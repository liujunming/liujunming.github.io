<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2024-11-24T03:18:01.764Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>virtio-net RSS Inner Header Hash</title>
    <link href="http://liujunming.github.io/2024/11/24/virtio-net-Inner-Header-Hash/"/>
    <id>http://liujunming.github.io/2024/11/24/virtio-net-Inner-Header-Hash/</id>
    <published>2024-11-24T02:03:40.000Z</published>
    <updated>2024-11-24T03:18:01.764Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio-net RSS Inner Header Hash的相关notes。<a id="more"></a></p><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p><a href="/2024/11/23/Notes-about-virtio-net-RSS-featute/">virtio-net RSS feature</a></p><p>需要理解网络<a href="https://www.wikiwand.com/en/articles/Tunneling_protocol" target="_blank" rel="noopener">Tunnel协议</a>，清楚Outer Header与Inner Header的概念。<br><img src="/images/2024/11/021.png" alt></p><h2 id="distribute-different-flows"><a href="#distribute-different-flows" class="headerlink" title="distribute different flows"></a>distribute different flows</h2><p>传统隧道协议由于在外头部中缺少足够的熵，导致收包时报文汇聚到单个队列上，无法发挥多队列收包的优势。</p><p>For legacy systems, they may lack entropy fields which modern protocols have in the outer header, resulting in multiple flows with the same outer header but different inner headers being directed to the same receive queue. This results in poor receive performance.</p><p>To address this limitation, inner header hash can be used to enable the device to advertise the capability to calculate the hash for the inner packet, regaining better receive performance.</p><blockquote><p>Legacy tunneling protocols, lacking the outer header entropy, can use RSS with the inner header hash to distribute flows with identical outer but different inner headers across various queues, improving performance.</p></blockquote><h2 id="identify-same-flow"><a href="#identify-same-flow" class="headerlink" title="identify same flow"></a>identify same flow</h2><blockquote><p>Identify an inner flow distributed across multiple outer tunnels.</p></blockquote><p>现代隧道协议在某些场景需要通过将同一条流接收在同一个队列上以获得性能收益，而外头部不容易做到。</p><p>Currently, a received encapsulated packet has an outer and an inner header, but the virtio device is unable to calculate the hash for the inner header. The same flow can traverse through different tunnels, resulting in the encapsulated packets being spread across multiple receive queues (refer to the figure below). However, in certain scenarios, we may need to direct these encapsulated packets of the same flow to a single receive queue. This facilitates the processing of the flow by the same CPU to improve performance (warm caches, less locking, etc.).</p><pre><code>client1                    client2   |        +-------+         |   +-------&gt;|tunnels|&lt;--------+            +-------+               |  |               v  v       +-----------------+       | monitoring host |       +-----------------+</code></pre><p>To achieve this, the device can calculate a symmetric hash based on the inner headers of the same flow.</p><h3 id="symmetric-hash"><a href="#symmetric-hash" class="headerlink" title="symmetric hash"></a>symmetric hash</h3><blockquote><p>symmetric hash确保同一个五元组(无论方向)哈希到同一个桶中，即当源IP和目标IP、源端口和目标端口互换时，哈希值仍然能保持一致。</p></blockquote><h2 id="Spec描述"><a href="#Spec描述" class="headerlink" title="Spec描述"></a>Spec描述</h2><p>VIRTIO_NET_F_HASH_TUNNEL</p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2620004" target="_blank" rel="noopener">5.1.6.4.4 Inner Header Hash</a></p><blockquote><p>5.1.6.4.4.1 Encapsulated packet<br>Multiple tunneling protocols allow encapsulating an inner, payload packet in an outer, encapsulated packet. The encapsulated packet thus contains an outer header and an inner header, and the device calculates the hash over either the inner header or the outer header.<br>If VIRTIO_NET_F_HASH_TUNNEL is negotiated and a received encapsulated packet’s outer header matches one of the encapsulation types enabled in enabled_tunnel_types, then the device uses the inner header for hash calculations (only a single level of encapsulation is currently supported).<br>If VIRTIO_NET_F_HASH_TUNNEL is negotiated and a received packet’s (outer) header does not match any encapsulation types enabled in enabled_tunnel_types, then the device uses the outer header for hash calculations.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://networkdirection.net/articles/routingandswitching/gretunnels/" target="_blank" rel="noopener">GRE Tunnels</a></li><li><a href="https://lore.kernel.org/virtio-dev/20230703152711.106008-1-hengqi@linux.alibaba.com/" target="_blank" rel="noopener">[PATCH v21] virtio-net: support inner header hash</a></li><li><a href="https://developer.aliyun.com/article/1257786" target="_blank" rel="noopener">高性能网络 SIG 月度动态：联合 IBM 就 SMC v2.1 协议升级达成一致，ANCK 率先完成支持</a></li><li><a href="https://developer.aliyun.com/article/1305988" target="_blank" rel="noopener">高性能网络 SIG 月度动态：ANCK 首次支持 SMCv2.1，virtio 规范支持隧道报文内头部哈希</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio-net RSS Inner Header Hash的相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about virtio-net RSS feature</title>
    <link href="http://liujunming.github.io/2024/11/23/Notes-about-virtio-net-RSS-featute/"/>
    <id>http://liujunming.github.io/2024/11/23/Notes-about-virtio-net-RSS-featute/</id>
    <published>2024-11-23T10:33:03.000Z</published>
    <updated>2024-11-24T00:25:05.878Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio-net中<a href="/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/">RSS(Receive Side Scaling)</a>的具体实现。<a id="more"></a></p><h2 id="Spec描述"><a href="#Spec描述" class="headerlink" title="Spec描述"></a>Spec描述</h2><p>VIRTIO_NET_F_RSS</p><p><img src="/images/2024/11/020.png" alt></p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2570003" target="_blank" rel="noopener">5.1.6.4.3 Hash calculation for incoming packets</a></p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2580001" target="_blank" rel="noopener">5.1.6.4.3.1 Supported/enabled hash types</a></p><p>RSS需要通过ctrl q去下发配置参数，所以VIRTIO_NET_F_RSS Requires VIRTIO_NET_F_CTRL_VQ，需要ctrl q。<br><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2890007" target="_blank" rel="noopener">5.1.6.5.7 Receive-side scaling (RSS)</a></p><h2 id="struct-virtio-net-rss-config"><a href="#struct-virtio-net-rss-config" class="headerlink" title="struct virtio_net_rss_config"></a>struct virtio_net_rss_config</h2><p>研究明白<code>struct virtio_net_rss_config</code>即可理解virtio-net RSS的实现细节。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> &#123;</span> </span><br><span class="line">   le16 vq_index_1_16: <span class="number">15</span>; <span class="comment">/* Bits 1 to 16 of the virtqueue index */</span> </span><br><span class="line">   le16 reserved: <span class="number">1</span>; <span class="comment">/* Set to zero */</span> </span><br><span class="line">&#125;; </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">virtio_net_rss_config</span> &#123;</span> </span><br><span class="line">    le32 hash_types; </span><br><span class="line">    le16 indirection_table_mask; </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> <span class="title">unclassified_queue</span>;</span> </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> <span class="title">indirection_table</span>[<span class="title">indirection_table_length</span>];</span> </span><br><span class="line">    le16 max_tx_vq; </span><br><span class="line">    u8 hash_key_length; </span><br><span class="line">    u8 hash_key_data[hash_key_length]; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="hash-types"><a href="#hash-types" class="headerlink" title="hash_types"></a>hash_types</h3><p>通过hash_types，比如可以选择(Source IP address,Destination IP address,Source TCP port,Destination TCP port)这个四元组作为RSS Input Fields，也可以选择(Source IP address,Destination IP address)这个二元组作为RSS Input Fields。<br><img src="/images/2024/11/016.png" alt></p><h3 id="indirection-table-mask"><a href="#indirection-table-mask" class="headerlink" title="indirection_table_mask"></a>indirection_table_mask</h3><p><img src="/images/2024/11/017.png" alt><br>indirection_table_mask就是上图中的LSB。例如indirection_table_mask为0x111，那么RSS Redirection Table的size就是8。</p><h3 id="hash-key-length和hash-key-data"><a href="#hash-key-length和hash-key-data" class="headerlink" title="hash_key_length和hash_key_data"></a>hash_key_length和hash_key_data</h3><p><img src="/images/2024/11/018.png" alt></p><p>hash_key_length和hash_key_data就代表上图中的Hash Key。</p><h3 id="struct-rss-rq-id"><a href="#struct-rss-rq-id" class="headerlink" title="struct rss_rq_id"></a>struct rss_rq_id</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> &#123;</span> </span><br><span class="line">   le16 vq_index_1_16: <span class="number">15</span>; <span class="comment">/* Bits 1 to 16 of the virtqueue index */</span> </span><br><span class="line">   le16 reserved: <span class="number">1</span>; <span class="comment">/* Set to zero */</span> </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>rss_rq_id is a receive virtqueue id.vq_index_1_16 consists of bits 1 to 16 of a virtqueue index. For example, a vq_index_1_16 value of 3 corresponds to virtqueue index 6, which maps to receiveq4.</p><p><img src="/images/2024/11/019.png" alt></p><table><thead><tr><th>vq_id</th><th>vq</th></tr></thead><tbody><tr><td>0</td><td>rxq1</td></tr><tr><td>1</td><td>txq1</td></tr><tr><td>2</td><td>rxq2</td></tr><tr><td>3</td><td>txq2</td></tr><tr><td>4</td><td>rxq3</td></tr><tr><td>5</td><td>txq3</td></tr><tr><td>6</td><td>rxq4</td></tr><tr><td>7</td><td>txq4</td></tr></tbody></table><p>如果vq_index_1_16为3，那么rss_rq_id就是6(0x110)，对应于rxq4。</p><h3 id="unclassified-queue"><a href="#unclassified-queue" class="headerlink" title="unclassified_queue"></a>unclassified_queue</h3><p>Field unclassified_queue specifies the receive virtqueue id in which to place unclassified packets.</p><h3 id="indirection-table"><a href="#indirection-table" class="headerlink" title="indirection_table"></a>indirection_table</h3><p>Field indirection_table is an array of receive virtqueues ids.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>The device MUST determine the destination queue for a network packet as follows:</p><ol><li>Calculate the hash of the packet.</li><li>If the device did not calculate the hash for the specific packet, the device directs the packet to the receiveq specified by <code>unclassified_queue</code> of <code>virtio_net_rss_config</code> structure.</li><li>Apply <code>indirection_table_mask</code>to the calculated hash and use the result as the index in the indirection table to get the destination receive virtqueue id.</li><li>If the destination receive queue is being reset, the device MUST drop the packet.</li></ol><hr><p>参考资料:</p><ol><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html" target="_blank" rel="noopener">VIRTIO 1.3 spec</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio-net中&lt;a href=&quot;/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/&quot;&gt;RSS(Receive Side Scaling)&lt;/a&gt;的具体实现。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NUMA</title>
    <link href="http://liujunming.github.io/2024/11/17/%E8%BD%AC%E8%BD%BD-Linux-NUMA-Optimization-1/"/>
    <id>http://liujunming.github.io/2024/11/17/转载-Linux-NUMA-Optimization-1/</id>
    <published>2024-11-17T00:37:04.000Z</published>
    <updated>2024-11-17T08:26:21.697Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要转载自:<a href="https://oliveryang.net/2016/02/linux-numa-optimization-1/" target="_blank" rel="noopener">https://oliveryang.net/2016/02/linux-numa-optimization-1/</a><a id="more"></a></p><h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p>理解NUMA的概念首先要熟悉多处理器计算机系统的几个重要概念。</p><h3 id="1-1-SMP-vs-AMP"><a href="#1-1-SMP-vs-AMP" class="headerlink" title="1.1 SMP vs. AMP"></a>1.1 SMP vs. AMP</h3><p><a href="https://www.wikiwand.com/en/articles/Symmetric_multiprocessing" target="_blank" rel="noopener">SMP(Symmetric Multiprocessing)</a>， 即对称多处理器架构，是目前最常见的多处理器计算机架构。<br><a href="https://en.wikipedia.org/wiki/Asymmetric_multiprocessing" target="_blank" rel="noopener">AMP(Asymmetric Multiprocessing)</a>， 即非对称多处理器架构，则是与SMP相对的概念。</p><p>那么两者之间的主要区别是什么呢？ 总结下来有这么几点，</p><ol><li>SMP的多个处理器都是同构的，使用相同架构的CPU；而AMP的多个处理器则可能是异构的。</li><li>SMP的多个处理器共享同一内存地址空间；而AMP的每个处理器则拥有自己独立的地址空间。</li><li>SMP的多个处理器操通常共享一个操作系统的实例；而AMP的每个处理器可以有或者没有运行操作系统， 运行操作系统的CPU也是在运行多个独立的实例。</li><li>SMP的多处理器之间可以通过共享内存来协同通信；而AMP则需要提供一种处理器间的通信机制。</li></ol><p>SMP和AMP的深入介绍很多经典文章书籍可参考，此处不再赘述。现今主流的x86多处理器服务器都是SMP架构的， 而很多嵌入式系统则是AMP架构的。</p><h3 id="1-2-NUMA-vs-UMA"><a href="#1-2-NUMA-vs-UMA" class="headerlink" title="1.2 NUMA vs. UMA"></a>1.2 NUMA vs. UMA</h3><p><a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access" target="_blank" rel="noopener">NUMA(Non-Uniform Memory Access)</a> 非均匀内存访问架构是指多处理器系统中，内存的访问时间是依赖于处理器和内存之间的相对位置的。 这种设计里存在和处理器相对近的内存，通常被称作本地内存；还有和处理器相对远的内存， 通常被称为非本地内存。</p><p><a href="https://en.wikipedia.org/wiki/Uniform_memory_access" target="_blank" rel="noopener">UMA(Uniform Memory Access)</a> 均匀内存访问架构则是与NUMA相反，所以处理器对共享内存的访问距离和时间是相同的。</p><p>由此可知，不论是NUMA还是UMA都是SMP架构的一种设计和实现上的选择。</p><p>阅读文档时，也常常能看到<strong>ccNUMA(Cache Coherent NUMA)</strong>，即缓存一致性NUMA架构。 这种架构主要是在NUMA架构之上保证了多处理器之间的缓存一致性。降低了系统程序的编写难度。</p><p>x86多处理器发展历史上，早期的多核和多处理器系统都是UMA架构的。这种架构下， 多个CPU通过同一个北桥(North Bridge)芯片与内存链接。北桥芯片里集成了内存控制器(Memory Controller)，</p><p>下图是一个典型的早期 x86 UMA 系统，四路处理器通过 FSB (前端系统总线) 和主板上的内存控制器芯片 (MCH) 相连，DRAM 是以 UMA 方式组织的，延迟并无访问差异，<br><img src="/images/2024/11/011.png" alt></p><p>在 UMA 架构下，CPU 和内存控制器之间的前端总线 (FSB) 在系统 CPU 数量不断增加的前提下， 成为了系统性能的瓶颈。因此，AMD 在引入 64 位 x86 架构时，实现了 NUMA 架构。之后， Intel 也推出了 x64 的 Nehalem 架构，x86 终于全面进入到 NUMA 时代。x86 NUMA 目前的实现属于 ccNUMA。</p><p>从 Nehalem 架构开始，x86 开始转向 NUMA 架构，内存控制器芯片被集成到处理器内部，多个处理器通过 QPI 链路相连，从此 DRAM 有了远近之分。 而 Sandybridge 架构则更近一步，将片外的 IOH 芯片也集成到了处理器内部，至此，内存控制器和 PCIe Root Complex 全部在处理器内部了。 下图就是一个典型的 x86 的 NUMA 架构：</p><p><img src="/images/2024/11/012.png" alt></p><h2 id="2-NUMA-Hierarchy"><a href="#2-NUMA-Hierarchy" class="headerlink" title="2. NUMA Hierarchy"></a>2. NUMA Hierarchy</h2><p>NUMA Hierarchy就是NUMA的层级结构。一个Intel x86 NUMA系统就是由多个NUMA Node组成。</p><h3 id="2-1-NUMA-Node内部"><a href="#2-1-NUMA-Node内部" class="headerlink" title="2.1 NUMA Node内部"></a>2.1 NUMA Node内部</h3><p>一个NUMA Node内部是由一个<strong>物理CPU</strong>和它所有的<strong>本地内存(Local Memory)</strong>组成的。广义得讲， 一个NUMA Node内部还包含<strong>本地IO资源</strong>，对大多数Intel x86 NUMA平台来说，主要是PCIe总线资源。 ACPI规范就是这么抽象一个NUMA Node的。</p><h4 id="2-1-1-物理CPU"><a href="#2-1-1-物理CPU" class="headerlink" title="2.1.1 物理CPU"></a>2.1.1 物理CPU</h4><p>一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成。每个CPU Core内部又可以由两个CPU Thread组成。 每个CPU thread都是一个操作系统可见的逻辑CPU。对大多数操作系统来说，一个八核HT打开的CPU会被识别为16个CPU。 下面就说一说这里面相关的概念，</p><ul><li><p>Socket<br>一个Socket对应一个物理CPU。 这个词大概是从CPU在主板上的物理连接方式上来的。处理器通过主板的Socket来插到主板上。 尤其是有了多核(Multi-core)系统以后，Multi-socket系统被用来指明系统到底存在多少个物理CPU。</p></li><li><p>Core<br>CPU的运算核心。 x86的核包含了CPU运算的基本部件，如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存。 一个Socket里可以有多个Core。如今的多核时代，即使是Single Socket的系统， 也是逻辑上的SMP系统。但是，一个物理CPU的系统不存在非本地内存，因此相当于UMA系统。</p></li><li><p>Uncore<br>Intel x86物理CPU里没有放在Core里的部件都被叫做Uncore。Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能。 在Nehalem时代，内存控制器被集成到CPU里，叫做iMC(Integrated Memory Controller)。 而PCIe Root Complex还做为独立部件在IO Hub芯片里。到了SandyBridge时代，PCIe Root Complex也被集成到了CPU里。 现今的Uncore部分，除了iMC，PCIe Root Complex，还有QPI(QuickPath Interconnect)控制器， L3缓存，CBox(负责缓存一致性)，及其它外设控制器。</p></li><li><p>Threads<br>这里特指CPU的多线程技术。在Intel x86架构下，CPU的多线程技术被称作超线程(Hyper-Threading)技术。 Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor)， 每个逻辑处理器都有独立的处理器状态，但共享Core内部的计算资源，如ALU，FPU，L1，L2缓存。 这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能，提高了硬件使用效率。 x86的超线程技术出现早于NUMA架构。</p></li></ul><p>以下图为例，1 个 x86 CPU Socket 有 4 个物理 Core，每个 Core 有两个 HT (Hyper Thread)，L1 L2 Cache 被两个 HT 共享， 而 L3 Cache 则在 Socket 内，被所有 4 个 Core 共享，<br><img src="/images/2024/11/013.jpg" alt></p><h4 id="2-1-2-本地内存"><a href="#2-1-2-本地内存" class="headerlink" title="2.1.2 本地内存"></a>2.1.2 本地内存</h4><p>在Intel x86平台上，所谓本地内存，就是CPU可以经过Uncore部件里的iMC访问到的内存。而那些非本地的， 远程内存(Remote Memory)，则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问。 曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示，远程内存访问的延时是本地内存的一倍。</p><p>可以假设，操作系统应该尽量利用本地内存的低访问延迟特性来优化应用和系统的性能。</p><h4 id="2-1-3-本地IO资源"><a href="#2-1-3-本地IO资源" class="headerlink" title="2.1.3 本地IO资源"></a>2.1.3 本地IO资源</h4><p>如前所述，Intel自从SandyBridge处理器开始，已经把PCIe Root Complex集成到CPU里了。 正因为如此，从CPU直接引出PCIe Root Port的PCIe 3.0的链路可以直接与PCIe Switch或者PCIe Endpoint相连。 一个PCIe Endpoint就是一个PCIe外设。这就意味着，对某个PCIe外设来说，如果它直接与哪个CPU相连， 它就属于哪个CPU所在的NUMA Node。</p><p>与本地内存一样，所谓本地IO资源，就是CPU可以经过Uncore部件里的PCIe Root Complex直接访问到的IO资源。 如果是非本地IO资源，则需要经过QPI链路到该IO资源所属的CPU，再通过该CPU PCIe Root Complex访问。 如果同一个NUMA Node内的CPU和内存和另外一个NUMA Node的IO资源发生互操作，因为要跨越QPI链路， 会存在额外的访问延迟问题。</p><p>其它体系结构里，为降低外设访问延迟，也有将IB(Infiniband)总线集成到CPU里的。 这样IB设备也属于NUMA Node的一部分了。</p><p>可以假设，操作系统如果是NUMA Aware的话，应该会尽量针对本地IO资源低延迟的优点进行优化。</p><h3 id="2-2-NUMA-Node互联"><a href="#2-2-NUMA-Node互联" class="headerlink" title="2.2 NUMA Node互联"></a>2.2 NUMA Node互联</h3><p>在Intel x86上，NUMA Node之间的互联是通过<a href="https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect" target="_blank" rel="noopener">QPI(QuickPath Interconnect)</a> Link的。 CPU的Uncore部分有QPI的控制器来控制CPU到QPI的数据访问。</p><p>不借助第三方的 Node Controller，2 或 4 个 NUMA Node (取决于具体架构)可以通过 QPI(QuickPath Interconnect) 总线互联起来， 构成一个NUMA系统。例如，<a href="https://www.doit.com.cn/p/118059.html" target="_blank" rel="noopener">SGI UV计算机系统</a>， 它就是借助自家的 SGI NUMAlink® 互联技术来达到 4 到 256 个 CPU socket 扩展的能力的。这是一个 SMP 系统， 所以支持运行一个 Linux 操作系统实例去管理系统。</p><p>下图就是一个利用 QPI Switch 互联的 8 NUMA Node 的 x86 系统，</p><p><img src="/images/2024/11/014.png" alt></p><h2 id="3-NUMA-Affinity"><a href="#3-NUMA-Affinity" class="headerlink" title="3. NUMA Affinity"></a>3. NUMA Affinity</h2><p>NUMA Affinity(亲和性)是和NUMA Hierarchy(层级结构)直接相关的。对系统软件来说， 以下两个概念至关重要，</p><h3 id="3-1-CPU-NUMA-Affinity"><a href="#3-1-CPU-NUMA-Affinity" class="headerlink" title="3.1 CPU NUMA Affinity"></a>3.1 CPU NUMA Affinity</h3><p>CPU NUMA的亲和性是指从CPU角度看，哪些内存访问更快，有更低的延迟。如前所述， 和该CPU直接相连的本地内存是更快的。操作系统如果可以根据任务所在CPU去分配本地内存， 就是基于CPU NUMA亲和性的考虑。因此，CPU NUMA亲和性就是要尽量让任务运行在本地的NUMA Node里。</p><h3 id="3-2-Device-NUMA-Affinity"><a href="#3-2-Device-NUMA-Affinity" class="headerlink" title="3.2 Device NUMA Affinity"></a>3.2 Device NUMA Affinity</h3><p><img src="/images/2024/11/015.png" alt></p><p>设备NUMA亲和性是指从PCIe外设的角度看，如果和CPU和内存相关的IO活动都发生在外设所属的NUMA Node， 将会有更低延迟。这里有两种设备NUMA亲和性的问题，</p><h4 id="3-2-1-DMA-Buffer-NUMA-Affinity"><a href="#3-2-1-DMA-Buffer-NUMA-Affinity" class="headerlink" title="3.2.1 DMA Buffer NUMA Affinity"></a>3.2.1 DMA Buffer NUMA Affinity</h4><p>大部分PCIe设备支持DMA功能的。也就是说，设备可以直接把数据写入到位于内存中的DMA缓冲区。 显然，如果DMA缓冲区在PCIe外设所属的NUMA Node里分配，那么将会有最低的延迟。 否则，外设的DMA操作要跨越QPI链接去读写另外一个NUMA Node里的DMA缓冲区。 因此，操作系统如果可以根据PCIe设备所属的NUMA node分配DMA缓冲区， 将会有最好的DMA操作的性能。</p><h4 id="3-2-2-Interrupt-NUMA-Affinity"><a href="#3-2-2-Interrupt-NUMA-Affinity" class="headerlink" title="3.2.2 Interrupt NUMA Affinity"></a>3.2.2 Interrupt NUMA Affinity</h4><p>设备DMA操作完成后，需要在CPU上触发中断来通知驱动程序的中断处理例程(ISR)来读写DMA缓冲区。 很多时候，ISR触发下半部机制(SoftIRQ)来进入到协议栈相关(Network，Storage)的代码路径来传送数据。 对大部分操作系统来说，硬件中断(HardIRQ)和下半部机制的代码在同一个CPU上发生。 因此，DMA缓冲区的读写操作发生的位置和设备硬件中断(HardIRQ)密切相关。假设操作系统可以把设备的硬件中断绑定到自己所属的NUMA node， 那之后中断处理函数和协议栈代码对DMA缓冲区的读写将会有更低的延迟。</p><h2 id="4-Firmware接口"><a href="#4-Firmware接口" class="headerlink" title="4. Firmware接口"></a>4. Firmware接口</h2><p>由于NUMA的亲和性对应用的性能非常重要，那么硬件平台就需要给操作系统提供接口机制来感知硬件的NUMA层级结构。 在x86平台，ACPI规范提供了以下接口来让操作系统检测系统的NUMA层级结构。</p><p>ACPI 5.0a规范的第17章是有关NUMA的章节。ACPI规范里，NUMA Node被第9章定义的Module Device所描述。 ACPI规范里用<strong>Proximity Domain</strong>对NUMA Node做了抽象，两者的概念大多时候等同。</p><ul><li><p><strong>SRAT(System Resource Affinity Table)</strong><br>主要描述了系统boot时的CPU和内存都属于哪个Proximity Domain(NUMA Node)。 这个表格里的信息时静态的，如果是启动后热插拔，需要用OSPM的_PXM方法去获得相关信息。</p></li><li><p><strong>SLIT(System Locality Information Table)</strong><br>提供CPU和内存之间的位置远近信息。在SRAT表格里，只能告诉给定的CPU和内存是否在一个NUMA Node。 对某个CPU来说，不在本NUMA Node里的内存，即远程内存们是否都是一样的访问延迟取决于NUMA的拓扑有多复杂(QPI的跳数)。 总之，对于不能简单用远近来描述的NUMA系统(QPI存在0，1，2等不同跳数)， 需要SLIT表格给出进一步的说明。同样的，这个表格也是静态表格，热插拔需要使用OSPM的_SLI方法。</p></li><li><p><strong>DSDT(Differentiated System Description Table)</strong><br>从Device NUMA角度看，这个表格给出了系统boot时的外设都属于哪个Proximity Domain(NUMA Node)。</p></li></ul><p>ACPI规范OSPM(Operating System-directed configuration and Power Management) 和OSPM各种方法就是操作系统里的ACPI驱动和ACPI firmware之间的一个互动的接口。 x86启动OS后，没有ACPI之前，firmware(BIOS)的代码是无法被执行了，除非通过SMI中断处理程序。 但有了ACPI，BIOS提前把ACPI的一些静态表格和AML的bytecode代码装载到内存， 然后ACPI驱动就会加载AML的解释器，这样OS就可以通过ACPI驱动调用预先装载的AML代码。 AML(ACPI Machine Language)是和Java类似的一种虚拟机解释型语言，所以不同操作系统的ACPI驱动， 只要有相同的虚拟机解释器，就可以直接从操作系统调用ACPI写好的AML的代码了。 所以，前文所述的所有热插拔的OSPM方法，其实就是对应ACPI firmware的AML的一段函数代码而已。 (关于ACPI的简单介绍，这里给出两篇延伸阅读：<a href="http://rdist.root.org/2008/10/17/all-about-acpi/" target="_blank" rel="noopener">1</a> 和<a href="https://www.usenix.org/legacy/events/usenix02/tech/freenix/full_papers/watanabe/watanabe_html/index.html" target="_blank" rel="noopener">2</a>。)</p><p>至此，x86 NUMA平台所需的一些硬件知识基本就覆盖到了。需要说明的是， 虽然本文以Intel平台为例，但AMD平台的差异也只是CPU总线和内部结构的差异而已。 其它方面的NUMA概念AMD也是类似的。</p><hr><p>参考资料:</p><ol><li>IOctopus: outsmarting nonuniform DMA(ASPLOS’20)</li><li><a href="https://blog.csdn.net/qq_20817327/article/details/105925071" target="_blank" rel="noopener">NUMA架构详解</a></li><li><a href="https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="noopener">The MySQL “swap insanity” problem and the effects of the NUMA architecture</a></li><li><a href="https://frankdenneman.nl/2016/07/08/numa-deep-dive-part-2-system-architecture/" target="_blank" rel="noopener">NUMA Deep Dive Part 2: System Architecture</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要转载自:&lt;a href=&quot;https://oliveryang.net/2016/02/linux-numa-optimization-1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://oliveryang.net/2016/02/linux-numa-optimization-1/&lt;/a&gt;
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about AMD IOMMU IRTCache机制</title>
    <link href="http://liujunming.github.io/2024/11/10/Notes-about-AMD-IOMMU-IRTCache/"/>
    <id>http://liujunming.github.io/2024/11/10/Notes-about-AMD-IOMMU-IRTCache/</id>
    <published>2024-11-10T10:52:35.000Z</published>
    <updated>2024-11-10T11:52:37.336Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下AMD IOMMU IRTCache机制的相关notes。<a id="more"></a></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>在没有IRTCache机制之前，设备的MSI data到IRTE(Interrupt Remapping Table Entry)的映射，需要硬件从内存中读取Interrupt Remapping Table来获取IRTE。<br><img src="/images/2024/11/009.png" alt></p><p>在引入IRTCache机制之后，IOMMU硬件中就会缓存设备MSI data到IRTE的映射了,这样就可以避免IOMMU硬件从内存中读取IRTE。</p><h3 id="How"><a href="#How" class="headerlink" title="How"></a>How</h3><p>For IOMMU AVIC, the IOMMU driver needs to keep track of vcpu scheduling changes, and updates interrupt remapping table entry (IRTE) accordingly. The IRTE is normally cached by the hardware, which requires the IOMMU driver to issue IOMMU IRT invalidation command and wait for completion everytime it updates the table.</p><p>Enabling IOMMU AVIC on a large scale system with lots of vcpus and VFIO pass-through devices running interrupt-intensive workload, it could result in high IRT invalidation rate. In such case, the overhead from IRT invalidation could outweigh the benefit of IRTE caching.</p><p>Therefore, introduce a new AMD IOMMU driver option “amd_iommu=irtcachedis” to allow disabling IRTE caching, and avoid the need for IRTE invalidation.</p><p><img src="/images/2024/11/008.png" alt></p><p><img src="/images/2024/11/010.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://lore.kernel.org/lkml/20230530141137.14376-1-suravee.suthikulpanit@amd.com/" target="_blank" rel="noopener">[PATCH v3 0/5] iommu/amd: AVIC Interrupt Remapping Improvements</a></li><li>AMD I/O Virtualization Technology (IOMMU) Specification, 48882</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下AMD IOMMU IRTCache机制的相关notes。
    
    </summary>
    
      <category term="IOMMU" scheme="http://liujunming.github.io/categories/IOMMU/"/>
    
    
      <category term="AMD" scheme="http://liujunming.github.io/tags/AMD/"/>
    
      <category term="IOMMU" scheme="http://liujunming.github.io/tags/IOMMU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PCIe flow control机制</title>
    <link href="http://liujunming.github.io/2024/11/10/Notes-about-PCIe-credit%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2024/11/10/Notes-about-PCIe-credit机制/</id>
    <published>2024-11-10T02:16:57.000Z</published>
    <updated>2024-11-10T07:12:32.928Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PCIe flow control机制的相关notes。<a id="more"></a></p><p>The receiver of each port reports the size of its Flow Control buffers <strong>in units called credits</strong>.<br>在一条链路的两端设备分别向对方通知其自己可以使用的缓冲区大小或数量。这里通知的空间大小就是信用（credit）。</p><p><img src="/images/2024/11/007.png" alt></p><p>A定期的告诉B，我这里还有地方，来吧，来吧~。B因此知道A是有空余空间接收的。同样，B也是采用同样的方式告知A。流控信用会定期在两者间发送，这叫做更新流控信用（Update FC）。</p><p>The data link layer has a Flow Control (FC) mechanism, which makes sure that a TLP is transmitted only when the link partner has enough buffer space to accept it.</p><p>The Flow Control mechanism uses a credit‐based mechanism that allows the transmitting port to be aware of buffer space available at the receiving port. As part of its initialization, each receiver reports the size of its buffers to the transmitter on the other end of the Link, and then <strong>during run‐time it regularly updates the number of credits available using Flow Control DLLPs</strong>. Technically, of course, DLLPs are overhead because they don’t convey any data payload, but they are kept small to minimize their impact on performance.</p><hr><p>参考资料:</p><ol><li>PCI Express Technology(Mike Jackson, Ravi Budruk)</li><li><a href="https://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-2" target="_blank" rel="noopener">Down to the TLP: How PCI express devices talk (Part II)</a></li><li><a href="https://www.slideshare.net/slideshow/pciexpressbasicsbackgroundpdf/252660914#38" target="_blank" rel="noopener">PCI Express Basics Background</a></li><li><a href="https://mp.weixin.qq.com/s/WkRTqLOpqynHtOiaaOTplw" target="_blank" rel="noopener">Credit timeout &amp; Completion timeout</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PCIe flow control机制的相关notes。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PCIe prefetchable bar</title>
    <link href="http://liujunming.github.io/2024/11/03/Notes-about-PCIe-prefetchable-bar/"/>
    <id>http://liujunming.github.io/2024/11/03/Notes-about-PCIe-prefetchable-bar/</id>
    <published>2024-11-03T09:21:24.000Z</published>
    <updated>2024-11-03T09:43:15.866Z</updated>
    
    <content type="html"><![CDATA[<p>When a base address register is marked as <strong>Prefetchable</strong>, it means that:the region does not have read side effects (reading from that memory range doesn’t change any state), and it is allowed for the CPU to cache loads from that memory region and read it in bursts (typically cache line sized).<a id="more"></a> Hardware is also allowed to merge repeated stores to the same address into one store of the latest value. If you are using paging and want maximum performance, you should map prefetchable MMIO regions as WT (write-through) instead of UC (uncacheable). On x86, frame buffers are the exception, they should be almost always be mapped WC (write-combining).</p><p><img src="/images/2024/11/006.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://wiki.osdev.org/PCI" target="_blank" rel="noopener">wiki.osdev.org/PCI</a></li><li><a href="https://blog.csdn.net/redseazhaojianertao/article/details/79943494" target="_blank" rel="noopener">PCIE的prefetchable和nonprefetchable的理解</a></li><li>Intel SDM Vol3</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When a base address register is marked as &lt;strong&gt;Prefetchable&lt;/strong&gt;, it means that:the region does not have read side effects (reading from that memory range doesn’t change any state), and it is allowed for the CPU to cache loads from that memory region and read it in bursts (typically cache line sized).
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA Direct WQE与Inline data机制</title>
    <link href="http://liujunming.github.io/2024/11/03/Notes-about-RDMA-Inline-data%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2024/11/03/Notes-about-RDMA-Inline-data机制/</id>
    <published>2024-11-03T03:27:55.000Z</published>
    <updated>2024-11-03T09:20:09.475Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA Direct WQE与Inline data机制，主要内容转载自<a href="https://zhuanlan.zhihu.com/p/567720023" target="_blank" rel="noopener">知乎:RDMA 高级</a>。<a id="more"></a></p><h2 id="Normal-flow"><a href="#Normal-flow" class="headerlink" title="Normal flow"></a>Normal flow</h2><p>以发包为例:<br><img src="/images/2024/11/001.jpg" alt></p><ol><li>填充要发送的数据</li><li>填充WQE描述符</li><li>敲doorbell通知硬件有数据要发送</li><li>硬件通过DMA读取WQE</li><li>硬件通过DMA读取要发送的数据</li></ol><h2 id="Direct-WQE"><a href="#Direct-WQE" class="headerlink" title="Direct WQE"></a>Direct WQE</h2><p>以发包为例:<br><img src="/images/2024/11/002.jpg" alt><br>与Normal flow相比，Direct WQE先将WQE写入MMIO bar中，再写doorbell通知硬件有数据要发送。这样RNIC就无需通过DMA来读取WQE了。</p><p>需要将MMIO页以write combining方式映射，这里的MMIO页属于<a href="/2024/10/20/Notes-about-RDMA-Device-Memory/">RDMA Device Memory</a>。</p><p><img src="/images/2024/11/003.png" alt></p><h2 id="Inline-Send"><a href="#Inline-Send" class="headerlink" title="Inline-Send"></a>Inline-Send</h2><p><img src="/images/2024/11/004.jpg" alt><br><code>ibv_post_send</code>时带上<code>IBV_SEND_INLINE</code>标识，如果要发送的数据小于128字节则填WQE时会将这部分数据直接append在WQE的后面。这样硬件DMA WQE时就顺便将data也读出来了，这样就省去了单独DMA data的操作。</p><h2 id="Inline-Receive"><a href="#Inline-Receive" class="headerlink" title="Inline-Receive"></a>Inline-Receive</h2><p><img src="/images/2024/11/005.png" alt><br>与发送类似，如果接收的是一个小数据，则没有必要将其放入RQ的receive buffer中，而是可以直接将其放入CQE中。这可以省去硬件将数据DMA至RQ SGE list的过程。使用<code>ibv_exp_create_qp</code>创建QP时指定<code>max_inl_recv</code>即可开启此功能。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Direct WQE与Inline data机制的目标都是减少RDMA数据面上的PCIe TLP交互次数。</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/567720023" target="_blank" rel="noopener">RDMA 高级</a></li><li><a href="https://docs.nvidia.com/networking/display/mlnxofedv461000/optimized+memory+access#src-12013520_OptimizedMemoryAccess-Inline-Receive" target="_blank" rel="noopener">Inline-Receive</a></li><li><a href="https://lore.kernel.org/all/1622194379-59868-5-git-send-email-liweihang@huawei.com/" target="_blank" rel="noopener">[rdma-core,4/4] libhns: Add support for direct wqe</a></li><li>Design Guidelines for High Performance RDMA Systems(ATC’16)</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA Direct WQE与Inline data机制，主要内容转载自&lt;a href=&quot;https://zhuanlan.zhihu.com/p/567720023&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知乎:RDMA 高级&lt;/a&gt;。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVF</title>
    <link href="http://liujunming.github.io/2024/10/27/Notes-about-NVF/"/>
    <id>http://liujunming.github.io/2024/10/27/Notes-about-NVF/</id>
    <published>2024-10-27T08:55:46.000Z</published>
    <updated>2024-10-27T10:10:02.468Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下NVF(Network Function Virtualization)相关notes。<a id="more"></a></p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><img src="/images/2024/10/021.jpg" alt></p><p><img src="/images/2024/10/022.jpg" alt></p><p><img src="/images/2024/10/023.jpg" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Network Function Virtualization is a network architecture for virtualizing the entire class of network functions (NFs) on commodity off-the-shelf(现成的) general-purpose hardware.</p><p><img src="/images/2024/10/020.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/conference/nsdi18/presentation/zhang-kai" target="_blank" rel="noopener">G-NET: Effective GPU Sharing in NFV Systems</a></li><li><a href="https://dianshenseu.github.io/files/NFV.pdf" target="_blank" rel="noopener">NFV实验平台的技术方案及搭建过程介绍</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下NVF(Network Function Virtualization)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA ODP feature</title>
    <link href="http://liujunming.github.io/2024/10/20/Notes-about-RDMA-ODP-feature/"/>
    <id>http://liujunming.github.io/2024/10/20/Notes-about-RDMA-ODP-feature/</id>
    <published>2024-10-20T08:27:30.000Z</published>
    <updated>2024-10-20T11:42:27.410Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA ODP(On-Demand-Paging) feature相关notes。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>On-Demand-Paging (ODP) is a technique to alleviate much of the shortcomings of memory registration. Applications no longer need to pin down the underlying physical pages of the address space, and track the validity of the mappings. Rather, the HCA requests the latest translations from the OS when pages are not present, and the OS invalidates translations which are no longer valid due to either non-present pages or mapping changes.</p><p><img src="/images/2024/10/017.jpg" alt></p><h2 id="Synchronizing-between-CPU-and-RNIC-page-tables"><a href="#Synchronizing-between-CPU-and-RNIC-page-tables" class="headerlink" title="Synchronizing between CPU and RNIC page tables"></a>Synchronizing between CPU and RNIC page tables</h2><p><img src="/images/2024/10/018.jpg" alt></p><h3 id="Faulting"><a href="#Faulting" class="headerlink" title="Faulting"></a>Faulting</h3><p>When an RDMA request accesses data on invalid virtual pages, (1a) the RNIC stalls the QP and raises an RNIC page fault interrupt. (1b) The driver requests the OS kernel for virtual-to-physical mappings via <code>hmm_range_fault</code>. The OS kernel triggers CPU page faults on these virtual pages and fills the CPU page table if necessary. (1c) The driver updates the mappings on the RNIC page table and (1d) resumes the QP.</p><h3 id="Invalidation"><a href="#Invalidation" class="headerlink" title="Invalidation"></a>Invalidation</h3><p>When the OS kernel tries to unmap virtual pages in scenarios like swapping out or page migration, (2a)it notifies the RNIC driver to invalidate virtual pages via <code>mmu_interval_notifier</code>. (2b) The RNIC driver erases the virtual-to-physical mapping from the RNIC page table. (2c) The driver notifies the kernel that the physical pages are no longer used by the RNIC. Then, the OS kernel modifies the CPU page table and reuses the physical pages.</p><p>ODP MR(Memory Region) relies on faulting and invalidation flows to synchronize CPU and RNIC page tables. </p><h3 id="Advising"><a href="#Advising" class="headerlink" title="Advising"></a>Advising</h3><p>An application can proactively request the RNIC driver to populate a range in the RNIC page table. The RNIC driver completes advising by steps (3a) – (3b), which are identical to steps (1b) – (1c).</p><h2 id="Related-code"><a href="#Related-code" class="headerlink" title="Related code"></a>Related code</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> ib_odp_general_cap_bits &#123;</span><br><span class="line">IB_ODP_SUPPORT= <span class="number">1</span> &lt;&lt; <span class="number">0</span>,</span><br><span class="line">IB_ODP_SUPPORT_IMPLICIT = <span class="number">1</span> &lt;&lt; <span class="number">1</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">enum</span> ib_odp_transport_cap_bits &#123;</span><br><span class="line">IB_ODP_SUPPORT_SEND= <span class="number">1</span> &lt;&lt; <span class="number">0</span>,</span><br><span class="line">IB_ODP_SUPPORT_RECV= <span class="number">1</span> &lt;&lt; <span class="number">1</span>,</span><br><span class="line">IB_ODP_SUPPORT_WRITE= <span class="number">1</span> &lt;&lt; <span class="number">2</span>,</span><br><span class="line">IB_ODP_SUPPORT_READ= <span class="number">1</span> &lt;&lt; <span class="number">3</span>,</span><br><span class="line">IB_ODP_SUPPORT_ATOMIC= <span class="number">1</span> &lt;&lt; <span class="number">4</span>,</span><br><span class="line">IB_ODP_SUPPORT_SRQ_RECV= <span class="number">1</span> &lt;&lt; <span class="number">5</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://docs.nvidia.com/networking/display/mlnxofedv461000/optimized+memory+access" target="_blank" rel="noopener">Optimized Memory Access</a></li><li>TeRM: Extending RDMA-Attached Memory with SSD(FAST’24)</li><li><a href="https://dlsvr04.asus.com.cn/pub/ASUS/mb/accessory/PEM-FDR/Manual/Mellanox_OFED_Linux_User_Manual_v2_3-1_0_1.pdf" target="_blank" rel="noopener">Mellanox OFED for Linux User Manual</a></li><li><a href="https://lore.kernel.org/linux-rdma/1418310266-9584-1-git-send-email-haggaie@mellanox.com/" target="_blank" rel="noopener">[PATCH v3 00/17] On demand paging</a></li><li><a href="https://lore.kernel.org/linux-rdma/cover.1699503619.git.matsuda-daisuke@fujitsu.com/" target="_blank" rel="noopener">[PATCH for-next v7 0/7] On-Demand Paging on SoftRoCE</a></li><li><a href="https://cloud.tencent.com/developer/article/2428026" target="_blank" rel="noopener">RDMA - ODP按需分页设计原理-优点-源码浅析</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA ODP(On-Demand-Paging) feature相关notes。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA Device Memory</title>
    <link href="http://liujunming.github.io/2024/10/20/Notes-about-RDMA-Device-Memory/"/>
    <id>http://liujunming.github.io/2024/10/20/Notes-about-RDMA-Device-Memory/</id>
    <published>2024-10-20T00:29:55.000Z</published>
    <updated>2024-10-20T08:16:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA Device Memory相关notes。<a id="more"></a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Device Memory is a verbs API that allows using on-chip memory, located on the device, as a data buffer for send/receive and RDMA operations. The device memory can be mapped and accessed directly by user and kernel applications, and can be allocated in various sizes, registered as memory regions with local and remote access keys for performing the send/ receive and RDMA operations. Using the device memory to store packets for transmission can significantly reduce transmission latency compared to the host memory.</p><p><img src="/images/2024/10/014.jpg" alt></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><img src="/images/2024/10/015.jpg" alt></p><p>staging buffer: 暂存缓冲区</p><h3 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h3><p><img src="/images/2024/10/016.jpg" alt></p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>可以类比于NVMe的<a href="/2024/06/30/深入理解NVMe-CMB机制/">CMB</a>，RDMA Device Memory以mmio的形式expose给host，目的是让RDMA直接使用Device Memory，无需DMA到host的内存，减少了PCIe TLP的交互。</p><h3 id="相关paper"><a href="#相关paper" class="headerlink" title="相关paper"></a>相关paper</h3><p>Sherman(SIGMOD’22) is the first RDMA-based system that leverages on-chip memory of commodity RDMA NICs.</p><hr><p>参考资料:</p><ol><li><a href="https://docs.nvidia.com/networking/display/rdmacore50/device+memory" target="_blank" rel="noopener">Device Memory</a></li><li><a href="https://www.openfabrics.org/images/2018workshop/presentations/304_LLiss_OnDeviceMemory.pdf" target="_blank" rel="noopener">On-device memory usage for RDMA transactions</a></li><li><a href="http://lastweek.io/notes/source_code/rdma/" target="_blank" rel="noopener">On DPDK and RDMA Related Software</a></li><li><a href="https://www.man7.org/linux/man-pages/man3/ibv_alloc_dm.3.html" target="_blank" rel="noopener">ibv_alloc_dm(3)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA Device Memory相关notes。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA SRQ/XRC/DCT技术</title>
    <link href="http://liujunming.github.io/2024/10/19/Notes-about-RDMA-SRQ-XRC-DCT%E6%8A%80%E6%9C%AF/"/>
    <id>http://liujunming.github.io/2024/10/19/Notes-about-RDMA-SRQ-XRC-DCT技术/</id>
    <published>2024-10-19T02:47:47.000Z</published>
    <updated>2024-10-22T23:48:39.056Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA SRQ(Shared Receive Queue)/XRC(eXtended Reliable Connection)/DCT(Dynamically Connected Transport)技术相关notes。<a id="more"></a></p><h2 id="1-SRQ"><a href="#1-SRQ" class="headerlink" title="1. SRQ"></a>1. SRQ</h2><h3 id="1-1-为什么需要SRQ"><a href="#1-1-为什么需要SRQ" class="headerlink" title="1.1 为什么需要SRQ"></a>1.1 为什么需要SRQ</h3><p><img src="/images/2024/10/007.webp" alt></p><p>在没有SRQ的情况下，因为RC/UC/UD的接收方不知道对端什么时候会发送过来多少数据，所以必须做好最坏的打算，做好突发性收到大量数据的准备，也就是向RQ中下发足量的的接收WQE；另外RC服务类型可以利用流控机制来抑制发送方，也就是告诉对端”我这边RQ WQE不够了”，这样发送端就会暂时放缓或停止发送数据。</p><p>但是第一种方法由于是为最坏情况准备的，大部分时候有大量的RQ WQE处于空闲状态未被使用，这对内存是一种极大地浪费(主要是WQE指向的用于存放数据的内存空间)；第二种方法虽然不用下发那么多RQ WQE了，但是流控是有代价的，即会增加通信时延。</p><p>而SRQ通过允许很多QP共享接收WQE(本身其实不是很大)以及<strong>用于存放数据的内存空间</strong>(这可是很大一块内存)来解决上面的问题。当任何一个QP收到消息后，硬件会从SRQ中取出一个WQE，根据其内容存放接收到的数据，然后硬件通过Completion Queue来返回接收任务的完成信息给对应的上层用户。</p><p><img src="/images/2024/10/008.webp" alt></p><h3 id="1-2-SRQ-Limit"><a href="#1-2-SRQ-Limit" class="headerlink" title="1.2 SRQ Limit"></a>1.2 SRQ Limit</h3><p>SRQ可以设置一个阈值，当队列中剩余的WQE数量小于阈值时，这个SRQ就会上报一个异步事件。提醒用户“队列中的WQE快用完了，请下发更多WQE以防没有地方接收新的数据”。这个阈值就被称为SRQ Limit，这个上报的事件就被称为SRQ Limit Reached。</p><h2 id="2-XRC"><a href="#2-XRC" class="headerlink" title="2. XRC"></a>2. XRC</h2><h3 id="2-1-为什么需要XRC"><a href="#2-1-为什么需要XRC" class="headerlink" title="2.1 为什么需要XRC"></a>2.1 为什么需要XRC</h3><p>当前的计算节点一般都有多核，因此可以运行多进程。在这样的计算节点组成的集群中，如果想用RC连接建立full mesh的全连接拓扑时，每个节点就需要建立N*p*p个QP(这里假设集群有N个节点，每个节点上有p个进程，需要让任何2个进程都连通)。当集群扩张，N和p同时增长时，一个节点所需的RC QP资源将变得不可接受。</p><p>XRC的思想是<strong>当一个进程想与某个远程节点的p个进程通信时不需要跟各个进程建立p个连接而只需要跟对端节点建立一个连接</strong>，连接上传输的报文携带了对端目的进程号(XRC SRQ)，报文到达连接对端(XRC TGT QP)时根据进程号分发至各个进程对应的XRC SRQ。这样源端进程只需要创建一个源端连接(XRC INI QP)就能跟对端所有进程通信了，这样所需总的QP数量就会除以p。</p><h3 id="2-2-核心概念"><a href="#2-2-核心概念" class="headerlink" title="2.2 核心概念"></a>2.2 核心概念</h3><p><img src="/images/2024/10/009.webp" alt></p><p>上图中XRC下标xyz的含义:x代表发起端的node号，y代表发起端的进程号，z代表接收端的node号。</p><ul><li>XRC INI QP</li></ul><p>XRC发起端QP，是XRC操作的源端队列，用于发出XRC操作，但它没有接收XRC操作的功能，对比常规RC QP来说可以认为它是只有SQ没有RQ。XRC操作在对端由XRC TGT QP处理。</p><ul><li>XRC TGT QP</li></ul><p>XRC接收端QP，它处理XRC操作将其分发至报文SRQ number对应的SRQ。XRC TGT QP只能接收XRC操作，但它没有发出XRC操作的功能，对比常规RC QP来说可以认为它是只有RQ没有SQ。XRC操作在对端由XRC INI QP发出。</p><ul><li>XRC SRQ</li></ul><p>接收缓冲区(receive WQE)被放在XRC SRQ中以接收XRC请求，XRC请求中携带了XRC SRQ number，所以XRC TGT QP收到报文后会从报文指定的XRC SRQ中取receive WQE来存放XRC请求。</p><ul><li>XRC domain</li></ul><p>用于关联XRC TGT QP和XRC SRQ，XRC报文只能指定与XRC TGT QP在同一domain内的XRC SRQ，否则报文会被丢弃。这起到了隔离资源的作用，防止攻击报文随意指定XRC SRQ。</p><p>XRC INI QP和XRC TGT QP是一一对应的，host2上的每个进程在远端节点host0上都有自己对应的XRC TGT QP。<strong>XRC的共享体现在一个XRC TGT QP可以分发至多个XRC SRQ</strong>。一个进程一般只有一个XRC SRQ，它可以接收多个XRC TGT QP来的包。</p><h2 id="3-DCT"><a href="#3-DCT" class="headerlink" title="3. DCT"></a>3. DCT</h2><p>Dynamically Connected transport (DCT) service is an extension to transport services to enable a higher degree of scalability while maintaining high performance for <strong>sparse traffic</strong>. Utilization of DCT reduces the total number of QPs required system wide by <em>having Reliable type QPs dynamically connect and disconnect from any remote node</em>. <strong>DCT connections only stay connected while they are active</strong>. This results in smaller memory footprint, less overhead to set connections and higher on-chip cache utilization and hence increased performance. </p><h3 id="3-1-为什么需要DCT"><a href="#3-1-为什么需要DCT" class="headerlink" title="3.1 为什么需要DCT"></a>3.1 为什么需要DCT</h3><p><img src="/images/2024/10/011.jpg" alt></p><p><img src="/images/2024/10/010.webp" alt></p><p>UD虽然扩展性很好，但是不支持read/write单边语义。RC虽然支持read/write单边语义，但是扩展性不好。DCT的初衷就是融合2者的优点，保持RC的read/write单边语义和可靠连接特性，同时像UD一样<strong>用一个QP去跟多个远端通信，保持良好的可扩展性</strong>。DCT一般用于sparse traffic场景。</p><p><img src="/images/2024/10/012.jpg" alt></p><p>想用RC连接建立full mesh的全连接拓扑时:</p><ul><li>在RC机制下，每个节点就需要建立N*p*p个QP</li><li>在XRC机制下，每个节点就需要建立N*p个QP</li><li>在DCT机制下，每个节点就需要建立p(可能p+n)个QP</li></ul><h3 id="3-2-什么是DCT"><a href="#3-2-什么是DCT" class="headerlink" title="3.2 什么是DCT"></a>3.2 什么是DCT</h3><p><img src="/images/2024/10/013.jpg" alt></p><ul><li>Dynamic Connectivity</li><li>Each DC Initiator can be used to reach any remote DC Target</li></ul><p>DCT具有非对称的API：DC在发送侧的部分称为DC initiator(DCI)，在接收侧的部分称为DC target(DCT)。DCI和DCT不过是特殊类型的QP，它们依然遵循基本的QP操作，比如post send/receive。</p><p>DC意味着临时连接，在DCI上发送的每个send-WR都携带了目的地址信息，如果DCI当前连接的对端不是send-WR里携带的对端(node地址不一样)，则它会首先断开当前的连接，再连接到send-WR里携带的对端。只要后续的send-WR里携带的都是当前已连接对端，则都可以复用当前已建立的连接。如果DCI在一段指定的时间内都没有发送操作则也会断开当前连接。注意DCT每次临时建立的是一个RC可靠连接。</p><h3 id="3-3-思考"><a href="#3-3-思考" class="headerlink" title="3.3 思考"></a>3.3 思考</h3><p>DCT preserves their core connection-oriented design, but dynamically creates and destroys one-to-one connections. This provides software the illusion of using one QP to communicate with multiple remote machines, but at a prohibitively large performance cost for our workloads: DCT requires three additional network messages when the target machine of a DCT queue pair changes: a disconnect packet to the current machine, and a two-way handshake with the next machine to establish a connection[FaSST, OSDI’16].</p><p>所以DCT在<strong>sparse traffic</strong>场景中，性能才高。</p><h3 id="3-4-XRC-vs-DCT"><a href="#3-4-XRC-vs-DCT" class="headerlink" title="3.4 XRC vs DCT"></a>3.4 XRC vs DCT</h3><ul><li>XRC: 发起端进程与不同node通信时，需要与不同node都建立XRC连接</li><li>DCT: 发起端进程与不同node通信时，只需建立一个连接；当发起端进程需要与新node通信时，先与原先的node断连，再与新node建连，从而达到只用一个连接的目标</li></ul><h3 id="3-5-学术论文"><a href="#3-5-学术论文" class="headerlink" title="3.5 学术论文"></a>3.5 学术论文</h3><p>KRCORE: a microsecond-scale RDMA control plane for elastic computing(ATC’22)<br><img src="/images/2024/10/019.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/567720023" target="_blank" rel="noopener">RDMA 高级</a></li><li><a href="https://zhuanlan.zhihu.com/p/279904125" target="_blank" rel="noopener">Savir专栏:11. RDMA之Shared Receive Queue</a></li><li><a href="https://docs.nvidia.com/networking/display/mlnxofedv543100/advanced+transport" target="_blank" rel="noopener">Advanced Transport</a></li><li><a href="https://www.openfabrics.org/images/eventpresos/workshops2014/DevWorkshop/presos/Monday/pdf/05_DC_Verbs.pdf" target="_blank" rel="noopener">Dynamically Connected Transport</a></li><li><a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-kalia.pdf" target="_blank" rel="noopener">FaSST: Fast, Scalable and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA SRQ(Shared Receive Queue)/XRC(eXtended Reliable Connection)/DCT(Dynamically Connected Transport)技术相关notes。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Intel架构下TLB shutdown使用pause指令</title>
    <link href="http://liujunming.github.io/2024/10/19/Intel%E6%9E%B6%E6%9E%84%E4%B8%8BTLB-shutdown%E4%BD%BF%E7%94%A8pause%E6%8C%87%E4%BB%A4/"/>
    <id>http://liujunming.github.io/2024/10/19/Intel架构下TLB-shutdown使用pause指令/</id>
    <published>2024-10-19T00:12:05.000Z</published>
    <updated>2024-10-19T01:46:59.135Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel架构下TLB shutdown使用pause指令的相关notes。<a id="more"></a><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">smp_call_function_many_cond</span><span class="params">(<span class="keyword">const</span> struct cpumask *mask,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">smp_call_func_t</span> func, <span class="keyword">void</span> *info,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">unsigned</span> <span class="keyword">int</span> scf_flags,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">smp_cond_func_t</span> cond_func)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">if</span> (run_remote &amp;&amp; wait) &#123;</span><br><span class="line">                <span class="comment">// 按顺序等各个cpu修改csd的flag，不然死等</span></span><br><span class="line">for_each_cpu(cpu, cfd-&gt;cpumask) &#123;</span><br><span class="line"><span class="keyword">call_single_data_t</span> *csd;</span><br><span class="line"></span><br><span class="line">csd = per_cpu_ptr(cfd-&gt;csd, cpu);</span><br><span class="line">csd_lock_wait(csd);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>csd_lock_wait</code>会调用到pause命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">csd_lock_wait</span><br><span class="line">└── smp_cond_load_relaxed</span><br><span class="line">    └── cpu_relax</span><br><span class="line">        └── asm volatile(&quot;rep; nop&quot;)</span><br></pre></td></tr></table></figure></p><p><code>rep;nop</code>的机器码是f3 90，其实就是pause指令的机器码，相当于pause的一个”别名”。</p><p><img src="/images/2024/10/004.png" alt></p><p><img src="/images/2024/10/005.png" alt></p><p><img src="/images/2024/10/006.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://cloud.tencent.com/developer/article/1179606" target="_blank" rel="noopener">x86的cpu_relax解析</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel架构下TLB shutdown使用pause指令的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA UMR(User-Mode Memory Registration)</title>
    <link href="http://liujunming.github.io/2024/10/13/Notes-about-RDMA-User-Mode-Memory-Registration-UMR/"/>
    <id>http://liujunming.github.io/2024/10/13/Notes-about-RDMA-User-Mode-Memory-Registration-UMR/</id>
    <published>2024-10-13T10:49:06.000Z</published>
    <updated>2024-10-13T11:31:02.495Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA UMR(User-Mode Memory Registration)机制相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>User-Mode Memory Registration (UMR) supports the creation of memory keys for non-contiguous memory regions. This includes the concatenation(连接) of arbitrary contiguous regions of memory, as well as regions with regular structure.</p><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>Three examples of non-contiguous regions of memory that are used to <strong>form new contiguous regions of memory</strong> are described below.</p><h3 id="将多块非连续的MR拼接成一个VA连续的MR"><a href="#将多块非连续的MR拼接成一个VA连续的MR" class="headerlink" title="将多块非连续的MR拼接成一个VA连续的MR"></a>将多块非连续的MR拼接成一个VA连续的MR</h3><p><img src="/images/2024/10/001.jpg" alt></p><p>如上图所示，我们之前创建了3个常规的MR：MR1(green), MR2(purple), MR3(red)，现在我们想从这三个MR中各抽取一部分拼接起来形成一个新的连续的MR：第一块是MR1(v0-v1)部分，第二块是MR2(v2-v3)部分，第三块是MR3(v4-v5)部分。这个新的MR有一个新的base VA地址，长度是3个小块的长度之和。这样虽然内部是不连续的，但在外部访问者看来这个MR是连续的。</p><h3 id="将一个MR内有规律非连续的块拼接成一个连续的MR"><a href="#将一个MR内有规律非连续的块拼接成一个连续的MR" class="headerlink" title="将一个MR内有规律非连续的块拼接成一个连续的MR"></a>将一个MR内有规律非连续的块拼接成一个连续的MR</h3><p><img src="/images/2024/10/002.jpg" alt></p><p>如上图所示，当我们做一个矩阵的转置时，需要把一列的元素拼成新的行，这个行就成了新的连续的MR。</p><h3 id="将多个MR拼接成新的相互交织的连续MR"><a href="#将多个MR拼接成新的相互交织的连续MR" class="headerlink" title="将多个MR拼接成新的相互交织的连续MR"></a>将多个MR拼接成新的相互交织的连续MR</h3><p><img src="/images/2024/10/003.jpg" alt></p><p>如上图所示，2个老矩阵的列相互交织形成新的列，这是一个新的VA连续的MR，它有自己新的base address和length。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>UMR会创建新的memory keys、VA(Virtual Address)地址和MTT entries；在MTT entry中，保证新的VA地址指向目标PA(Physical Address)即可。</p><hr><p>参考资料:</p><ol><li><a href="https://docs.nvidia.com/rdma-aware-networks-programming-user-manual-1-7.pdf" target="_blank" rel="noopener">RDMA Aware Networks Programming User Manual</a></li><li><a href="https://zhuanlan.zhihu.com/p/567720023" target="_blank" rel="noopener">RDMA 高级</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA UMR(User-Mode Memory Registration)机制相关notes。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ARM VHE mode</title>
    <link href="http://liujunming.github.io/2024/09/08/Notes-about-ARM-VHE-mode/"/>
    <id>http://liujunming.github.io/2024/09/08/Notes-about-ARM-VHE-mode/</id>
    <published>2024-09-08T12:57:19.000Z</published>
    <updated>2024-09-11T13:08:22.298Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下ARM VHE(Virtualization Host Extensions)mode相关notes。<a id="more"></a></p><p><img src="/images/2024/09/001.jpg" alt></p><p>通常，寄主操作系统的内核部分运行在EL1，控制虚拟化的部分运行在EL2。然而，这种设计有一个明显的问题。VHE之前的Hypervisor通常需要设计成high-visor和low-visor两部分，前者运行在EL1，后者运行在EL2。分层设计在系统运行时会造成很多不必要的上下文切换，带来不少设计上的复杂性和性能开销。为了解决这个问题，虚拟化主机扩展 （Virtualization Host Extensions, VHE）应运而生。该特性由Armv8.1-A引入，可以让寄主操作系统的内核部分直接运行在EL2上。</p><p><img src="/images/2024/09/002.jpg" alt></p><p><img src="/images/2024/09/003.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.trustedfirmware.org/docs/VHE-Support-updated.pdf" target="_blank" rel="noopener">Hafnium – VHE Support</a></li><li><a href="http://events17.linuxfoundation.org/sites/events/files/slides/To%20EL2%20and%20Beyond_0.pdf" target="_blank" rel="noopener">To EL2, and Beyond!</a></li><li><a href="https://developer.arm.com/documentation/102142/0100/Virtualization-host-extensions" target="_blank" rel="noopener">Virtualization host extensions</a></li><li><a href="http://linux-kernel.uio.no/pub/linux/kernel/people/will/slides/kvmforum-2020-edited.pdf" target="_blank" rel="noopener">Virtualisation for the Masses:Exposing KVM on Android</a></li><li><a href="https://calinyara.github.io/technology/2019/11/03/armv8-virtualization.html" target="_blank" rel="noopener">Armv8架构虚拟化介绍</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下ARM VHE(Virtualization Host Extensions)mode相关notes。
    
    </summary>
    
      <category term="ARM" scheme="http://liujunming.github.io/categories/ARM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="ARM" scheme="http://liujunming.github.io/tags/ARM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA Event Queue mechanism</title>
    <link href="http://liujunming.github.io/2024/08/25/Notes-about-RDMA-Event-Queue-mechanism/"/>
    <id>http://liujunming.github.io/2024/08/25/Notes-about-RDMA-Event-Queue-mechanism/</id>
    <published>2024-08-25T10:50:35.000Z</published>
    <updated>2024-08-29T12:59:50.164Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA中的Event Queue机制。<a id="more"></a><br>Host Channel Adapter(HCA) device, HCA device, NIC, NIC device and adapter device are used interchangeably.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>HCA has multiple sources that can generate events (completion events, asynchronous events/<br>errors). Once an event is generated internally, it can be reported to the host software via the Event<br>Queue mechanism. The EQ is a memory-resident circular buffer used by hardware to write event<br>cause information for consumption by the host software. Once event reporting is enabled, event<br>cause information is written by hardware to the EQ when the event occurs. If EQ is armed, HW<br>will subsequently generate an interrupt on the device interface (send MSI-X message or assert<br>the pin) as configured in the EQ.</p></blockquote><h2 id="Q-amp-amp-A"><a href="#Q-amp-amp-A" class="headerlink" title="Q &amp;&amp; A"></a>Q &amp;&amp; A</h2><p>Q1: 都有cq了，为什么还要有completion EQ？<br>A1: 如果只有cq，就只能用轮询方式了，加上ceq之后，中断上来就能从ceqe中拿到哪个cq有数据</p><p>Q2: 为什么不能为每个cq分配一个中断vector，这样就无需eq机制了?<br>A2: 一个RDMA设备的CQ会很多，大概率会超过2048个，此时就超过了MSI-x table的上限，因而引入了eq机制，将多个cq绑定到1个eq上，然后为每个eq分配一个中断vector，控制eq的数量，就会保证vector个数不超过MSI-x table的上限</p><p>Q3: CQ与EQ是如何绑定的？<br>A3: While creating a CQ, software configures the EQ number to which this CQ will report completion events.</p><h2 id="eRDMA-example"><a href="#eRDMA-example" class="headerlink" title="eRDMA example"></a>eRDMA example</h2><p>Event queue (EQ) is the main notification way from erdma hardware to its driver. Each erdma device contains 2 kinds EQs: asynchronous EQ (AEQ) and completion EQ (CEQ). Per device has 1 AEQ, which used for RDMA async event report, and max to 32 CEQs (numbered for CEQ0 to CEQ31). CEQ0 is used for cmdq completion event report, and the rest CEQs are used for RDMA completion event report.</p><h3 id="CQ与EQ的绑定"><a href="#CQ与EQ的绑定" class="headerlink" title="CQ与EQ的绑定"></a>CQ与EQ的绑定</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">create_cq_cmd</span><span class="params">(struct erdma_ucontext *uctx, struct erdma_cq *cq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">erdma_dev</span> *<span class="title">dev</span> = <span class="title">to_edev</span>(<span class="title">cq</span>-&gt;<span class="title">ibcq</span>.<span class="title">device</span>);</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">erdma_cmdq_create_cq_req</span> <span class="title">req</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">erdma_mem</span> *<span class="title">mem</span>;</span></span><br><span class="line">u32 page_size;</span><br><span class="line"></span><br><span class="line">erdma_cmdq_build_reqhdr(&amp;req.hdr, CMDQ_SUBMOD_RDMA,</span><br><span class="line">CMDQ_OPCODE_CREATE_CQ);</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">req.cfg1 = FIELD_PREP(ERDMA_CMD_CREATE_CQ_EQN_MASK, cq-&gt;assoc_eqn);</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">erdma_create_cq</span><span class="params">(struct ib_cq *ibcq, <span class="keyword">const</span> struct ib_cq_init_attr *attr,</span></span></span><br><span class="line"><span class="function"><span class="params">    struct ib_udata *udata)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        ...</span><br><span class="line">        cq-&gt;assoc_eqn = attr-&gt;comp_vector + <span class="number">1</span>;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/2024/08/030.jpg" alt></p><h3 id="eqe获取CQ-number"><a href="#eqe获取CQ-number" class="headerlink" title="eqe获取CQ number"></a>eqe获取CQ number</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> irqreturn_t <span class="title">erdma_intr_ceq_handler</span><span class="params">(<span class="keyword">int</span> irq, <span class="keyword">void</span> *data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">erdma_eq_cb</span> *<span class="title">ceq_cb</span> = <span class="title">data</span>;</span></span><br><span class="line"></span><br><span class="line">tasklet_schedule(&amp;ceq_cb-&gt;tasklet); <span class="comment">//会调用erdma_intr_ceq_task</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> IRQ_HANDLED;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">erdma_intr_ceq_task</span><br><span class="line">└── erdma_ceq_completion_handler</span><br><span class="line">    ├── get_next_valid_eqe</span><br><span class="line">    └── cqn = FIELD_GET(ERDMA_CEQE_HDR_CQN_MASK, READ_ONCE(*ceqe))</span><br></pre></td></tr></table></figure><h2 id="mellanox-mlx4-example"><a href="#mellanox-mlx4-example" class="headerlink" title="mellanox mlx4 example"></a>mellanox mlx4 example</h2><h3 id="CQ与EQ的绑定-1"><a href="#CQ与EQ的绑定-1" class="headerlink" title="CQ与EQ的绑定"></a>CQ与EQ的绑定</h3><p><img src="/images/2024/08/031.jpg" alt></p><p><img src="/images/2024/08/027.jpg" alt></p><p><img src="/images/2024/08/028.jpg" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mlx4_ib_create_cq[mlx4_ib_dev_ops.create_cq]</span><br><span class="line">└── mlx4_cq_alloc</span><br><span class="line">    └── cq_context-&gt;comp_eqn = ...</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">mlx4_cq_alloc</span><span class="params">(struct mlx4_dev *dev, <span class="keyword">int</span> nent,</span></span></span><br><span class="line"><span class="function"><span class="params">  struct mlx4_mtt *mtt, struct mlx4_uar *uar, u64 db_rec,</span></span></span><br><span class="line"><span class="function"><span class="params">  struct mlx4_cq *cq, <span class="keyword">unsigned</span> <span class="built_in">vector</span>, <span class="keyword">int</span> collapsed,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">int</span> timestamp_en, <span class="keyword">void</span> *buf_addr, <span class="keyword">bool</span> user_cq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">bool</span> sw_cq_init = dev-&gt;caps.flags2 &amp; MLX4_DEV_CAP_FLAG2_SW_CQ_INIT;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mlx4_priv</span> *<span class="title">priv</span> = <span class="title">mlx4_priv</span>(<span class="title">dev</span>);</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mlx4_cq_table</span> *<span class="title">cq_table</span> = &amp;<span class="title">priv</span>-&gt;<span class="title">cq_table</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mlx4_cmd_mailbox</span> *<span class="title">mailbox</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mlx4_cq_context</span> *<span class="title">cq_context</span>;</span></span><br><span class="line">u64 mtt_addr;</span><br><span class="line"><span class="keyword">int</span> err;</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">mailbox = mlx4_alloc_cmd_mailbox(dev);</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">cq_context = mailbox-&gt;buf;</span><br><span class="line">        ...</span><br><span class="line">cq_context-&gt;comp_eqn    = priv-&gt;eq_table.eq[MLX4_CQ_TO_EQ_VECTOR(<span class="built_in">vector</span>)].eqn;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="eqe获取CQ-number-1"><a href="#eqe获取CQ-number-1" class="headerlink" title="eqe获取CQ number"></a>eqe获取CQ number</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> mlx4_event &#123;</span><br><span class="line">MLX4_EVENT_TYPE_COMP   = <span class="number">0x00</span>,</span><br><span class="line">MLX4_EVENT_TYPE_PATH_MIG   = <span class="number">0x01</span>,</span><br><span class="line">MLX4_EVENT_TYPE_COMM_EST   = <span class="number">0x02</span>,</span><br><span class="line">MLX4_EVENT_TYPE_SQ_DRAINED   = <span class="number">0x03</span>,</span><br><span class="line">MLX4_EVENT_TYPE_SRQ_QP_LAST_WQE   = <span class="number">0x13</span>,</span><br><span class="line">MLX4_EVENT_TYPE_SRQ_LIMIT   = <span class="number">0x14</span>,</span><br><span class="line">MLX4_EVENT_TYPE_CQ_ERROR   = <span class="number">0x04</span>,</span><br><span class="line">MLX4_EVENT_TYPE_WQ_CATAS_ERROR   = <span class="number">0x05</span>,</span><br><span class="line">MLX4_EVENT_TYPE_EEC_CATAS_ERROR   = <span class="number">0x06</span>,</span><br><span class="line">MLX4_EVENT_TYPE_PATH_MIG_FAILED   = <span class="number">0x07</span>,</span><br><span class="line">MLX4_EVENT_TYPE_WQ_INVAL_REQ_ERROR = <span class="number">0x10</span>,</span><br><span class="line">MLX4_EVENT_TYPE_WQ_ACCESS_ERROR   = <span class="number">0x11</span>,</span><br><span class="line">MLX4_EVENT_TYPE_SRQ_CATAS_ERROR   = <span class="number">0x12</span>,</span><br><span class="line">MLX4_EVENT_TYPE_LOCAL_CATAS_ERROR  = <span class="number">0x08</span>,</span><br><span class="line">MLX4_EVENT_TYPE_PORT_CHANGE   = <span class="number">0x09</span>,</span><br><span class="line">MLX4_EVENT_TYPE_EQ_OVERFLOW   = <span class="number">0x0f</span>,</span><br><span class="line">MLX4_EVENT_TYPE_ECC_DETECT   = <span class="number">0x0e</span>,</span><br><span class="line">MLX4_EVENT_TYPE_CMD   = <span class="number">0x0a</span>,</span><br><span class="line">MLX4_EVENT_TYPE_VEP_UPDATE   = <span class="number">0x19</span>,</span><br><span class="line">MLX4_EVENT_TYPE_COMM_CHANNEL   = <span class="number">0x18</span>,</span><br><span class="line">MLX4_EVENT_TYPE_OP_REQUIRED   = <span class="number">0x1a</span>,</span><br><span class="line">MLX4_EVENT_TYPE_FATAL_WARNING   = <span class="number">0x1b</span>,</span><br><span class="line">MLX4_EVENT_TYPE_FLR_EVENT   = <span class="number">0x1c</span>,</span><br><span class="line">MLX4_EVENT_TYPE_PORT_MNG_CHG_EVENT = <span class="number">0x1d</span>,</span><br><span class="line">MLX4_EVENT_TYPE_RECOVERABLE_ERROR_EVENT  = <span class="number">0x3e</span>,</span><br><span class="line">MLX4_EVENT_TYPE_NONE   = <span class="number">0xff</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">mlx4_eq_int</span><span class="params">(struct mlx4_dev *dev, struct mlx4_eq *eq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mlx4_priv</span> *<span class="title">priv</span> = <span class="title">mlx4_priv</span>(<span class="title">dev</span>);</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mlx4_eqe</span> *<span class="title">eqe</span>;</span></span><br><span class="line"><span class="keyword">int</span> cqn;</span><br><span class="line"><span class="keyword">int</span> eqes_found = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> set_ci = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> port;</span><br><span class="line"><span class="keyword">int</span> slave = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> ret;</span><br><span class="line"><span class="keyword">int</span> flr_slave;</span><br><span class="line">u8 update_slave_state;</span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="keyword">enum</span> slave_port_gen_event gen_event;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> flags;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mlx4_vport_state</span> *<span class="title">s_info</span>;</span></span><br><span class="line"><span class="keyword">int</span> eqe_size = dev-&gt;caps.eqe_size;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> ((eqe = next_eqe_sw(eq, dev-&gt;caps.eqe_factor, eqe_size))) &#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Make sure we read EQ entry contents after we've</span></span><br><span class="line"><span class="comment"> * checked the ownership bit.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">dma_rmb();</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> (eqe-&gt;type) &#123;</span><br><span class="line"><span class="keyword">case</span> MLX4_EVENT_TYPE_COMP:</span><br><span class="line">cqn = be32_to_cpu(eqe-&gt;event.comp.cqn) &amp; <span class="number">0xffffff</span>;</span><br><span class="line">mlx4_cq_completion(dev, cqn);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">                        ...</span><br></pre></td></tr></table></figure><p><img src="/images/2024/08/022.jpg" alt></p><p><img src="/images/2024/08/023.jpg" alt></p><p><img src="/images/2024/08/024.jpg" alt></p><p><code>cqn = be32_to_cpu(eqe-&gt;event.comp.cqn) &amp; 0xffffff</code>保证了cq number就是event data的0~23位。</p><hr><p>参考资料:</p><ol><li><a href="https://network.nvidia.com/files/doc-2020/ethernet-adapters-programming-manual.pdf" target="_blank" rel="noopener">Mellanox Adapters Programmer’s Reference Manual (PRM)</a></li><li><a href="https://lore.kernel.org/all/20220711131316.3449-6-chengyou@linux.alibaba.com/" target="_blank" rel="noopener">RDMA/erdma: Add event queue implementation</a></li><li><a href="https://www.rdmamojo.com/2012/11/03/ibv_create_cq/" target="_blank" rel="noopener">ibv_create_cq()</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA中的Event Queue机制。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA cmdq</title>
    <link href="http://liujunming.github.io/2024/08/25/Notes-about-RDMA-cmdq/"/>
    <id>http://liujunming.github.io/2024/08/25/Notes-about-RDMA-cmdq/</id>
    <published>2024-08-25T07:23:08.000Z</published>
    <updated>2024-08-29T12:20:48.512Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA中cmdq(command queue)相关notes。<a id="more"></a><br>Host Channel Adapter(HCA) device, HCA device, NIC, NIC device and adapter device are used interchangeably.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The HCA command interface is used for:</p><ul><li>configuring the HCA</li><li>the handshake between hardware and system software</li><li>handling (querying, configuring, modifying) HCA objects</li></ul><p>The HCA is configured using the command queues. Each function has its own command queues to get commands from its HCA driver.</p><p>The command queue is the transport that is used to pass commands to the HCA.</p><p>cmdq其实属于一种sq(Send Queue)，可以类比于NVMe的admin sq(submission queue)。</p><p>对于不同类型的RDMA设备，cmdq的具体实现是存在差异的。</p><h2 id="mellanox-mlx4-example"><a href="#mellanox-mlx4-example" class="headerlink" title="mellanox mlx4 example"></a>mellanox mlx4 example</h2><p>mellanox mlx4 cmdq的细节，可以参考spec中的7.14 Command Interface一节。<br><img src="/images/2024/08/029.jpg" alt></p><h2 id="eRDMA-example"><a href="#eRDMA-example" class="headerlink" title="eRDMA example"></a>eRDMA example</h2><p>Cmdq is the main control plane channel between erdma driver and hardware. After erdma device is initialized, the cmdq channel will be active in the whole lifecycle of this driver.</p><h3 id="cmdq命令"><a href="#cmdq命令" class="headerlink" title="cmdq命令"></a>cmdq命令</h3><p>eRDMA支持如下命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">enum CMDQ_RDMA_OPCODE &#123;</span><br><span class="line">CMDQ_OPCODE_QUERY_DEVICE = 0,</span><br><span class="line">CMDQ_OPCODE_CREATE_QP = 1,</span><br><span class="line">CMDQ_OPCODE_DESTROY_QP = 2,</span><br><span class="line">CMDQ_OPCODE_MODIFY_QP = 3,</span><br><span class="line">CMDQ_OPCODE_CREATE_CQ = 4,</span><br><span class="line">CMDQ_OPCODE_DESTROY_CQ = 5,</span><br><span class="line">CMDQ_OPCODE_REFLUSH = 6,</span><br><span class="line">CMDQ_OPCODE_REG_MR = 8,</span><br><span class="line">CMDQ_OPCODE_DEREG_MR = 9</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">enum CMDQ_COMMON_OPCODE &#123;</span><br><span class="line">CMDQ_OPCODE_CREATE_EQ = 0,</span><br><span class="line">CMDQ_OPCODE_DESTROY_EQ = 1,</span><br><span class="line">CMDQ_OPCODE_QUERY_FW_INFO = 2,</span><br><span class="line">CMDQ_OPCODE_CONF_MTU = 3,</span><br><span class="line">CMDQ_OPCODE_CONF_DEVICE = 5,</span><br><span class="line">CMDQ_OPCODE_ALLOC_DB = 8,</span><br><span class="line">CMDQ_OPCODE_FREE_DB = 9,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="erdma-device-ops"><a href="#erdma-device-ops" class="headerlink" title="erdma_device_ops"></a>erdma_device_ops</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">ib_device_ops</span> <span class="title">erdma_device_ops</span> = &#123;</span></span><br><span class="line">.owner = THIS_MODULE,</span><br><span class="line">.driver_id = RDMA_DRIVER_ERDMA,</span><br><span class="line">.uverbs_abi_ver = ERDMA_ABI_VERSION,</span><br><span class="line"></span><br><span class="line">.alloc_mr = erdma_ib_alloc_mr,</span><br><span class="line">.alloc_pd = erdma_alloc_pd,</span><br><span class="line">.alloc_ucontext = erdma_alloc_ucontext,</span><br><span class="line">.create_cq = erdma_create_cq,</span><br><span class="line">.create_qp = erdma_create_qp,</span><br><span class="line">.dealloc_pd = erdma_dealloc_pd,</span><br><span class="line">.dealloc_ucontext = erdma_dealloc_ucontext,</span><br><span class="line">.dereg_mr = erdma_dereg_mr,</span><br><span class="line">.destroy_cq = erdma_destroy_cq,</span><br><span class="line">.destroy_qp = erdma_destroy_qp,</span><br><span class="line">.get_dma_mr = erdma_get_dma_mr,</span><br><span class="line">.get_port_immutable = erdma_get_port_immutable,</span><br><span class="line">.iw_accept = erdma_accept,</span><br><span class="line">.iw_add_ref = erdma_qp_get_ref,</span><br><span class="line">.iw_connect = erdma_connect,</span><br><span class="line">.iw_create_listen = erdma_create_listen,</span><br><span class="line">.iw_destroy_listen = erdma_destroy_listen,</span><br><span class="line">.iw_get_qp = erdma_get_ibqp,</span><br><span class="line">.iw_reject = erdma_reject,</span><br><span class="line">.iw_rem_ref = erdma_qp_put_ref,</span><br><span class="line">.map_mr_sg = erdma_map_mr_sg,</span><br><span class="line">.mmap = erdma_mmap,</span><br><span class="line">.mmap_free = erdma_mmap_free,</span><br><span class="line">.modify_qp = erdma_modify_qp,</span><br><span class="line">.post_recv = erdma_post_recv,</span><br><span class="line">.post_send = erdma_post_send,</span><br><span class="line">.poll_cq = erdma_poll_cq,</span><br><span class="line">.query_device = erdma_query_device,</span><br><span class="line">.query_gid = erdma_query_gid,</span><br><span class="line">.query_port = erdma_query_port,</span><br><span class="line">.query_qp = erdma_query_qp,</span><br><span class="line">.req_notify_cq = erdma_req_notify_cq,</span><br><span class="line">.reg_user_mr = erdma_reg_user_mr,</span><br><span class="line"></span><br><span class="line">INIT_RDMA_OBJ_SIZE(ib_cq, erdma_cq, ibcq),</span><br><span class="line">INIT_RDMA_OBJ_SIZE(ib_pd, erdma_pd, ibpd),</span><br><span class="line">INIT_RDMA_OBJ_SIZE(ib_ucontext, erdma_ucontext, ibucontext),</span><br><span class="line">INIT_RDMA_OBJ_SIZE(ib_qp, erdma_qp, ibqp),</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><code>struct ib_device_ops</code> - InfiniBand device operations, 其实是内核与cmdq的交互接口。以<code>alloc_mr</code>为例，用户态下发创建Memory Region的请求到内核，此时<code>erdma_ib_alloc_mr</code>就会被调用。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">erdma_ib_alloc_mr</span><br><span class="line">└── regmr_cmd</span><br><span class="line">    ├── erdma_cmdq_build_reqhdr(&amp;req.hdr, CMDQ_SUBMOD_RDMA, CMDQ_OPCODE_REG_MR)</span><br><span class="line">    └── erdma_post_cmd_wait</span><br></pre></td></tr></table></figure></p><p>最终，eRDMA driver会往cmdq中下发<code>CMDQ_OPCODE_REG_MR</code>命令来创建Memory Region。</p><h3 id="erdma-post-cmd-wait"><a href="#erdma-post-cmd-wait" class="headerlink" title="erdma_post_cmd_wait"></a>erdma_post_cmd_wait</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">erdma_post_cmd_wait</span><br><span class="line">├── push_cmdq_sqe</span><br><span class="line">│   └── kick_cmdq_db <span class="comment">//更新sq的db寄存器</span></span><br><span class="line">├── erdma_wait_cmd_completion <span class="comment">// 如果使用cmdq eq中断</span></span><br><span class="line">│   └── wait_for_completion_timeout <span class="comment">//当前进程等待eq中断handler来唤醒(complete(&amp;comp_wait-&gt;wait_event))</span></span><br><span class="line">└── erdma_poll_cmd_completion <span class="comment">// 如果使用polling</span></span><br><span class="line">    └── erdma_polling_cmd_completions</span><br></pre></td></tr></table></figure><h3 id="cmdq初始化"><a href="#cmdq初始化" class="headerlink" title="cmdq初始化"></a>cmdq初始化</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">erdma_probe</span><br><span class="line">└── erdma_probe_dev</span><br><span class="line">    ├── erdma_comm_irq_init</span><br><span class="line">    │   └── request_irq(...erdma_comm_irq_handler...)</span><br><span class="line">    └── erdma_cmdq_init</span><br><span class="line">        ├── erdma_cmdq_sq_init</span><br><span class="line">        ├── erdma_cmdq_cq_init</span><br><span class="line">        └── erdma_cmdq_eq_init</span><br></pre></td></tr></table></figure><h3 id="cmdq中断通知"><a href="#cmdq中断通知" class="headerlink" title="cmdq中断通知"></a>cmdq中断通知</h3><p>Q: cmdq已经有cq了，为什么还需要eq(CEQ0)?<br>A: 如果只有cq，就只能用轮询模式了，加上ceq之后，cmdq与eq配合就能完成中断通知。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> irqreturn_t <span class="title">erdma_comm_irq_handler</span><span class="params">(<span class="keyword">int</span> irq, <span class="keyword">void</span> *data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">erdma_dev</span> *<span class="title">dev</span> = <span class="title">data</span>;</span></span><br><span class="line"></span><br><span class="line">erdma_cmdq_completion_handler(&amp;dev-&gt;cmdq);</span><br><span class="line">erdma_aeq_event_handler(dev);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> IRQ_HANDLED;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">erdma_cmdq_completion_handler</span><br><span class="line">├── erdma_polling_cmd_completions</span><br><span class="line">│   ├── erdma_poll_single_cmd_completion</span><br><span class="line">│   │   ├── get_next_valid_cmdq_cqe</span><br><span class="line">│   │   └── complete(&amp;comp_wait-&gt;wait_event) <span class="comment">//唤醒等待wait_event的进程(erdma_wait_cmd_completion)</span></span><br><span class="line">│   └── arm_cmdq_cq <span class="comment">//更新cq的db寄存器</span></span><br><span class="line">└── notify_eq <span class="comment">//更新eq的db寄存器</span></span><br></pre></td></tr></table></figure><h3 id="cmdq相关寄存器"><a href="#cmdq相关寄存器" class="headerlink" title="cmdq相关寄存器"></a>cmdq相关寄存器</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_SQ_ADDR_L_REG 0x20</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_SQ_ADDR_H_REG 0x24</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_CQ_ADDR_L_REG 0x28</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_CQ_ADDR_H_REG 0x2C</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_DEPTH_REG 0x30</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_EQ_DEPTH_REG 0x34</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_EQ_ADDR_L_REG 0x38</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ERDMA_REGS_CMDQ_EQ_ADDR_H_REG 0x3C</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://network.nvidia.com/files/doc-2020/ethernet-adapters-programming-manual.pdf" target="_blank" rel="noopener">Mellanox Adapters Programmer’s Reference Manual (PRM)</a></li><li><a href="https://lore.kernel.org/all/20220711131316.3449-5-chengyou@linux.alibaba.com/" target="_blank" rel="noopener">RDMA/erdma: Add cmdq implementation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA中cmdq(command queue)相关notes。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>RDMA 资料合集</title>
    <link href="http://liujunming.github.io/2024/08/24/RDMA-spec%E5%90%88%E9%9B%86/"/>
    <id>http://liujunming.github.io/2024/08/24/RDMA-spec合集/</id>
    <published>2024-08-24T10:40:54.000Z</published>
    <updated>2024-10-19T06:57:40.025Z</updated>
    
    <content type="html"><![CDATA[<p>本文将持续记录RDMA资料合集。<a id="more"></a></p><ul><li><a href="https://network.nvidia.com/files/doc-2020/ethernet-adapters-programming-manual.pdf" target="_blank" rel="noopener">Mellanox Adapters Programmer’s Reference Manual (PRM)</a></li><li><a href="https://gist.github.com/aagontuk/705315c94eeaf657b3f35b011c233c19" target="_blank" rel="noopener">RDMA Resources</a></li><li><a href="https://docs.nvidia.com/rdma-aware-networks-programming-user-manual-1-7.pdf" target="_blank" rel="noopener">RDMA Aware Networks Programming User Manual</a></li><li><a href="https://www.ngdcn.com/category-13.html" target="_blank" rel="noopener">NGDCN:RDMA技术</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将持续记录RDMA资料合集。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>network: USO vs UFO</title>
    <link href="http://liujunming.github.io/2024/08/18/network-USO-vs-UFO/"/>
    <id>http://liujunming.github.io/2024/08/18/network-USO-vs-UFO/</id>
    <published>2024-08-18T12:24:36.000Z</published>
    <updated>2024-08-18T12:56:39.938Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下USO(UDP Segmentation offload) vs UFO(UDP Fragmentation Offload)相关notes。<a id="more"></a><br>需阅读<a href="/2024/08/11/Network-Segmentation-vs-Fragmentation/">Network Segmentation vs Fragmentation</a>，值得注意的是，UDP也是存在Segmentation的。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>UDP Segmentation Offload (USO)is a feature that enables network interface cards (NICs) to offload the segmentation of UDP datagrams that are larger than the maximum transmission unit (MTU) of the network medium.</p><p>UDP fragmentation offload allows a device to fragment an oversized UDP datagram into multiple IPv4 fragments.</p><h2 id="USO-vs-UFO"><a href="#USO-vs-UFO" class="headerlink" title="USO vs UFO"></a>USO vs UFO</h2><p>There is a USO feature that is different from existing UFO:</p><ul><li>UFO fragments the UDP packet and only first fragment carries the UDP header (SKB_GSO_UDP in the Linux network stack)</li><li>USO segments the UDP packet, each segment has a UDP header and IP identification field is incremented for each segment. It is designated as SKB_GSO_UDP_L4 in the Linux network stack</li></ul><blockquote><p>VIRTIO_NET_F_HOST_USO (56)<br>Device can receive USO packets. Unlike UFO (fragmenting the packet) the USO splits large UDP packet to several segments when each of these smaller packets has UDP header.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">Virtual I/O Device (VIRTIO) Version 1.2</a></li><li><a href="https://github.com/oasis-tcs/virtio-spec/issues/104" target="_blank" rel="noopener">virtio-net: define USO feature</a></li><li><a href="https://lore.kernel.org/netdev/20221207113558.19003-4-andrew@daynix.com/T/" target="_blank" rel="noopener">udp: allow header check for dodgy GSO_UDP_L4 packets</a></li><li><a href="https://www.slideshare.net/slideshow/20140928-gso-eurobsdcon2014/43398725" target="_blank" rel="noopener">Software segmentation offloading for FreeBSD by Stefano Garzarella</a></li><li><a href="https://www.cnblogs.com/sammyliu/p/5227121.html" target="_blank" rel="noopener">理解 Linux 网络栈（2）：非虚拟化Linux 环境中的 Segmentation Offloading 技术</a></li><li><a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/network/udp-segmentation-offload-uso-" target="_blank" rel="noopener">UDP Segmentation Offload (USO)</a></li><li><a href="https://www.kernel.org/doc/html/next/networking/segmentation-offloads.html" target="_blank" rel="noopener">Segmentation Offloads</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下USO(UDP Segmentation offload) vs UFO(UDP Fragmentation Offload)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about network checksum offload</title>
    <link href="http://liujunming.github.io/2024/08/18/Notes-about-network-checksum-offload/"/>
    <id>http://liujunming.github.io/2024/08/18/Notes-about-network-checksum-offload/</id>
    <published>2024-08-18T11:21:20.000Z</published>
    <updated>2024-08-18T12:11:53.952Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下network checksum offload技术的相关notes。<a id="more"></a></p><h2 id="软件协议checksum"><a href="#软件协议checksum" class="headerlink" title="软件协议checksum"></a>软件协议checksum</h2><p>很多网络协议，例如IP、TCP、UDP都有自己的校验和(checksum)。</p><h3 id="TCP-checksum"><a href="#TCP-checksum" class="headerlink" title="TCP checksum"></a>TCP checksum</h3><p>TCP校验和计算三部分：TCP头部、TCP数据和TCP伪头部。TCP校验和是必须的。<br><img src="/images/2024/08/016.jpg" alt></p><p><img src="/images/2024/08/017.jpg" alt></p><h3 id="UDP-checksum"><a href="#UDP-checksum" class="headerlink" title="UDP checksum"></a>UDP checksum</h3><p>UDP校验和计算三部分：UDP头部、UDP数据和UDP伪头部。UDP校验和是可选的。</p><p><img src="/images/2024/08/018.jpg" alt></p><p><img src="/images/2024/08/019.jpg" alt></p><h3 id="IP-checksum"><a href="#IP-checksum" class="headerlink" title="IP checksum"></a>IP checksum</h3><p>IP校验和只计算检验IP数据报的首部，但不包括IP数据报中的数据部分。</p><p><img src="/images/2024/08/020.jpg" alt></p><p><img src="/images/2024/08/021.jpg" alt></p><h2 id="checksum-offload"><a href="#checksum-offload" class="headerlink" title="checksum offload"></a>checksum offload</h2><p>传统上，校验和的计算（发送数据包）和验证（接收数据包）是通过CPU完成的。这对CPU的影响很大，因为校验和需要每个字节的数据都参与计算。对于一个100G带宽的网络，需要CPU最多每秒计算大约12G的数据。</p><p>为了减轻这部分的影响，现在的网卡，都支持校验和的计算和验证。系统内核在封装网络数据包的时候，可以跳过校验和。网卡收到网络数据包之后，根据网络协议的规则，进行计算，再将校验和填入相应的位置。</p><p>因为Checksum offload的存在，在用tcpdump之类的抓包分析工具时，有时会发现抓到的包提示校验和错误（checksum incorrect）。tcpdump抓到的网络包就是系统内核发给网卡的网络包，如果校验和放到网卡去计算，那么tcpdump抓到包的时刻，校验和还没有被计算出来，自然看到的是错误的值。</p><h2 id="virtio-net"><a href="#virtio-net" class="headerlink" title="virtio-net"></a>virtio-net</h2><blockquote><p>VIRTIO_NET_F_CSUM (0)<br>Device handles packets with partial checksum. This “checksum offload” is a common feature on modern network cards.</p></blockquote><blockquote><p>VIRTIO_NET_F_HOST_TSO4<br>Requires VIRTIO_NET_F_CSUM.</p></blockquote><p>由上述描述可知<a href="/2023/04/23/Notes-about-TSO、GSO、LRO、GRO/#TSO">TSO</a>需要Checksum offload的支持。因为在enable TSO时，TCP/IP协议栈并不知道最终的网络数据包是什么样，自然也没办法完成校验和计算。</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/44635205" target="_blank" rel="noopener">常见网络加速技术浅谈（一）</a></li><li><a href="https://zhuanlan.zhihu.com/p/106400339" target="_blank" rel="noopener">Wireshark 提示和技巧 | Checksum Offload</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc9293" target="_blank" rel="noopener">RFC 9293: Transmission Control Protocol</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc768" target="_blank" rel="noopener">RFC 768: User Datagram Protocol</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc791" target="_blank" rel="noopener">RFC 791: INTERNET PROTOCOL</a></li><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">Virtual I/O Device (VIRTIO) Version 1.2</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下network checksum offload技术的相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Network Segmentation vs Fragmentation</title>
    <link href="http://liujunming.github.io/2024/08/11/Network-Segmentation-vs-Fragmentation/"/>
    <id>http://liujunming.github.io/2024/08/11/Network-Segmentation-vs-Fragmentation/</id>
    <published>2024-08-11T09:28:22.000Z</published>
    <updated>2024-08-11T12:03:43.222Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下计算机网络中的分段(Segmentation)与分片(Fragmentation)操作。<a id="more"></a>本文主要内容转载自<a href="https://cloud.tencent.com/developer/article/1828823" target="_blank" rel="noopener">动图图解！既然IP层会分片，为什么TCP层也还要分段？</a>。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>分段特指发生在使用TCP协议的传输层中的数据切分行为<br><img src="/images/2024/08/011.gif" alt></p><p>分片特指发生在使用IP协议的网络IP层中的数据切分行为<br><img src="/images/2024/08/012.gif" alt></p><p>TCP协议在将用户数据传给IP层之前，会先将大段的数据根据MSS（Maximum Segment Size）分成多个小段，这个过程是Segmentation，分出来的数据是Segments。IP协议因为MTU（Maximum Transmission Unit）的限制，会将上层传过来的并且超过MTU的数据，分成多个分片，这个过程是Fragmentation，分出来的数据是Fragments。这两个过程都是大块的数据分成多个小块数据，区别就是一个在TCP（L4），一个在IP（L3）完成。</p><h2 id="MSS与MTU的区别"><a href="#MSS与MTU的区别" class="headerlink" title="MSS与MTU的区别"></a>MSS与MTU的区别</h2><p>TCP 提交给 IP 层最大分段大小，不包含 TCP Header 和  TCP Option，只包含 TCP Payload ，MSS 是 TCP 用来限制应用层最大的发送字节数。<br>假设 MTU= 1500 byte，那么 MSS = 1500- 20(IP Header) -20 (TCP Header) = 1460 byte，如果应用层有 2000 byte 发送，那么需要两个切片才可以完成发送，第一个 TCP 切片 = 1460，第二个 TCP 切片 = 540。</p><p>MTU是由<strong>数据链路层</strong>提供，为了告诉上层IP层，自己的传输能力是多大。IP层就会根据它进行数据包切分。一般 MTU=1500 Byte。<br>假设IP层有 &lt;= 1500 byte 需要发送，只需要一个 IP 包就可以完成发送任务；假设 IP 层有 &gt; 1500 byte 数据需要发送，需要分片才能完成发送，分片后的 IP Header ID 相同，同时为了分片后能在接收端把切片组装起来，还需要在分片后的IP包里加上各种信息。比如这个分片在原来的IP包里的偏移offset。</p><p><img src="/images/2024/08/015.jpg" alt></p><p>在一台机器的应用层到这台机器的网卡，<strong>这条链路上</strong>，基本上可以保证，MSS &lt; MTU。</p><p><img src="/images/2024/08/013.png" alt></p><h2 id="为什么MTU一般是1500"><a href="#为什么MTU一般是1500" class="headerlink" title="为什么MTU一般是1500"></a>为什么MTU一般是1500</h2><p>这其实是由传输效率决定的。虽然我们平时用的网络感觉挺稳定的，但其实这是因为TCP在背地里做了各种重传等保证了传输的可靠，其实背地里线路是动不动就丢包的，而越大的包，发生丢包的概率就越大。</p><p>那是不是包越小就越好？也不是</p><p>如果选择一个比较小的长度，假设选择MTU为300Byte，TCP payload = 300 - IP Header - TCP Header = 300 - 20 - 20 = 260 byte。那有效传输效率= 260 / 300 = 86%</p><p>而如果以太网MTU长度为1500，那有效传输效率= 1460 / 1500 = 96% ，显然比 86% 高多了。</p><p>所以，包越小越不容易丢包，包越大，传输效率又越高，因此权衡之下，选了1500。</p><h2 id="为什么IP层会分片，TCP还要分段"><a href="#为什么IP层会分片，TCP还要分段" class="headerlink" title="为什么IP层会分片，TCP还要分段"></a>为什么IP层会分片，TCP还要分段</h2><p>由于本身IP层就会做分片这件事情。就算TCP不分段，到了IP层，数据包也会被分片，数据也能正常传输。</p><p>既然网络层就会分片了，那么TCP为什么还要分段？是不是有些多此一举？</p><p>假设有一份数据，较大，且在TCP层不分段，如果这份数据在发送的过程中出现丢包现象，TCP会发生重传，那么重传的就是这一大份数据（虽然IP层会把数据切分为MTU长度的N多个小包，但是TCP重传的单位却是那一大份数据）。</p><p>如果TCP把这份数据，分段为N个小于等于MSS长度的数据包，到了IP层后加上IP头和TCP头，还是小于MTU，那么IP层也不会再进行分片。此时在传输路上发生了丢包，那么TCP重传的时候也只是重传那一小部分的MSS段。效率会比TCP不分段时更高。</p><p>类似的，传输层除了TCP外，还有UDP协议，但UDP本身不会分段，所以当数据量较大时，只能交给IP层去分片，然后传到底层进行发送。</p><p>正常情况下，在一台机器的传输层到网络层<strong>这条链路</strong>上，如果传输层对数据做了分段，那么IP层就不会再分片。如果传输层没分段，那么IP层就可能会进行分片。</p><p><strong>数据在TCP分段，就是为了在IP层不需要分片，同时发生重传的时候只重传分段后的小份数据</strong>。</p><h2 id="TCP分段了，IP层就一定不会分片了吗"><a href="#TCP分段了，IP层就一定不会分片了吗" class="headerlink" title="TCP分段了，IP层就一定不会分片了吗"></a>TCP分段了，IP层就一定不会分片了吗</h2><p>在发送端，TCP分段后，IP层就不会再分片了。</p><p>但是整个传输链路中，可能还会有其他网络层设备，而这些设备的MTU可能小于发送端的MTU。此时虽然数据包在发送端已经分段过了，但是在IP层还会再分片一次。</p><p>如果链路上还有设备<strong>有更小的MTU</strong>，那么还会再分片，最后所有的分片都会在<strong>接收端</strong>进行组装。</p><p><img src="/images/2024/08/014.gif" alt></p><p>因此，就算TCP分段过后，在链路上的其他节点的IP层也是有可能再分片的，而且哪怕数据被第一次IP分片过了，也是有可能被其他机器的IP层进行二次、三次、四次….分片的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>(TCP)分段和(IP)分片各自发生在不同的协议层(分段-TCP传输层，分片-IP层)</li><li>TCP分段的原因是TCP报文段大小受MSS限制，IP分片则是因为IP数据报大小受MTU限制</li><li>在发送方，数据在TCP分段，在IP层就不需要分片，同时发生重传的时候只重传分段后的小份数据</li><li>虽然分段和分片不会在发送方同时发生，但却可能在同一次通信过程中分别在发送主机(分段)和转发设备(分片)中发生</li><li>IP分片是<strong>不得已</strong>的行为，尽量不在IP层分片，尤其是链路上中间设备的IP分片</li><li>UDP不会分段，就由IP来分片</li></ul><hr><p>参考资料:</p><ol><li><a href="https://cloud.tencent.com/developer/article/1828823" target="_blank" rel="noopener">动图图解！既然IP层会分片，为什么TCP层也还要分段？</a></li><li><a href="https://cloud.tencent.com/developer/article/1173790" target="_blank" rel="noopener">TCP分段与IP分片的区别与联系</a></li><li><a href="https://zhuanlan.zhihu.com/p/44635205" target="_blank" rel="noopener">常见网络加速技术浅谈（一）</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下计算机网络中的分段(Segmentation)与分片(Fragmentation)操作。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
