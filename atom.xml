<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2026-01-03T02:23:26.646Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes about SSD basic knowledge</title>
    <link href="http://liujunming.github.io/2026/01/03/Notes-about-SSD-components/"/>
    <id>http://liujunming.github.io/2026/01/03/Notes-about-SSD-components/</id>
    <published>2026-01-03T01:42:59.000Z</published>
    <updated>2026-01-03T02:23:26.646Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下SSD basic knowledge。<a id="more"></a></p><h2 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h2><p><img src="/images/2026/01/001.png" alt></p><ul><li>Flash Controller</li><li>Channel</li><li>Chip</li><li>Die</li><li>Plane(Bank)</li><li>Block</li><li>Page</li></ul><h2 id="Basic-Operations"><a href="#Basic-Operations" class="headerlink" title="Basic Operations"></a>Basic Operations</h2><p>There are three low-level operations that a flash chip supports. The <strong>read</strong> command is used to read a page from the flash; <strong>erase</strong> and <strong>program</strong> are used in tandem to write.</p><ul><li><p><strong>Read (a page)</strong>: A client of the flash chip can read any page, simply by specifying the read command and appropriate page number to the device.</p></li><li><p><strong>Erase (a block)</strong>: Before writing to a <code>page</code> within a flash, the nature of the device requires that you first <strong>erase</strong> the entire block the page lies within. Erase, importantly, destroys the contents of the block(by setting each bit to the value 1); therefore, you must be sure that any data you care about in the block has been copied elsewhere (to memory, or perhaps to another flash block) before executing the erase. The erase command is quite expensive, taking a few milliseconds to complete. Once finished, the entire block is reset and each page is ready to be programmed.</p></li><li><p><strong>Program (a page)</strong>: Once a block has been erased, the program command can be used to write the desired contents of a page to the flash.</p></li></ul><hr><p>参考资料:</p><ol><li><a href="https://pages.cs.wisc.edu/~remzi/OSTEP/file-ssd.pdf" target="_blank" rel="noopener">Flash-based SSDs</a></li><li><a href="https://www.usenix.org/system/files/fast24_slides-jun.pdf" target="_blank" rel="noopener">We Ain’t Afraid of No File Fragmentation: Causes and Prevention of Its Performance Impact on Modern Flash SSDs</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下SSD basic knowledge。
    
    </summary>
    
      <category term="存储" scheme="http://liujunming.github.io/categories/%E5%AD%98%E5%82%A8/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="存储" scheme="http://liujunming.github.io/tags/%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVIDIA TMA(Tensor Memory Access)</title>
    <link href="http://liujunming.github.io/2025/12/07/Notes-about-NVIDIA-TMA-Tensor-Memory-Access/"/>
    <id>http://liujunming.github.io/2025/12/07/Notes-about-NVIDIA-TMA-Tensor-Memory-Access/</id>
    <published>2025-12-07T01:16:22.000Z</published>
    <updated>2025-12-07T08:21:45.277Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下NVIDIA TMA(Tensor Memory Access)技术的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/12/003.webp" alt></p><p>在GPU中，数据存储在Global Memory(一般为HBM)，计算单元(CUDA core/ Tensor core)位于SM中，数据需要搬运到SM内的Shared Memory(SMEM)中用于计算，计算后再搬回Global Memory中。</p><p>SM中的线程除了负责发起计算外，还要先负责搬运数据。线程需要承担地址计算、越界处理、同步管理等一系列任务。据调查，开发者90%的时间花费在编写数据访问的代码上，10%以上的性能损耗源于内存访问开销。</p><p>为了解决这一问题，英伟达在Hopper架构中引入了 Tensor Memory Accelerator(TMA)，将数据访问的复杂性从软件层(线程)剥离，交由专用硬件(TMA)处理，实现数据搬运与计算解耦。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 A100（Ampere 架构）及更早的 GPU 中，线程块（thread block）需要通过 CUDA 线程显式地从全局内存（global memory）将数据加载到共享内存（shared memory），再供给 Tensor Core 使用。这个过程存在以下问题：</p><ul><li>地址计算开销大：每个线程需手动计算多维张量（如矩阵分块）的内存地址</li><li>同步复杂：需协调多个 warp 协同完成数据搬运，并使用 <code>__syncthreads()</code> 同步</li><li>带宽利用率受限：非最优的访存模式可能导致内存带宽未被充分利用</li></ul><h2 id="核心原理"><a href="#核心原理" class="headerlink" title="核心原理"></a>核心原理</h2><p>TMA 是一个<strong>独立于 CUDA 核心的硬件单元</strong>，可由软件通过<strong>异步描述符（descriptor）+ 坐标（coordinate）</strong>的方式发起高维张量的内存拷贝操作。</p><h3 id="Tensor-Descriptor"><a href="#Tensor-Descriptor" class="headerlink" title="Tensor Descriptor"></a>Tensor Descriptor</h3><p>开发者首先定义一个张量描述符，包含以下信息：</p><ul><li>张量的形状（shape）</li><li>内存布局（layout，如行主序、列主序、分块布局等）</li><li>数据类型（如 FP16、BF16、INT8）</li><li>起始地址（base address）</li></ul><p>该描述符一次性配置后，可被多次复用。</p><h3 id="Copy-by-Coordinate"><a href="#Copy-by-Coordinate" class="headerlink" title="Copy by Coordinate"></a>Copy by Coordinate</h3><p>TMA 不使用传统的一维地址，而是通过逻辑坐标（如 <code>(block_row, block_col)</code>）指定要拷贝的子张量（tile）。例如：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 伪代码示意</span></span><br><span class="line">tma_load(desc, shared_mem_ptr, coord = &#123;tile_m, tile_n&#125;);</span><br></pre></td></tr></table></figure></p><p>硬件根据描述符自动：</p><ul><li>将逻辑坐标映射为物理地址</li><li>生成最优的内存访问模式（合并访问、对齐等）</li><li>执行从全局内存到共享内存（或反之）的 DMA 式传输</li></ul><h3 id="异步-amp-零线程开销"><a href="#异步-amp-零线程开销" class="headerlink" title="异步 &amp; 零线程开销"></a>异步 &amp; 零线程开销</h3><ul><li>TMA 操作由专用硬件引擎执行，不占用SM中的 CUDA 核心资源</li><li>CUDA 线程只需发出TMA指令(通过<code>cp.async.bulk</code>PTX指令触发传输)，即可继续执行其他计算，实现计算与数据搬运重叠</li><li>无需线程参与地址计算或数据搬移，显著降低软件开销</li></ul><h3 id="支持高维与非规则布局"><a href="#支持高维与非规则布局" class="headerlink" title="支持高维与非规则布局"></a>支持高维与非规则布局</h3><p>TMA 原生支持最多 5 维张量，并能处理：</p><ul><li>分块循环布局（swizzled/tiled layout）</li><li>填充（padding）</li><li>跨步（strides）</li></ul><p>这使其特别适合 Transformer、卷积等复杂神经网络中的内存访问模式。</p><h2 id="典型应用场景"><a href="#典型应用场景" class="headerlink" title="典型应用场景"></a>典型应用场景</h2><ul><li>GEMM（通用矩阵乘）：将 A、B 矩阵的分块自动加载到 shared memory，供 Tensor Core 使用</li><li>Attention 机制：高效搬运 Q、K、V 的分块数据</li><li>大模型推理/训练：减少数据搬运瓶颈，提升计算吞吐</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>TMA核心目标是<strong>优化张量运算中的数据搬运效率</strong>，特别是为 Tensor Core 提供高效、低开销的数据加载与存储能力。</p><p>TMA的技术本质是：将“基于地址的、线程驱动的”内存访问，转变为“基于张量语义的、硬件加速的”异步数据传输。</p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s/0gkeKmRYU-fX4gWtScAjvQ" target="_blank" rel="noopener">NVIDIA TMA 全面解读</a></li><li><a href="https://zhuanlan.zhihu.com/p/709750258" target="_blank" rel="noopener">[Hopper 架构特性学习笔记 Part2] Tensor Memory Access（TMA）</a></li><li><a href="https://www.qianwen.com/share?shareId=2762294c-c203-4efe-b262-5e29dce5a0bb" target="_blank" rel="noopener">千问对话</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下NVIDIA TMA(Tensor Memory Access)技术的相关notes。
    
    </summary>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/categories/AI-Infra/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="CUDA" scheme="http://liujunming.github.io/tags/CUDA/"/>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/tags/AI-Infra/"/>
    
  </entry>
  
  <entry>
    <title>Notes about linux MTD(Memory Technology Devices)</title>
    <link href="http://liujunming.github.io/2025/12/07/Notes-about-linux-MTD-Memory-Technology-Devices/"/>
    <id>http://liujunming.github.io/2025/12/07/Notes-about-linux-MTD-Memory-Technology-Devices/</id>
    <published>2025-12-07T00:21:49.000Z</published>
    <updated>2025-12-07T00:57:06.916Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下linux MTD(Memory Technology Devices)的相关notes。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In embedded Linux systems, raw flash memory,such as NOR and NAND flash,is accessed using the Memory Technology Device (MTD) subsystem. MTD provides low-level interfaces for reading, erasing, and writing to flash blocks.</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/images/2025/12/001.jpg" alt></p><h2 id="MTD-Layers"><a href="#MTD-Layers" class="headerlink" title="MTD Layers"></a>MTD Layers</h2><p>The MTD subsystem is organized into three main layers:</p><ul><li>Chip Drivers: Interfaces for specific NOR/NAND flash chips.</li><li>MTD Core: Common APIs and internal logic.</li><li>User-Space Interfaces: Exposes flash as character (/dev/mtdX) or block (/dev/mtdblockX) devices.</li></ul><p><img src="/images/2025/12/002.webp" alt></p><p>MTD character drivers allow us to access the flash memory as an array of bytes so that we can read and write (program) the flash.</p><p>The MTD block driver’s purpose is to present flash memory as a block device, which we can use to format and mount a filesystem.</p><hr><p>参考资料:</p><ol><li><a href="https://medium.com/@akashsainisaini37/a-beginners-guide-to-mtd-and-flash-management-in-linux-7e6c8dd15493" target="_blank" rel="noopener">A Beginner’s Guide to MTD and Flash Management in Linux</a></li><li><a href="http://kcchao.wikidot.com/mtd" target="_blank" rel="noopener">http://kcchao.wikidot.com/mtd</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下linux MTD(Memory Technology Devices)的相关notes。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes of Basic Concepts in GPU Computing</title>
    <link href="http://liujunming.github.io/2025/11/30/Notes-of-Basic-Concepts-in-GPU-Computing/"/>
    <id>http://liujunming.github.io/2025/11/30/Notes-of-Basic-Concepts-in-GPU-Computing/</id>
    <published>2025-11-30T07:13:25.000Z</published>
    <updated>2025-11-30T14:01:27.568Z</updated>
    
    <content type="html"><![CDATA[<p>本文将以NVIDIA GPU为例，结合CUDA，记录下GPU中的基本概念。<a id="more"></a>本文默认以Fermi architecture为例，同时内容会逐步完善！</p><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><ul><li>SP(Streaming Processor)</li><li>SM(Streaming MultiProcessor)</li><li>Warp(线程束)</li></ul><h2 id="CPU-vs-GPU"><a href="#CPU-vs-GPU" class="headerlink" title="CPU vs GPU"></a>CPU vs GPU</h2><p><img src="/images/2025/11/014.png" alt></p><p>CPU has complex core structure and pack several cores on a single chip. GPU cores are very simple in comparison, they also share data and control between each other. This allows to pack more cores on a single chip, thus achieving very high compute density.</p><h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p><img src="/images/2025/11/012.png" alt></p><h3 id="Hardware’s-View"><a href="#Hardware’s-View" class="headerlink" title="Hardware’s View"></a>Hardware’s View</h3><ul><li><p>SP(Streaming Processor):是GPU最基本的处理单元，在fermi架构开始被叫做CUDA core。</p></li><li><p>SM(Streaming MultiProcessor): 一个SM由多个CUDA core组成，每个SM根据GPU架构不同有不同数量的CUDA core，Pascal架构中一个SM有128个CUDA core。</p></li></ul><h3 id="Software’s-View"><a href="#Software’s-View" class="headerlink" title="Software’s View"></a>Software’s View</h3><p><img src="/images/2025/11/013.png" alt></p><ul><li>thread: 一个CUDA并行程序会以许多个thread来执行</li><li>block: 数个thread会组成一个block</li><li>grid: 多个block则会再构成grid</li></ul><h2 id="GPU-Architecture"><a href="#GPU-Architecture" class="headerlink" title="GPU Architecture"></a>GPU Architecture</h2><p>The following graph shows the Fermi architecture. This GPU has 16 streaming multiprocessor (SM), which contains 32 cuda cores each. Every cuda core is an execute unit for integer and float numbers.</p><p><img src="/images/2025/11/015.png" alt></p><p>As shown in the following chart, every SM has 32 cuda cores, 2 Warp Scheduler and dispatch unit, a bunch of registers, 64 KB configurable shared memory and L1 cache. Cuda cores is the execute unit which has one float and one integer compute processor. The SM schedules threads in group of 32 threads called warps.  The Warp Schedulers means two warps can be issued at the same time.</p><p><img src="/images/2025/11/016.png" alt></p><h2 id="Warp"><a href="#Warp" class="headerlink" title="Warp"></a>Warp</h2><p>Each SM features two <em>warp schedulers</em> and two <em>instruction dispatch units</em>. When a thread block is assigned to an SM, all threads in a thread block are divided into warps. The two warp schedulers select two warps and issue one instruction from each warp to a group of 16 CUDA cores, 16 load/store units, or 4 special function units. (虽然硬件上1个warp只有16个CUDA cores，逻辑上1个warp有32个thread，可以通过硬件花费两个周期来处理完32个thread，也就是16个cuda cores执行了两个周期来运行这32个thread)<br><img src="/images/2025/11/019.png" alt></p><p>Warps are the basic unit of execution in an SM. When you launch a grid of thread blocks, the thread blocks in the grid are distributed among SMs. Once a thread block is scheduled to an SM, threads in the thread block are further partitioned into warps. A warp consists of 32 consecutive threads and all threads in a warp are executed in Single Instruction Multiple Thread (SIMT) fashion; that is, all threads execute the same instruction, and each thread carries out that operation on its own private data. </p><p><img src="/images/2025/11/020.png" alt></p><p>The number of warps for a thread block can be determined as follows:<br><img src="/images/2025/11/021.png" alt></p><ul><li><p>SM 是 Warp 的物理载体<br>SM 作为 GPU 的核心计算单元，负责承载并执行 Warp。每个 SM 可同时管理多个 Warp，通过硬件调度器动态分配资源。</p></li><li><p>Warp 是 SM 的调度单位<br>Warp 由 32 个同步线程组成，是 SM 内指令发射的最小单元。SM 的 Warp 调度器 会从就绪的 Warp 中选择指令，分发至 CUDA Core、Tensor Core 等执行单元。</p></li><li><p>资源竞争与负载平衡<br>SM 的寄存器、共享内存等资源限制了可驻留的 Warp 数量。若 Warp 因内存访问延迟阻塞（约 400-800 周期），调度器会立即切换至其他就绪 Warp，避免计算单元空闲。</p></li><li><p>指令分发</p><ul><li>SM 的 Warp 调度器 从就绪的 Warp 中选择指令，分发至 Core 执行</li><li>例如：一个 Warp 的 32 个线程同时执行加法指令，调度器将其分配到 32 个 CUDA Core 并行处理</li></ul></li><li><p>资源映射</p><ul><li>Hopper 架构：每个 SM 子分区含 32 个 FP32 Core，可直接匹配一个 Warp 的 32 线程，单周期完成指令</li><li>旧架构（如 Fermi）：16 个 FP32 Core 需两周期处理一个 Warp</li></ul></li></ul><h2 id="Memory-Hierarchy"><a href="#Memory-Hierarchy" class="headerlink" title="Memory Hierarchy"></a>Memory Hierarchy</h2><p><img src="/images/2025/11/018.webp" alt></p><p>Shared memory is allocated per block. The shared memory per block is limited by the shared memory per SM.</p><p><img src="/images/2025/11/017.webp" alt></p><p>The fastest memory is registers just as in CPU. L1 cache and shared memory is second, which is also pretty limited in size. The SM above can have 48KB shared memory and 16KB L1 cache, or 16KB shared memory and 48KB L1 cache. L1 cache caches local and global memory, or only local memory varying among different GPU model. L2 cache caches local and global memory. Global memory acts like the ram in CPU computation, which is much slower than L1 and L2 cache.</p><hr><p>参考资料:</p><ol><li><a href="https://www.cs.utexas.edu/~rossbach/cs380p/papers/cuda-programming.pdf" target="_blank" rel="noopener">PROFESSIONAL CUDA® C Programming</a></li><li><a href="https://www.qianwen.com/share?shareId=3a7b3810-b745-4be5-99c9-5ba131ccffac" target="_blank" rel="noopener">千问对话</a></li><li><a href="https://zhuanlan.zhihu.com/p/123170285" target="_blank" rel="noopener">理解CUDA中的thread,block,grid和warp</a></li><li><a href="https://medium.com/@smallfishbigsea/basic-concepts-in-gpu-computing-3388710e9239" target="_blank" rel="noopener">Basic Concepts in GPU Computing</a></li><li><a href="https://enccs.github.io/gpu-programming/2-gpu-ecosystem/" target="_blank" rel="noopener">The GPU hardware and software ecosystem</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将以NVIDIA GPU为例，结合CUDA，记录下GPU中的基本概念。
    
    </summary>
    
      <category term="GPU" scheme="http://liujunming.github.io/categories/GPU/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="CUDA" scheme="http://liujunming.github.io/tags/CUDA/"/>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/tags/AI-Infra/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel UMIP</title>
    <link href="http://liujunming.github.io/2025/11/29/Notes-about-Intel-UMIP/"/>
    <id>http://liujunming.github.io/2025/11/29/Notes-about-Intel-UMIP/</id>
    <published>2025-11-29T08:56:16.000Z</published>
    <updated>2025-11-29T10:20:31.916Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel UMIP(User-Mode Instruction Prevention)的相关notes。<a id="more"></a></p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p><img src="/images/2025/11/010.png" alt></p><p><code>SIDT</code>指令用于读取IDTR的内容到内存，常用于虚拟机检测和安全分析，且可在用户态执行。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/11/011.png" alt></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>User-Mode Instruction Prevention (UMIP) is a <strong>security feature</strong> present in new Intel Processors. If enabled, it prevents the execution of certain instructions if the Current Privilege Level (CPL) is greater than 0. If these instructions were executed while in CPL &gt; 0, user space applications could have access to system-wide settings such as the global and local descriptor tables, the task register and the interrupt descriptor table.</p><p>These are the instructions covered by UMIP:</p><ul><li>SGDT - Store Global Descriptor Table</li><li>SIDT - Store Interrupt Descriptor Table</li><li>SLDT - Store Local Descriptor Table</li><li>SMSW - Store Machine Status Word</li><li>STR - Store Task Register</li></ul><p>这五条指令都是 “S开头 + 描述符/状态寄存器名”，作用是 “Store（存储）系统寄存器到内存/寄存器”，且全部可在用户态执行，是窥探系统底层状态的“后门窗口”。</p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>UMIP则可以防止用户态执行上述命令，堵住了用户态窥探系统底层状态的“后门窗口”。</p><hr><p>参考资料:</p><ol><li>Intel SDM</li><li><a href="https://lwn.net/Articles/705877/" target="_blank" rel="noopener">x86: enable User-Mode Instruction Prevention</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel UMIP(User-Mode Instruction Prevention)的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ACPI ERST</title>
    <link href="http://liujunming.github.io/2025/11/23/Notes-about-ACPI-ERST/"/>
    <id>http://liujunming.github.io/2025/11/23/Notes-about-ACPI-ERST/</id>
    <published>2025-11-23T12:09:40.000Z</published>
    <updated>2025-12-06T00:35:01.544Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下ACPI ERST(Error Record Serialization Table)的相关notes。<a id="more"></a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Linux uses the persistent storage filesystem, <code>pstore</code>, to record information (eg. dmesg tail) upon panics and shutdowns. Pstore is independent of, and runs before, kdump. In certain scenarios (ie. hosts/guests with root filesystems on NFS/iSCSI where networking software and/or hardware fails, and thus kdump fails), pstore may contain information available for post-mortem debugging(崩溃后调试).</p><p>Save the hardware error log into flash via ERST before go panic, the hardware error log can be gotten from the flash after system boot successful again, which is very useful in production.</p><h2 id="Linux-pstore"><a href="#Linux-pstore" class="headerlink" title="Linux pstore"></a>Linux pstore</h2><p>PSTORE provides a file system interface that allows you to read and delete the contents of ramoops through the file system in the user layer.</p><p>Supports saving dmesg, console, ftrace and pmsg (frontend) to the backend devices like ram, blk, mtd (memory technology devices). PSTORE core would provide relevant interfaces to communicate between frontend and backend.</p><p><img src="/images/2025/11/022.webp" alt></p><p>Linux pstore是一种持久化存储机制，专为系统崩溃或重启时自动保存内核日志而设计，使开发者能在系统恢复后分析崩溃原因，特别适用于无法实时监控的远程设备或小概率崩溃问题。</p><p>ACPI ERST是pstore filesystem的一种storage backend。</p><h2 id="ACPI-ERST-DEVICE-PCI-Interface"><a href="#ACPI-ERST-DEVICE-PCI-Interface" class="headerlink" title="ACPI ERST DEVICE PCI Interface"></a>ACPI ERST DEVICE PCI Interface</h2><p>The ERST device is a PCI device with two BARs, one for accessing the programming registers, and the other for accessing the record exchange buffer.</p><p>BAR0 contains the programming interface consisting of ACTION and VALUE 64-bit registers. All ERST actions/operations/side effects happen on the write to the ACTION, by design. Any data needed by the action must be placed into VALUE prior to writing ACTION. Reading the VALUE simply returns the register contents, which can be updated by a previous ACTION.</p><p>BAR1 contains the 8KiB record exchange buffer, which is the implemented maximum record size.</p><h2 id="serialize-and-deserialize-MCE-error-record"><a href="#serialize-and-deserialize-MCE-error-record" class="headerlink" title="serialize and deserialize MCE error record"></a>serialize and deserialize MCE error record</h2><p>On X86 platform, the kernel has supported to serialize and deserialize MCE error record by commit <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=482908b49ebf" target="_blank" rel="noopener">482908b49ebf (“ACPI, APEI, Use ERST for persistent storage of MCE”)</a>. The process involves two steps:</p><ul><li>MCE Producer: When a hardware error is detected, MCE raised and its handler writes MCE error record into flash via ERST before panic</li><li>MCE Consumor: After system reboot, /sbin/mcelog run, it reads /dev/mcelog to check flash for error record of previous boot via ERST</li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.qemu.org/docs/master/specs/acpi_erst.html" target="_blank" rel="noopener">ACPI ERST DEVICE</a></li><li><a href="https://patchew.org/linux/20230925074426.97856-1-xueshuai@linux.alibaba.com/" target="_blank" rel="noopener">Use ERST for persistent storage of MCE and APEI errors</a></li><li><a href="https://lore.kernel.org/lkml/1272350481-27951-1-git-send-email-ying.huang@intel.com/#t" target="_blank" rel="noopener">ACPI, APEI support</a></li><li>千问</li><li><a href="https://medium.com/@aravindchetla/do-you-know-how-pstore-works-f9a661b674c7" target="_blank" rel="noopener">Do you know how ‘PSTORE’ works</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下ACPI ERST(Error Record Serialization Table)的相关notes。
    
    </summary>
    
      <category term="ACPI" scheme="http://liujunming.github.io/categories/ACPI/"/>
    
    
      <category term="ACPI" scheme="http://liujunming.github.io/tags/ACPI/"/>
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel Enhance SpeedStep Technology</title>
    <link href="http://liujunming.github.io/2025/11/15/Notes-about-Intel-Enhance-SpeedStep-Technology/"/>
    <id>http://liujunming.github.io/2025/11/15/Notes-about-Intel-Enhance-SpeedStep-Technology/</id>
    <published>2025-11-15T03:04:35.000Z</published>
    <updated>2025-11-15T03:35:27.305Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel Enhance SpeedStep Technology的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Enhance Intel SpeedStep Technology enables operating system to control and select <a href="/2020/02/20/Introduction-to-power-management/#Processor-Performance-state-P-state">P-state</a>, it allows the processor to lower the speed when it is in idle or low workload, this will allow the system to save power consumption.</p><h2 id="Key-features"><a href="#Key-features" class="headerlink" title="Key features"></a>Key features</h2><ul><li>Multiple frequency and voltage points for optimal performance and power efficiency. These operating points are known as P-states.</li><li><p>Frequency selection is software-controlled by <strong>writing to processor model-specific registers (MSRs)</strong>. </p><p>The voltage is optimized based on the selected frequency and the number of active processor IA cores.</p><ul><li>All active processor IA cores share the same frequency and voltage. In a multicore processor, the highest frequency P-state requested among all active IA cores is selected.</li><li>Software-requested transitions are accepted at any time. If a previous transition is in progress, the new transition is deferred until the previous transition is completed.</li></ul></li><li>The processor controls voltage ramp rates internally to ensure glitch-free transitions(处理器内部控制电压升降速率，确保无毛刺平滑转换).</li><li>Because there is low transition latency between P-states, a significant number of transitions per-second are possible.</li></ul><h2 id="Spec"><a href="#Spec" class="headerlink" title="Spec"></a>Spec</h2><p><img src="/images/2025/11/008.png" alt></p><p><img src="/images/2025/11/009.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/support/articles/000007073/processors.html" target="_blank" rel="noopener">Overview of Enhanced Intel SpeedStep® Technology for Intel® Processors</a></li><li><a href="https://community.intel.com/t5/Mobile-and-Desktop-Processors/Question-about-SpeedStep-and-Turbo-Boost/td-p/202521" target="_blank" rel="noopener">Question about SpeedStep and Turbo Boost</a></li><li>Intel SDM vol3</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel Enhance SpeedStep Technology的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="ACPI" scheme="http://liujunming.github.io/tags/ACPI/"/>
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel Advanced Performance Extensions(APX)</title>
    <link href="http://liujunming.github.io/2025/11/15/Notes-about-Intel-Advanced-Performance-Extensions-APX/"/>
    <id>http://liujunming.github.io/2025/11/15/Notes-about-Intel-Advanced-Performance-Extensions-APX/</id>
    <published>2025-11-15T01:27:49.000Z</published>
    <updated>2025-11-15T02:38:53.771Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel Advanced Performance Extensions(APX)的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Intel Advanced Performance Extensions (Intel APX) <strong>expands the Intel 64 instruction set architecture with access to more registers and adds various new features that improve general-purpose performance</strong>. The extensions are designed to provide efficient performance gains across a variety of workloads without significantly increasing silicon area or power consumption of the core.</p><p>Intel APX doubles the number of general-purpose registers (GPRs) from 16 to 32. This allows the compiler to keep more values in registers. As a result, code compiled with Intel APX contains 10% fewer loads and more than 20% fewer stores than the same code compiled for an Intel® 64 baseline. Register accesses are not only faster, but they also consume significantly less dynamic power than complex load and store operations.</p><h2 id="Changes"><a href="#Changes" class="headerlink" title="Changes"></a>Changes</h2><ul><li>Doubles the number of general purpose registers from 16 to 32</li><li>New three operand instructions (e.g. adding ability to subtract register1 from register2, and place the result in register3)</li><li>New instructions to PUSH/POP two general purpose registers at once</li></ul><h2 id="XSAVE-enabled"><a href="#XSAVE-enabled" class="headerlink" title="XSAVE-enabled"></a>XSAVE-enabled</h2><h3 id="New-Register-State"><a href="#New-Register-State" class="headerlink" title="New Register State"></a>New Register State</h3><p>APX register state is managed by the XSAVE instruction set, with its state component number assigned to 19, following XTILEDATA.</p><h3 id="XSAVE-Buffer-Offset"><a href="#XSAVE-Buffer-Offset" class="headerlink" title="XSAVE Buffer Offset"></a>XSAVE Buffer Offset</h3><ul><li>In the compacted format used (for the in-kernel buffer), the APX state will appear at a later position in the buffer.</li><li>In the non-compacted format (used for signal, ptrace, and KVM ABIs), APX is assigned a lower offset, occupying the space previously reserved for the deprecated MPX state.</li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html" target="_blank" rel="noopener">https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html</a></li><li><a href="https://mp.weixin.qq.com/s/yZ6iVIDJ7718XGi9sBGdeA" target="_blank" rel="noopener">Introducing Intel® Advanced Performance Extensions (Intel® APX)</a></li><li><a href="https://lore.kernel.org/kvm/20251110180131.28264-1-chang.seok.bae@intel.com/" target="_blank" rel="noopener">[PATCH RFC v1 00/20] KVM: x86: Support APX feature for guests</a></li><li><a href="https://lore.kernel.org/lkml/20250227184502.10288-1-chang.seok.bae@intel.com/T/#m44e1ec0c544417adddac2b87451a8c3e9d1bf322" target="_blank" rel="noopener">[PATCH RFC v1 00/11] x86: Support Intel Advanced Performance Extensions</a></li><li><a href="https://thechipletter.substack.com/p/intel-apx" target="_blank" rel="noopener">Intel APX</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel Advanced Performance Extensions(APX)的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about virtio-9p</title>
    <link href="http://liujunming.github.io/2025/11/02/Notes-about-virtio-9p/"/>
    <id>http://liujunming.github.io/2025/11/02/Notes-about-virtio-9p/</id>
    <published>2025-11-02T09:23:44.000Z</published>
    <updated>2025-11-02T11:54:08.995Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio-9p的相关notes。<a id="more"></a></p><h2 id="什么是9P？"><a href="#什么是9P？" class="headerlink" title="什么是9P？"></a>什么是9P？</h2><p>9P(Plan 9 File System Protocol)是一种网络文件系统协议。它最初是为贝尔实验室的Plan 9 from Bell Labs分布式操作系统而设计的。</p><p>其核心思想是：<strong>“将所有资源都表示为文件”</strong>。在 Plan 9 中，不仅是磁盘上的数据，包括进程、设备、网络接口、甚至窗口系统接口，都被统一抽象为文件系统中的文件。9P 就是用来访问这些”文件”的协议。</p><h2 id="9P核心特性与设计哲学"><a href="#9P核心特性与设计哲学" class="headerlink" title="9P核心特性与设计哲学"></a>9P核心特性与设计哲学</h2><h3 id="简单与精简"><a href="#简单与精简" class="headerlink" title="简单与精简"></a>简单与精简</h3><ul><li>9P 的消息类型非常少（大约 10 种左右，如 <code>Tversion</code>, <code>Tauth</code>, <code>Tattach</code>, <code>Tclunk</code>, <code>Twalk</code>, <code>Topen</code>, <code>Tread</code>, <code>Twrite</code>, <code>Tstat</code>, <code>Twstat</code>），协议本身简洁明了。</li><li>这种简单性使得其实现轻量，开销小。</li></ul><h3 id="分层与状态性"><a href="#分层与状态性" class="headerlink" title="分层与状态性"></a>分层与状态性</h3><ul><li>9P 是一个有状态的协议。客户端首先通过 <code>Tattach</code> 消息与服务器建立一个”会话”，获取一个根文件句柄（<code>fid</code>）。</li><li>随后，客户端通过 <code>Twalk</code> 消息在这个根句柄的基础上”遍历”路径，逐步获取目标文件的句柄。</li><li>这种基于 <code>fid</code> 的交互模式，非常类似于在本地文件系统中使用文件描述符。</li></ul><h3 id="传输无关性"><a href="#传输无关性" class="headerlink" title="传输无关性"></a>传输无关性</h3><ul><li>9P 协议定义的是消息格式和语义，并不关心底层传输介质。</li><li>它可以运行在多种传输层之上，最常见的是 TCP/IP，也可以是 Unix Domain Socket，甚至是 serial port connections。</li></ul><p><strong>在虚拟化场景下，virtio-9p就是这个传输通道！</strong>而virtio-9p就是本文要介绍的内容。 </p><h2 id="virtio-9p-usage"><a href="#virtio-9p-usage" class="headerlink" title="virtio-9p usage"></a>virtio-9p usage</h2><p>通过virtio-9p(也称为 9pfs)功能，宿主机可以将一个目录直接”共享”给虚拟机。虚拟机内的客户端可以挂载这个 9P 文件系统，从而实现宿主机与虚拟机之间高效的文件共享。</p><h2 id="virtio-9p-architecture"><a href="#virtio-9p-architecture" class="headerlink" title="virtio-9p architecture"></a>virtio-9p architecture</h2><p><img src="/images/2025/11/007.png" alt></p><p>The following figure shows the basic structure of the 9pfs implementation in QEMU.</p><p><img src="/images/2025/11/006.png" alt></p><p>The 9P transport driver is the communication channel between host system and guest system. </p><p>virtio transport driver: The 9p virtio transport driver uses e.g. a virtual PCI device and ontop the virtio protocol to transfer the 9p messages between clients (guest systems) and 9p server (host system). <a href="https://gitlab.com/qemu-project/qemu/-/blob/master/hw/9pfs/virtio-9p-device.c" target="_blank" rel="noopener">virtio-9p-device.c</a></p><hr><p>参考资料:</p><ol><li><a href="https://www.kernel.org/doc/ols/2010/ols2010-pages-109-120.pdf" target="_blank" rel="noopener">VirtFS—A virtualization aware File System pass-through</a></li><li><a href="https://wiki.qemu.org/Documentation/9p" target="_blank" rel="noopener">Documentation/9p</a></li><li>deepseek prompt:简要介绍下9P (protocol)</li><li><a href="https://www.wikiwand.com/en/articles/9P_(protocol)" target="_blank" rel="noopener">https://www.wikiwand.com/en/articles/9P_(protocol)</a></li><li><a href="https://gitlab.com/qemu-project/qemu/-/blob/master/hw/9pfs/virtio-9p-device.c" target="_blank" rel="noopener">virtio-9p-device.c</a></li><li><a href="https://www.kernel.org/doc/Documentation/filesystems/9p.txt" target="_blank" rel="noopener">v9fs: Plan 9 Resource Sharing for Linux</a></li><li><a href="https://github.com/torvalds/linux/tree/master/fs/9p" target="_blank" rel="noopener">linux/fs/9p</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio-9p的相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes about gVisor</title>
    <link href="http://liujunming.github.io/2025/11/02/Notes-about-gVisor/"/>
    <id>http://liujunming.github.io/2025/11/02/Notes-about-gVisor/</id>
    <published>2025-11-02T05:41:03.000Z</published>
    <updated>2025-11-02T07:35:40.715Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下gVisor的相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>gVisor provides a strong layer of isolation between running applications and the host operating system. It is an application kernel that implements a Linux-like interface. Unlike Linux, it is written in a memory-safe language (Go) and runs in userspace.</p><h2 id="How-is-this-different"><a href="#How-is-this-different" class="headerlink" title="How is this different?"></a>How is this different?</h2><h3 id="Machine-level-virtualization"><a href="#Machine-level-virtualization" class="headerlink" title="Machine-level virtualization"></a>Machine-level virtualization</h3><p>such as KVM and Xen, exposes virtualized hardware to a guest kernel via a Virtual Machine Monitor (VMM). This virtualized hardware is generally enlightened (paravirtualized) and additional mechanisms can be used to improve the visibility between the guest and host (e.g. balloon drivers, paravirtualized spinlocks). Running containers in distinct virtual machines can provide great isolation, compatibility and performance, but for containers it often requires additional proxies and agents, and may require a larger resource footprint and slower start-up times.</p><p><img src="/images/2025/11/003.png" alt></p><h3 id="Rule-based-execution"><a href="#Rule-based-execution" class="headerlink" title="Rule-based execution"></a>Rule-based execution</h3><p>such as <a href="https://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt" target="_blank" rel="noopener">seccomp</a>, <a href="https://github.com/selinuxproject" target="_blank" rel="noopener">SELinux</a> and <a href="https://wiki.ubuntu.com/AppArmor" target="_blank" rel="noopener">AppArmor</a>, allows the specification of a fine-grained security policy for an application or container. These schemes typically rely on hooks implemented inside the host kernel to enforce the rules. If the surface can be made small enough, then this is an excellent way to sandbox applications and maintain native performance. However, in practice it can be extremely difficult (if not impossible) to reliably define a policy for arbitrary, previously unknown applications, making this approach challenging to apply universally.</p><p><img src="/images/2025/11/004.png" alt></p><h3 id="gVisor"><a href="#gVisor" class="headerlink" title="gVisor"></a>gVisor</h3><p>provides a third isolation mechanism, distinct from those above.</p><p>gVisor intercepts application system calls and acts as the guest kernel, without the need for translation through virtualized hardware. gVisor may be thought of as either a merged guest kernel and VMM, or as seccomp on steroids(强化版). This architecture allows it to provide a flexible resource footprint (i.e. one based on threads and memory mappings, not fixed guest physical resources) while also lowering the fixed costs of virtualization. However, this comes at the price of reduced application compatibility and higher per-system call overhead.</p><p><img src="/images/2025/11/005.png" alt></p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>Google gVisor is a sandboxed container runtime that uses para-virtualization to isolate containerized applications from the host system without the heavy-weight resource allocation that comes with full virtual machines. It implements a user space kernel, <code>Sentry</code>, that is written in the Go Language and runs in a restricted <a href="/2025/04/05/Notes-about-Seccomp-filter/">seccomp</a> container. </p><p><img src="/images/2025/11/002.png" alt></p><p>Figure 2 shows gVisor’s architecture. All syscalls made by the application are redirected into the Sentry, which implements most system call functionality itself for the 237 syscalls it supports. Sentry makes calls to 53 host syscalls to support its operations. This prevents the application from having any direct interaction with the host through syscalls. gVisor supports two methods of redirecting syscalls: <code>ptrace-mode</code> uses ptrace in the Linux kernel to forward syscalls to the sentry and <code>KVM-mode</code> uses KVM to trap syscalls before they hit the Linux kernel so they can be forwarded to the sentry. KVM-mode performs better than ptrace for many workloads and has several benefits over the ptrace platform according to the gVisor documentation.</p><p>gVisor starts a Gofer process with each container that provides the Sentry with access to file system resources. Thus, a compromised Sentry cannot directly read or write any files. A writable tmpfs can be overlaid on the entire file system to provide complete isolation from the host file system. To enable sharing between the running containers and with the host, a shared file access mode may be used.</p><p>gVisor has its own user-space networking stack written in Go called <code>netstack</code>. The Sentry uses netstack to handle almost all networking, including TCP connection state, control messages, and packet assembly, rather than relying on kernel code that shares much more state across containers. gVisor also provides an option to use host networking for higher performance</p><hr><p>参考资料:</p><ol><li><a href="https://gvisor.dev/docs/" target="_blank" rel="noopener">What is gVisor?</a></li><li><a href="https://pages.cs.wisc.edu/~swift/papers/vee20-isolation.pdf" target="_blank" rel="noopener">Blending Containers and Virtual Machines:A Study of Firecracker and gVisor</a></li><li><a href="https://ipads.se.sjtu.edu.cn/_media/pub/members/paper.pdf" target="_blank" rel="noopener">A Hardware-Software Co-Design for Efficient Secure Containers</a></li><li><a href="https://mp.weixin.qq.com/s/b8KFCv4jwdzcq4T44566HA" target="_blank" rel="noopener">Google 发布 gVisor – 容器沙箱运行时</a></li><li><a href="https://www.usenix.org/system/files/hotcloud19-paper-young.pdf" target="_blank" rel="noopener">The True Cost of Containing: A gVisor Case Study</a></li><li><a href="https://gvisor.dev/blog/2023/04/28/systrap-release/" target="_blank" rel="noopener">Releasing Systrap - A high-performance gVisor platform</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下gVisor的相关notes。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Skip List</title>
    <link href="http://liujunming.github.io/2025/11/01/Notes-about-Skip-List/"/>
    <id>http://liujunming.github.io/2025/11/01/Notes-about-Skip-List/</id>
    <published>2025-11-01T14:40:28.000Z</published>
    <updated>2025-11-01T14:56:43.624Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Skip List的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>可以看下视频<a href="https://www.youtube.com/watch?v=ol-FaNLXlR0" target="_blank" rel="noopener">Skip List Explained</a>。</p><h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>跳跃表的核心思想非常直观：<strong>为有序的链表增加多级“索引”</strong>，来加速查找过程。它是对“二分查找”思想的一种链表实现。</p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>对于一个有序链表，查找一个元素只能从头到尾遍历，时间复杂度是 O(n)，效率很低。我们希望能像数组的二分查找一样，实现 O(log n) 的查找效率，但链表不支持随机访问。</p><p>跳跃表通过“空间换时间”的策略，完美地解决了这个问题。</p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>跳跃表由多层链表组成:</p><ol><li>最底层（第0层）：是一个包含所有元素的有序单链表</li><li>第1层：是底层链表的一个“索引”子集，它只包含底层中的部分节点，节点密度更低</li><li>第2层：是第1层索引的索引，节点更稀疏</li><li>以此类推… 直到最顶层</li></ol><p>每一层都是一个有序的链表，高层链表是低层链表的“快速通道”。</p><p><img src="/images/2025/11/001.png" alt></p><hr><p>参考资料:</p><ol><li>deepseek prompt:简要介绍下跳跃表这个数据结构</li><li><a href="https://www.youtube.com/watch?v=ol-FaNLXlR0" target="_blank" rel="noopener">Skip List Explained</a></li><li><a href="https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/skiplists.pdf" target="_blank" rel="noopener">Skip Lists CMSC 420</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Skip List的相关notes。
    
    </summary>
    
      <category term="数据结构" scheme="http://liujunming.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据结构" scheme="http://liujunming.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about CRIU(Checkpoint/Restore In Userspace)</title>
    <link href="http://liujunming.github.io/2025/10/01/Notes-about-CRIU-Checkpoint-Restore-In-Userspace/"/>
    <id>http://liujunming.github.io/2025/10/01/Notes-about-CRIU-Checkpoint-Restore-In-Userspace/</id>
    <published>2025-10-01T01:19:54.000Z</published>
    <updated>2025-10-01T13:36:03.255Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下CRIU(Checkpoint/Restore In Userspace)的相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>CRIU(Checkpoint/Restore In Userspace) is a Linux software. It can freeze a running container (or an individual application) and checkpoint its state to disk. The data saved can be used to restore the application and run it exactly as it was during the time of the freeze. Using this functionality, application or container live migration, snapshots, remote debugging, and <a href="https://criu.org/Usage_scenarios" target="_blank" rel="noopener">many other things</a> are now possible.</p><p><img src="/images/2025/10/001.png" alt></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CRIU is a software framework for transparent checkpointing and restoring of Linux processes. It enables live migration, snapshots, or accelerated start-up of processes and containers. To extract the user-space application state, CRIU uses conventional debugging mechanisms, like <a href="http://man7.org/linux/man-pages/man2/ptrace.2.html" target="_blank" rel="noopener">ptrace</a>. However, to extract the state of process-specific kernel objects, CRIU depends on special Linux kernel interfaces.</p><p><img src="/images/2025/10/003.png" alt></p><p>During recovery, CRIU runs inside the target process and recreates all OS objects on behalf of the target. This way, CRIU utilises the available OS mechanisms to run most of the recovery without the need for significant kernel modifications. Finally, CRIU removes any traces of itself from the process.</p><p>CRIU can also <a href="https://lwn.net/Articles/495304/" target="_blank" rel="noopener">restore the state of TCP connections</a>, a crucial feature for live migration of distributed applications. The Linux kernel introduced a new TCP connection state, <code>TCP_REPAIR</code>, for that purpose. In this state, a user-level process can modify the send and receive message queues, get and set the message sequence numbers and timestamps, or open and close a connection without notifying the other side.</p><h2 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h2><p>CRIU checkpoint与restore时需要考虑的进程状态:</p><ul><li>All of the threads running within the process, where they are executing, their priority, and their signal handling state.</li><li>A complete memory map of the process: which mappings exist at which addresses and the protections that apply to each.</li><li>A list of the process’s open files, including the actual files that have been opened, whether each was opened for read, write, or append access, the current file position, and the file-descriptor number.</li><li>Every open network connection, who the peer is, which protocol is in use, and any in-transit data.</li><li>The configuration of the namespaces in which the process is running.</li><li>Active file notifications, terminal configurations, active timers, and no end of other details.</li></ul><h2 id="CRIU-for-RDMA-and-GPU"><a href="#CRIU-for-RDMA-and-GPU" class="headerlink" title="CRIU for RDMA and GPU"></a>CRIU for RDMA and GPU</h2><ul><li><a href="https://www.usenix.org/system/files/atc21-planeta.pdf" target="_blank" rel="noopener">MigrOS: Transparent Live-Migration Support for Containerised RDMA Applications</a></li><li><a href="https://developer.nvidia.com/blog/checkpointing-cuda-applications-with-criu/" target="_blank" rel="noopener">Checkpointing CUDA Applications with CRIU</a></li><li><a href="https://arxiv.org/pdf/2502.16631" target="_blank" rel="noopener">CRIUgpu</a></li></ul><p>更多学术paper，可以参考<a href="https://criu.org/Academic_Research" target="_blank" rel="noopener">Academic Research</a></p><h2 id="CRIB"><a href="#CRIB" class="headerlink" title="CRIB"></a>CRIB</h2><ul><li><a href="https://lwn.net/Articles/984313/" target="_blank" rel="noopener">CRIB: checkpoint/restore in BPF</a></li><li><a href="https://mp.weixin.qq.com/s/XtCebIlGOvugvP35R3pNpw" target="_blank" rel="noopener">CRIB: checkpoint/restore in BPF</a></li><li><a href="https://lpc.events/event/18/contributions/1812/" target="_blank" rel="noopener">Checkpoint/Restore In eBPF (CRIB)</a></li></ul><p>CRIB (Checkpoint/Restore In eBPF) is a proposed eBPF-based system for checkpointing and restoring process state, offering improved performance, flexibility, and extensibility over traditional Linux kernel methods like <code>procfs</code>. It consists of user-space programs, eBPF programs, and kernel functions to facilitate efficient dumping and restoring of process data directly within the kernel.</p><p><img src="/images/2025/10/002.png" alt></p><p>CRIB is an innovative approach to process checkpointing and restoring, designed to address the limitations of existing kernel mechanisms. It leverages the power of eBPF to provide a more performant, flexible, and extensible solution for managing process states. </p><hr><p>参考资料:</p><ol><li><a href="https://criu.org/Main_Page" target="_blank" rel="noopener">https://criu.org/Main_Page</a></li><li><a href="https://github.com/checkpoint-restore/criu" target="_blank" rel="noopener">source code</a></li><li><a href="https://www.usenix.org/system/files/atc21-planeta.pdf" target="_blank" rel="noopener">MigrOS: Transparent Live-Migration Support for Containerised RDMA Applications</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下CRIU(Checkpoint/Restore In Userspace)的相关notes。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Notes about CUDA Dynamic Parallelism</title>
    <link href="http://liujunming.github.io/2025/09/21/Notes-about-CUDA-Dynamic-Parallelism/"/>
    <id>http://liujunming.github.io/2025/09/21/Notes-about-CUDA-Dynamic-Parallelism/</id>
    <published>2025-09-21T12:45:49.000Z</published>
    <updated>2025-09-21T13:27:55.592Z</updated>
    
    <content type="html"><![CDATA[<p>CUDA Dynamic Parallelism(也简称DP，动态并行)是CUDA编程模型的一个高级特性，它允许<strong>GPU内核（Kernel）在设备端自行创建和启动新的内核</strong>，而无需CPU的干预。<a id="more"></a></p><h2 id="为什么引入Dynamic-Parallelism？"><a href="#为什么引入Dynamic-Parallelism？" class="headerlink" title="为什么引入Dynamic Parallelism？"></a>为什么引入Dynamic Parallelism？</h2><p>传统 CUDA 模型要求所有内核（Kernel）必须由 CPU 预先启动，在面对递归算法、自适应计算、不规则数据结构等场景时存在显著局限性。</p><p>CUDA Dynamic Parallelism就是来解决这个问题的。</p><h2 id="DP基本概念"><a href="#DP基本概念" class="headerlink" title="DP基本概念"></a>DP基本概念</h2><p>CUDA DP是CUDA编程模型的一个扩展，CUDA5.0引入的关键特性。允许CUDA内核直接在GPU上创建和同步新的工作（即启动新的内核），实现运行时动态任务生成与调度，从而实现更灵活的并行计算模式。</p><p>与静态并行相比，DP 的核心创新在于：</p><ol><li>任务生成的本地化：任务调度逻辑完全在 GPU 上执行，无需 CPU 介入</li><li>动态资源分配：根据运行时数据特征实时调整并行粒度</li><li>算法自然映射：支持递归、条件分支等不规则控制流的并行化</li></ol><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><ol><li>父内核启动：整个过程仍然始于CPU。CPU启动一个普通的内核（称为父内核）</li><li>设备端启动：在父内核的执行过程中，GPU上的线程可以根据其计算得到的结果，使用标准的语法来启动一个新的子内核</li><li>设备端同步：父内核中的线程可以使用 <code>cudaDeviceSynchronize()</code> 来等待其启动的所有子内核执行完毕，然后再继续自己的工作</li><li>层次结构：子内核还可以继续启动自己的子内核，从而形成一个嵌套的、层次化的内核启动结构。CUDA确保了这种嵌套执行的正确性</li></ol><h2 id="DP核心优势"><a href="#DP核心优势" class="headerlink" title="DP核心优势"></a>DP核心优势</h2><ol><li>消除主机端瓶颈: 在传统模式中，每次内核启动需经历 “CPU 准备参数→PCIe 传输→GPU 执行” 的固定流程。DP 将任务启动逻辑迁移至设备端，避免了频繁的主机 - 设备同步。例如，在递归计算中，子内核启动延迟从 PCIe 级（微秒级）降至 GPU 内核内函数调用级（纳秒级）。</li><li>细粒度任务调度: 线程可根据局部数据特征动态决定是否启动子任务。如在自适应网格计算中，仅当网格单元误差超过阈值时才启动子内核进行细化，实现计算资源的精准分配。</li><li>支持复杂算法结构: 传统CUDA难以高效实现的递归算法（如快速傅里叶变换、光线追踪的光线分叉），可通过DP直接映射为内核层级调用，避免繁琐的迭代化改造。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CUDA Dynamic Parallelism 是一项强大的技术，它通过允许GPU内核自主启动新工作，深化了GPU的独立计算能力。它主要解决了减少CPU-GPU通信瓶颈和简化复杂算法实现的问题，但随之而来的是需要开发者对开销和复杂性进行仔细权衡。在合适的场景下使用，它能带来显著的性能提升和编程便利性。</p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s/teAouWA11-uJ5N3MKEyXcQ" target="_blank" rel="noopener">NVIDIA | BaM与CUDA Dynamic Parallelism的核心区别</a></li><li>deepseek prompt:简要介绍下cuda的Dynamic Parallelism技术</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CUDA Dynamic Parallelism(也简称DP，动态并行)是CUDA编程模型的一个高级特性，它允许&lt;strong&gt;GPU内核（Kernel）在设备端自行创建和启动新的内核&lt;/strong&gt;，而无需CPU的干预。
    
    </summary>
    
      <category term="GPU" scheme="http://liujunming.github.io/categories/GPU/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="CUDA" scheme="http://liujunming.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel Advanced Matrix Extensions (AMX)</title>
    <link href="http://liujunming.github.io/2025/09/20/Notes-about-Intel-Advanced-Matrix-Extensions-AMX/"/>
    <id>http://liujunming.github.io/2025/09/20/Notes-about-Intel-Advanced-Matrix-Extensions-AMX/</id>
    <published>2025-09-20T09:18:05.000Z</published>
    <updated>2025-09-20T10:19:11.388Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel Advanced Matrix Extensions (AMX)的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>To significantly improve the throughput of the CPU for machine learning (ML) applications, Intel has integrated AMX, an on-chip matrix-multiplication(矩阵乘法) accelerator, along with Instruction Set Architecture (ISA) support, starting from the 4th generation Xeon CPUs (SPR), released in 2023. </p><p><img src="/images/2025/09/013.png" alt></p><p>The above Figure depicts the accelerator architecture consisting of two core components: (1) a 2D array of registers (tiles) and (2) Tile matrix multiply unit (TMUL), which are designed to support INT8 and BF16 formats. The tiles store sub-arrays of matrices and the TMUL, a 2D array of multiply-add units, operate on these tiles. The 2D structure of TMUL delivers significantly higher operations/cycle through large tile-based matrix computation, which operates with greater parallelism than 1D compute engines such as AVX engines. The CPU dispatches AMX instructions, such as tile load/store and accelerator commands, to the multi-cycle AMX units. The accelerator’s memory accesses remain coherent with the CPU’s memory accesses.</p><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p>AMX 通过其两个核心组件协同工作来加速矩阵运算:</p><ul><li>Tile:这是一组 8 个二维寄存器（TMM0-TMM7），每个大小为 1 KB，专门用于存储较大的数据块（矩阵）</li><li>TMUL(Tile Matrix Multiply Unit，Tile矩阵乘法单元):这是一个专用的矩阵乘法加速引擎，直接与 Tile 寄存器连接，用于执行 AI 计算中至关重要的矩阵乘法运算</li></ul><p>AMX 支持 BF16 和 INT8 两种数据类型。BF16 在保持足够精度的同时提高了计算效率，适用于训练和推理；INT8 则主要用于推理场景，以进一步追求速度和能效。</p><h2 id="XSAVE的支持"><a href="#XSAVE的支持" class="headerlink" title="XSAVE的支持"></a>XSAVE的支持</h2><p>Tile的状态分为两部分: TILECFG和TILEDATA</p><p><img src="/images/2025/09/014.png" alt></p><p><img src="/images/2025/09/015.png" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/qemu/qemu/blob/master/target/i386/cpu.h</span></span><br><span class="line"><span class="comment">/* Ext. save area 17: AMX XTILECFG state */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">XSaveXTILECFG</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint8_t</span> xtilecfg[<span class="number">64</span>];</span><br><span class="line">&#125; XSaveXTILECFG;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Ext. save area 18: AMX XTILEDATA state */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">XSaveXTILEDATA</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint8_t</span> xtiledata[<span class="number">8</span>][<span class="number">1024</span>];</span><br><span class="line">&#125; XSaveXTILEDATA;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Intel AMX 是 Intel内置在其现代服务器 CPU 中的专用矩阵计算加速硬件，它通过专门的寄存器和执行单元，大幅提升了 CPU 执行 AI 训练和推理任务（尤其是矩阵乘法）的效率。对于希望利用现有服务器基础设施进行高效 AI 计算、同时控制成本和复杂性的企业来说，AMX 提供了一项值得关注的技术。</p><hr><p>参考资料:</p><ol><li>Intel SDM vol1</li><li>LIA: A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading(ISCA’25)</li><li>deepseek prompt:简要介绍下Intel的amx技术</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel Advanced Matrix Extensions (AMX)的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel Processor Trace (IPT)</title>
    <link href="http://liujunming.github.io/2025/09/14/Notes-about-Intel-Processor-Trace-Intel-PT/"/>
    <id>http://liujunming.github.io/2025/09/14/Notes-about-Intel-Processor-Trace-Intel-PT/</id>
    <published>2025-09-14T12:47:52.000Z</published>
    <updated>2025-09-21T02:13:43.460Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel Processor Trace (IPT)的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/09/009.png" alt></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Currently, there are several control flow tracing mechanisms in hardware, each representing a different set of tradeoffs (as shown in Table 1). Branch Trace Store (BTS) can capture all control transfer events (e.g., call, return and all types of jumps) to a memory-resident BTS buffer. Each record contains the addresses of source and target of the branch instruction, thus there is no need to decode it. However, BTS introduces very high overhead during tracing and is inflexible due to the lack of event filtering mechanisms. While Last Brach Record (LBR) has some support of event filtering (e.g., filtering out conditional branches), it can only record 16 or 32 most recent branch pairs (source and target) into a register stack. Though it incurs very low tracing overhead, it can hardly provide precise protection.</p><p><img src="/images/2025/09/010.png" alt></p><p>Due to the capability of dynamically tracing control flow, BTS and LBR have been exploited to defend against ROP like attacks, which, however, either incur high overhead (those using BTS) or sacrifice security due to imprecise tracing(LBR).</p><p>IPT能以比BTS低得多的性能开销（通常在 5% 左右或更低），持续、近乎无遗漏地记录程序执行的控制流。它在很多场景下正在替代 BTS。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>IPT is introduced in Intel Core M and 5th generation Intel Core processors. Each CPU core has its own IPT hardware that generates trace information of running programs in the form of <code>packets</code>. IPT configuration can only be done by the privileged agents (e.g., OS) using certain model-specific registers (MSRs). The traced packets are written to the pre-configured memory buffer in a compressed form to minimize the output bandwidth and reduce the tracing overhead. The software decoder can decode the traced packets based on pre-defined format, with the extra information like the program binaries, as well as some runtime data provided by the control agent, to precisely reconstruct the program flow. Thanks to the aggressive compression of traces, it can collect more control flow tracing information including control flow, execution modes, and timings than BTS, yet incurring much less tracing overhead compared to BTS. This, however, also incurs orders of magnitude slower decoding speed than tracing.</p><p><img src="/images/2025/09/011.png" alt></p><p><img src="/images/2025/09/012.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://ipads.se.sjtu.edu.cn/zh/publications/LiuHPCA17.pdf" target="_blank" rel="noopener">Transparent and Efficient CFI Enforcement with Intel Processor Trace</a></li><li><a href="https://halobates.de/blog/p/406" target="_blank" rel="noopener">Intel Processor Trace resources</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel Processor Trace (IPT)的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PMU(Performance Monitoring Unit)</title>
    <link href="http://liujunming.github.io/2025/09/14/Notes-about-PMU-Performance-Monitoring-Unit/"/>
    <id>http://liujunming.github.io/2025/09/14/Notes-about-PMU-Performance-Monitoring-Unit/</id>
    <published>2025-09-14T10:48:56.000Z</published>
    <updated>2025-09-14T12:35:31.177Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PMU(Performance Monitoring Unit)的相关notes，内容源于DSN’12 paper，时效性可能不太准确。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>There are generally two working modes of PMUs: interrupt-based mode and precision mode. In the first mode, a counter will automatically increase and generate an interrupt when it has reached a predefined threshold (i.e., event-based sampling) or predefined time has elapsed (i.e., time-based sampling). This is the basic performance counter mode, which supports most types of events, but lacks precise instruction pointer information, resulting in that the reported IP (instruction pointer) is up to tens of instructions away from the instruction causing the event, due to the out-of-order execution in modern processors. For example, according to AMD’s manual, the reported IP may be up to 72 instructions away from the actual IP causing the event.</p><p>To improve the precision and flexibility of PMUs,most commodity processors also support a precise mode of performance monitoring, including the Precise Event-Based Sampling (PEBS), Branch Trace Store (BTS),Last Branch Record (LBR) and Event Filtering (EF). Currently, most existing commodity processors support parts of the features mentioned above.</p><h2 id="Precise-Performance-Counter"><a href="#Precise-Performance-Counter" class="headerlink" title="Precise Performance Counter"></a>Precise Performance Counter</h2><p>In PEBS, the samples of performance counters are written into a preregistered memory region. When the memory region is nearly full, an interrupt is generated to trigger the handler. By batching the samples and processing them together, this mechanism improves the performance of monitoring significantly. Meanwhile, thanks to the <em>atomic-freeze</em> feature, the IP addresses recorded in traces are exactly the ones causing the event.<br><img src="/images/2025/09/008.png" alt></p><h2 id="Branch-Trace-Store"><a href="#Branch-Trace-Store" class="headerlink" title="Branch Trace Store"></a>Branch Trace Store</h2><p>Intel’s BTS mechanism provides the capability of capturing all control transfer events and saving the events in a memory-resident BTS buffer. The events include all types of jump, call, return, interrupt and exception. The recorded information includes the addresses of branch source and target. Thus, it enables the monitoring of the whole control flow of an application. Similar as PEBS, the branch trace is also recorded in a pre-registered memory region, which makes the batching processing possible.</p><h2 id="Last-Branch-Record"><a href="#Last-Branch-Record" class="headerlink" title="Last Branch Record"></a>Last Branch Record</h2><p>LBR in Intel Core and Core i7, records the most recent branches into a register stack. This mechanism records similar data as in BTS. It records the source address and target address of each branch, thus provides the ability to trace the control flow of a program as well. However, due to the small size of the register stack (e.g., Intel Core has 4 pairs, Core i7 has 16 pairs), previous samples may be overwritten by upcoming samples during monitoring.</p><p><img src="/images/2025/09/007.png" alt></p><p>可以把 LBR 想象成一个容量很小的高速暂存器，只记录最近发生的几次分支，但查看速度极快，几乎不影响当前工作。而 BTS 则像一个庞大的流水账本，忠实记录下所有的分支，但往账本上写字的过程本身会消耗一些精力。</p><h2 id="Event-Filtering"><a href="#Event-Filtering" class="headerlink" title="Event Filtering"></a>Event Filtering</h2><p>The Event Filtering mechanism provides additional constraints to record events. It is used to filter events not concerned with. For example, latency constraints can be applied in Itanium2’s cache related events, which only count on high latency cache misses. Further, constraints such as “do not capture conditional branches”, “do not capture near return branches” are generally available on recent processors, which support LBR such as Intel Core i7. However, this mechanism is currently only available in LBR, control transfers recorded in BTS lack this type of filtering support.</p><h2 id="Conditional-Counting"><a href="#Conditional-Counting" class="headerlink" title="Conditional Counting"></a>Conditional Counting</h2><p>To separate user-level events from kernel-level ones, PMUs also support conditional event counting: they only increment counter while the processor is running at a specific privilege level (e.g. user, kernel or both). Further, to isolate possible interferences in performance counters among multiple processes/threads, operating systems are usually enhanced by saving and restoring performance counters during context switches.</p><hr><p>参考资料:</p><ol><li><a href="https://ipads.se.sjtu.edu.cn/_media/publications/cfimon-dsn12.pdf" target="_blank" rel="noopener">CFIMon: Detecting Violation of Control Flow Integrity using Performance Counters</a></li><li><a href="https://llvm.org/devmtg/2024-04/slides/TechnicalTalks/Xiao-EnablingHW-BasedPGO.pdf" target="_blank" rel="noopener">Enabling HW-based PGO</a></li><li>deepssek prompt: pmu的pebs与lbr，如何理解</li><li>deepssek prompt: Intel Branch Trace Store与Last Branch Record的区别</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PMU(Performance Monitoring Unit)的相关notes，内容源于DSN’12 paper，时效性可能不太准确。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about qemu pxb(pci expander bridge)</title>
    <link href="http://liujunming.github.io/2025/09/13/Notes-about-qemu-pxb-pci-expander-bridge/"/>
    <id>http://liujunming.github.io/2025/09/13/Notes-about-qemu-pxb-pci-expander-bridge/</id>
    <published>2025-09-13T11:16:54.000Z</published>
    <updated>2025-09-13T11:40:19.492Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下qemu pxb(pci expander bridge)的相关notes。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>PXB is a “light-weight” host bridge whose purpose is to enable the main host bridge to support multiple PCI root buses.</p><p>As oposed to PCI-2-PCI bridge’s secondary bus, PXB’s bus is a primary bus and can be associated with a NUMA node(different from the main host bridge) allowing the guest OS to recognize the proximity of a pass-through device to other resources as RAM and CPUs.</p><p>The PXB is composed from:</p><ul><li>A primary PCI bus (can be associated with a NUMA node)<br>Acts like a normal pci bus and from the functionality point<br>of view is an “expansion” of the bus behind the<br>main host bridge.</li><li>A pci-2-pci bridge behind the primary PCI bus where the actual<br>devices will be attached.</li><li>A host-bridge PCI device<br>Situated on the bus behind the main host bridge, allows<br>the BIOS to configure the bus number and IO/mem resources.<br>It does not have its own config/data register for configuration<br>cycles, this being handled by the main host bridge.</li></ul><ul><li>A host-bridge sysbus to comply with QEMU current design.</li></ul><h2 id="PCI-Root-Bus"><a href="#PCI-Root-Bus" class="headerlink" title="PCI Root Bus"></a>PCI Root Bus</h2><p>This section demonstrates how to create extra PCI root buses through the “light-weight” PXB (PCI Expander Bridge) host bridge. It is “pxb” in QEMU command line. It is implemented only for i440fx and can be placed only on bus 0.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -machine pc,accel=kvm -vnc :8 -smp 4 -m 4096M \</span><br><span class="line">-net nic -net user,hostfwd=tcp::5028-:22 \</span><br><span class="line">-hda ol8.qcow2 -serial stdio \</span><br><span class="line">-device pxb,id=bridge1,bus=pci.0,bus_nr=3 \</span><br><span class="line">-device virtio-scsi-pci,bus=bridge1,addr=0x3 \</span><br><span class="line">-device pxb,id=bridge2,bus=pci.0,bus_nr=8 \</span><br><span class="line">-device virtio-scsi-pci,bus=bridge2,addr=0x3 \</span><br><span class="line">-device virtio-scsi-pci,bus=bridge2,addr=0x4</span><br></pre></td></tr></table></figure></p><p>The above QEMU command line creates two extra PCI root buses. The first root bus (04:00.0) has one virtio-scsi-pci HBA (04:03.0), and the second root bus (09:00.0) has two virtio-scsi-pci HBAs (09:03.0 and 09:04.0).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci</span><br><span class="line">00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)</span><br><span class="line">00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]</span><br><span class="line">00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]</span><br><span class="line">00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)</span><br><span class="line">00:02.0 VGA compatible controller: Device 1234:1111 (rev 02)</span><br><span class="line">00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 03)</span><br><span class="line">00:04.0 Host bridge: Red Hat, Inc. QEMU PCI Expander bridge</span><br><span class="line">00:05.0 Host bridge: Red Hat, Inc. QEMU PCI Expander bridge</span><br><span class="line">03:00.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge</span><br><span class="line">04:03.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI</span><br><span class="line">08:00.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge</span><br><span class="line">09:03.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI</span><br><span class="line">09:04.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI</span><br></pre></td></tr></table></figure></p><p>The below <code>lspci</code> output and figure depict the PCI bus topology for this example.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci -t</span><br><span class="line">-+-[0000:08]---00.0-[09]--+-03.0</span><br><span class="line"> |                        \-04.0</span><br><span class="line"> +-[0000:03]---00.0-[04]----03.0</span><br><span class="line"> \-[0000:00]-+-00.0</span><br><span class="line">             +-01.0</span><br><span class="line">             +-01.1</span><br><span class="line">             +-01.3</span><br><span class="line">             +-02.0</span><br><span class="line">             +-03.0</span><br><span class="line">             +-04.0</span><br><span class="line">             \-05.0</span><br></pre></td></tr></table></figure><p><img src="/images/2025/09/005.avif" alt></p><h2 id="PCIe-Root-Complex"><a href="#PCIe-Root-Complex" class="headerlink" title="PCIe Root Complex"></a>PCIe Root Complex</h2><p>This section demonstrates how to create extra PCIe root buses through extra Root Complexes. According to QEMU source code, PCIe features are supported only by ‘q35’ machine type on x86 architecture and the ‘virt’ machine type on AArch64. The root complex is created by using “pxb-pcie” on the QEMU command line.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -machine q35,accel=kvm -vnc :8 -smp 4 -m 4096M \</span><br><span class="line">-net nic -net user,hostfwd=tcp::5028-:22 \</span><br><span class="line">-hda ol8.qcow2 -serial stdio \</span><br><span class="line">-device pxb-pcie,id=pcie.1,bus_nr=2,bus=pcie.0 \</span><br><span class="line">-device ioh3420,id=pcie_port1,bus=pcie.1,chassis=1 \</span><br><span class="line">-device virtio-scsi-pci,bus=pcie_port1 \</span><br><span class="line">-device ioh3420,id=pcie_port2,bus=pcie.1,chassis=2 \</span><br><span class="line">-device virtio-scsi-pci,bus=pcie_port2 \</span><br><span class="line">-device pxb-pcie,id=pcie.2,bus_nr=8,bus=pcie.0 \</span><br><span class="line">-device ioh3420,id=pcie_port3,bus=pcie.2,chassis=3 \</span><br><span class="line">-device virtio-scsi-pci,bus=pcie_port3</span><br></pre></td></tr></table></figure></p><p>The above QEMU command line creates two extra PCIe root complexes. The first root complex has one virtio-scsi-pci HBA (09:00.0), and the second has two virtio-scsi-pci HBAs (03:00.0 and 04:00.0).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci</span><br><span class="line">00:00.0 Host bridge: Intel Corporation 82G33/G31/P35/P31 Express DRAM Controller</span><br><span class="line">00:01.0 VGA compatible controller: Device 1234:1111 (rev 02)</span><br><span class="line">00:02.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection</span><br><span class="line">00:03.0 Host bridge: Red Hat, Inc. QEMU PCIe Expander bridge</span><br><span class="line">00:04.0 Host bridge: Red Hat, Inc. QEMU PCIe Expander bridge</span><br><span class="line">00:1f.0 ISA bridge: Intel Corporation 82801IB (ICH9) LPC Interface Controller (rev 02)</span><br><span class="line">00:1f.2 SATA controller: Intel Corporation 82801IR/IO/IH (ICH9R/DO/DH) 6 port SATA Controller [AHCI mode] (rev 02)</span><br><span class="line">00:1f.3 SMBus: Intel Corporation 82801I (ICH9 Family) SMBus Controller (rev 02)</span><br><span class="line">02:00.0 PCI bridge: Intel Corporation 7500/5520/5500/X58 I/O Hub PCI Express Root Port 0 (rev 02)</span><br><span class="line">02:01.0 PCI bridge: Intel Corporation 7500/5520/5500/X58 I/O Hub PCI Express Root Port 0 (rev 02)</span><br><span class="line">03:00.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI (rev 01)</span><br><span class="line">04:00.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI (rev 01)</span><br><span class="line">08:00.0 PCI bridge: Intel Corporation 7500/5520/5500/X58 I/O Hub PCI Express Root Port 0 (rev 02)</span><br><span class="line">09:00.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI (rev 01)</span><br></pre></td></tr></table></figure></p><p>The below <code>lspci</code> output and figure depict the PCIe topology for this example.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci -t</span><br><span class="line">-+-[0000:08]---00.0-[09]----00.0</span><br><span class="line"> +-[0000:02]-+-00.0-[03]----00.0</span><br><span class="line"> |           \-01.0-[04]----00.0</span><br><span class="line"> \-[0000:00]-+-00.0</span><br><span class="line">             +-01.0</span><br><span class="line">             +-02.0</span><br><span class="line">             +-03.0</span><br><span class="line">             +-04.0</span><br><span class="line">             +-1f.0</span><br><span class="line">             +-1f.2</span><br><span class="line">             \-1f.3</span><br></pre></td></tr></table></figure></p><p><img src="/images/2025/09/006.avif" alt></p><hr><p>参考资料:</p><ol><li><a href="https://github.com/qemu/qemu/blob/master/docs/pci_expander_bridge.txt" target="_blank" rel="noopener">qemu/docs/pci_expander_bridge.txt</a></li><li><a href="https://blogs.oracle.com/linux/post/a-study-of-the-linux-kernel-pci-subsystem-with-qemu" target="_blank" rel="noopener">A study of the Linux kernel PCI subsystem with QEMU</a></li><li><a href="https://mail.coreboot.org/archives/list/seabios@seabios.org/message/7QA3NUQ7PMU445BWS6FGI54CFWED66GM/" target="_blank" rel="noopener">[PATCH RFC V2 12/17] hw/pci: introduce PCI Expander Bridge (PXB)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下qemu pxb(pci expander bridge)的相关notes。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="QEMU" scheme="http://liujunming.github.io/tags/QEMU/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ARM pointer authentication</title>
    <link href="http://liujunming.github.io/2025/09/07/Notes-about-ARM-pointer-authentication/"/>
    <id>http://liujunming.github.io/2025/09/07/Notes-about-ARM-pointer-authentication/</id>
    <published>2025-09-07T00:21:35.000Z</published>
    <updated>2025-09-07T01:31:56.528Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下ARM pointer authentication的相关notes。<a id="more"></a></p><h2 id="核心目的"><a href="#核心目的" class="headerlink" title="核心目的"></a>核心目的</h2><p>Pointer Authentication(PA)的核心目的是验证指针(如返回地址、函数指针等)是否被恶意篡改。它通过为指针添加加密认证码(PAC, Pointer Authentication Code)并在使用前验证其有效性来实现。</p><p><img src="/images/2025/09/001.png" alt></p><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><ol><li>生成PAC(签名)：<ul><li>使用一个密钥(CPU寄存器存储)、指针的虚拟地址和额外的上下文信息(如堆栈指针)，通过加密算法(ARM选择的是QARMA算法)为指针生成一个短小的PAC</li><li>PAC被存储在指针值中未使用的最高位中。因为64位架构中，虚拟地址通常不会全部使用(如48位)，高位空间是空闲的</li></ul></li><li>验证与恢复(验证)：<ul><li>在使用指针前(如函数返回前)，CPU会执行验证指令(如<code>AUT*</code>)</li><li>系统会使用相同的密钥和上下文重新计算PAC，并与指针中存储的PAC进行比较</li><li>如果匹配，则移除PAC，恢复原始指针，程序正常执行</li><li>如果不匹配，则指针会被置为一个非法值，使用时通常会触发一个异常(如段错误)，从而阻止攻击</li></ul></li></ol><h2 id="Pointer-Authentication-in-ARMv8-3-A"><a href="#Pointer-Authentication-in-ARMv8-3-A" class="headerlink" title="Pointer Authentication in ARMv8.3-A"></a>Pointer Authentication in ARMv8.3-A</h2><p><img src="/images/2025/09/003.png" alt></p><p><img src="/images/2025/09/004.png" alt></p><p>PA is intended for checking the integrity of pointers with minimal size and performance impact. It is available when the processor executes in 64-bit ARM state(AArch64). PA adds instructions for creating and authenticating pointer authentication codes (PACs). The PAC is a tweakable message authentication code (MAC) calculated over the pointer value and a 64-bit modifier as the tweak. Different combinations of key and modifier pairs allow domain separation among different classes of authenticated pointers. This prevents authenticated pointer values from being arbitrarily interchangeable with one another.</p><p>PA provides five different keys for PAC generation: two for code pointers, two for data pointers, and one for generic use. The keys are stored in hardware registers configured to be accessible only from a higher privilege level: e.g., the kernel maintains the keys for a user space process, generating keys for each process at process <code>exec</code>. The keys remain constant throughout the process lifetime, whereas the modifier is given in an instruction-specific register operand on each PAC creation and authentication (i.e., MAC verification). Thus it can be used to describe the run-time context in which the pointer is created and used. The modifier value is not necessarily confidential but ideally such that it 1) precisely describes the context of use in which the pointer is valid, and 2) cannot be influenced by the attacker</p><h2 id="Example-PA-based-return-address-signing"><a href="#Example-PA-based-return-address-signing" class="headerlink" title="Example: PA-based return address signing"></a>Example: PA-based return address signing</h2><p><img src="/images/2025/09/002.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/conference/usenixsecurity19/presentation/liljestrand" target="_blank" rel="noopener">PAC it up: Towards Pointer Integrity using ARM Pointer Authentication</a></li><li>deepseek prompt:简要介绍下arm Pointer Authentication</li><li><a href="https://learn.arm.com/learning-paths/servers-and-cloud-computing/pac/pac/" target="_blank" rel="noopener">Pointer Authentication on Arm</a></li><li><a href="https://lwn.net/Articles/718888/" target="_blank" rel="noopener">https://lwn.net/Articles/718888/</a></li><li><a href="https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/pointer-auth-v7.pdf" target="_blank" rel="noopener">Pointer Authentication on ARMv8.3: Design and Analysis of the New Software Security Instructions</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下ARM pointer authentication的相关notes。
    
    </summary>
    
      <category term="ARM" scheme="http://liujunming.github.io/categories/ARM/"/>
    
    
      <category term="ARM" scheme="http://liujunming.github.io/tags/ARM/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Linux kernel 内存分配函数</title>
    <link href="http://liujunming.github.io/2025/08/17/Notes-about-Linux-kernel-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%87%BD%E6%95%B0/"/>
    <id>http://liujunming.github.io/2025/08/17/Notes-about-Linux-kernel-内存分配函数/</id>
    <published>2025-08-17T09:24:12.000Z</published>
    <updated>2025-08-24T12:26:57.280Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux kernel内存分配函数的相关notes。<a id="more"></a></p><h2 id="内核函数"><a href="#内核函数" class="headerlink" title="内核函数"></a>内核函数</h2><h3 id="kmalloc系列"><a href="#kmalloc系列" class="headerlink" title="kmalloc系列"></a>kmalloc系列</h3><p>kmalloc() 申请的内存位于物理内存映射区域，而且在物理上也是连续的，它们与真实的物理地址只有一个固定的偏移，因而存在较简单的转换关系。</p><ul><li>kmalloc</li><li>kzalloc</li><li>kcalloc</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kzalloc - allocate memory. The memory is set to zero.</span></span><br><span class="line"><span class="comment"> * @size: how many bytes of memory are required.</span></span><br><span class="line"><span class="comment"> * @flags: the type of memory to allocate (see kmalloc).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kzalloc</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kmalloc(size, flags | __GFP_ZERO);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kcalloc - allocate memory for an array. The memory is set to zero.</span></span><br><span class="line"><span class="comment"> * @n: number of elements.</span></span><br><span class="line"><span class="comment"> * @size: element size.</span></span><br><span class="line"><span class="comment"> * @flags: the type of memory to allocate (see kmalloc).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kcalloc</span><span class="params">(<span class="keyword">size_t</span> n, <span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kmalloc_array(n, size, flags | __GFP_ZERO);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="vmalloc系列"><a href="#vmalloc系列" class="headerlink" title="vmalloc系列"></a>vmalloc系列</h3><p>物理地址不连续：通过映射非连续的物理页帧实现。<br>虚拟地址连续：虚拟地址空间是连续的。<br>逐页映射: 用 <code>alloc_page()</code> 从伙伴系统获取多个不连续的物理页帧。 修改内核页表，将这些物理页映射到连续的虚拟地址空间（<code>VMALLOC_START</code> ~ <code>VMALLOC_END</code>）。</p><ul><li>vmalloc</li><li>vzalloc</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *vmalloc  -  allocate virtually contiguous memory</span></span><br><span class="line"><span class="comment"> *@size:allocation size</span></span><br><span class="line"><span class="comment"> *Allocate enough pages to cover @size from the page level</span></span><br><span class="line"><span class="comment"> *allocator and map them into contiguous kernel virtual space.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *For tight control over page level allocator and protection flags</span></span><br><span class="line"><span class="comment"> *use __vmalloc() instead.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">vmalloc</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> __vmalloc_node_flags(size, NUMA_NO_NODE,</span><br><span class="line">    GFP_KERNEL);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *vzalloc - allocate virtually contiguous memory with zero fill</span></span><br><span class="line"><span class="comment"> *@size:allocation size</span></span><br><span class="line"><span class="comment"> *Allocate enough pages to cover @size from the page level</span></span><br><span class="line"><span class="comment"> *allocator and map them into contiguous kernel virtual space.</span></span><br><span class="line"><span class="comment"> *The memory allocated is set to zero.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *For tight control over page level allocator and protection flags</span></span><br><span class="line"><span class="comment"> *use __vmalloc() instead.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">vzalloc</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> __vmalloc_node_flags(size, NUMA_NO_NODE,</span><br><span class="line">GFP_KERNEL | __GFP_ZERO);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="kvzalloc系列"><a href="#kvzalloc系列" class="headerlink" title="kvzalloc系列"></a>kvzalloc系列</h3><ul><li>kvzalloc</li><li>kvcalloc</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kvzalloc</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kvmalloc(size, flags | __GFP_ZERO);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kvmalloc</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kvmalloc_node(size, flags, NUMA_NO_NODE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kvmalloc_node - attempt to allocate physically contiguous memory, but upon</span></span><br><span class="line"><span class="comment"> * failure, fall back to non-contiguous (vmalloc) allocation.</span></span><br><span class="line"><span class="comment"> * @size: size of the request.</span></span><br><span class="line"><span class="comment"> * @flags: gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.</span></span><br><span class="line"><span class="comment"> * @node: numa node to allocate from</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Uses kmalloc to get the memory but if the allocation fails then falls back</span></span><br><span class="line"><span class="comment"> * to the vmalloc allocator. Use kvfree for freeing the memory.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported.</span></span><br><span class="line"><span class="comment"> * __GFP_RETRY_MAYFAIL is supported, and it should be used only if kmalloc is</span></span><br><span class="line"><span class="comment"> * preferable to the vmalloc fallback, due to visible performance drawbacks.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Please note that any use of gfp flags outside of GFP_KERNEL is careful to not</span></span><br><span class="line"><span class="comment"> * fall back to vmalloc.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">kvmalloc_node</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags, <span class="keyword">int</span> node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">gfp_t</span> kmalloc_flags = flags;</span><br><span class="line"><span class="keyword">void</span> *ret;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * vmalloc uses GFP_KERNEL for some internal allocations (e.g page tables)</span></span><br><span class="line"><span class="comment"> * so the given set of flags has to be compatible.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> ((flags &amp; GFP_KERNEL) != GFP_KERNEL)</span><br><span class="line"><span class="keyword">return</span> kmalloc_node(size, flags, node);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * We want to attempt a large physically contiguous block first because</span></span><br><span class="line"><span class="comment"> * it is less likely to fragment multiple larger blocks and therefore</span></span><br><span class="line"><span class="comment"> * contribute to a long term fragmentation less than vmalloc fallback.</span></span><br><span class="line"><span class="comment"> * However make sure that larger requests are not too disruptive - no</span></span><br><span class="line"><span class="comment"> * OOM killer and no allocation failure warnings as we have a fallback.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (size &gt; PAGE_SIZE) &#123;</span><br><span class="line">kmalloc_flags |= __GFP_NOWARN;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!(kmalloc_flags &amp; __GFP_RETRY_MAYFAIL))</span><br><span class="line">kmalloc_flags |= __GFP_NORETRY;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ret = kmalloc_node(size, kmalloc_flags, node);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * It doesn't really make sense to fallback to vmalloc for sub page</span></span><br><span class="line"><span class="comment"> * requests</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (ret || size &lt;= PAGE_SIZE)</span><br><span class="line"><span class="keyword">return</span> ret;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> __vmalloc_node_flags_caller(size, node, flags,</span><br><span class="line">__builtin_return_address(<span class="number">0</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="物理地址获取"><a href="#物理地址获取" class="headerlink" title="物理地址获取"></a>物理地址获取</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> phys_addr_t <span class="title">kvm_kaddr_to_phys</span><span class="params">(<span class="keyword">void</span> *kaddr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!is_vmalloc_addr(kaddr)) &#123;</span><br><span class="line"><span class="keyword">return</span> __pa(kaddr);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> page_to_phys(vmalloc_to_page(kaddr)) +</span><br><span class="line">       offset_in_page(kaddr);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note that the virtual address may come from different kernel memory zones, including the <code>vmalloc region</code> or the <code>direct memory region</code>. Checking whether the a virtual address belongs to the vmalloc region by invoking the <code>is_vmalloc_addr</code> function. If so, the <code>vmalloc_to_page</code> function is used to get the corresponding physical page structure; otherwise, the <code>virt_to_page</code> function is used to obtain the right page.</p><hr><p>参考资料:</p><ol><li><a href="https://elixir.bootlin.com/linux/v4.19/" target="_blank" rel="noopener">https://elixir.bootlin.com/linux/v4.19/</a></li><li><a href="https://stackoverflow.com/questions/5153173/what-are-differences-between-kmalloc-kcalloc-vmalloc-and-kzalloc" target="_blank" rel="noopener">what are differences between kmalloc() kcalloc() vmalloc() and kzalloc()?</a></li><li><a href="https://zhuanlan.zhihu.com/p/470783392" target="_blank" rel="noopener">带你看懂Linux内核空间内存申请函数kmalloc.、kzalloc、 vmalloc的区别（一篇就够了）</a></li><li><a href="https://blog.csdn.net/sinat_37817094/article/details/149230334" target="_blank" rel="noopener">傻傻分不清楚 kmalloc、vmalloc和malloc之间有什么区别以及实现上的差异</a></li><li><a href="https://www.usenix.org/system/files/atc19-hu.pdf" target="_blank" rel="noopener">QZFS: QAT Accelerated Compression in File System for Application Agnostic and Cost Efficient Data Storage</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux kernel内存分配函数的相关notes。
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about GPUDirect Async family</title>
    <link href="http://liujunming.github.io/2025/07/05/Notes-about-GPUDirect-Async-family/"/>
    <id>http://liujunming.github.io/2025/07/05/Notes-about-GPUDirect-Async-family/</id>
    <published>2025-07-05T03:24:40.000Z</published>
    <updated>2025-07-05T07:31:12.661Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下GPUDirect Async family技术的相关notes。<a id="more"></a></p><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><ul><li><a href="/2022/04/02/Introduction-to-GPUDirect-RDMA/">GPUDirect RDMA</a></li><li><a href="/2023/05/01/Notes-about-GPU-Direct-Storage/">GPU Direct Storage</a></li><li><a href="/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/">IBGDA</a></li></ul><h2 id="GPUDirect-Async-Kernel-Initiated-Network"><a href="#GPUDirect-Async-Kernel-Initiated-Network" class="headerlink" title="GPUDirect Async Kernel-Initiated Network"></a>GPUDirect Async Kernel-Initiated Network</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>为了让GPU和网卡并行起来，CPU仍然扮演了厚重的调度角色，而且GPU空转时间比较长。</p><p><img src="/images/2025/07/004.png" alt></p><p>能否把控制面也offload一部分？于是乎GPUDirect Async Kernel-Initiated Network概念被提了出来。</p><p>GPUDirect Async Kernel-Initiated Network整体的逻辑如下:<br><img src="/images/2025/07/005.png" alt></p><p>GPUDirect Async Kernel-Initiated Network消除了CPU在通信控制路径中的作用。</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>The following is an example diagram of a CPU-centric approach:<br><img src="/images/2025/07/006.webp" alt></p><p>The following is an example diagram of a GPU-centric approach:<br><img src="/images/2025/07/007.webp" alt></p><p>在此处，网卡既可以是RDMA网卡，也可以是Ethernet网卡。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/images/2025/07/008.png" alt></p><ol><li>对于GDR，GPUDirect Async Kernel-Initiated Network可以offload critical控制路径到GPU中(也就是<a href="/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/">IBGDA</a>)</li><li>对于GPU与Ethernet网卡的p2p，GPUDirect Async Kernel-Initiated Network可以offload critical控制路径到GPU中</li></ol><h2 id="GPUDirect-Async-Kernel-Initiated-Storage"><a href="#GPUDirect-Async-Kernel-Initiated-Storage" class="headerlink" title="GPUDirect Async Kernel Initiated Storage"></a>GPUDirect Async Kernel Initiated Storage</h2><p><a href="https://mp.weixin.qq.com/s/l0PxXtDV8trFyi87Ucerww" target="_blank" rel="noopener">BaM</a>是首个以加速器为中心的方法，使GPU能够按需访问存储在内存或存储设备中的数据，而无需依赖CPU来发起或触发这些访问。</p><h3 id="BaM架构"><a href="#BaM架构" class="headerlink" title="BaM架构"></a>BaM架构</h3><p>BaM的<strong>设计目标</strong>是为GPU线程提供高效的存储访问抽象，以便其能够按需、细粒度且高吞吐量地访问存储设备，同时提升存储访问性能。为此，如下图所示，BaM在GPU内存中配置了专门的存储I/O队列和缓冲区，并借助GPU的内存映射功能，将存储DB reg映射到GPU地址空间。</p><p><img src="/images/2025/07/003.webp" alt></p><h3 id="与传统GDS对比"><a href="#与传统GDS对比" class="headerlink" title="与传统GDS对比"></a>与传统GDS对比</h3><p><img src="/images/2025/07/001.webp" alt></p><p>与传统的存储数据访问模式（GDS）相比，BaM带来了显著的变革，使GPU线程能够直接访问存储，从而实现了细粒度的计算与I/O重叠。这一设计理念带来了多方面的优势。</p><ul><li>首先，减少CPU-GPU同步开销，以及GPU内核的启动频率，从而消除了每次数据访问时CPU的启动和调度需求。</li><li>其次，降低I/O放大开销，由于CPU调度通常以大块数据任务为单位，而非GPU实际所需的随机数据，BaM中GPU线程仅在需要时才获取，因此有效避免了IO放大问题。</li><li>最后，简化编程并隐藏延迟，过去，为了处理不同规模的数据，开发人员可能需要计算应用层面的复杂数据分块和分割策略；而Bam允许程序员通过数组抽象自然地访问数据，并<u>利用GPU线程在大规模数据集上的并行性来隐藏存储访问延迟</u>，从而简化了编程逻辑。</li></ul><h3 id="BaM与NVIDIA-GDS的性能对比"><a href="#BaM与NVIDIA-GDS的性能对比" class="headerlink" title="BaM与NVIDIA GDS的性能对比"></a>BaM与NVIDIA GDS的性能对比</h3><p><img src="/images/2025/07/002.webp" alt></p><p>BaM与NVIDIA GDS的性能对比显示：当访问粒度小于32KB时，受传统 CPU 软件栈开销限制，GDS无法使PCIe接口饱和。相比之下，BaM 即使在4KB的I/O粒度下也能使接口饱和（约25GBps）。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>对于GDS，GPUDirect Async Kernel-Initiated Storage可以offload critical控制路径到GPU中</p><h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><p>GPU正朝着更高自主性和异步性的趋势发展。GPUDirect Async技术族在将数据从内存或存储直接移动到GPU内存时，可加速控制路径。</p><hr><p>参考资料:</p><ol><li><a href="https://downloads.openfabrics.org/ofv/ofv_presentation_GPU.pdf" target="_blank" rel="noopener">OFVWG:GPUDirect and PeerDirect</a></li><li><a href="https://docs.nvidia.com/doca/sdk/doca+gpunetio/index.html" target="_blank" rel="noopener">DOCA GPUNetIO</a></li><li><a href="https://joyxu.github.io/2022/06/06/gpu-direct/" target="_blank" rel="noopener">gpu-direct</a></li><li><a href="http://blog.chinaunix.net/uid-28541347-id-5886592.html" target="_blank" rel="noopener">GPU Direct相关技术和原理</a></li><li>GPU-Initiated On-Demand High-Throughput Storage Access in the BaM System Architecture(ASPLOS’23)</li><li><a href="http://nvidia.zhidx.com/index.php?m=content&amp;c=index&amp;a=show&amp;catid=6&amp;id=3190" target="_blank" rel="noopener">使用 NVIDIA DOCA GPUNetIO 实现实时网络处理功能</a></li><li><a href="https://github.com/NVIDIA/nccl/issues/1380" target="_blank" rel="noopener">Does NCCL support DOCA GPUNetIO?</a></li><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0743731517303386" target="_blank" rel="noopener">GPUDirect Async: Exploring GPU synchronous communication techniques for InfiniBand clusters</a></li><li><a href="https://zhuanlan.zhihu.com/p/430101220" target="_blank" rel="noopener">【研究综述】浅谈GPU通信和PCIe P2P DMA</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下GPUDirect Async family技术的相关notes。
    
    </summary>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/categories/AI-Infra/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/tags/AI-Infra/"/>
    
  </entry>
  
</feed>
