<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2019-04-21T10:02:55.501Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CPU cache</title>
    <link href="http://liujunming.github.io/2019/04/20/CPU-cache/"/>
    <id>http://liujunming.github.io/2019/04/20/CPU-cache/</id>
    <published>2019-04-20T10:30:09.000Z</published>
    <updated>2019-04-21T10:02:55.501Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-为什么要有CPU-Cache"><a href="#1-为什么要有CPU-Cache" class="headerlink" title="1. 为什么要有CPU Cache"></a>1. 为什么要有CPU Cache</h2><ol><li>CPU的处理速度和内存的访问速度差距</li><li>内存数据访问的局部性原理<a id="more"></a><img src="/images/2019/4/8.png" alt=""><center>Cache Performance</center></li></ol><h2 id="2-为什么要有多级CPU-Cache"><a href="#2-为什么要有多级CPU-Cache" class="headerlink" title="2. 为什么要有多级CPU Cache"></a>2. 为什么要有多级CPU Cache</h2><blockquote><p>Soon after the introduction of the cache the system got more complicated. The speed difference between the cache and the main memory increased again, to a point that another level of cache was added, bigger and slower than the first-level cache. Only increasing the size of the first-level cache was not an option for economical reasons.</p></blockquote><p>此外，又由于程序指令和程序数据的行为和热点分布差异很大，因此L1 Cache也被划分成L1i (i for instruction)和L1d (d for data)两种专门用途的缓存。</p><h2 id="3-CPU-Cache-是如何存放数据的"><a href="#3-CPU-Cache-是如何存放数据的" class="headerlink" title="3. CPU Cache 是如何存放数据的"></a>3. CPU Cache 是如何存放数据的</h2><p>如果对这部分知识有些遗忘，可以看下<a href="https://courses.cs.washington.edu/courses/cse378/09wi/lectures/lec15.pdf" target="_blank" rel="noopener">cse378</a>即可。</p><h3 id="3-1-为什么Cache不能做成Direct-Mapped"><a href="#3-1-为什么Cache不能做成Direct-Mapped" class="headerlink" title="3.1 为什么Cache不能做成Direct Mapped"></a>3.1 为什么Cache不能做成Direct Mapped</h3><p><img src="/images/2019/4/7.png" alt=""></p><blockquote><p>Direct-Mapped Cache is simplier (requires just one comparator and one multiplexer), as a result is cheaper and works faster. Given any address, it is easy to identify the single entry in cache, where it can be. A major drawback when using DM cache is called a conflict miss, when two different addresses correspond to one entry in the cache. Even if the cache is big and contains many stale entries, it can’t simply evict those, because the position within cache is predetermined by the address.</p></blockquote><h3 id="3-2-为什么Cache不能做成Fully-Associative"><a href="#3-2-为什么Cache不能做成Fully-Associative" class="headerlink" title="3.2 为什么Cache不能做成Fully Associative"></a>3.2 为什么Cache不能做成Fully Associative</h3><blockquote><p>Full Associative Cache is much more complex, and it allows to store an address into any entry. There is a price for that. In order to check if a particular address is in the cache, it has to compare all current entries (the tags to be exact).</p></blockquote><p><img src="/images/2019/4/9.png" alt=""></p><h3 id="3-3-什么是N-Way-Set-Associative"><a href="#3-3-什么是N-Way-Set-Associative" class="headerlink" title="3.3 什么是N-Way Set Associative"></a>3.3 什么是N-Way Set Associative</h3><p><img src="/images/2019/4/10.png" alt=""></p><p>阅读<a href="http://igoro.com/archive/gallery-of-processor-cache-effects/" target="_blank" rel="noopener">Gallery of Processor Cache Effects</a>和<a href="https://coolshell.cn/articles/10249.html" target="_blank" rel="noopener">7个示例科普CPU CACHE</a>可以加深对N-Way Set Associative的理解。</p><hr><p>参考资料：</p><ol><li><a href="https://courses.cs.washington.edu/courses/cse378/09wi/lectures/lec15.pdf" target="_blank" rel="noopener">cse378</a></li><li><a href="http://cenalulu.github.io/linux/all-about-cpu-cache/" target="_blank" rel="noopener">关于CPU Cache – 程序猿需要知道的那些事</a></li><li><a href="http://igoro.com/archive/gallery-of-processor-cache-effects/" target="_blank" rel="noopener">Gallery of Processor Cache Effects</a></li><li><a href="https://coolshell.cn/articles/10249.html" target="_blank" rel="noopener">7个示例科普CPU CACHE</a></li><li><a href="https://stackoverflow.com/questions/30097648/difference-between-a-direct-mapped-cache-and-fully-associative-cache" target="_blank" rel="noopener">Difference Between a Direct-Mapped Cache and Fully Associative Cache</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-为什么要有CPU-Cache&quot;&gt;&lt;a href=&quot;#1-为什么要有CPU-Cache&quot; class=&quot;headerlink&quot; title=&quot;1. 为什么要有CPU Cache&quot;&gt;&lt;/a&gt;1. 为什么要有CPU Cache&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;CPU的处理速度和内存的访问速度差距&lt;/li&gt;
&lt;li&gt;内存数据访问的局部性原理
    
    </summary>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="计算机系统" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>(转)CPU Cache Line伪共享问题的总结和分析</title>
    <link href="http://liujunming.github.io/2019/04/10/%E8%BD%AC-CPU-Cache-Line%E4%BC%AA%E5%85%B1%E4%BA%AB%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93%E5%92%8C%E5%88%86%E6%9E%90/"/>
    <id>http://liujunming.github.io/2019/04/10/转-CPU-Cache-Line伪共享问题的总结和分析/</id>
    <published>2019-04-10T08:45:04.000Z</published>
    <updated>2019-04-16T03:56:03.449Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自：<a href="https://mp.weixin.qq.com/s/RHIjnQDqC-2AOjjBAoMKMg" target="_blank" rel="noopener">CPU Cache Line伪共享问题的总结和分析</a></p><h2 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h2><p>要搞清楚 Cache Line 伪共享的概念及其性能影响，需要对现代理器架构和硬件实现有一个基本的了解。 <a id="more"></a> </p><h3 id="1-1-存储器层次结构"><a href="#1-1-存储器层次结构" class="headerlink" title="1.1 存储器层次结构"></a>1.1 存储器层次结构</h3><p>众所周知，现代计算机体系结构，通过存储器层次结构 (Memory Hierarchy) 的设计，使系统在性能，成本和制造工艺之间作出取舍，从而达到一个平衡。<br>下图给出了不同层次的硬件访问延迟，可以看到，各个层次硬件访问延迟存在数量级上的差异，越高的性能，往往意味着更高的成本和更小的容量：<br><img src="/images/2019/4/1.jpg" alt=""></p><p>通过上图，可以对各级存储器 Cache Miss 带来的性能惩罚有个大致的概念。</p><h3 id="1-2-多核架构"><a href="#1-2-多核架构" class="headerlink" title="1.2 多核架构"></a>1.2 多核架构</h3><p>随着多核架构的普及，对称多处理器 (SMP) 系统成为主流。例如，一个物理 CPU 可以存在多个物理 Core，而每个 Core 又可以存在多个硬件线程。<br>x86 以下图为例，1 个 x86 CPU 有 4 个物理 Core，每个 Core 有两个 HT (Hyper Thread)，</p><p><img src="/images/2019/4/2.png" alt=""></p><p>从硬件的角度，上图的 L1 和 L2 Cache 都被两个 HT 共享，且在同一个物理 Core。而 L3 Cache 则在物理 CPU 里，被多个 Core 来共享。<br>而从 OS 内核角度，每个 HT 都是一个逻辑 CPU，因此，这个处理器在 OS 来看，就是一个 8 个 CPU 的 SMP 系统。</p><h3 id="1-3-NUMA-架构"><a href="#1-3-NUMA-架构" class="headerlink" title="1.3 NUMA 架构"></a>1.3 NUMA 架构</h3><p>按照 CPU 和内存的互连方式，可以分为 UMA (均匀内存访问) 和 NUMA (非均匀内存访问) 两种架构。<br>其中，在多个物理 CPU 之间保证 Cache 一致性的 NUMA 架构，又被称做 ccNUMA (Cache Coherent NUMA) 架构。值得注意的是：SMP也被称为UMA。</p><p>以 x86 为例，早期的 x86 就是典型的 UMA 架构。例如下图，四路处理器通过 FSB (前端系统总线) 和主板上的内存控制器芯片 (MCH) 相连，DRAM 是以 UMA 方式组织的，延迟并无访问差异。</p><p><img src="/images/2019/4/3.png" alt=""></p><p>然而，这种架构带来了严重的内存总线的性能瓶颈，影响了 x86 在多路服务器上的可扩展性和性能。<a href="http://cenalulu.github.io/linux/numa/" target="_blank" rel="noopener">NUMA架构的CPU – 你真的用好了么？</a></p><p>因此，从 Nehalem 架构开始，x86 开始转向 NUMA 架构，内存控制器芯片被集成到处理器内部，多个处理器通过 QPI 链路相连，从此 DRAM 有了远近之分。<br>而 Sandybridge 架构则更近一步，将片外的 IOH 芯片也集成到了处理器内部，至此，内存控制器和 PCIe Root Complex 全部在处理器内部了。 </p><p>下图就是一个典型的 x86 的 NUMA 架构：</p><p><img src="/images/2019/4/4.png" alt=""></p><p>由于 NUMA 架构的引入，以下主要部件产生了因物理链路的远近带来的延迟差异：</p><ul><li>Cache</li></ul><p>除物理 CPU 有本地的 Cache 的层级结构以外，还存在跨越系统总线 (QPI) 的远程 Cache 命中访问的情况。需要注意的是，远程的 Cache 命中，对发起 Cache 访问的 CPU 来说，还是被记入了 LLC Cache Miss。</p><ul><li>DRAM</li></ul><p>在两路及以上的服务器，远程 DRAM 的访问延迟，远远高于本地 DRAM 的访问延迟，有些系统可以达到 2 倍的差异。<br>需要注意的是，即使服务器 BIOS 里关闭了 NUMA 特性，也只是对 OS 内核屏蔽了这个特性，这种延迟差异还是存在的。</p><ul><li>Device</li></ul><p>对 CPU 访问设备内存，及设备发起 DMA 内存的读写活动而言，存在本地 Device 和远程 Device 的差别，有显著的延迟访问差异。</p><p>因此，对以上 NUMA 系统，一个 NUMA 节点通常可以被认为是一个物理 CPU 加上它本地的 DRAM 和 Device 组成。那么，四路服务器就拥有四个 NUMA 节点。<br>如果 BIOS 打开了 NUMA 支持，Linux 内核则会根据 ACPI 提供的表格，针对 NUMA 节点做一系列的 NUMA 亲和性的优化。</p><p>在 Linux 上，<code>numactl --hardware</code> 可以返回当前系统的 NUMA 节点信息，特别是 CPU 和 NUMA 节点的对应信息。</p><h3 id="1-4-Cache-的结构"><a href="#1-4-Cache-的结构" class="headerlink" title="1.4 Cache 的结构"></a>1.4 Cache 的结构</h3><p>Cache Line 是 CPU 和主存之间数据传输的最小单位。当一行 Cache Line 被从内存拷贝到 Cache 里，Cache 里会为这个 Cache Line 创建一个条目。<br>这个 Cache 条目里既包含了拷贝的内存数据，即 Cache Line，又包含了这行数据在内存里的位置等元数据信息。</p><p>详情可以参考<a href="https://courses.cs.washington.edu/courses/cse378/09wi/lectures/lec15.pdf" target="_blank" rel="noopener">cse378</a></p><p>Cache Line 的大小和处理器硬件架构有关。在 Linux 上，通过 <code>getconf</code> 就可以拿到 CPU 的 Cache Line 的大小。</p><p><img src="/images/2019/4/5.png" alt=""></p><p>除了 *_LINESIZE 指示了系统的 Cache Line 的大小是 64 字节外，还给出了 Cache 类别，大小。<br>其中 *_ASSOC 则指示了该 Cache 是几路关联 (Way Associative) 的。</p><h3 id="1-5-Cache-一致性"><a href="#1-5-Cache-一致性" class="headerlink" title="1.5 Cache 一致性"></a>1.5 Cache 一致性</h3><p>如前所述，在 SMP 系统里，每个 CPU 都有自己本地的 Cache。因此，同一个变量，或者同一行 Cache Line，有在多个处理器的本地 Cache 里存在多份拷贝的可能性，因此就存在数据一致性问题。<br>通常，处理器都实现了 Cache 一致性 (Cache Coherence）协议。如历史上 x86 曾实现了 MESI 协议以及 MESIF 协议。</p><p>假设两个处理器 A 和 B, 都在各自本地 Cache Line 里有同一个变量的拷贝时，此时该 Cache Line 处于 Shared 状态。当处理器 A 在本地修改了变量，除去把本地变量所属的 Cache Line 置为 Modified 状态以外，<br>还必须在另一个处理器 B 读同一个变量前，对该变量所在的 B 处理器本地 Cache Line 发起 Invaidate 操作，标记 B 处理器的那条 Cache Line 为 Invalidate 状态。<br>随后，若处理器 B 在对变量做读写操作时，如果遇到这个标记为 Invalidate 的状态的 Cache Line，即会引发 Cache Miss，<br>从而将内存中最新的数据拷贝到 Cache Line 里，然后处理器 B 再对此 Cache Line 对变量做读写操作。</p><p>本文中的 Cache Line 伪共享场景，就基于上述场景来讲解，关于 Cache 一致性协议更多的细节，请参考相关文档。</p><h3 id="1-6-Cache-Line-伪共享"><a href="#1-6-Cache-Line-伪共享" class="headerlink" title="1.6 Cache Line 伪共享"></a>1.6 Cache Line 伪共享</h3><p>Cache Line 伪共享问题，就是由多个 CPU 上的多个线程同时修改自己的变量引发的。这些变量表面上是不同的变量，但是实际上却存储在同一条 Cache Line 里。<br>在这种情况下，由于 Cache 一致性协议，两个处理器都存储有相同的 Cache Line 拷贝的前提下，本地 CPU 变量的修改会导致本地 Cache Line 变成 Modified 状态，然后在其它共享此 Cache Line 的 CPU 上，<br>引发 Cache Line 的 Invaidate 操作，导致 Cache Line 变为 Invalidate 状态，从而使 Cache Line 再次被访问时，发生本地 Cache Miss，从而伤害到应用的性能。<br>在此场景下，多个线程在不同的 CPU 上高频反复访问这种 Cache Line 伪共享的变量，则会因 Cache 颠簸引发严重的性能问题。</p><p>下图即为两个线程间的 Cache Line 伪共享问题的示意图。</p><p><img src="/images/2019/4/6.png" alt=""></p><h2 id="2-Perf-c2c-发现伪共享"><a href="#2-Perf-c2c-发现伪共享" class="headerlink" title="2 Perf c2c 发现伪共享"></a>2 Perf c2c 发现伪共享</h2><p>当应用在 NUMA 环境中运行，或者应用是多线程的，又或者是多进程间有共享内存，满足其中任意一条，那么这个应用就可能因为 Cache Line 伪共享而性能下降。</p><p>但是，要怎样才能知道一个应用是不是受伪共享所害呢？<a href="https://joemario.github.io/blog/2016/09/01/c2c-blog/" target="_blank" rel="noopener">Joe Mario</a> 提交的 patch 能够解决这个问题。Joe 的 patch 是在 Linux 的著名的 perf 工具上，添加了一些新特性，叫做 c2c，意思是“缓存到缓存” (cache-2-cache)。</p><p>Redhat 在很多 Linux 的大型应用上使用了 c2c 的原型，成功地发现了很多热的伪共享的 Cache Line。<br>Joe 在博客里总结了一下 perf c2c 的主要功能：</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自：&lt;a href=&quot;https://mp.weixin.qq.com/s/RHIjnQDqC-2AOjjBAoMKMg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CPU Cache Line伪共享问题的总结和分析&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-背景知识&quot;&gt;&lt;a href=&quot;#1-背景知识&quot; class=&quot;headerlink&quot; title=&quot;1 背景知识&quot;&gt;&lt;/a&gt;1 背景知识&lt;/h2&gt;&lt;p&gt;要搞清楚 Cache Line 伪共享的概念及其性能影响，需要对现代理器架构和硬件实现有一个基本的了解。
    
    </summary>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="计算机系统" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>ppt制作资源集锦</title>
    <link href="http://liujunming.github.io/2019/01/08/ppt%E5%88%B6%E4%BD%9C%E8%B5%84%E6%BA%90%E9%9B%86%E9%94%A6/"/>
    <id>http://liujunming.github.io/2019/01/08/ppt制作资源集锦/</id>
    <published>2019-01-08T04:48:29.000Z</published>
    <updated>2019-01-12T04:02:47.416Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录ppt制作过程中的心得以及遇到的优质资源，将持续更新。</p><a id="more"></a><h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2><p>学术、技术报告：</p><p>英文 <code>Arial Unicode MS</code></p><p>中文 <code>微软雅黑</code></p><h2 id="配色"><a href="#配色" class="headerlink" title="配色"></a>配色</h2><p><a href="https://brandcolors.net/" target="_blank" rel="noopener">美国世界知名品牌颜色收集网</a></p><p><img src="/images/2019/1/23.png" alt=""></p><h2 id="表情"><a href="#表情" class="headerlink" title="表情"></a>表情</h2><p>在说明问题的优缺点时，可以添加下表情。</p><p><a href="https://emojipedia.org/apple/" target="_blank" rel="noopener">emoji表情</a></p><p><img src="/images/2019/1/24.png" alt=""></p><h2 id="图片素材"><a href="#图片素材" class="headerlink" title="图片素材"></a>图片素材</h2><ul><li>Google图片</li></ul><p>有时候，ppt中需要添加一些高大上图片，比如需要添加数据中心的图片，此刻，可以去谷歌中搜索<code>datacenter</code>。</p><p><img src="/images/2019/1/25.png" alt=""></p><ul><li><a href="https://www.pexels.com/" target="_blank" rel="noopener">pexels</a></li></ul><p>  免费高清素材下载网站</p><ul><li><a href="https://pixabay.com/" target="_blank" rel="noopener">pixabay</a></li></ul><p>免费高清矢量图片资源</p><ul><li><a href="https://www.ssyer.com/home" target="_blank" rel="noopener">莎莎野</a></li></ul><p>免费高清素材下载网站</p><ul><li><a href="https://www.flaticon.com/" target="_blank" rel="noopener">flaticon</a></li></ul><p>免费图标下载资源</p><ul><li><a href="https://www.iconfont.cn/" target="_blank" rel="noopener">阿里巴巴图标素材库</a></li></ul><h2 id="ppt排版"><a href="#ppt排版" class="headerlink" title="ppt排版"></a>ppt排版</h2><ul><li><a href="https://www.pinterest.com/a0802725pinyu/ppt-typesetting/" target="_blank" rel="noopener">PPT typesetting</a></li><li><a href="http://huaban.com/boards/19375308/" target="_blank" rel="noopener">观海PPT收集</a></li><li><a href="https://www.zhihu.com/question/47791746/answer/108730202" target="_blank" rel="noopener">如何做好一张包括一段字和几张图片的ppt,应该如何排版?</a></li></ul><h2 id="神器"><a href="#神器" class="headerlink" title="神器"></a>神器</h2><ul><li>PHOTOZOOM PRO</li></ul><p>做PPT时，我们经常需要用到图片，一张清晰出彩的图片可以为演示增分不少，而往往我们找到一张称心的图片，却发现分辨率过低，插入到PPT中变得模糊，着实令人懊恼。PHOTOZOOM PRO，就是为解决这个痛点而生的。</p><ul><li>PPTMinimizer</li></ul><p>PPT压缩软件</p><ul><li>iSlide 插件</li><li>文字云制作工具</li></ul><p><a href="https://wordart.com/" target="_blank" rel="noopener">Word Cloud Art Creator</a>，是一个支持中文的文字云生成工具</p><ul><li>Collagelt</li></ul><p>这款软件就是可以轻松将很多图片拼接在一起的一款工具。</p><h2 id="PPT-模板下载网站"><a href="#PPT-模板下载网站" class="headerlink" title="PPT 模板下载网站"></a>PPT 模板下载网站</h2><ul><li><a href="http://www.officeplus.cn/Template/Home.shtml" target="_blank" rel="noopener">officeplus</a></li><li><a href="https://slidemodel.com/" target="_blank" rel="noopener">slidemodel</a></li><li><a href="https://www.presentationmagazine.com/" target="_blank" rel="noopener">presentationmagazine</a></li><li><a href="https://slidehunter.com/" target="_blank" rel="noopener">slidehunter</a></li><li><a href="https://www.free-powerpoint-templates-design.com/" target="_blank" rel="noopener">AllPPT</a></li></ul><h2 id="keynote"><a href="#keynote" class="headerlink" title="keynote"></a>keynote</h2><p><a href="http://keynotetemplate.com/" target="_blank" rel="noopener">keynote template</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录ppt制作过程中的心得以及遇到的优质资源，将持续更新。&lt;/p&gt;
    
    </summary>
    
      <category term="设计" scheme="http://liujunming.github.io/categories/%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="设计" scheme="http://liujunming.github.io/tags/%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-The Page Cache</title>
    <link href="http://liujunming.github.io/2019/01/04/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-The-Page-Cache/"/>
    <id>http://liujunming.github.io/2019/01/04/Understanding-the-Linux-Kernel-读书笔记-The-Page-Cache/</id>
    <published>2019-01-04T02:36:01.000Z</published>
    <updated>2019-01-04T02:44:18.527Z</updated>
    
    <content type="html"><![CDATA[<p>A disk cache is a software mechanism that allows the system to keep in RAM some data that is normally stored on a disk, so that further accesses to that data can be sat- isfied quickly without accessing the disk.<br><a id="more"></a><br>The dentry cache, which stores dentry objects representing filesystem pathnames, and the inode cache, which stores inode objects representing disk inodes. The <em>page cache</em>, which is a disk cache working on whole pages of data.</p><h2 id="1-The-Page-Cache"><a href="#1-The-Page-Cache" class="headerlink" title="1 The Page Cache"></a>1 The Page Cache</h2><p>The <em>page cache</em> is the main disk cache used by the Linux kernel. In most cases, the kernel refers to the page cache when reading from or writing to disk. New pages are added to the page cache to satisfy User Mode processes’s read requests. If the page is not already in the cache, a new entry is added to the cache and filled with the data read from the disk.</p><p>Kernel designers have implemented the page cache to fulfill two main requirements:</p><ul><li>Quickly locate a specific page containing data relative to a given owner. </li><li>Keep track of how every page in the cache should be handled when reading or writing its content.</li></ul><p>A page does not necessarily contain physically adjacent disk blocks, so it cannot be identified by a device number and a block number. Instead, a page in the page cache is identified by an owner and by an index within the owner’s data—usually, an inode and an offset inside the corresponding file.</p><h3 id="1-1-The-address-space-Object"><a href="#1-1-The-address-space-Object" class="headerlink" title="1.1 The address_space Object"></a>1.1 The address_space Object</h3><p><a href="http://liujunming.top/2017/06/25/address-space-%E9%A1%B5%E9%AB%98%E9%80%9F%E7%BC%93%E5%AD%98/" target="_blank" rel="noopener">address_space,页高速缓存</a><br>The core data structure of the page cache is the <code>address_space</code> object, a data structure embedded in the inode object that owns the page. Many pages in the cache may refer to the same owner, thus they may be linked to the same <code>address_space</code> object. This object also establishes a link between the owner’s pages and a set of methods that operate on these pages.</p><h3 id="1-2-The-Radix-Tree"><a href="#1-2-The-Radix-Tree" class="headerlink" title="1.2 The Radix Tree"></a>1.2 The Radix Tree</h3><h3 id="1-3-Page-Cache-Handling-Functions"><a href="#1-3-Page-Cache-Handling-Functions" class="headerlink" title="1.3 Page Cache Handling Functions"></a>1.3 Page Cache Handling Functions</h3><h3 id="1-4-The-Tags-of-the-Radix-Tree"><a href="#1-4-The-Tags-of-the-Radix-Tree" class="headerlink" title="1.4 The Tags of the Radix Tree"></a>1.4 The Tags of the Radix Tree</h3><h2 id="2-Storing-Blocks-in-the-Page-Cache"><a href="#2-Storing-Blocks-in-the-Page-Cache" class="headerlink" title="2 Storing Blocks in the Page Cache"></a>2 Storing Blocks in the Page Cache</h2><p><em>page cache</em> vs <em>buffer cache</em>.</p><p>Starting from stable version 2.4.10, the buffer cache does not really exist anymore. In fact, for reasons of efficiency, block buffers are no longer allocated individually; instead, they are stored in dedicated pages called “buffer pages,” which are kept in the page cache.</p><p>A buffer page is a page of data associated with additional descriptors called “buffer heads,” whose main purpose is to quickly locate the disk address of each individual block in the page. In fact, the chunks of data stored in a page belonging to the page cache are not necessarily adjacent on disk.</p><h3 id="2-1-Block-Buffers-and-Buffer-Heads"><a href="#2-1-Block-Buffers-and-Buffer-Heads" class="headerlink" title="2.1 Block Buffers and Buffer Heads"></a>2.1 Block Buffers and Buffer Heads</h3><h3 id="2-2-Managing-the-Buffer-Heads"><a href="#2-2-Managing-the-Buffer-Heads" class="headerlink" title="2.2 Managing the Buffer Heads"></a>2.2 Managing the Buffer Heads</h3><h3 id="2-3-Buffer-Pages"><a href="#2-3-Buffer-Pages" class="headerlink" title="2.3 Buffer Pages"></a>2.3 Buffer Pages</h3><p><img src="/images/2019/1/21.png" alt=""></p><p><a href="http://sundayhut.is-programmer.com/posts/62477" target="_blank" rel="noopener">buffer_head理解、解析</a></p><p>bio 相对 buffer_head 的好处有：bio 可以更方便的使用高端内存，因为它只与 page 打交道，并不直接使用地址。bio 可以表示 direct I/O。对向量形式的 I/O支持更好，防止 I/O 被打散。但是 buffer_head 还是需要的，它用于映射磁盘块到内存，因为 bio 中并没有包含 kernel 需要的 buffer 状态的成员以及一些其它信息。</p><h2 id="3-Writing-Dirty-Pages-to-Disk"><a href="#3-Writing-Dirty-Pages-to-Disk" class="headerlink" title="3 Writing Dirty Pages to Disk"></a>3 Writing Dirty Pages to Disk</h2><hr><p>参考资料：</p><ol><li><a href="http://sundayhut.is-programmer.com/posts/62477" target="_blank" rel="noopener">buffer_head理解、解析</a></li><li><a href="http://www.ilinuxkernel.com/files/Linux.Generic.Block.Layer.pdf" target="_blank" rel="noopener">Linux通用块设备层</a></li><li><a href="https://zohead.com/archives/linux-kernel-learning-block-layer/" target="_blank" rel="noopener">Linux kernel学习-block层</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A disk cache is a software mechanism that allows the system to keep in RAM some data that is normally stored on a disk, so that further accesses to that data can be sat- isfied quickly without accessing the disk.&lt;br&gt;
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-Block Device Drivers</title>
    <link href="http://liujunming.github.io/2019/01/03/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Block-Device-Drivers/"/>
    <id>http://liujunming.github.io/2019/01/03/Understanding-the-Linux-Kernel-读书笔记-Block-Device-Drivers/</id>
    <published>2019-01-03T05:12:09.000Z</published>
    <updated>2019-01-03T10:14:56.327Z</updated>
    
    <content type="html"><![CDATA[<p>We start in the first section “Block Devices Handling” to explain the general architecture of the Linux block I/O subsystem. In the sections “The Generic Block Layer,” “The I/O Scheduler,” and “Block Device Drivers,” we will describe the main components of the block I/O subsystem. Finally, in the last section, “Opening a Block Device File,” we will outline the steps performed by the kernel when opening a block device file.<a id="more"></a></p><p><a href="http://www.ilinuxkernel.com/files/IO.Data.Structure.pdf" target="_blank" rel="noopener">Linux内核I/O流程主要数据结构</a></p><p>阅读时注意request的上下文，可能代表普遍意义上的请求，也可能代表<code>request</code>结构体。</p><h2 id="1-Block-Devices-Handling"><a href="#1-Block-Devices-Handling" class="headerlink" title="1 Block Devices Handling"></a>1 Block Devices Handling</h2><p>Each operation on a block device driver involves a large number of kernel components; the most important ones are shown in Figure 14-1.<br><img src="/images/2019/1/16.png" alt=""><br>Let us suppose, for instance, that a process issued a read() system call on some disk file. Here is what the kernel typically does to service the process request:</p><ol><li>The service routine of the <code>read()</code> system call activates a suitable VFS function, passing to it a file descriptor and an offset inside the file. </li><li>The VFS function determines if the requested data is already available and, if necessary, how to perform the read operation.</li><li>Let’s assume that the kernel must read the data from the block device, thus it must determine the physical location of that data. To do this, the kernel relies on the <em>mapping layer</em>, which typically executes two steps:<ul><li>It determines the block size of the filesystem including the file and computes the extent of the requested data in terms of <em>file block numbers</em>. Essentially, the file is seen as split in many blocks, and the kernel determines the numbers (indices relative to the beginning of file) of the blocks containing the requested data.</li><li>Next, the mapping layer invokes a filesystem-specific function that accesses the file’s disk inode and determines the position of the requested data on disk in terms of <em>logical block numbers</em>. Essentially, the disk is seen as split in blocks, and the kernel determines the numbers (indices relative to the beginning of the disk or partition) corresponding to the blocks storing the requested data. Because a file may be stored in nonadjacent blocks on disk, a data structure stored in the disk inode maps each file block number to a logical block number.</li></ul></li><li>The kernel can now issue the read operation on the block device. It makes use of the <em>generic block layer</em>, which starts the I/O operations that transfer the requested data. In general, each I/O operation involves a group of blocks that are adjacent on disk. Because the requested data is not necessarily adjacent on disk, the generic block layer might start several I/O operations. Each I/O operation is represented by a “block I/O” (in short, “bio”) structure, which collects all information needed by the lower components to satisfy the request.<br>The generic block layer hides the peculiarities of each hardware block device, thus offering an abstract view of the block devices. Because almost all block devices are disks, the generic block layer also provides some general data structures that describe “disks” and “disk partitions.” </li><li>Below the generic block layer, the “I/O scheduler” sorts the pending I/O data transfer requests according to predefined kernel policies. The purpose of the scheduler is to group requests of data that lie near each other on the physical medium.</li><li>Finally, the <em>block device drivers</em> take care of the actual data transfer by sending suitable commands to the hardware interfaces of the disk controllers.</li></ol><p>As you can see, there are many kernel components that are concerned with data stored in block devices; each of them manages the disk data using chunks of different length:</p><ul><li>The controllers of the hardware block devices transfer data in chunks of fixed length called “sectors.” Therefore, the I/O scheduler and the block device drivers must manage sectors of data.</li><li>The Virtual Filesystem, the mapping layer, and the filesystems group the disk data in logical units called “blocks.” A block corresponds to the minimal disk storage unit inside a filesystem.</li><li>Block device drivers should be able to cope with “segments” of data: each segment is a memory page—or a portion of a memory page—including chunks of data that are physically adjacent on disk.</li><li>The disk caches work on “pages” of disk data, each of which fits in a page frame.</li><li>The generic block layer glues together all the upper and lower components, thus it knows about sectors, blocks, segments, and pages of data.</li></ul><p><img src="/images/2019/1/17.png" alt=""></p><h3 id="1-1-Sectors"><a href="#1-1-Sectors" class="headerlink" title="1.1 Sectors"></a>1.1 Sectors</h3><h3 id="1-2-Blocks"><a href="#1-2-Blocks" class="headerlink" title="1.2 Blocks"></a>1.2 Blocks</h3><h3 id="1-3-Segments"><a href="#1-3-Segments" class="headerlink" title="1.3 Segments"></a>1.3 Segments</h3><h2 id="2-The-Generic-Block-Layer"><a href="#2-The-Generic-Block-Layer" class="headerlink" title="2 The Generic Block Layer"></a>2 The Generic Block Layer</h2><p>The generic block layer is a kernel component that handles the requests for all block devices in the system. Thanks to its functions, the kernel may easily:</p><ul><li>Implement—with some additional effort—a “zero-copy” schema, where disk data is directly put in the User Mode address space without being copied to kernel memory first.</li><li>Manage logical volumes—such as those used by LVM(the Logical Volume Manager) and RAID (Redundant Array of Inexpensive Disks): several disk partitions, even on different block devices, can be seen as a single partition.</li><li>Exploit the advanced features of the most recent disk controllers.</li></ul><h3 id="2-1-The-Bio-Structure"><a href="#2-1-The-Bio-Structure" class="headerlink" title="2.1 The Bio Structure"></a>2.1 The Bio Structure</h3><p>The core data structure of the generic block layer is a descriptor of an ongoing I/O block device operation called <em>bio</em>. Each bio essentially includes an identifier for a disk storage area—the initial sector number and the number of sectors included in the storage area—and one or more segments describing the memory areas involved in the I/O operation. A bio is implemented by the <code>bio</code> data structure.</p><p>Each segment in a bio is represented by a <code>bio_vec</code> data structure.<br><img src="/images/2019/1/18.png" alt=""></p><h3 id="2-2-Representing-Disks-and-Disk-Partitions"><a href="#2-2-Representing-Disks-and-Disk-Partitions" class="headerlink" title="2.2 Representing Disks and Disk Partitions"></a>2.2 Representing Disks and Disk Partitions</h3><p>A <em>disk</em> is a logical block device that is handled by the generic block layer. Usually a disk corresponds to a hardware block device such as a hard disk, a floppy disk, or a CD-ROM disk. However, a disk can be a virtual device built upon several physical disk partitions, or a storage area living in some dedicated pages of RAM. In any case, the upper kernel components operate on all disks in the same way thanks to the services offered by the generic block layer.</p><p>A disk is represented by the <code>gendisk</code> object.</p><p>Hard disks are commonly split into <em>logical partitions</em>. Each block device file may represent either a whole disk or a partition inside the disk. If a disk is split in partitions, their layout is kept in an array of <code>hd_struct</code> structures whose address is stored in the <code>part</code> field of the <code>gendisk</code> object.</p><h3 id="2-3-Submitting-a-Request"><a href="#2-3-Submitting-a-Request" class="headerlink" title="2.3 Submitting a Request"></a>2.3 Submitting a Request</h3><h2 id="3-The-I-O-Scheduler"><a href="#3-The-I-O-Scheduler" class="headerlink" title="3 The I/O Scheduler"></a>3 The I/O Scheduler</h2><p><img src="/images/2019/1/20.jpg" alt=""></p><p><img src="/images/2019/1/19.png" alt=""></p><p>Although block device drivers are able to transfer a single sector at a time, the block I/O layer does not perform an individual I/O operation for each sector to be accessed on disk; this would lead to poor disk performance, because locating the physical position of a sector on the disk surface is quite time-consuming. Instead, the kernel tries, whenever possible, to cluster several sectors and handle them as a whole, thus reducing the average number of head movements.</p><p>When a kernel component wishes to read or write some disk data, it actually creates a block device request. That request essentially describes the requested sectors and the kind of operation to be performed on them (read or write). However, the kernel does not satisfy a request as soon as it is created—the I/O operation is just scheduled and will be performed at a later time.</p><p>Each block device driver maintains its own <em>request queue</em>, which contains the list of pending requests for the device. If the disk controller is handling several disks, there is usually one request queue for each physical block device. I/O scheduling is performed separately on each request queue, thus increasing disk performance.</p><h3 id="3-1-Request-Queue-Descriptors"><a href="#3-1-Request-Queue-Descriptors" class="headerlink" title="3.1 Request Queue Descriptors"></a>3.1 Request Queue Descriptors</h3><p>The <code>backing_dev_info</code> field is a small object of type <code>backing_dev_info</code>, which stores information about the I/O data flow traffic for the underlying hardware block device. For instance, it holds information about read-ahead and about request queue congestion state.</p><h3 id="3-2-Request-Descriptors"><a href="#3-2-Request-Descriptors" class="headerlink" title="3.2 Request Descriptors"></a>3.2 Request Descriptors</h3><p>每个bio到达了磁盘设备的request_queue，接下来需要对该bio进行深加工，提高IO效率。这里的关键在于将bio合并至已存在request内，所谓的合并指的是该bio所请求的io是否与当前已有request在物理磁盘块上连续，如果是，无需分配新的request，直接将该请求添加至已有request，这样一次便可传输更多数据，提升IO效率，这其实也是整个IO系统的核心所在。</p><h3 id="3-3-Activating-the-Block-Device-Driver"><a href="#3-3-Activating-the-Block-Device-Driver" class="headerlink" title="3.3 Activating the Block Device Driver"></a>3.3 Activating the Block Device Driver</h3><h3 id="3-4-I-O-Scheduling-Algorithms"><a href="#3-4-I-O-Scheduling-Algorithms" class="headerlink" title="3.4 I/O Scheduling Algorithms"></a>3.4 I/O Scheduling Algorithms</h3><p>When a new request is added to a request queue, the generic block layer invokes the I/O scheduler to determine that exact position of the new element in the queue. The I/O scheduler tries to keep the request queue sorted sector by sector. If the requests to be processed are taken sequentially from the list, the amount of disk seeking is significantly reduced because the disk head moves in a linear way from the inner track to the outer one (or vice versa) instead of jumping randomly from one track to another.</p><h3 id="3-5-Issuing-a-Request-to-the-I-O-Scheduler"><a href="#3-5-Issuing-a-Request-to-the-I-O-Scheduler" class="headerlink" title="3.5 Issuing a Request to the I/O Scheduler"></a>3.5 Issuing a Request to the I/O Scheduler</h3><h2 id="4-Block-Device-Drivers"><a href="#4-Block-Device-Drivers" class="headerlink" title="4 Block Device Drivers"></a>4 Block Device Drivers</h2><h3 id="4-1-Block-Devices"><a href="#4-1-Block-Devices" class="headerlink" title="4.1 Block Devices"></a>4.1 Block Devices</h3><h3 id="4-2-Device-Driver-Registration-and-Initialization"><a href="#4-2-Device-Driver-Registration-and-Initialization" class="headerlink" title="4.2 Device Driver Registration and Initialization"></a>4.2 Device Driver Registration and Initialization</h3><h3 id="4-3-The-Strategy-Routine"><a href="#4-3-The-Strategy-Routine" class="headerlink" title="4.3 The Strategy Routine"></a>4.3 The Strategy Routine</h3><h3 id="4-4-The-Interrupt-Handler"><a href="#4-4-The-Interrupt-Handler" class="headerlink" title="4.4 The Interrupt Handler"></a>4.4 The Interrupt Handler</h3><h2 id="5-Opening-a-Block-Device-File"><a href="#5-Opening-a-Block-Device-File" class="headerlink" title="5 Opening a Block Device File"></a>5 Opening a Block Device File</h2><hr><p>参考资料：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/39199521" target="_blank" rel="noopener">Linux IO请求处理流程-bio和request</a></li><li><a href="http://sundayhut.is-programmer.com/posts/62921.html" target="_blank" rel="noopener">bio，request，request_queue的学习</a></li><li><a href="http://www.ilinuxkernel.com/files/IO.Data.Structure.pdf" target="_blank" rel="noopener">Linux内核I/O流程主要数据结构</a></li><li><a href="http://ilinuxkernel.com/?p=1693" target="_blank" rel="noopener">Linux内核I/O调度层</a></li><li><a href="http://sundayhut.is-programmer.com/posts/49095.html" target="_blank" rel="noopener">文件系统写浅析</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We start in the first section “Block Devices Handling” to explain the general architecture of the Linux block I/O subsystem. In the sections “The Generic Block Layer,” “The I/O Scheduler,” and “Block Device Drivers,” we will describe the main components of the block I/O subsystem. Finally, in the last section, “Opening a Block Device File,” we will outline the steps performed by the kernel when opening a block device file.
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-I/O Architecture and Device Drivers</title>
    <link href="http://liujunming.github.io/2019/01/03/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-I-O-Architecture-and-Device-Drivers/"/>
    <id>http://liujunming.github.io/2019/01/03/Understanding-the-Linux-Kernel-读书笔记-I-O-Architecture-and-Device-Drivers/</id>
    <published>2019-01-03T01:50:38.000Z</published>
    <updated>2019-01-03T04:59:27.272Z</updated>
    
    <content type="html"><![CDATA[<p>In the section “I/O Architecture,” we give a brief survey of the 80×86 I/O architecture. In the section “The Device Driver Model,” we introduce the Linux device driver model.<a id="more"></a>  Next, in the section “Device Files,” we show how the VFS associates a special file called “device file” with each different hardware device, so that application programs can use all kinds of devices in the same way. We then introduce in the section “Device Drivers” some common characteristics of device drivers. Finally, in the section “Character Device Drivers,” we illustrate the overall organization of character device drivers in Linux.</p><h2 id="1-I-O-Architecture"><a href="#1-I-O-Architecture" class="headerlink" title="1 I/O Architecture"></a>1 I/O Architecture</h2><p>The data path that connects a CPU to an I/O device is generically called an <em>I/O bus</em>. The 80 × 86 microprocessors use 16 of their address pins to address I/O devices and 8, 16, or 32 of their data pins to transfer data. The I/O bus, in turn, is connected to each I/O device by means of a hierarchy of hardware components including up to three elements: I/O ports, interfaces, and device controllers. Figure 13-1 shows the components of the I/O architecture.<br><img src="/images/2019/1/12.png" alt=""></p><h3 id="1-1-I-O-Ports"><a href="#1-1-I-O-Ports" class="headerlink" title="1.1 I/O Ports"></a>1.1 I/O Ports</h3><p>Each device connected to the I/O bus has its own set of I/O addresses, which are usually called <em>I/O ports</em>.  Four special assembly language instructions called <code>in</code>, <code>ins</code>, <code>out</code>, and <code>outs</code> allow the CPU to read from and write into an I/O port.</p><p>I/O ports may also be mapped into addresses of the physical address space. </p><p>The I/O ports of each device are structured into a set of specialized registers, as shown in Figure 13-2.<br><img src="/images/2019/1/13.png" alt=""></p><h4 id="1-1-1-Accessing-I-O-ports"><a href="#1-1-1-Accessing-I-O-ports" class="headerlink" title="1.1.1 Accessing I/O ports"></a>1.1.1 Accessing I/O ports</h4><p>The <code>in</code>, <code>out</code>, <code>ins</code>, and <code>outs</code> assembly language instructions access I/O ports. </p><p>While accessing I/O ports is simple, detecting which I/O ports have been assigned to I/O devices may not be easy, in particular, for systems based on an ISA bus. Often a device driver must blindly write into some I/O port to probe the hardware device; if, however, this I/O port is already used by some other hardware device, a system crash could occur. To prevent such situations, the kernel keeps track of I/O ports assigned to each hardware device by means of “resources.”</p><h3 id="1-2-I-O-Interfaces"><a href="#1-2-I-O-Interfaces" class="headerlink" title="1.2 I/O Interfaces"></a>1.2 I/O Interfaces</h3><p>An <em>I/O interface</em> is a hardware circuit inserted between a group of I/O ports and the corresponding device controller. It acts as an interpreter that translates the values in the I/O ports into commands and data for the device. In the opposite direction, it detects changes in the device state and correspondingly updates the I/O port that plays the role of status register. This circuit can also be connected through an IRQ line to a Programmable Interrupt Controller, so that it issues interrupt requests on behalf of the device.</p><p>There are two types of interfaces:</p><ul><li><em>Custom I/O interfaces</em></li><li><em>General-purpose I/O interfaces</em></li></ul><h3 id="1-3-Device-Controllers"><a href="#1-3-Device-Controllers" class="headerlink" title="1.3 Device Controllers"></a>1.3 Device Controllers</h3><p>A complex device may require a <em>device controller</em> to drive it. Essentially, the controller plays two important roles:</p><ul><li>It interprets the high-level commands received from the I/O interface and forces the device to execute specific actions by sending proper sequences of electrical signals to it.</li><li>It converts and properly interprets the electrical signals received from the device and modifies (through the I/O interface) the value of the status register.</li></ul><p>A typical device controller is the <em>disk controller</em>.</p><p>Simpler devices do not have a device controller.</p><p>Several hardware devices include their own memory, which is often called <em>I/O shared memory</em>. </p><h2 id="2-The-Device-Driver-Model"><a href="#2-The-Device-Driver-Model" class="headerlink" title="2 The Device Driver Model"></a>2 The Device Driver Model</h2><p>Recent hardware devices, even of different classes, support similar functionalities. Drivers for such devices should typically take care of:</p><ul><li>Power management </li><li>Plug and play </li><li>Hot-plugging</li></ul><p>Linux 2.6 provides some data structures and helper functions that offer a unifying view of all buses, devices, and device drivers in the system; this framework is called the <em>device driver model</em>.</p><h3 id="2-1-The-sysfs-Filesystem"><a href="#2-1-The-sysfs-Filesystem" class="headerlink" title="2.1 The sysfs Filesystem"></a>2.1 The sysfs Filesystem</h3><p>A goal of the <code>sysfs</code> filesystem is to expose the hierarchical relationships among the components of the device driver model. The related top-level directories of this filesystem are:</p><ul><li><em>block</em></li><li><em>devices</em></li><li><em>bus</em></li><li><em>drivers</em></li><li><em>class</em></li><li><em>power</em></li><li><em>firmware</em></li></ul><h3 id="2-2-Kobjects"><a href="#2-2-Kobjects" class="headerlink" title="2.2 Kobjects"></a>2.2 Kobjects</h3><p>The core data structure of the device driver model is a generic data structure named <em>kobject</em>, which is inherently tied to the <code>sysfs</code> filesystem: each kobject corresponds to a directory in that filesystem.</p><h3 id="2-3-Components-of-the-Device-Driver-Model"><a href="#2-3-Components-of-the-Device-Driver-Model" class="headerlink" title="2.3 Components of the Device Driver Model"></a>2.3 Components of the Device Driver Model</h3><p>The device driver model is built upon a handful of basic data structures, which represent buses, devices, device drivers, etc. </p><h2 id="3-Device-Files"><a href="#3-Device-Files" class="headerlink" title="3 Device Files"></a>3 Device Files</h2><p>I/O devices are treated as special files called <em>device file</em>.</p><p>According to the characteristics of the underlying device drivers, device files can be of two types: <em>block</em> or <em>character</em>.</p><ul><li>The data of a block device can be addressed randomly, and the time needed to transfer a data block is small and roughly the same, at least from the point of view of the human user. Typical examples of block devices are hard disks, floppy disks, CD-ROM drives, and DVD players.</li><li>The data of a character device either cannot be addressed randomly, or they can be addressed randomly, but the time required to access a random datum largely depends on its position inside the device.</li></ul><p>A device file is usually a real file stored in a filesystem. Its inode, however, doesn’t need to include pointers to blocks of data on the disk (the file’s data) because there are none. Instead, the inode must include an identifier of the hardware device corresponding to the character or block device file.</p><p>Traditionally, this identifier consists of the type of device file (character or block) and a pair of numbers. The first number, called the <em>major number</em>, identifies the device type. Traditionally, all device files that have the same major number and the same type share the same set of file operations, because they are handled by the same device driver. The second number, called the <em>minor number</em>, identifies a specific device among a group of devices that share the same major number. For instance, a group of disks managed by the same disk controller have the same major number and different minor numbers.</p><p><img src="/images/2019/1/14.png" alt=""></p><h3 id="3-1-User-Mode-Handling-of-Device-Files"><a href="#3-1-User-Mode-Handling-of-Device-Files" class="headerlink" title="3.1 User Mode Handling of Device Files"></a>3.1 User Mode Handling of Device Files</h3><h3 id="3-2-VFS-Handling-of-Device-Files"><a href="#3-2-VFS-Handling-of-Device-Files" class="headerlink" title="3.2 VFS Handling of Device Files"></a>3.2 VFS Handling of Device Files</h3><p>Device files live in the system directory tree but are intrinsically different from regular files and directories. When a process accesses a regular file, it is accessing some data blocks in a disk partition through a filesystem; when a process accesses a device file, it is just driving a hardware device. </p><h2 id="4-Device-Drivers"><a href="#4-Device-Drivers" class="headerlink" title="4 Device Drivers"></a>4 Device Drivers</h2><p>A <em>device driver</em> is the set of kernel routines that makes a hardware device respond to the programming interface defined by the canonical set of VFS functions (<code>open</code>, <code>read</code>, <code>lseek</code>, <code>ioctl</code>, and so forth) that control a device. The actual implementation of all these functions is delegated to the device driver. Because each device has a different I/O controller, and thus different commands and different state information, most I/O devices have their own drivers.</p><p>A device driver does not consist only of the functions that implement the device file operations. Before using a device driver, several activities must have taken place.</p><h3 id="4-1-Device-Driver-Registration"><a href="#4-1-Device-Driver-Registration" class="headerlink" title="4.1 Device Driver Registration"></a>4.1 Device Driver Registration</h3><h3 id="4-2-Device-Driver-Initialization"><a href="#4-2-Device-Driver-Initialization" class="headerlink" title="4.2 Device Driver Initialization"></a>4.2 Device Driver Initialization</h3><h3 id="4-3-Monitoring-I-O-Operations"><a href="#4-3-Monitoring-I-O-Operations" class="headerlink" title="4.3 Monitoring I/O Operations"></a>4.3 Monitoring I/O Operations</h3><h3 id="4-4-Accessing-the-I-O-Shared-Memory"><a href="#4-4-Accessing-the-I-O-Shared-Memory" class="headerlink" title="4.4 Accessing the I/O Shared Memory"></a>4.4 Accessing the I/O Shared Memory</h3><h3 id="4-5-Direct-Memory-Access-DMA"><a href="#4-5-Direct-Memory-Access-DMA" class="headerlink" title="4.5 Direct Memory Access (DMA)"></a>4.5 Direct Memory Access (DMA)</h3><p><strong>Bus addresses</strong><br>Every DMA transfer involves (at least) one memory buffer, which contains the data to be read or written by the hardware device. In general, before activating the transfer, the device driver must ensure that the DMA circuit can directly access the RAM locations.</p><p>Until now we have distinguished three kinds of memory addresses: logical and linear addresses, which are used internally by the CPU, and physical addresses, which are the memory addresses used by the CPU to physically drive the data bus. However, there is a fourth kind of memory address: the so-called <em>bus address</em>. It corresponds to the memory addresses used by all hardware devices except the CPU to drive the data bus.</p><p>In the 80×86 architecture, bus addresses coincide with physical addresses.</p><h3 id="4-6-Levels-of-Kernel-Support"><a href="#4-6-Levels-of-Kernel-Support" class="headerlink" title="4.6 Levels of Kernel Support"></a>4.6 Levels of Kernel Support</h3><h2 id="5-Character-Device-Drivers"><a href="#5-Character-Device-Drivers" class="headerlink" title="5 Character Device Drivers"></a>5 Character Device Drivers</h2><p><img src="/images/2019/1/15.png" alt=""></p><h3 id="5-1-Assigning-Device-Numbers"><a href="#5-1-Assigning-Device-Numbers" class="headerlink" title="5.1 Assigning Device Numbers"></a>5.1 Assigning Device Numbers</h3><h3 id="5-2-Accessing-a-Character-Device-Driver"><a href="#5-2-Accessing-a-Character-Device-Driver" class="headerlink" title="5.2 Accessing a Character Device Driver"></a>5.2 Accessing a Character Device Driver</h3><h3 id="5-3-Buffering-Strategies-for-Character-Devices"><a href="#5-3-Buffering-Strategies-for-Character-Devices" class="headerlink" title="5.3 Buffering Strategies for Character Devices"></a>5.3 Buffering Strategies for Character Devices</h3><hr><p>参考资料：</p><ol><li><a href="https://sysplay.github.io/books/LinuxDrivers/book/Content/Part04.html" target="_blank" rel="noopener">Linux Character Drivers</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In the section “I/O Architecture,” we give a brief survey of the 80×86 I/O architecture. In the section “The Device Driver Model,” we introduce the Linux device driver model.
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-The Virtual Filesystem</title>
    <link href="http://liujunming.github.io/2019/01/02/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-The-Virtual-Filesystem/"/>
    <id>http://liujunming.github.io/2019/01/02/Understanding-the-Linux-Kernel-读书笔记-The-Virtual-Filesystem/</id>
    <published>2019-01-02T08:20:40.000Z</published>
    <updated>2019-01-02T12:01:00.699Z</updated>
    
    <content type="html"><![CDATA[<p>The idea behind the Virtual Filesystem is to put a wide range of information in the kernel to represent many different types of filesystems.<br><a id="more"></a> </p><h2 id="1-The-Role-of-the-Virtual-Filesystem-VFS"><a href="#1-The-Role-of-the-Virtual-Filesystem-VFS" class="headerlink" title="1 The Role of the Virtual Filesystem (VFS)"></a>1 The Role of the Virtual Filesystem (VFS)</h2><p>The Virtual Filesystem is a kernel software layer that handles all system calls related to a standard Unix filesystem. Its main strength is providing a common interface to several kinds of filesystems.</p><p><img src="/images/2019/1/8.png" alt=""></p><p>Filesystems supported by the VFS may be grouped into three main classes:</p><ul><li><em>Disk-based filesystems</em></li></ul><p>These manage storage space available in a local disk or in some other device that emulates a disk (such as a USB flash drive).</p><ul><li><em>Network filesystems</em></li></ul><p>These allow easy access to files included in filesystems belonging to other networked computers.</p><ul><li><em>Special filesystems</em></li></ul><p>These do not manage disk space, either locally or remotely.</p><h3 id="1-1-The-Common-File-Model"><a href="#1-1-The-Common-File-Model" class="headerlink" title="1.1 The Common File Model"></a>1.1 The Common File Model</h3><p>The key idea behind the VFS consists of introducing a <em>common file model</em> capable of representing all supported filesystems.</p><p>The common file model consists of the following object types:</p><ul><li><em>The superblock object</em></li><li><em>The inode object</em></li><li><em>The file object</em></li><li><em>The dentry object</em></li></ul><p>Figure 12-2 illustrates with a simple example how processes interact with files. Three different processes have opened the same file, two of them using the same hard link. In this case, each of the three processes uses its own file object, while only two dentry objects are required—one for each hard link. Both dentry objects refer to the same inode object, which identifies the superblock object and, together with the latter, the common disk file.<br><img src="/images/2019/1/9.png" alt=""></p><h3 id="1-2-System-Calls-Handled-by-the-VFS"><a href="#1-2-System-Calls-Handled-by-the-VFS" class="headerlink" title="1.2 System Calls Handled by the VFS"></a>1.2 System Calls Handled by the VFS</h3><h2 id="2-VFS-Data-Structures"><a href="#2-VFS-Data-Structures" class="headerlink" title="2 VFS Data Structures"></a>2 VFS Data Structures</h2><h3 id="2-1-Superblock-Objects"><a href="#2-1-Superblock-Objects" class="headerlink" title="2.1 Superblock Objects"></a>2.1 Superblock Objects</h3><p>A superblock object consists of a <a href="https://elixir.bootlin.com/linux/v2.6.11/source/include/linux/fs.h#L754" target="_blank" rel="noopener">super_block</a> structure.</p><h3 id="2-2-Inode-Objects"><a href="#2-2-Inode-Objects" class="headerlink" title="2.2 Inode Objects"></a>2.2 Inode Objects</h3><h3 id="2-3-File-Objects"><a href="#2-3-File-Objects" class="headerlink" title="2.3 File Objects"></a>2.3 File Objects</h3><h3 id="2-4-dentry-Objects"><a href="#2-4-dentry-Objects" class="headerlink" title="2.4 dentry Objects"></a>2.4 dentry Objects</h3><h3 id="2-5-The-dentry-Cache"><a href="#2-5-The-dentry-Cache" class="headerlink" title="2.5 The dentry Cache"></a>2.5 The dentry Cache</h3><h3 id="2-6-Files-Associated-with-a-Process"><a href="#2-6-Files-Associated-with-a-Process" class="headerlink" title="2.6 Files Associated with a Process"></a>2.6 Files Associated with a Process</h3><p><img src="/images/2019/1/10.png" alt=""></p><h2 id="3-Filesystem-Types"><a href="#3-Filesystem-Types" class="headerlink" title="3 Filesystem Types"></a>3 Filesystem Types</h2><h3 id="3-1-Special-Filesystems"><a href="#3-1-Special-Filesystems" class="headerlink" title="3.1 Special Filesystems"></a>3.1 Special Filesystems</h3><p>While network and disk-based filesystems enable the user to handle information stored outside the kernel, special filesystems may provide an easy way for system programs and administrators to manipulate the data structures of the kernel and to implement special features of the operating system. Table 12-8 lists the most common special filesystems used in Linux; for each of them, the table reports its suggested mount point and a short description.<br><img src="/images/2019/1/11.png" alt=""></p><h3 id="3-2-Filesystem-Type-Registration"><a href="#3-2-Filesystem-Type-Registration" class="headerlink" title="3.2 Filesystem Type Registration"></a>3.2 Filesystem Type Registration</h3><h2 id="4-Filesystem-Handling"><a href="#4-Filesystem-Handling" class="headerlink" title="4 Filesystem Handling"></a>4 Filesystem Handling</h2><h3 id="4-1-Namespaces"><a href="#4-1-Namespaces" class="headerlink" title="4.1 Namespaces"></a>4.1 Namespaces</h3><p>Every process might have its own tree of mounted filesystems—the socalled <em>namespace</em> of the process.</p><h3 id="4-2-Filesystem-Mounting"><a href="#4-2-Filesystem-Mounting" class="headerlink" title="4.2 Filesystem Mounting"></a>4.2 Filesystem Mounting</h3><h3 id="4-3-Mounting-a-Generic-Filesystem"><a href="#4-3-Mounting-a-Generic-Filesystem" class="headerlink" title="4.3 Mounting a Generic Filesystem"></a>4.3 Mounting a Generic Filesystem</h3><h3 id="4-4-Mounting-the-Root-Filesystem"><a href="#4-4-Mounting-the-Root-Filesystem" class="headerlink" title="4.4 Mounting the Root Filesystem"></a>4.4 Mounting the Root Filesystem</h3><h3 id="4-5-Unmounting-a-Filesystem"><a href="#4-5-Unmounting-a-Filesystem" class="headerlink" title="4.5 Unmounting a Filesystem"></a>4.5 Unmounting a Filesystem</h3><h2 id="5-Pathname-Lookup"><a href="#5-Pathname-Lookup" class="headerlink" title="5 Pathname Lookup"></a>5 Pathname Lookup</h2><h3 id="5-1-Standard-Pathname-Lookup"><a href="#5-1-Standard-Pathname-Lookup" class="headerlink" title="5.1 Standard Pathname Lookup"></a>5.1 Standard Pathname Lookup</h3><h3 id="5-2-Parent-Pathname-Lookup"><a href="#5-2-Parent-Pathname-Lookup" class="headerlink" title="5.2 Parent Pathname Lookup"></a>5.2 Parent Pathname Lookup</h3><h3 id="5-3-Lookup-of-Symbolic-Links"><a href="#5-3-Lookup-of-Symbolic-Links" class="headerlink" title="5.3 Lookup of Symbolic Links"></a>5.3 Lookup of Symbolic Links</h3><h2 id="6-Implementations-of-VFS-System-Calls"><a href="#6-Implementations-of-VFS-System-Calls" class="headerlink" title="6 Implementations of VFS System Calls"></a>6 Implementations of VFS System Calls</h2><h3 id="6-1-The-open-System-Call"><a href="#6-1-The-open-System-Call" class="headerlink" title="6.1 The open() System Call"></a>6.1 The open() System Call</h3><h3 id="6-2-The-read-and-write-System-Calls"><a href="#6-2-The-read-and-write-System-Calls" class="headerlink" title="6.2 The read() and write() System Calls"></a>6.2 The read() and write() System Calls</h3><h3 id="6-3-The-close-System-Call"><a href="#6-3-The-close-System-Call" class="headerlink" title="6.3 The close() System Call"></a>6.3 The close() System Call</h3><h2 id="7-File-Locking"><a href="#7-File-Locking" class="headerlink" title="7 File Locking"></a>7 File Locking</h2><p>The POSIX standard requires a file-locking mechanism based on the <code>fcntl()</code>system call. It is possible to lock an arbitrary region of a file (even a single byte) or to lock the whole file (including data appended in the future). Because a process can choose to lock only a part of a file, it can also hold multiple locks on different parts of the file.</p><h3 id="7-1-Linux-File-Locking"><a href="#7-1-Linux-File-Locking" class="headerlink" title="7.1 Linux File Locking"></a>7.1 Linux File Locking</h3><h3 id="7-2-File-Locking-Data-Structures"><a href="#7-2-File-Locking-Data-Structures" class="headerlink" title="7.2 File-Locking Data Structures"></a>7.2 File-Locking Data Structures</h3><h3 id="7-3-FL-FLOCK-Locks"><a href="#7-3-FL-FLOCK-Locks" class="headerlink" title="7.3 FL_FLOCK Locks"></a>7.3 FL_FLOCK Locks</h3><h3 id="7-4-FL-POSIX-Locks"><a href="#7-4-FL-POSIX-Locks" class="headerlink" title="7.4 FL_POSIX Locks"></a>7.4 FL_POSIX Locks</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The idea behind the Virtual Filesystem is to put a wide range of information in the kernel to represent many different types of filesystems.&lt;br&gt;
    
    </summary>
    
      <category term="文件系统" scheme="http://liujunming.github.io/categories/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>文件系统概述</title>
    <link href="http://liujunming.github.io/2019/01/02/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/"/>
    <id>http://liujunming.github.io/2019/01/02/文件系统概述/</id>
    <published>2019-01-02T05:15:06.000Z</published>
    <updated>2019-01-02T08:02:50.058Z</updated>
    
    <content type="html"><![CDATA[<p>文件系统是纯软件。文件系统的具体定位可以参考<a href="https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram" target="_blank" rel="noopener">Linux Storage Stack Diagram</a>和<a href="http://ilinuxkernel.com/?p=1559" target="_blank" rel="noopener">Linux存储I/O栈</a><br><a id="more"></a><br><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-implementation.pdf" target="_blank" rel="noopener">File System Implementation</a>很好地阐述了文件系统的实现。</p><p><img src="/images/2019/1/1.png" alt=""></p><ul><li>data region<br><img src="/images/2019/1/2.png" alt=""></li><li>inodes<br><img src="/images/2019/1/3.png" alt=""></li><li>inode bitmap and data bitmap<br><img src="/images/2019/1/4.png" alt=""></li><li>superblock</li></ul><p>The superblock contains information about this particular file system, including, for example, how many inodes and data blocks are in the file system, where the inode table begins and so forth.<br><img src="/images/2019/1/5.png" alt=""></p><h2 id="Fast-File-System-FFS"><a href="#Fast-File-System-FFS" class="headerlink" title="Fast File System (FFS)"></a>Fast File System (FFS)</h2><p>详情可参见<a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-ffs.pdf" target="_blank" rel="noopener">Fast File System</a>.</p><p>The idea was to design the file system structures and allocation policies to be “disk aware” and thus improve performance.</p><p>FFS divides the disk into a number of <strong>cylinder groups</strong>.<br><img src="/images/2019/1/6.png" alt=""></p><p>Modern file systems (such as Linux ext2, ext3, and ext4) instead organize the drive into <strong>block groups</strong>, each of which is just a consecutive portion of the disk’s address space.<br><img src="/images/2019/1/7.png" alt=""></p><p>By placing two files within the same group, FFS can ensure that accessing one after the other will not result in long seeks across the disk.</p><p>FFS has to decide what is “related” and place it within the same block group.</p><p>All modern systems account for the main lesson of FFS: treat the disk like it’s a disk.</p><h2 id="crash-consistency-problem"><a href="#crash-consistency-problem" class="headerlink" title="crash-consistency problem"></a>crash-consistency problem</h2><p>详情可参见<a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-journaling.pdf" target="_blank" rel="noopener">FSCK and Journaling</a>.</p><p>One major challenge faced by a file system is how to update persistent data structures despite the presence of a <strong>power loss</strong> or <strong>system crash</strong>.</p><p>This problem is quite simple to understand. Imagine you have to update two on-disk structures, A and B, in order to complete a particular operation. Because the disk only services a single request at a time, one of these requests will reach the disk first (either A or B). If the system crashes or loses power after one write completes, the on-disk structure will be left in an <strong>inconsistent</strong> state. And thus, we have a problem that all file systems need to solve.</p><p><strong>Journaling</strong> (also known as <strong>write-ahead logging</strong>) is a technique which adds a little bit of overhead to each write but recovers more quickly from crashes or power losses.</p><h2 id="Log-structured-File-Systems"><a href="#Log-structured-File-Systems" class="headerlink" title="Log-structured File Systems"></a>Log-structured File Systems</h2><p>本质上是将随机写转换为顺序写，详情可参见<a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-lfs.pdf" target="_blank" rel="noopener">Log-structured File Systems</a>。</p><p>LFS introduces a new approach to updating the disk. Instead of overwriting files in places, LFS always writes to an unused portion of the disk, and then later reclaims that old space through cleaning. LFS can gather all updates into an in-memory segment and then write them out together sequentially.</p><p>The downside to this approach is that it generates garbage; old copies of the data are scattered throughout the disk, and if one wants to reclaim such space for subsequent usage, one must clean old segments periodically.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文件系统是纯软件。文件系统的具体定位可以参考&lt;a href=&quot;https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linux Storage Stack Diagram&lt;/a&gt;和&lt;a href=&quot;http://ilinuxkernel.com/?p=1559&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linux存储I/O栈&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="文件系统" scheme="http://liujunming.github.io/categories/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-Signals</title>
    <link href="http://liujunming.github.io/2018/12/29/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Signals/"/>
    <id>http://liujunming.github.io/2018/12/29/Understanding-the-Linux-Kernel-读书笔记-Signals/</id>
    <published>2018-12-29T05:06:57.000Z</published>
    <updated>2018-12-29T07:22:20.437Z</updated>
    
    <content type="html"><![CDATA[<p>Signals were introduced by the first Unix systems to allow interactions between User Mode processes; the kernel also uses them to notify processes of system events.<a id="more"></a> </p><h2 id="1-The-Role-of-Signals"><a href="#1-The-Role-of-Signals" class="headerlink" title="1 The Role of Signals"></a>1 The Role of Signals</h2><p>A <em>signal</em> is a very short message that may be sent to a process or a group of processes. The only information given to the process is usually a number identifying the signal.</p><p>A set of macros whose names start with the prefix <code>SIG</code> is used to identify signals.</p><p>Signals serve two main purposes:</p><ul><li>To make a process aware that a specific event has occurred</li><li>To cause a process to execute a signal handler function included in its code</li></ul><p><img src="/images/2018/12/71.png" alt=""></p><p>Besides the <em>regular signals</em> described in this table, the POSIX standard has introduced a new class of signals denoted as <em>real-time signals</em>; their signal numbers range from 32 to 64 on Linux. They mainly differ from regular signals because they are always queued so that multiple signals sent will be received. On the other hand, regular signals of the same kind are not queued: if a regular signal is sent many times in a row, just one of them is delivered to the receiving process. Although the Linux kernel does not use real-time signals, it fully supports the POSIX standard by means of several specific system calls.</p><p>A number of system calls allow programmers to send signals and determine how their processes respond to the signals they receive.<br><img src="/images/2018/12/72.png" alt=""></p><p>An important characteristic of signals is that they may be sent at any time to a process whose state is usually unpredictable. Signals sent to a process that is not currently executing must be saved by the kernel until that process resumes execution. Blocking a signal requires that delivery of the signal be held off until it is later unblocked.</p><p>Therefore, the kernel distinguishes two different phases related to signal transmission:</p><ul><li><em>Signal generation</em></li></ul><p>The kernel updates a data structure of the destination process to represent that a new signal has been sent.</p><ul><li><em>Signal delivery</em></li></ul><p>The kernel forces the destination process to react to the signal by changing its execution state, by starting the execution of a specified signal handler, or both.</p><p>Signals that have been generated but not yet delivered are called <em>pending signals</em>. At any time, only one pending signal of a given type may exist for a process; additional pending signals of the same type to the same process are not queued but simply discarded. Real-time signals are different, though: there can be several pending signals of the same type.</p><h3 id="1-1-Actions-Performed-upon-Delivering-a-Signal"><a href="#1-1-Actions-Performed-upon-Delivering-a-Signal" class="headerlink" title="1.1 Actions Performed upon Delivering a Signal"></a>1.1 Actions Performed upon Delivering a Signal</h3><p>There are three ways in which a process can respond to a signal:</p><ol><li>Explicitly ignore the signal.</li><li>Execute the default action associated with the signal.</li><li>Catch the signal by invoking a corresponding signal-handler function.</li></ol><p>The <code>SIGKILL</code> and <code>SIGSTOP</code> signals cannot be ignored, caught, or blocked, and their default actions must always be executed. </p><h3 id="1-2-POSIX-Signals-and-Multithreaded-Applications"><a href="#1-2-POSIX-Signals-and-Multithreaded-Applications" class="headerlink" title="1.2 POSIX Signals and Multithreaded Applications"></a>1.2 POSIX Signals and Multithreaded Applications</h3><p>Furthermore, a pending signal is <em>private</em> if it has been sent to a specific process; it is <em>shared</em> if it has been sent to a whole thread group.</p><h3 id="1-3-Data-Structures-Associated-with-Signals"><a href="#1-3-Data-Structures-Associated-with-Signals" class="headerlink" title="1.3 Data Structures Associated with Signals"></a>1.3 Data Structures Associated with Signals</h3><p>For each process in the system, the kernel must keep track of what signals are currently pending or masked; the kernel must also keep track of how every thread group is supposed to handle every signal.<br><img src="/images/2018/12/73.png" alt=""><br><img src="/images/2018/12/77.png" alt=""><br><strong>The signal descriptor and the signal handler descriptor</strong><br>The <code>signal</code> field of the process descriptor points to a <em>signal descriptor</em>, a <code>signal_struct</code> structure that keeps track of the shared pending signals. The signal descriptor is shared by all processes belonging to the same thread group.<br>Besides the signal descriptor, every process refers also to a <em>signal handler descriptor</em>, which is a <code>sighand_struct</code> structure describing how each signal must be handled by the thread group.</p><p><strong>The sigaction data structure</strong></p><p><strong>The pending signal queues</strong></p><h3 id="1-4-Operations-on-Signal-Data-Structures"><a href="#1-4-Operations-on-Signal-Data-Structures" class="headerlink" title="1.4 Operations on Signal Data Structures"></a>1.4 Operations on Signal Data Structures</h3><h2 id="2-Generating-a-Signal"><a href="#2-Generating-a-Signal" class="headerlink" title="2 Generating a Signal"></a>2 Generating a Signal</h2><p>Many kernel functions generate signals: they accomplish the first phase of signal handling——by updating one or more process descriptors as needed. They do not directly perform the second phase of delivering the signal but, depending on the type of signal and the state of the destination processes, may wake up some processes and force them to receive the signal.</p><ul><li>When a signal is sent to a process, either from the kernel or from another process.</li><li>When a signal is sent to a whole thread group, either from the kernel or from another process.</li></ul><h2 id="3-Delivering-a-Signal"><a href="#3-Delivering-a-Signal" class="headerlink" title="3 Delivering a Signal"></a>3 Delivering a Signal</h2><p>We assume that the kernel noticed the arrival of a signal and prepareD the process descriptor of the process that is supposed to receive the signal. But in case that process was not running on the CPU at that moment, the kernel deferred the task of delivering the signal. </p><p>The kernel checks for the existence of pending signals every time it finishes handling an interrupt or an exception.</p><h3 id="3-1-Executing-the-Default-Action-for-the-Signal"><a href="#3-1-Executing-the-Default-Action-for-the-Signal" class="headerlink" title="3.1 Executing the Default Action for the Signal"></a>3.1 Executing the Default Action for the Signal</h3><h3 id="3-2-Catching-the-Signal"><a href="#3-2-Catching-the-Signal" class="headerlink" title="3.2 Catching the Signal"></a>3.2 Catching the Signal</h3><p>Linux kernel每隔固定周期会发出timer interrupt。<br><img src="/images/2018/12/76.png" alt=""></p><h3 id="3-3-Reexecution-of-System-Calls"><a href="#3-3-Reexecution-of-System-Calls" class="headerlink" title="3.3 Reexecution of System Calls"></a>3.3 Reexecution of System Calls</h3><p>The request associated with a system call cannot always be immediately satisfied by the kernel; when this happens, the process that issued the system call is put in a <code>TASK_INTERRUPTIBLE</code> or <code>TASK_UNINTERRUPTIBLE</code> state.</p><p>If the process is put in a <code>TASK_INTERRUPTIBLE</code> state and some other process sends a signal to it, the kernel puts it in the <code>TASK_RUNNING</code> state without completing the system call.The signal is delivered to the process while switching back to User Mode. When this happens, the system call service routine does not complete its job, but returns an <code>EINTR</code>, <code>ERESTARTNOHAND</code>, <code>ERESTART_RESTARTBLOCK</code>, <code>ERESTARTSYS</code>, or <code>ERESTARTNOINTR</code> error code.</p><p>In practice, the only error code a User Mode process can get in this situation is <code>EINTR</code>, which means that the system call has not been completed.The remaining error codes are used internally by the kernel to specify whether the system call may be reexecuted automatically after the signal handler termination.</p><p><strong>Restarting a system call interrupted by a non-caught signal</strong></p><p><strong>Restarting a system call for a caught signal</strong></p><h2 id="4-System-Calls-Related-to-Signal-Handling"><a href="#4-System-Calls-Related-to-Signal-Handling" class="headerlink" title="4 System Calls Related to Signal Handling"></a>4 System Calls Related to Signal Handling</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Signals were introduced by the first Unix systems to allow interactions between User Mode processes; the kernel also uses them to notify processes of system events.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-System Calls</title>
    <link href="http://liujunming.github.io/2018/12/29/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-System-Calls/"/>
    <id>http://liujunming.github.io/2018/12/29/Understanding-the-Linux-Kernel-读书笔记-System-Calls/</id>
    <published>2018-12-29T04:04:37.000Z</published>
    <updated>2018-12-29T05:03:41.821Z</updated>
    
    <content type="html"><![CDATA[<p>Unix systems implement most interfaces between User Mode processes and hardware devices by means of system calls issued to the kernel.<a id="more"></a> </p><h2 id="1-POSIX-APIs-and-System-Calls"><a href="#1-POSIX-APIs-and-System-Calls" class="headerlink" title="1 POSIX APIs and System Calls"></a>1 POSIX APIs and System Calls</h2><p>Let’s start by stressing the difference between an application programmer interface (API) and a system call. The former is a function definition that specifies how to obtain a given service, while the latter is an explicit request to the kernel made via a software interrupt.</p><p>Unix systems include several libraries of functions that provide APIs to programmers. Some of the APIs defined by the <em>libc</em> standard C library refer to <em>wrapper routines</em>(routines whose only purpose is to issue a system call). Usually, each system call has a corresponding wrapper routine, which defines the API that application programs should employ.</p><p>The converse is not true, by the way—an API does not necessarily correspond to a specific system call. First of all, the API could offer its services directly in User Mode. (For something abstract such as math functions, there may be no reason to make system calls.) Second, a single API function could make several system calls. Moreover,several API functions could make the same system call, but wrap extra functionality around it. For instance, in Linux, the <code>malloc()</code>, <code>calloc()</code>, and <code>free()</code> APIs are implemented in the <em>libc</em> library. The code in this library keeps track of the allocation and deallocation requests and uses the <code>brk()</code> system call to enlarge or shrink the process heap.</p><p>The POSIX standard refers to APIs and not to system calls. </p><h2 id="2-System-Call-Handler-and-Service-Routines"><a href="#2-System-Call-Handler-and-Service-Routines" class="headerlink" title="2 System Call Handler and Service Routines"></a>2 System Call Handler and Service Routines</h2><p>When a User Mode process invokes a system call, the CPU switches to Kernel Mode and starts the execution of a kernel function. In the 80 × 86 architecture a Linux system call can be invoked in two different ways. The net result of both methods, however, is a jump to an assembly language function called the <em>system call handler</em>.</p><p>Because the kernel implements many different system calls, the User Mode process must pass a parameter called the <em>system call number</em> to identify the required system call; the eax register is used by Linux for this purpose. </p><p>In the kernel, positive or 0 values denote a successful termination of the system call, while negative values denote an error condition.</p><p>The system call handler, which has a structure similar to that of the other exception handlers, performs the following operations:</p><ul><li>Saves the contents of most registers in the Kernel Mode stack.</li><li>Handles the system call by invoking a corresponding C function called the <em>system call service routine</em>.</li><li>Exits from the handler: the registers are loaded with the values saved in the Kernel Mode stack, and the CPU is switched back from Kernel Mode to User Mode.</li></ul><p>The name of the service routine associated with the <code>xyz()</code> system call is usually <code>sys_ xyz()</code>; there are, however, a few exceptions to this rule.</p><p>Figure 10-1 illustrates the relationships between the application program that invokes a system call, the corresponding wrapper routine, the system call handler, and the system call service routine.<br><img src="/images/2018/12/70.png" alt=""></p><p>To associate each system call number with its corresponding service routine, the kernel uses a <em>system call dispatch table</em>.</p><h2 id="3-Entering-and-Exiting-a-System-Call"><a href="#3-Entering-and-Exiting-a-System-Call" class="headerlink" title="3 Entering and Exiting a System Call"></a>3 Entering and Exiting a System Call</h2><p>Native applications can invoke a system call in two different ways:</p><ul><li>By executing the int <code>$0x80</code> assembly language instruction.</li><li>By executing the <code>sysenter</code> assembly language instruction.</li></ul><p>Similarly, the kernel can exit from a system call—thus switching the CPU back to User Mode—in two ways:</p><ul><li>By executing the <code>iret</code> assembly language instruction.</li><li>By executing the <code>sysexit</code> assembly language instruction.</li></ul><p>The <code>int</code> assembly language instruction is inherently slow because it performs several consistency and security checks. The <code>sysenter</code> instruction, dubbed in Intel documentation as “Fast System Call,” provides a faster way to switch from User Mode to Kernel Mode.</p><h2 id="4-Parameter-Passing"><a href="#4-Parameter-Passing" class="headerlink" title="4 Parameter Passing"></a>4 Parameter Passing</h2><p>Like ordinary functions, system calls often require some input/output parameters, which may consist of actual values (i.e., numbers), addresses of variables in the address space of the User Mode process, or even addresses of data structures including pointers to User Mode functions.</p><h3 id="4-1-Verifying-the-Parameters"><a href="#4-1-Verifying-the-Parameters" class="headerlink" title="4.1 Verifying the Parameters"></a>4.1 Verifying the Parameters</h3><p>All system call parameters must be carefully checked before the kernel attempts to satisfy a user request. </p><h3 id="4-2-Accessing-the-Process-Address-Space"><a href="#4-2-Accessing-the-Process-Address-Space" class="headerlink" title="4.2 Accessing the Process Address Space"></a>4.2 Accessing the Process Address Space</h3><p>System call service routines often need to read or write data contained in the process’s address space.</p><h3 id="4-3-Generating-the-Exception-Tables-and-the-Fixup-Code"><a href="#4-3-Generating-the-Exception-Tables-and-the-Fixup-Code" class="headerlink" title="4.3 Generating the Exception Tables and the Fixup Code"></a>4.3 Generating the Exception Tables and the Fixup Code</h3><h2 id="5-Kernel-Wrapper-Routines"><a href="#5-Kernel-Wrapper-Routines" class="headerlink" title="5 Kernel Wrapper Routines"></a>5 Kernel Wrapper Routines</h2><p>Although system calls are used mainly by User Mode processes, they can also be invoked by kernel threads, which cannot use library functions. To simplify the declarations of the corresponding wrapper routines, Linux defines a set of seven macros called <code>_syscall0</code> through <code>_syscall6</code>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Unix systems implement most interfaces between User Mode processes and hardware devices by means of system calls issued to the kernel.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-Process Address Space</title>
    <link href="http://liujunming.github.io/2018/12/28/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Address-Space/"/>
    <id>http://liujunming.github.io/2018/12/28/Understanding-the-Linux-Kernel-读书笔记-Process-Address-Space/</id>
    <published>2018-12-28T06:19:06.000Z</published>
    <updated>2018-12-29T04:11:06.398Z</updated>
    
    <content type="html"><![CDATA[<p>A kernel function gets dynamic memory in a fairly straightforward manner by invoking one of a variety of functions: <code>__get_free_pages()</code> or <code>alloc_pages()</code> to get pages from the zoned page frame allocator, <code>kmem_cache_ alloc()</code> or <code>kmalloc()</code> to use the slab allocator for specialized or general-purpose objects, and <code>vmalloc()</code> or <code>vmalloc_32()</code> to get a noncontiguous memory area. If the request can be satisfied, each of these functions returns a page descriptor address or a linear address identifying the beginning of the allocated dynamic memory area.<a id="more"></a> </p><p>These simple approaches work for two reasons:</p><ul><li>The kernel is the highest-priority component of the operating system.</li><li>The kernel trusts itself.</li></ul><p>When allocating memory to User Mode processes, the situation is entirely different:</p><ul><li>Process requests for dynamic memory are considered non-urgent.</li><li>Because user programs cannot be trusted, the kernel must be prepared to catch all addressing errors caused by processes in User Mode.</li></ul><p>The kernel succeeds in deferring the allocation of dynamic memory to processes by using a new kind of resource. When a User Mode process asks for dynamic memory, it doesn’t get additional page frames; instead, it gets the right to use a new range of linear addresses, which become part of its address space. This interval is called a “memory region.”</p><h2 id="1-The-Process’s-Address-Space"><a href="#1-The-Process’s-Address-Space" class="headerlink" title="1 The Process’s Address Space"></a>1 The Process’s Address Space</h2><p>The <em>address space</em> of a process consists of all linear addresses that the process is allowed to use.</p><p>The kernel represents intervals of linear addresses by means of resources called <em>memory regions</em>, which are characterized by an initial linear address, a length, and some access rights.</p><p><img src="/images/2018/12/65.png" alt=""></p><h2 id="2-The-Memory-Descriptor"><a href="#2-The-Memory-Descriptor" class="headerlink" title="2 The Memory Descriptor"></a>2 The Memory Descriptor</h2><p>All information related to the process address space is included in an object called the <em>memory descriptor</em> of type <code>mm_struct</code>. This object is referenced by the <code>mm</code> field of the process descriptor.</p><h3 id="2-1-Memory-Descriptor-of-Kernel-Threads"><a href="#2-1-Memory-Descriptor-of-Kernel-Threads" class="headerlink" title="2.1 Memory Descriptor of Kernel Threads"></a>2.1 Memory Descriptor of Kernel Threads</h3><p>Contrary to regular processes, kernel threads do not use memory regions, therefore most of the fields of a memory descriptor are meaningless for them.</p><h2 id="3-Memory-Regions"><a href="#3-Memory-Regions" class="headerlink" title="3 Memory Regions"></a>3 Memory Regions</h2><p>Linux implements a memory region by means of an object of type <code>vm_area_struct</code>.<br><img src="/images/2018/12/67.png" alt=""></p><h3 id="3-1-Memory-Region-Data-Structures"><a href="#3-1-Memory-Region-Data-Structures" class="headerlink" title="3.1 Memory Region Data Structures"></a>3.1 Memory Region Data Structures</h3><h3 id="3-2-Memory-Region-Access-Rights"><a href="#3-2-Memory-Region-Access-Rights" class="headerlink" title="3.2 Memory Region Access Rights"></a>3.2 Memory Region Access Rights</h3><h3 id="3-3-Memory-Region-Handling"><a href="#3-3-Memory-Region-Handling" class="headerlink" title="3.3 Memory Region Handling"></a>3.3 Memory Region Handling</h3><ul><li>Finding the closest region to a given address: <code>find_vma()</code></li><li>Finding a region that overlaps a given interval: <code>find_vma_intersection()</code></li><li>Finding a free interval: <code>get_unmapped_area()</code></li><li>Inserting a region in the memory descriptor list: <code>insert_vm_struct()</code></li></ul><h3 id="3-4-Allocating-a-Linear-Address-Interval"><a href="#3-4-Allocating-a-Linear-Address-Interval" class="headerlink" title="3.4 Allocating a Linear Address Interval"></a>3.4 Allocating a Linear Address Interval</h3><h3 id="3-5-Releasing-a-Linear-Address-Interval"><a href="#3-5-Releasing-a-Linear-Address-Interval" class="headerlink" title="3.5 Releasing a Linear Address Interval"></a>3.5 Releasing a Linear Address Interval</h3><h2 id="4-Page-Fault-Exception-Handler"><a href="#4-Page-Fault-Exception-Handler" class="headerlink" title="4 Page Fault Exception Handler"></a>4 Page Fault Exception Handler</h2><p>The Linux Page Fault exception handler must distinguish exceptions caused by programming errors from those caused by a reference to a page that legitimately belongs to the process address space but simply hasn’t been allocated yet.</p><p><img src="/images/2018/12/68.png" alt=""></p><p><img src="/images/2018/12/69.png" alt=""></p><h3 id="4-1-Handling-a-Faulty-Address-Outside-the-Address-Space"><a href="#4-1-Handling-a-Faulty-Address-Outside-the-Address-Space" class="headerlink" title="4.1 Handling a Faulty Address Outside the Address Space"></a>4.1 Handling a Faulty Address Outside the Address Space</h3><h3 id="4-2-Handling-a-Faulty-Address-Inside-the-Address-Space"><a href="#4-2-Handling-a-Faulty-Address-Inside-the-Address-Space" class="headerlink" title="4.2 Handling a Faulty Address Inside the Address Space"></a>4.2 Handling a Faulty Address Inside the Address Space</h3><h3 id="4-3-Demand-Paging"><a href="#4-3-Demand-Paging" class="headerlink" title="4.3 Demand Paging"></a>4.3 Demand Paging</h3><p>The term <em>demand paging</em> denotes a dynamic memory allocation technique that consists of deferring page frame allocation until the last possible moment—until the process attempts to address a page that is not present in RAM, thus causing a Page Fault exception.</p><h3 id="4-4-Copy-On-Write"><a href="#4-4-Copy-On-Write" class="headerlink" title="4.4 Copy On Write"></a>4.4 Copy On Write</h3><h3 id="4-5-Handling-Noncontiguous-Memory-Area-Accesses"><a href="#4-5-Handling-Noncontiguous-Memory-Area-Accesses" class="headerlink" title="4.5 Handling Noncontiguous Memory Area Accesses"></a>4.5 Handling Noncontiguous Memory Area Accesses</h3><h2 id="5-Creating-and-Deleting-a-Process-Address-Space"><a href="#5-Creating-and-Deleting-a-Process-Address-Space" class="headerlink" title="5 Creating and Deleting a Process Address Space"></a>5 Creating and Deleting a Process Address Space</h2><h3 id="5-1-Creating-a-Process-Address-Space"><a href="#5-1-Creating-a-Process-Address-Space" class="headerlink" title="5.1 Creating a Process Address Space"></a>5.1 Creating a Process Address Space</h3><h3 id="5-2-Deleting-a-Process-Address-Space"><a href="#5-2-Deleting-a-Process-Address-Space" class="headerlink" title="5.2 Deleting a Process Address Space"></a>5.2 Deleting a Process Address Space</h3><h2 id="6-Managing-the-Heap"><a href="#6-Managing-the-Heap" class="headerlink" title="6 Managing the Heap"></a>6 Managing the Heap</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A kernel function gets dynamic memory in a fairly straightforward manner by invoking one of a variety of functions: &lt;code&gt;__get_free_pages()&lt;/code&gt; or &lt;code&gt;alloc_pages()&lt;/code&gt; to get pages from the zoned page frame allocator, &lt;code&gt;kmem_cache_ alloc()&lt;/code&gt; or &lt;code&gt;kmalloc()&lt;/code&gt; to use the slab allocator for specialized or general-purpose objects, and &lt;code&gt;vmalloc()&lt;/code&gt; or &lt;code&gt;vmalloc_32()&lt;/code&gt; to get a noncontiguous memory area. If the request can be satisfied, each of these functions returns a page descriptor address or a linear address identifying the beginning of the allocated dynamic memory area.
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-Memory Management</title>
    <link href="http://liujunming.github.io/2018/12/28/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Memory-Management/"/>
    <id>http://liujunming.github.io/2018/12/28/Understanding-the-Linux-Kernel-读书笔记-Memory-Management/</id>
    <published>2018-12-28T04:49:01.000Z</published>
    <updated>2018-12-29T00:56:41.574Z</updated>
    
    <content type="html"><![CDATA[<p>The sections “Page Frame Management” and “Memory Area Management” illustrate two different techniques for handling physically contiguous memory areas, while the section “Noncontiguous Memory Area Management” illustrates a third technique that handles noncontiguous memory areas. In these sections we’ll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.<a id="more"></a> </p><p><img src="/images/2017/10/8.gif" alt=""></p><h2 id="1-Page-Frame-Management"><a href="#1-Page-Frame-Management" class="headerlink" title="1 Page Frame Management"></a>1 Page Frame Management</h2><h3 id="1-1-Page-Descriptors"><a href="#1-1-Page-Descriptors" class="headerlink" title="1.1 Page Descriptors"></a>1.1 Page Descriptors</h3><p>State information of a page frame is kept in a page descriptor of type <code>page</code>.All page descriptors are stored in the <code>mem_map</code> array.</p><h3 id="1-2-Non-Uniform-Memory-Access-NUMA"><a href="#1-2-Non-Uniform-Memory-Access-NUMA" class="headerlink" title="1.2 Non-Uniform Memory Access (NUMA)"></a>1.2 Non-Uniform Memory Access (NUMA)</h3><p>The physical memory of the system is partitioned in several <em>nodes</em>.The physical memory inside each node can be split into several zones, each node has a descriptor of type <code>pg_data_t</code>.</p><h3 id="1-3-Memory-Zones"><a href="#1-3-Memory-Zones" class="headerlink" title="1.3 Memory Zones"></a>1.3 Memory Zones</h3><p>Each memory zone has its own descriptor of type <code>zone</code>.</p><h3 id="1-4-The-Pool-of-Reserved-Page-Frames"><a href="#1-4-The-Pool-of-Reserved-Page-Frames" class="headerlink" title="1.4 The Pool of Reserved Page Frames"></a>1.4 The Pool of Reserved Page Frames</h3><p>Some kernel control paths cannot be blocked while requesting memory—this happens, for instance, when handling an interrupt or when executing code inside a critical region. In these cases, a kernel control path should issue <em>atomic memory allocation requests</em>.  An atomic request never blocks: if there are not enough free pages, the allocation simply fails. The kernel reserves a pool of page frames for atomic memory allocation requests to be used only on low-on-memory conditions.</p><h3 id="1-5-The-Zoned-Page-Frame-Allocator"><a href="#1-5-The-Zoned-Page-Frame-Allocator" class="headerlink" title="1.5 The Zoned Page Frame Allocator"></a>1.5 The Zoned Page Frame Allocator</h3><p>The kernel subsystem that handles the memory allocation requests for groups of contiguous page frames is called the <em>zoned page frame allocator</em>. Its main components are shown in Figure 8-2.<br><img src="/images/2018/12/62.png" alt=""><br>The component named “zone allocator” receives the requests for allocation and deallocation of dynamic memory. In the case of allocation requests, the component searches a memory zone that includes a group of contiguous page frames that can satisfy the request. Inside each zone, page frames are handled by a component named “buddy system”. To get better system performance, a small number of page frames are kept in cache to quickly satisfy the allocation requests for single page frames.</p><h4 id="1-5-1-The-Zone-Allocator"><a href="#1-5-1-The-Zone-Allocator" class="headerlink" title="1.5.1 The Zone Allocator"></a>1.5.1 The Zone Allocator</h4><p>The zone allocator is the frontend of the kernel page frame allocator. This component must locate a memory zone that includes a number of free page frames large enough to satisfy the memory request. </p><h4 id="1-5-2-The-Buddy-System-Algorithm"><a href="#1-5-2-The-Buddy-System-Algorithm" class="headerlink" title="1.5.2 The Buddy System Algorithm"></a>1.5.2 The Buddy System Algorithm</h4><p>The kernel must establish a robust and efficient strategy for allocating groups of contiguous page frames. In doing so, it must deal with a well-known memory management problem called <em>external fragmentation</em>. The technique adopted by Linux to solve the external fragmentation problem is based on the well-known <em>buddy system</em> algorithm.</p><h4 id="1-5-3-The-Per-CPU-Page-Frame-Cache"><a href="#1-5-3-The-Per-CPU-Page-Frame-Cache" class="headerlink" title="1.5.3 The Per-CPU Page Frame Cache"></a>1.5.3 The Per-CPU Page Frame Cache</h4><p>The kernel often requests and releases single page frames. To boost system performance, each memory zone defines a <em>per-CPU page frame cache</em>. Each per-CPU cache includes some pre-allocated page frames to be used for single memory requests issued by the local CPU.</p><h3 id="1-6-Kernel-Mappings-of-High-Memory-Page-Frames"><a href="#1-6-Kernel-Mappings-of-High-Memory-Page-Frames" class="headerlink" title="1.6 Kernel Mappings of High-Memory Page Frames"></a>1.6 Kernel Mappings of High-Memory Page Frames</h3><p>见<a href="http://liujunming.top/2017/10/10/Linux%E5%86%85%E6%A0%B8%E9%AB%98%E7%AB%AF%E5%86%85%E5%AD%98/" target="_blank" rel="noopener">Linux内核高端内存</a></p><h2 id="2-Memory-Area-Management"><a href="#2-Memory-Area-Management" class="headerlink" title="2 Memory Area Management"></a>2 Memory Area Management</h2><p><em>Memory areas</em> is with sequences of memory cells having contiguous physical addresses and an arbitrary length.</p><h3 id="2-1-The-Slab-Allocator"><a href="#2-1-The-Slab-Allocator" class="headerlink" title="2.1 The Slab Allocator"></a>2.1 The Slab Allocator</h3><p>The buddy system algorithm adopts the page frame as the basic memory area. This is fine for dealing with relatively large memory requests, but how are we going to deal with requests for small memory areas, say a few tens or hundreds of bytes?</p><p>The slab allocator groups objects into <code>caches</code>. Each cache is a “store” of objects of the same type.</p><p>The area of main memory that contains a cache is divided into <code>slabs</code>; each slab consists of one or more contiguous page frames that contain both allocated and free objects.</p><p><img src="/images/2018/12/63.png" alt=""></p><p><img src="/images/2018/12/64.png" alt=""></p><h4 id="2-1-1-Local-Caches-of-Free-Slab-Objects"><a href="#2-1-1-Local-Caches-of-Free-Slab-Objects" class="headerlink" title="2.1.1 Local Caches of Free Slab Objects"></a>2.1.1 Local Caches of Free Slab Objects</h4><p>To reduce spin lock contention among processors and to make better use of the hardware caches, each cache of the slab allocator includes a per-CPU data structure consisting of a small array of pointers to freed objects called the <em>slab local cache</em>.</p><h4 id="2-2-General-Purpose-Objects"><a href="#2-2-General-Purpose-Objects" class="headerlink" title="2.2 General Purpose Objects"></a>2.2 General Purpose Objects</h4><p><code>kmalloc()</code></p><h3 id="2-3-Memory-Pools"><a href="#2-3-Memory-Pools" class="headerlink" title="2.3 Memory Pools"></a>2.3 Memory Pools</h3><p>A memory pool allows a kernel component—such as the block device subsystem—to allocate some dynamic memory to be used only in low-on-memory emergencies.</p><p>Memory pools should not be confused with the reserved page frames described in the earlier section “The Pool of Reserved Page Frames.” In fact, those page frames can be used only to satisfy atomic memory allocation requests issued by interrupt handlers or inside critical regions. Instead, a memory pool is a reserve of dynamic memory that can be used only by a specific kernel component.</p><p>Often, a memory pool is stacked over the slab allocator—that is, it is used to keep a reserve of slab objects. Generally speaking, however, a memory pool can be used to allocate every kind of dynamic memory.</p><h2 id="3-Noncontiguous-Memory-Area-Management"><a href="#3-Noncontiguous-Memory-Area-Management" class="headerlink" title="3 Noncontiguous Memory Area Management"></a>3 Noncontiguous Memory Area Management</h2><p><code>vmalloc()</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The sections “Page Frame Management” and “Memory Area Management” illustrate two different techniques for handling physically contiguous memory areas, while the section “Noncontiguous Memory Area Management” illustrates a third technique that handles noncontiguous memory areas. In these sections we’ll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Process Scheduling</title>
    <link href="http://liujunming.github.io/2018/12/27/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Scheduling/"/>
    <id>http://liujunming.github.io/2018/12/27/Understanding-the-Linux-Kernel-读书笔记-Process-Scheduling/</id>
    <published>2018-12-27T02:43:29.000Z</published>
    <updated>2018-12-27T06:43:51.834Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter deals with <em>scheduling</em>, which is concerned with when to switch and which process to choose.<a id="more"></a> </p><h2 id="1-Scheduling-Policy"><a href="#1-Scheduling-Policy" class="headerlink" title="1 Scheduling Policy"></a>1 Scheduling Policy</h2><p>The set of rules used to determine when and how to select a new process to run is called <em>scheduling policy</em>.<br>Linux scheduling is based on the <em>time sharing</em> technique: several processes run in “time multiplexing” because the CPU time is divided into <em>slices</em>, one for each runnable process.</p><p>Programmers may change the scheduling priorities by means of the system calls illustrated in Table 7-1.</p><p><img src="/images/2018/12/59.png" alt=""></p><h3 id="1-1-Process-Preemption"><a href="#1-1-Process-Preemption" class="headerlink" title="1.1 Process Preemption"></a>1.1 Process Preemption</h3><p>Linux processes are <em>preemptable</em>. When a process enters the <code>TASK_RUNNING</code> state, the kernel checks whether its dynamic priority is greater than the priority of the currently running process. If it is, the execution of <code>current</code> is interrupted and the scheduler is invoked to select another process to run (usually the process that just became runnable).</p><p>Be aware that a preempted process is not suspended, because it remains in the <code>TASK_ RUNNING</code> state; it simply no longer uses the CPU.</p><h3 id="1-2-How-Long-Must-a-Quantum-Last"><a href="#1-2-How-Long-Must-a-Quantum-Last" class="headerlink" title="1.2 How Long Must a Quantum Last?"></a>1.2 How Long Must a Quantum Last?</h3><p>The quantum[时间片] duration is critical for system performance: it should be neither too long nor too short.</p><h2 id="2-The-Scheduling-Algorithm"><a href="#2-The-Scheduling-Algorithm" class="headerlink" title="2 The Scheduling Algorithm"></a>2 The Scheduling Algorithm</h2><p><a href="http://liujunming.top/junming/os/8.cpu-sched-mlfq.pdf" target="_blank" rel="noopener">The Multi-Level Feedback Queue</a></p><h3 id="2-1-Scheduling-of-Conventional-Processes"><a href="#2-1-Scheduling-of-Conventional-Processes" class="headerlink" title="2.1 Scheduling of Conventional Processes"></a>2.1 Scheduling of Conventional Processes</h3><ul><li><strong>Base time quantum</strong></li><li><strong>Dynamic priority and average sleep time</strong></li><li><strong>Active and expired processes</strong></li></ul><h3 id="2-2-Scheduling-of-Real-Time-Processes"><a href="#2-2-Scheduling-of-Real-Time-Processes" class="headerlink" title="2.2 Scheduling of Real-Time Processes"></a>2.2 Scheduling of Real-Time Processes</h3><h2 id="3-Data-Structures-Used-by-the-Scheduler"><a href="#3-Data-Structures-Used-by-the-Scheduler" class="headerlink" title="3 Data Structures Used by the Scheduler"></a>3 Data Structures Used by the Scheduler</h2><p>The runqueue lists link the process descriptors of all runnable processes—that is, of those in a <code>TASK_RUNNING</code> state—except the <em>swapper</em> process (idle process).</p><h3 id="3-1-The-runqueue-Data-Structure"><a href="#3-1-The-runqueue-Data-Structure" class="headerlink" title="3.1 The runqueue Data Structure"></a>3.1 The <code>runqueue</code> Data Structure</h3><p>The <a href="https://elixir.bootlin.com/linux/v2.6.11/source/kernel/sched.c#L198" target="_blank" rel="noopener">runqueue</a> data structure.</p><p>Each CPU in the system has its own runqueue; all <code>runqueue</code> structures are stored in the <code>runqueues</code> per-CPU variable.</p><p>The most important fields of the <code>runqueue</code> data structure are those related to the lists of runnable processes.</p><p>The <code>arrays</code> field of the runqueue is an array consisting of two <code>prio_array_t</code> structures. Each data structure represents a set of runnable processes, and includes 140 doubly linked list heads (one list for each possible process priority), a priority bitmap, and a counter of the processes included in the set.<br><img src="/images/2018/12/60.png" alt=""></p><h3 id="3-2-Process-Descriptor"><a href="#3-2-Process-Descriptor" class="headerlink" title="3.2 Process Descriptor"></a>3.2 Process Descriptor</h3><p>Each process descriptor includes several fields related to scheduling.</p><h2 id="4-Functions-Used-by-the-Scheduler"><a href="#4-Functions-Used-by-the-Scheduler" class="headerlink" title="4 Functions Used by the Scheduler"></a>4 Functions Used by the Scheduler</h2><p>The scheduler relies on several functions in order to do its work; the most important are:</p><ul><li><code>scheduler_tick()</code></li></ul><p>Keeps the <code>time_slice</code> counter of <code>current</code> up-to-date</p><ul><li><code>try_to_wake_up()</code></li></ul><p>Awakens a sleeping process</p><ul><li><code>recalc_task_prio()</code></li></ul><p>Updates the dynamic priority of a process</p><ul><li><code>schedule()</code></li></ul><p>Selects a new process to be executed</p><ul><li><code>load_balance()</code></li></ul><p>Keeps the runqueues of a multiprocessor system balanced</p><h2 id="5-Runqueue-Balancing-in-Multiprocessor-Systems"><a href="#5-Runqueue-Balancing-in-Multiprocessor-Systems" class="headerlink" title="5 Runqueue Balancing in Multiprocessor Systems"></a>5 Runqueue Balancing in Multiprocessor Systems</h2><p>Multiprocessor machines come in many different flavors, and the scheduler behaves differently depending on the hardware characteristics. In particular, we will consider the following three types of multiprocessor machines:</p><ul><li><em>Classic multiprocessor architecture</em></li></ul><p>Until recently, this was the most common architecture for multiprocessor machines. These machines have a common set of RAM chips shared by all CPUs.</p><ul><li><em>Hyper-threading</em></li></ul><p>A hyper-threaded chip is a microprocessor that executes several threads of execution at once; it includes several copies of the internal registers and quickly switches between them. This technology allows the processor to exploit the machine cycles to execute another thread while the current thread is stalled for a memory access. A hyper-threaded physical CPU is seen by Linux as several different logical CPUs.</p><ul><li><em>NUMA</em></li></ul><hr><p>These basic kinds of multiprocessor systems are often combined. For instance, a motherboard that includes two different hyper-threaded CPUs is seen by the kernel as four logical CPUs.</p><p>A runnable process is always stored in exactly one runqueue: no runnable process ever appears in two or more runqueues. Therefore, until a process remains runnable, it is usually bound to one CPU.</p><p>This design choice is usually beneficial for system performance, because the hardware cache of every CPU is likely to be filled with data owned by the runnable processes in the runqueue. In some cases, however, binding a runnable process to a given CPU might induce a severe performance penalty. For instance, consider a large number of batch processes that make heavy use of the CPU: if most of them end up in the same runqueue, one CPU in the system will be overloaded, while the others will be nearly idle.</p><p>Therefore, the kernel periodically checks whether the workloads of the runqueues are balanced and, if necessary, moves some process from one runqueue to another. However, to get the best performance from a multiprocessor system, the load balancing algorithm should take into consideration the topology of the CPUs in the system. Linux sports a sophisticated runqueue balancing algorithm based on the notion of “scheduling domains.” Thanks to the scheduling domains, the algorithm can be easily tuned for all kinds of existing multiprocessor architectures.</p><h3 id="5-1-Scheduling-Domains"><a href="#5-1-Scheduling-Domains" class="headerlink" title="5.1 Scheduling Domains"></a>5.1 Scheduling Domains</h3><p>Essentially, a <em>scheduling domain</em> is a set of CPUs whose workloads should be kept balanced by the kernel. Generally speaking, scheduling domains are hierarchically organized: the top-most scheduling domain, which usually spans all CPUs in the system, includes children scheduling domains, each of which include a subset of the CPUs. Thanks to the hierarchy of scheduling domains, workload balancing can be done in a rather efficient way.</p><p>Figure 7-2 illustrates three examples of scheduling domain hierarchies, corresponding to the three main architectures of multiprocessor machines.</p><p><img src="/images/2018/12/61.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter deals with &lt;em&gt;scheduling&lt;/em&gt;, which is concerned with when to switch and which process to choose.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Interprocess Communication</title>
    <link href="http://liujunming.github.io/2018/12/26/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Interprocess-Communication/"/>
    <id>http://liujunming.github.io/2018/12/26/apue-读书笔记-Interprocess-Communication/</id>
    <published>2018-12-26T06:56:24.000Z</published>
    <updated>2018-12-26T07:46:37.942Z</updated>
    
    <content type="html"><![CDATA[<p>Interprocess Communication (IPC).<br>In this chapter, we examine classical IPC: pipes, FIFOs, message queues, semaphores, and shared memory.<a id="more"></a> </p><!-- ![](/images/2018/12/26.png) --><h2 id="1-Pipes"><a href="#1-Pipes" class="headerlink" title="1 Pipes"></a>1 Pipes</h2><p>Pipes are the oldest form of UNIX System IPC and are provided by all UNIX systems. Pipes have two limitations：</p><ol><li>Historically, they have been half duplex[半双工] (i.e., data flows in only one direction).</li><li>Pipes can be used only between processes that have a common ancestor.</li></ol><h2 id="2-FIFOs"><a href="#2-FIFOs" class="headerlink" title="2 FIFOs"></a>2 FIFOs</h2><p>FIFOs are sometimes called named pipes. Unnamed pipes can be used only between related processes when a common ancestor has created the pipe. With FIFOs, however, unrelated processes can exchange data.</p><h2 id="3-Message-Queues"><a href="#3-Message-Queues" class="headerlink" title="3 Message Queues"></a>3 Message Queues</h2><p>A message queue is a linked list of messages stored within the kernel and identified by a message queue identifier.</p><h2 id="4-Semaphores"><a href="#4-Semaphores" class="headerlink" title="4 Semaphores"></a>4 Semaphores</h2><p>A semaphore isn’t a form of IPC similar to the others that we’ve described (pipes, FIFOs, and message queues). A semaphore is a counter used to provide access to a shared data object for multiple processes.</p><h2 id="5-Shared-Memory"><a href="#5-Shared-Memory" class="headerlink" title="5 Shared Memory"></a>5 Shared Memory</h2><p>Shared memory allows two or more processes to share the same pages of memory. No kernel intervention is required to exchange data via shared memory. Once a process has copied data into a shared memory segment, that data is immediately visible to other processes. Shared memory provides fast IPC, although this speed advantage is somewhat offset by the fact that normally we must use some type of synchronization technique, such as a System V semaphore, to synchronize access to the shared memory.</p><p>A POSIX shared memory object is used to share a region of memory between unrelated processes without creating an underlying disk file.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Interprocess Communication (IPC).&lt;br&gt;In this chapter, we examine classical IPC: pipes, FIFOs, message queues, semaphores, and shared memory.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Advanced I/O</title>
    <link href="http://liujunming.github.io/2018/12/26/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Advanced-I-O/"/>
    <id>http://liujunming.github.io/2018/12/26/apue-读书笔记-Advanced-I-O/</id>
    <published>2018-12-26T02:47:52.000Z</published>
    <updated>2018-12-26T07:01:10.468Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter covers numerous topics and functions that we lump under the term <em>advanced I/O</em> : nonblocking I/O, record locking, I/O multiplexing (the <code>select</code> and <code>poll</code> functions), asynchronous I/O, the <code>readv</code> and <code>writev</code> functions, and memory-mapped I/O (<code>mmap</code>).<a id="more"></a> </p><h2 id="1-Nonblocking-I-O"><a href="#1-Nonblocking-I-O" class="headerlink" title="1 Nonblocking I/O"></a>1 Nonblocking I/O</h2><p>参考<a href="http://liujunming.top/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/" target="_blank" rel="noopener">IO - 同步，异步，阻塞，非阻塞</a></p><h2 id="2-Record-Locking"><a href="#2-Record-Locking" class="headerlink" title="2 Record Locking"></a>2 Record Locking</h2><p><em>Record locking</em> is the term normally used to describe the ability of a process to prevent other processes from modifying a region of a file while the first process is reading or modifying that portion of the file. Under the UNIX System, ‘‘record’’ is a misnomer; the UNIX kernel does not have a notion of records in a file. A better term is <em>byte-range locking</em>, given that it is a range of a file (possibly the entire file) that is locked.</p><p>demo可以参考<a href="https://blog.csdn.net/anonymalias/article/details/9197641" target="_blank" rel="noopener">Linux进程同步之记录锁</a>和书中示例。对于像锁的隐含继承与实现这些细节问题可参考书籍。</p><h2 id="3-I-O-Multiplexing"><a href="#3-I-O-Multiplexing" class="headerlink" title="3 I/O Multiplexing"></a>3 I/O Multiplexing</h2><p>参考<a href="http://liujunming.top/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/" target="_blank" rel="noopener">IO - 同步，异步，阻塞，非阻塞</a></p><ul><li><code>select</code> and <code>pselect</code> Functions</li><li><code>poll</code> Function</li></ul><h2 id="4-Asynchronous-I-O"><a href="#4-Asynchronous-I-O" class="headerlink" title="4 Asynchronous I/O"></a>4 Asynchronous I/O</h2><p>参考<a href="http://liujunming.top/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/" target="_blank" rel="noopener">IO - 同步，异步，阻塞，非阻塞</a></p><h2 id="5-readv-and-writev-Functions"><a href="#5-readv-and-writev-Functions" class="headerlink" title="5 readv and writev Functions"></a>5 <code>readv</code> and <code>writev</code> Functions</h2><p>The <code>readv</code> and <code>writev</code> functions let us read into and write from multiple noncontiguous buffers in a single function call. These operations are called <em>scatter read</em> and <em>gather write</em>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> readv(<span class="keyword">int</span> fd, <span class="keyword">const</span> struct iovec *iov, <span class="keyword">int</span> iovcnt); </span><br><span class="line"><span class="keyword">ssize_t</span> writev(<span class="keyword">int</span> fd, <span class="keyword">const</span> struct iovec *iov, <span class="keyword">int</span> iovcnt);</span><br></pre></td></tr></table></figure></p><p>The second argument to both functions is a pointer to an array of <code>iovec</code> structures:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> &#123;</span></span><br><span class="line">    <span class="keyword">void</span>   *iov_base;  <span class="comment">/* starting address of buffer */</span></span><br><span class="line">    <span class="keyword">size_t</span>  iov_len;   <span class="comment">/* size of buffer */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The number of elements in the iov array is specified by iovcnt.Figure 14.22 shows a diagram relating the arguments to these two functions and the <code>iovec</code> structure.<br><img src="/images/2018/12/58.png" alt=""></p><h2 id="6-readn-and-writen-Functions"><a href="#6-readn-and-writen-Functions" class="headerlink" title="6 readn and writen Functions"></a>6 <code>readn</code> and <code>writen</code> Functions</h2><p>Pipes, FIFOs, and some devices—notably terminals and networks—have the following two properties:</p><ol><li>A <code>read</code> operation may return less than asked for, even though we have not encountered the end of file. This is not an error, and we should simply continue reading from the device.</li><li>A <code>write</code> operation can return less than we specified. Again, it’s not an error, and we should continue writing the remainder of the data.</li></ol><p>Generally, when we read from or write to a pipe, network device, or terminal, we need to take these characteristics into consideration. We can use the <code>readn</code> and <code>writen</code> functions to read and write N bytes of data, respectively, letting these functions handle a return value that’s possibly less than requested. These two functions simply call read or write as many times as required to <code>read</code> or <code>write</code> the entire N bytes of data.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> readn(<span class="keyword">int</span> fd, <span class="keyword">void</span> *buf, <span class="keyword">size_t</span> nbytes); </span><br><span class="line"><span class="keyword">ssize_t</span> writen(<span class="keyword">int</span> fd, <span class="keyword">void</span> *buf, <span class="keyword">size_t</span> nbytes);</span><br></pre></td></tr></table></figure></p><h2 id="7-Memory-Mapped-I-O"><a href="#7-Memory-Mapped-I-O" class="headerlink" title="7 Memory-Mapped I/O"></a>7 Memory-Mapped I/O</h2><p>Memory-mapped I/O lets us map a file on disk into a buffer in memory so that, when we fetch bytes from the buffer, the corresponding bytes of the file are read. Similarly, when we store data in the buffer, the corresponding bytes are automatically written to the file. This lets us perform I/O without using <code>read</code> or <code>write</code>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter covers numerous topics and functions that we lump under the term &lt;em&gt;advanced I/O&lt;/em&gt; : nonblocking I/O, record locking, I/O multiplexing (the &lt;code&gt;select&lt;/code&gt; and &lt;code&gt;poll&lt;/code&gt; functions), asynchronous I/O, the &lt;code&gt;readv&lt;/code&gt; and &lt;code&gt;writev&lt;/code&gt; functions, and memory-mapped I/O (&lt;code&gt;mmap&lt;/code&gt;).
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>IO - 同步，异步，阻塞，非阻塞</title>
    <link href="http://liujunming.github.io/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/"/>
    <id>http://liujunming.github.io/2018/12/26/IO-同步，异步，阻塞，非阻塞/</id>
    <published>2018-12-26T01:58:29.000Z</published>
    <updated>2018-12-26T06:57:52.226Z</updated>
    
    <content type="html"><![CDATA[<p>本文是对阻塞、非阻塞、同步、异步、I/O多路复用的总结。<a id="more"></a> </p><h2 id="1-阻塞-I-O（blocking-IO）"><a href="#1-阻塞-I-O（blocking-IO）" class="headerlink" title="1 阻塞 I/O（blocking IO）"></a>1 阻塞 I/O（blocking IO）</h2><p><img src="/images/2018/12/53.png" alt=""><br>blocking IO的特点就是在IO执行的两个阶段都被block了。</p><h2 id="2-非阻塞-I-O（nonblocking-IO）"><a href="#2-非阻塞-I-O（nonblocking-IO）" class="headerlink" title="2 非阻塞 I/O（nonblocking IO）"></a>2 非阻塞 I/O（nonblocking IO）</h2><p><img src="/images/2018/12/54.png" alt=""><br>nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。</p><h2 id="3-I-O-多路复用（-I-O-multiplexing）"><a href="#3-I-O-多路复用（-I-O-multiplexing）" class="headerlink" title="3 I/O 多路复用（ I/O multiplexing）"></a>3 I/O 多路复用（ I/O multiplexing）</h2><p><img src="/images/2018/12/55.png" alt=""><br>I/O multiplexing本质上是多条I/O路径共用同一个线程。<br>详情可以参考<a href="https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/" target="_blank" rel="noopener">select, poll, and epoll</a></p><h2 id="4-异步-I-O（asynchronous-IO）"><a href="#4-异步-I-O（asynchronous-IO）" class="headerlink" title="4 异步 I/O（asynchronous IO）"></a>4 异步 I/O（asynchronous IO）</h2><p><img src="/images/2018/12/56.png" alt=""><br>异步I/O通知可以采取两种方式：</p><ol><li>使用信号进行异步通知，如上图所示，示例可以参考<a href="http://www.informit.com/articles/article.aspx?p=607373&amp;seqNum=4" target="_blank" rel="noopener">POSIX Asynchronous I/O</a></li><li>使用回调函数进行异步通知，示例可以参考<a href="https://blog.csdn.net/Shreck66/article/details/48765533" target="_blank" rel="noopener">linux下aio异步读写详解与实例</a>和<a href="https://stackoverflow.com/questions/5153972/unix-linux-signal-handling-sigev-thread" target="_blank" rel="noopener">UNIX/Linux signal handling: SIGEV_THREAD</a></li></ol><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><h3 id="5-1-blocking和non-blocking的区别"><a href="#5-1-blocking和non-blocking的区别" class="headerlink" title="5.1 blocking和non-blocking的区别"></a>5.1 blocking和non-blocking的区别</h3><p>调用blocking IO会一直block住对应的进程，直到操作完成；而non-blocking IO在kernel还未准备好数据的情况下会立刻返回。</p><h3 id="5-2-synchronous-IO和asynchronous-IO的区别"><a href="#5-2-synchronous-IO和asynchronous-IO的区别" class="headerlink" title="5.2 synchronous IO和asynchronous IO的区别"></a>5.2 synchronous IO和asynchronous IO的区别</h3><p>在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的：</p><ul><li>A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;</li><li>An asynchronous I/O operation does not cause the requesting process to be blocked;</li></ul><p>两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。</p><p>有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的<code>read</code>这个system call。non-blocking IO在执行<code>read</code>这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，<code>read</code>会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。</p><p>而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。</p><h3 id="5-3-各个IO-Model的比较"><a href="#5-3-各个IO-Model的比较" class="headerlink" title="5.3 各个IO Model的比较"></a>5.3 各个IO Model的比较</h3><p><img src="/images/2018/12/57.png" alt=""><br>通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用<code>read</code>来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知或者调用回调函数。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。</p><hr><p>参考资料：</p><ol><li><a href="https://segmentfault.com/a/1190000003063859" target="_blank" rel="noopener">Linux IO模式及 select、poll、epoll详解</a></li><li><a href="https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/" target="_blank" rel="noopener">select, poll, and epoll</a></li><li><a href="http://davmac.org/davpage/linux/async-io.html" target="_blank" rel="noopener">Asynchronous I/O and event notification on linux</a></li><li><a href="https://fwheel.net/aio.html" target="_blank" rel="noopener">Asynchronous File I/O on Linux</a></li><li><a href="https://blog.csdn.net/Shreck66/article/details/48765533" target="_blank" rel="noopener">linux下aio异步读写详解与实例</a></li><li><a href="https://www.systutorials.com/docs/linux/man/7-aio/" target="_blank" rel="noopener">aio (7) - Linux Man Pages</a></li><li><a href="https://stackoverflow.com/questions/5153972/unix-linux-signal-handling-sigev-thread" target="_blank" rel="noopener">UNIX/Linux signal handling: SIGEV_THREAD</a></li><li><a href="http://www.informit.com/articles/article.aspx?p=607373&amp;seqNum=4" target="_blank" rel="noopener">POSIX Asynchronous I/O</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是对阻塞、非阻塞、同步、异步、I/O多路复用的总结。
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Signals</title>
    <link href="http://liujunming.github.io/2018/12/25/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Signals/"/>
    <id>http://liujunming.github.io/2018/12/25/apue-读书笔记-Signals/</id>
    <published>2018-12-25T01:42:04.000Z</published>
    <updated>2018-12-25T05:54:14.341Z</updated>
    
    <content type="html"><![CDATA[<p>Signals are software interrupts. Most nontrivial application programs need to deal with signals. Signals provide a way of handling asynchronous events—for example, a user at a terminal typing the interrupt key to stop a program.<a id="more"></a> </p><h2 id="1-Signal-Concepts"><a href="#1-Signal-Concepts" class="headerlink" title="1 Signal Concepts"></a>1 Signal Concepts</h2><p>First, every signal has a name. These names all begin with the three characters <code>SIG</code>. For example, <code>SIGABRT</code> is the abort signal that is generated when a process calls the <code>abort</code> function. <code>SIGALRM</code> is the alarm signal that is generated when the timer set by the <code>alarm</code> function goes off.</p><p>Signal names are all defined by positive integer constants (the signal number) in the header <code>&lt;signal.h&gt;</code>.</p><p>Numerous conditions can generate a signal:</p><ul><li>The terminal-generated signals occur when users press certain terminal keys. Pressing the Control-C key on the terminal normally causes the interrupt signal (<code>SIGINT</code>) to be generated. This is how to stop a runaway program.</li><li>Hardware exceptions generate signals: divide by 0, invalid memory reference, and the like. These conditions are usually detected by the hardware, and the kernel is notified. The kernel then generates the appropriate signal for the process that was running at the time the condition occurred. For example, <code>SIGSEGV</code> is generated for a process that executes an invalid memory reference.</li><li>The <code>kill</code> function allows a process to send any signal to another process or process group. Naturally, there are limitations: we have to be the owner of the process that we’re sending the signal to, or we have to be the superuser.</li><li>The <code>kill</code> command allows us to send signals to other processes. This command is often used to terminate a runaway background process.</li><li>Software conditions can generate signals when a process should be notified of various events. </li></ul><p>Signals are classic examples of asynchronous events. They occur at what appear to be random times to the process. The process can’t simply test a variable (such as <code>errno</code>) to see whether a signal has occurred; instead, the process has to tell the kernel ‘‘if and when this signal occurs, do the following.’’</p><p>We can tell the kernel to do one of three things when a signal occurs. We call this the <code>disposition</code> of the signal, or the <code>action</code> associated with a signal.</p><ol><li>Ignore the signal. </li><li>Catch the signal. To do this, we tell the kernel to call a function of ours whenever the signal occurs. In our function, we can do whatever we want to handle the condition.</li><li>Let the default action apply. Every signal has a default action, shown in Figure 10.1. Note that the default action for most signals is to terminate the process.</li></ol><p><img src="/images/2018/12/52.png" alt=""></p><p>When the default action is labeled ‘‘terminate+core,’’ it means that a memory image of the process is left in the file named <code>core</code> of the current working directory of the process. This file can be used with most UNIX System debuggers to examine the state of the process at the time it terminated.<br><a href="https://book.douban.com/annotation/15103265/" target="_blank" rel="noopener">core笔记</a></p><h2 id="2-signal-Function"><a href="#2-signal-Function" class="headerlink" title="2 signal Function"></a>2 <code>signal</code> Function</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> (*signal(<span class="keyword">int</span> signo, <span class="keyword">void</span> (*func)(<span class="keyword">int</span>)))(<span class="keyword">int</span>);</span><br></pre></td></tr></table></figure><p>The <code>signo</code> argument is just the name of the signal from Figure 10.1. The value of <code>func</code> is (a) the constant <code>SIG_IGN</code>, (b) the constant <code>SIG_DFL</code>, or (c) the address of a function to be called when the signal occurs. If we specify <code>SIG_IGN</code>, we are telling the system to ignore the signal. (Remember that we cannot ignore the two signals <code>SIGKILL</code> and <code>SIGSTOP</code>.) When we specify <code>SIG_DFL</code>, we are setting the action associated with the signal to its default value. When we specify the address of a function to be called when the signal occurs, we are arranging to ‘‘catch’’ the signal. We call the function either the <em>signal handler</em> or the <em>signal-catching function</em>.</p><h2 id="3-Unreliable-Signals"><a href="#3-Unreliable-Signals" class="headerlink" title="3 Unreliable Signals"></a>3 Unreliable Signals</h2><p>Signals were unreliable. By this we mean that signals could get lost: a signal could occur and the process would never know about it.</p><h2 id="4-Interrupted-System-Calls"><a href="#4-Interrupted-System-Calls" class="headerlink" title="4 Interrupted System Calls"></a>4 Interrupted System Calls</h2><h2 id="5-Reentrant-Functions"><a href="#5-Reentrant-Functions" class="headerlink" title="5 Reentrant Functions"></a>5 Reentrant Functions</h2><p><a href="https://stackoverflow.com/questions/34758863/what-is-reentrant-function-in-c" target="_blank" rel="noopener">re-entrancy</a></p><h2 id="6-Reliable-Signal-Terminology-and-Semantics"><a href="#6-Reliable-Signal-Terminology-and-Semantics" class="headerlink" title="6 Reliable-Signal Terminology and Semantics"></a>6 Reliable-Signal Terminology and Semantics</h2><p>First, a signal is <em>generated</em> for a process (or sent to a process) when the event that causes the signal occurs. The event could be a hardware exception (e.g., divide by 0), a software condition (e.g., an <code>alarm</code> timer expiring), a terminal-generated signal, or a call to the <code>kill</code> function. When the signal is generated, the kernel usually sets a flag of some form in the process table.</p><p>We say that a signal is <em>delivered</em> to a process when the action for a signal is taken. During the time between the generation of a signal and its delivery, the signal is said to be <em>pending</em>.</p><p>A process has the option of <em>blocking</em> the delivery of a signal. If a signal that is blocked is generated for a process, and if the action for that signal is either the default action or to catch the signal, then the signal remains pending for the process until the process either (a) unblocks the signal or (b) changes the action to ignore the signal.  The system determines what to do with a blocked signal when the signal is delivered, not when it’s generated.This allows the process to change the action for the signal before it’s delivered. The <code>sigpending</code> function  can be called by a process to determine which signals are blocked and pending.</p><p>Each process has a signal mask that defines the set of signals currently blocked from delivery to that process. We can think of this mask as having one bit for each possible signal. If the bit is on for a given signal, that signal is currently blocked.A process can examine and change its current signal mask by calling <code>sigprocmask</code>.</p><p>Since it is possible for the number of signals to exceed the number of bits in an integer, POSIX.1 defines a data type, called <code>sigset_t</code>, that holds a <em>signal set</em>. The signal mask, for example, is stored in one of these signal sets.</p><h2 id="7-kill-and-raise-Functions"><a href="#7-kill-and-raise-Functions" class="headerlink" title="7 kill and raise Functions"></a>7 <code>kill</code> and <code>raise</code> Functions</h2><p>The <code>kill</code> function sends a signal to a process or a group of processes. The <code>raise</code> function allows a process to send a signal to itself.</p><h2 id="8-alarm-and-pause-Functions"><a href="#8-alarm-and-pause-Functions" class="headerlink" title="8 alarm and pause Functions"></a>8 <code>alarm</code> and <code>pause</code> Functions</h2><p>The <code>alarm</code> function allows us to set a timer that will expire at a specified time in the future. When the timer expires, the <code>SIGALRM</code> signal is generated. If we ignore or don’t catch this signal, its default action is to terminate the process.</p><p>The <code>pause</code> function suspends the calling process until a signal is caught.</p><h2 id="9-Signal-Sets"><a href="#9-Signal-Sets" class="headerlink" title="9 Signal Sets"></a>9 Signal Sets</h2><h2 id="10-sigprocmask-Function"><a href="#10-sigprocmask-Function" class="headerlink" title="10 sigprocmask Function"></a>10 <code>sigprocmask</code> Function</h2><h2 id="11-sigpending-Function"><a href="#11-sigpending-Function" class="headerlink" title="11 sigpending Function"></a>11 <code>sigpending</code> Function</h2><h2 id="12-sigaction-Function"><a href="#12-sigaction-Function" class="headerlink" title="12 sigaction Function"></a>12 <code>sigaction</code> Function</h2><p>The <code>sigaction</code> function allows us to examine or modify (or both) the action associated with a particular signal.</p><h2 id="13-sigsetjmp-and-siglongjmp-Functions"><a href="#13-sigsetjmp-and-siglongjmp-Functions" class="headerlink" title="13 sigsetjmp and siglongjmp Functions"></a>13 <code>sigsetjmp</code> and <code>siglongjmp</code> Functions</h2><h2 id="14-sigsuspend-Function"><a href="#14-sigsuspend-Function" class="headerlink" title="14 sigsuspend Function"></a>14 <code>sigsuspend</code> Function</h2><h2 id="15-abort-Function"><a href="#15-abort-Function" class="headerlink" title="15 abort Function"></a>15 <code>abort</code> Function</h2><h2 id="16-system-Function"><a href="#16-system-Function" class="headerlink" title="16 system Function"></a>16 <code>system</code> Function</h2><h2 id="17-sleep-nanosleep-and-clock-nanosleep-Functions"><a href="#17-sleep-nanosleep-and-clock-nanosleep-Functions" class="headerlink" title="17 sleep, nanosleep, and clock_nanosleep Functions"></a>17 <code>sleep</code>, <code>nanosleep</code>, and <code>clock_nanosleep</code> Functions</h2><h2 id="18-sigqueue-Function"><a href="#18-sigqueue-Function" class="headerlink" title="18 sigqueue Function"></a>18 <code>sigqueue</code> Function</h2><h2 id="19-Job-Control-Signals"><a href="#19-Job-Control-Signals" class="headerlink" title="19 Job-Control Signals"></a>19 Job-Control Signals</h2><h2 id="20-Signal-Names-and-Numbers"><a href="#20-Signal-Names-and-Numbers" class="headerlink" title="20 Signal Names and Numbers"></a>20 Signal Names and Numbers</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Signals are software interrupts. Most nontrivial application programs need to deal with signals. Signals provide a way of handling asynchronous events—for example, a user at a terminal typing the interrupt key to stop a program.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Process Relationships</title>
    <link href="http://liujunming.github.io/2018/12/24/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Relationships/"/>
    <id>http://liujunming.github.io/2018/12/24/apue-读书笔记-Process-Relationships/</id>
    <published>2018-12-24T02:13:44.000Z</published>
    <updated>2018-12-25T01:52:01.137Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we’ll look at process groups in more detail and the concept of sessions that was introduced by POSIX.1. We’ll also look at the relationship between the login shell that is invoked for us when we login and all the processes that we start from our login shell.<a id="more"></a> </p><h2 id="1-Terminal-Logins"><a href="#1-Terminal-Logins" class="headerlink" title="1 Terminal Logins"></a>1 Terminal Logins</h2><p>Let’s start by looking at the programs that are executed when we login to a UNIX system.</p><p><strong>BSD Terminal Logins</strong><br>The system administrator creates a file, usually <code>/etc/ttys</code>, that has one line per terminal device.Each line specifies the name of the device and other parameters that are passed to the <code>getty</code> program. When the system is bootstrapped, the kernel creates process ID 1, the <code>init</code> process, and it is <code>init</code> that brings the system up in multiuser mode.The <code>init</code> process reads the file <code>/etc/ttys</code> and, for every terminal device that allows a login, does a <code>fork</code> followed by an <code>exec</code> of the program <code>getty</code>. This gives us the processes shown in Figure 9.1.<br><img src="/images/2018/12/47.png" alt=""></p><p>It is <code>getty</code> that calls <code>open</code> for the terminal device. The terminal is opened for reading and writing.Once the device is open, file descriptors 0, 1, and 2 are set to the device. Then <code>getty</code> outputs something like <code>login:</code> and waits for us to enter our user name.<br>When we enter our user name, <code>getty</code>’s job is complete, and it then invokes the <code>login</code> program, similar to:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">execle(<span class="string">"/bin/login"</span>, <span class="string">"login"</span>, <span class="string">"-p"</span>, username, (<span class="keyword">char</span> *)<span class="number">0</span>, envp);</span><br></pre></td></tr></table></figure></p><p>Figure 9.2 shows the state of these processes right after <code>login</code> has been invoked.<br><img src="/images/2018/12/48.png" alt=""><br><img src="/images/2018/12/49.png" alt=""></p><h2 id="2-Network-Logins"><a href="#2-Network-Logins" class="headerlink" title="2 Network Logins"></a>2 Network Logins</h2><p>The main (physical) difference between logging in to a system through a serial terminal and logging in to a system through a network is that the connection between the terminal and the computer isn’t point-to-point.</p><h2 id="3-Process-Groups"><a href="#3-Process-Groups" class="headerlink" title="3 Process Groups"></a>3 Process Groups</h2><p>In addition to having a process ID, each process belongs to a process group.</p><p>A process group is a collection of one or more processes, usually associated with the same job, that can receive signals from the same terminal. Each process group has a unique process group ID. Process group IDs are similar to process IDs: they are positive integers and can be stored in a <code>pid_t</code> data type.</p><p>Each process group can have a process group leader. The leader is identified by its process group ID being equal to its process ID.</p><p>It is possible for a process group leader to create a process group, create processes in the group, and then terminate.</p><p>A process joins an existing process group or creates a new process group by calling <code>setpgid</code>.</p><h2 id="4-Sessions"><a href="#4-Sessions" class="headerlink" title="4 Sessions"></a>4 Sessions</h2><p>A session is a collection of one or more process groups. For example, we could have the arrangement shown in Figure 9.6. Here we have three process groups in a single session.<br><img src="/images/2018/12/50.png" alt=""><br>The processes in a process group are usually placed there by a shell pipeline. For example, the arrangement shown in Figure 9.6 could have been generated by shell commands of the form<br><code>proc1 | proc2 &amp; proc3 | proc4 | proc5</code></p><p>A process establishes a new session by calling the <code>setsid</code> function.<br>If the calling process is not a process group leader, this function creates a new session. Three things happen.</p><ol><li>The process becomes the session leader of this new session. (A session leader is the process that creates a session.) The process is the only process in this new session.</li><li>The process becomes the process group leader of a new process group. The new process group ID is the process ID of the calling process.</li><li>The process has no controlling terminal. If the process had a controlling terminal before calling setsid, that association is broken.</li></ol><h2 id="5-Controlling-Terminal"><a href="#5-Controlling-Terminal" class="headerlink" title="5 Controlling Terminal"></a>5 Controlling Terminal</h2><p>Sessions and process groups have a few other characteristics.</p><ul><li>A session can have a single <em>controlling terminal</em>. This is usually the terminal device (in the case of a terminal login) or pseudo terminal device (in the case of a network login) on which we login.</li><li>The session leader that establishes the connection to the controlling terminal is called the <em>controlling process</em>.</li><li>The process groups within a session can be divided into a single <em>foreground process group</em> and one or more <em>background process groups</em>.</li><li>If a session has a controlling terminal, it has a single foreground process group and all other process groups in the session are background process groups.</li><li>Whenever we press the terminal’s interrupt key (often Control-C), the interrupt signal is sent to all processes in the foreground process group.</li><li>Whenever we press the terminal’s quit key (often Control-backslash), the quit signal is sent to all processes in the foreground process group.</li><li>If a modem (or network) disconnect is detected by the terminal interface, the hang-up signal is sent to the controlling process (the session leader).</li></ul><p>These characteristics are shown in Figure 9.7.<br><img src="/images/2018/12/51.png" alt=""></p><h2 id="6-tcgetpgrp-tcsetpgrp-and-tcgetsid-Functions"><a href="#6-tcgetpgrp-tcsetpgrp-and-tcgetsid-Functions" class="headerlink" title="6 tcgetpgrp, tcsetpgrp, and tcgetsid Functions"></a>6 <code>tcgetpgrp</code>, <code>tcsetpgrp</code>, and <code>tcgetsid</code> Functions</h2><p>We need a way to tell the kernel which process group is the foreground process group, so that the terminal device driver knows where to send the terminal input and the terminal-generated signals (Figure 9.7).</p><h2 id="7-Job-Control"><a href="#7-Job-Control" class="headerlink" title="7 Job Control"></a>7 Job Control</h2><p>Job control allows us to start multiple jobs (groups of processes) from a single terminal and to control which jobs can access the terminal and which jobs are run in the background. </p><h2 id="8-Shell-Execution-of-Programs"><a href="#8-Shell-Execution-of-Programs" class="headerlink" title="8 Shell Execution of Programs"></a>8 Shell Execution of Programs</h2><p>Let’s examine how the shells execute programs and how this relates to the concepts of process groups, controlling terminals, and sessions.</p><h2 id="9-Orphaned-Process-Groups"><a href="#9-Orphaned-Process-Groups" class="headerlink" title="9 Orphaned Process Groups"></a>9 Orphaned Process Groups</h2><p>We’ve mentioned that a process whose parent terminates is called an orphan and is inherited by the <code>init</code> process. We now look at entire process groups that can be orphaned and see how POSIX.1 handles this situation.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we’ll look at process groups in more detail and the concept of sessions that was introduced by POSIX.1. We’ll also look at the relationship between the login shell that is invoked for us when we login and all the processes that we start from our login shell.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记- Process Control</title>
    <link href="http://liujunming.github.io/2018/12/23/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Control/"/>
    <id>http://liujunming.github.io/2018/12/23/apue-读书笔记-Process-Control/</id>
    <published>2018-12-23T05:36:08.000Z</published>
    <updated>2018-12-24T10:25:33.274Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Process-Identifiers"><a href="#1-Process-Identifiers" class="headerlink" title="1 Process Identifiers"></a>1 Process Identifiers</h2><p>Every process has a unique process ID, a non-negative integer. The process ID is the only well-known identifier of a process that is always unique.<a id="more"></a> </p><p>Process ID 0 is usually the scheduler process.<br>Process ID 1 is usually the <code>init</code> process and is invoked by the kernel at the end of the bootstrap procedure. </p><h2 id="2-fork-Function"><a href="#2-fork-Function" class="headerlink" title="2 fork Function"></a>2 <code>fork</code> Function</h2><p>An existing process can create a new one by calling the <code>fork</code> function.<br>The new process created by <code>fork</code> is called the <em>child process</em>. This function is called once but returns twice. The only difference in the returns is that the return value in the child is 0, whereas the return value in the parent is the process ID of the new child.</p><p><strong>File Sharing</strong><br>One characteristic of <code>fork</code> is that all file descriptors that are open in the parent are duplicated in the child. We say ‘‘duplicated’’ because it’s as if the <code>dup</code> function had been called for each descriptor. The parent and the child share a file table entry for every open descriptor.</p><p>Consider a process that has three different files opened for standard input, standard output, and standard error. On return from <code>fork</code>, we have the arrangement shown in Figure 8.2.<br><img src="/images/2018/12/45.png" alt=""><br>It is important that the parent and the child share the same file offset.</p><h2 id="3-exit-Functions"><a href="#3-exit-Functions" class="headerlink" title="3 exit Functions"></a>3 <code>exit</code> Functions</h2><p>Regardless of how a process terminates, the same code in the kernel is eventually executed. This kernel code closes all the open descriptors for the process, releases the memory that it was using, and so on.</p><p>For any of the preceding cases, we want the terminating process to be able to notify its parent how it terminated. For the three exit functions (<code>exit</code>, <code>_exit</code>, and <code>_Exit</code>), this is done by passing an exit status as the argument to the function. In the case of an abnormal termination, however, the kernel—not the process—generates a termination status to indicate the reason for the abnormal termination. In any case, the parent of the process can obtain the termination status from either the <code>wait</code> or the <code>waitpid</code> function.</p><p>What happens if the parent terminates before the child? The answer is that the <code>init</code> process becomes the parent process of any process whose parent terminates. In such a case, we say that the process has been inherited by init.</p><p>Another condition we have to worry about is when a child terminates before its parent. If the child completely disappeared, the parent wouldn’t be able to fetch its termination status when and if the parent was finally ready to check if the child had terminated. The kernel keeps a small amount of information for every terminating process, so that the information is available when the parent of the terminating process calls <code>wait</code> or <code>waitpid</code>. Minimally, this information consists of the process ID, the termination status of the process, and the amount of CPU time taken by the process. The kernel can discard all the memory used by the process and close its open files. In UNIX System terminology, a process that has terminated, but whose parent has not yet waited for it, is called a <em>zombie</em>. The <code>ps</code> command prints the state of a zombie process as Z.</p><p>The final condition to consider is this: What happens when a process that has been inherited by <code>init</code> terminates? Does it become a zombie? The answer is ‘‘no,’’ because <code>init</code> is written so that whenever one of its children terminates, <code>init</code> calls one of the wait functions to fetch the termination status. By doing this, <code>init</code> prevents the system from being clogged by zombies.</p><h2 id="4-wait-and-waitpid-Functions"><a href="#4-wait-and-waitpid-Functions" class="headerlink" title="4 wait and waitpid Functions"></a>4 <code>wait</code> and <code>waitpid</code> Functions</h2><p>When a process terminates, either normally or abnormally, the kernel notifies the parent by sending the <code>SIGCHLD</code> signal to the parent. Because the termination of a child is an asynchronous event—it can happen at any time while the parent is running—this signal is the asynchronous notification from the kernel to the parent. The parent can choose to ignore this signal, or it can provide a function that is called when the signal occurs: a signal handler. The default action for this signal is to be ignored. For now, we need to be aware that a process that calls <code>wait</code> or <code>waitpid</code> can:</p><ul><li>Block, if all of its children are still running</li><li>Return immediately with the termination status of a child, if a child has terminated and is waiting for its termination status to be fetched</li><li>Return immediately with an error, if it doesn’t have any child processes</li></ul><p>If the process is calling <code>wait</code> because it received the <code>SIGCHLD</code> signal, we expect <code>wait</code> to return immediately. But if we call it at any random point in time, it can block.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pid_t</span> wait(<span class="keyword">int</span> *statloc);</span><br><span class="line"><span class="keyword">pid_t</span> waitpid(<span class="keyword">pid_t</span> pid, <span class="keyword">int</span> *statloc, <span class="keyword">int</span> options);</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/12/46.png" alt=""></p><p>The <code>waitpid</code> function provides three features that aren’t provided by the <code>wait</code><br>function.</p><ol><li>The <code>waitpid</code> function lets us wait for one particular process, whereas the <code>wait</code> function returns the status of any terminated child.</li><li>The <code>waitpid</code> function provides a nonblocking version of <code>wait</code>. There are times when we want to fetch a child’s status, but we don’t want to block.</li><li>The <code>waitpid</code> function provides support for job control with the <code>WUNTRACED</code> and <code>WCONTINUED</code> options.</li></ol><h2 id="5-waitid-Function"><a href="#5-waitid-Function" class="headerlink" title="5 waitid Function"></a>5 <code>waitid</code> Function</h2><h2 id="6-wait3-and-wait4-Functions"><a href="#6-wait3-and-wait4-Functions" class="headerlink" title="6 wait3 and wait4 Functions"></a>6 <code>wait3</code> and <code>wait4</code> Functions</h2><h2 id="7-exec-Functions"><a href="#7-exec-Functions" class="headerlink" title="7 exec Functions"></a>7 <code>exec</code> Functions</h2><p>One use of the <code>fork</code> function is to create a new process (the child) that then causes another program to be executed by calling one of the <code>exec</code> functions. When a process calls one of the <code>exec</code> functions, that process is completely replaced by the new program, and the new program starts executing at its <code>main</code> function. The process ID does not change across an <code>exec</code>, because a new process is not created; <code>exec</code> merely replaces the current process—its text, data, heap, and stack segments—with a brand-new program from disk.</p><p>With <code>fork</code>, we can create new processes; and with the <code>exec</code> functions, we can initiate new programs. The <code>exit</code> function and the <code>wait</code> functions handle termination and waiting for termination. These are the only process control primitives we need.</p><h2 id="8-Changing-User-IDs-and-Group-IDs"><a href="#8-Changing-User-IDs-and-Group-IDs" class="headerlink" title="8 Changing User IDs and Group IDs"></a>8 Changing User IDs and Group IDs</h2><h2 id="9-Interpreter-Files"><a href="#9-Interpreter-Files" class="headerlink" title="9 Interpreter Files"></a>9 Interpreter Files</h2><h2 id="10-system-Function"><a href="#10-system-Function" class="headerlink" title="10 system Function"></a>10 <code>system</code> Function</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">system</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *cmdstring)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="11-Process-Accounting"><a href="#11-Process-Accounting" class="headerlink" title="11 Process Accounting"></a>11 Process Accounting</h2><p>Most UNIX systems provide an option to do process accounting. When enabled, the kernel writes an accounting record each time a process terminates. These accounting records typically contain a small amount of binary data with the name of the command, the amount of CPU time used, the user ID and group ID, the starting time, and so on.</p><h2 id="12-User-Identification"><a href="#12-User-Identification" class="headerlink" title="12 User Identification"></a>12 User Identification</h2><h2 id="13-Process-Scheduling"><a href="#13-Process-Scheduling" class="headerlink" title="13 Process Scheduling"></a>13 Process Scheduling</h2><p>The UNIX System provided processes with only coarse control over their scheduling priority. The scheduling policy and priority were determined by the kernel. A process could choose to run with lower priority by adjusting its <em>nice value</em> (thus a process could be ‘‘nice’’ and reduce its share of the CPU by adjusting its nice value). Only a privileged process was allowed to increase its scheduling priority.</p><h2 id="14-Process-Times"><a href="#14-Process-Times" class="headerlink" title="14 Process Times"></a>14 Process Times</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">clock_t</span> times(struct tms *buf);</span><br><span class="line"><span class="comment">//Returns: elapsed wall clock time in clock ticks if OK, −1 on error</span></span><br></pre></td></tr></table></figure><p>This function fills in the <code>tms</code> structure pointed to by <code>buf</code> :<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">tms</span> &#123;</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_utime;  <span class="comment">/* user CPU time */</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_stime;  <span class="comment">/* system CPU time */</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_cutime; <span class="comment">/* user CPU time, terminated children */</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_cstime; <span class="comment">/* system CPU time, terminated children */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Process-Identifiers&quot;&gt;&lt;a href=&quot;#1-Process-Identifiers&quot; class=&quot;headerlink&quot; title=&quot;1 Process Identifiers&quot;&gt;&lt;/a&gt;1 Process Identifiers&lt;/h2&gt;&lt;p&gt;Every process has a unique process ID, a non-negative integer. The process ID is the only well-known identifier of a process that is always unique.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Process Environment</title>
    <link href="http://liujunming.github.io/2018/12/23/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Environment/"/>
    <id>http://liujunming.github.io/2018/12/23/apue-读书笔记-Process-Environment/</id>
    <published>2018-12-23T03:42:28.000Z</published>
    <updated>2018-12-23T05:45:27.532Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we’ll see how the <code>main</code> function is called when the program is executed, how command-line arguments are passed to the new program, what the typical memory layout looks like, how to allocate additional memory, how the process can use environment variables, and various ways for the process to terminate. Additionally, we’ll look at the <code>longjmp</code> and <code>setjmp</code> functions and their interaction with the stack. We finish the chapter by examining the resource limits of a process.<a id="more"></a> </p><h2 id="1-main-Function"><a href="#1-main-Function" class="headerlink" title="1 main Function"></a>1 <code>main</code> Function</h2><p>A C program starts execution with a function called <code>main</code>. The prototype for the <code>main</code> function is<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="2-Process-Termination"><a href="#2-Process-Termination" class="headerlink" title="2 Process Termination"></a>2 Process Termination</h2><p>There are eight ways for a process to terminate. Normal termination occurs in five ways:</p><ol><li>Return from <code>main</code></li><li>Calling <code>exit</code></li><li>Calling <code>_exit</code> or <code>_Exit</code></li><li>Return of the last thread from its start routine</li><li>Calling <code>pthread_exit</code> from the last thread</li></ol><p>Abnormal termination occurs in three ways:</p><ol start="6"><li>Calling <code>abort</code> </li><li>Receipt of a signal </li><li>Response of the last thread to a cancellation request </li></ol><h3 id="2-1-Exit-Functions"><a href="#2-1-Exit-Functions" class="headerlink" title="2.1 Exit Functions"></a>2.1 Exit Functions</h3><p>Three functions terminate a program normally: <code>_exit</code> and <code>_Exit</code>, which return to the kernel immediately, and <code>exit</code>, which performs certain cleanup processing and then returns to the kernel.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">exit</span><span class="params">(<span class="keyword">int</span> status)</span></span>; </span><br><span class="line"><span class="keyword">void</span> _Exit(<span class="keyword">int</span> status);</span><br><span class="line"><span class="keyword">void</span> _exit(<span class="keyword">int</span> status);</span><br></pre></td></tr></table></figure></p><p>Historically, the <code>exit</code> function has always performed a clean shutdown of the standard I/O library: the <code>fclose</code> function is called for all open streams.</p><p><code>exit(0)</code> is the same as <code>return(0)</code> from the main function.</p><h3 id="2-2-atexit-Function"><a href="#2-2-atexit-Function" class="headerlink" title="2.2 atexit Function"></a>2.2 <code>atexit</code> Function</h3><p>With ISO C, a process can register at least 32 functions that are automatically called by <code>exit</code>. These are called <em>exit handlers</em> and are registered by calling the <code>atexit</code> function.<br><img src="/images/2018/12/42.png" alt=""></p><h2 id="3-Command-Line-Arguments"><a href="#3-Command-Line-Arguments" class="headerlink" title="3 Command-Line Arguments"></a>3 Command-Line Arguments</h2><p>When a program is executed, the process that does the <code>exec</code> can pass command-line arguments to the new program.</p><h2 id="4-Environment-List"><a href="#4-Environment-List" class="headerlink" title="4 Environment List"></a>4 Environment List</h2><p>Each program is also passed an <em>environment list</em>. Like the argument list, the environment list is an array of character pointers, with each pointer containing the address of a null-terminated C string. The address of the array of pointers is contained in the global variable <code>environ</code>:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="keyword">char</span> **environ;</span><br></pre></td></tr></table></figure></p><p>For example, if the environment consisted of five strings, it could look like Figure 7.5. Here we explicitly show the null bytes at the end of each string. We’ll call <code>environ</code> the <em>environment pointer</em>, the array of pointers the environment list, and the strings they point to the <em>environment strings</em>.<br><img src="/images/2018/12/43.png" alt=""></p><h2 id="5-Memory-Layout-of-a-C-Program"><a href="#5-Memory-Layout-of-a-C-Program" class="headerlink" title="5 Memory Layout of a C Program"></a>5 Memory Layout of a C Program</h2><p><img src="/images/2018/12/44.png" alt=""></p><h2 id="6-Shared-Libraries"><a href="#6-Shared-Libraries" class="headerlink" title="6 Shared Libraries"></a>6 Shared Libraries</h2><p>Shared libraries remove the common library routines from the executable file, instead maintaining a single copy of the library routine somewhere in memory that all processes reference.This reduces the size of each executable file but may add some runtime overhead, either when the program is first executed or the first time each shared library function is called. Another advantage of shared libraries is that library functions can be replaced with new versions without having to relink edit every program that uses the library (assuming that the number and type of arguments haven’t changed).</p><h2 id="7-Memory-Allocation"><a href="#7-Memory-Allocation" class="headerlink" title="7 Memory Allocation"></a>7 Memory Allocation</h2><p>ISO C specifies three functions for memory allocation:</p><ol><li><code>malloc</code>, which allocates a specified number of bytes of memory. The initial value of the memory is indeterminate.</li><li><code>calloc</code>, which allocates space for a specified number of objects of a specified size. The space is initialized to all 0 bits.</li><li><code>realloc</code>, which increases or decreases the size of a previously allocated area. </li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">malloc</span><span class="params">(<span class="keyword">size_t</span> size)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">calloc</span><span class="params">(<span class="keyword">size_t</span> nobj, <span class="keyword">size_t</span> size)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">realloc</span><span class="params">(<span class="keyword">void</span> *ptr, <span class="keyword">size_t</span> newsize)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="8-Environment-Variables"><a href="#8-Environment-Variables" class="headerlink" title="8 Environment Variables"></a>8 Environment Variables</h2><p> the environment strings are usually of the form:</p><p> <code>name=value</code></p><p>The UNIX kernel never looks at these strings; their interpretation is up to the various applications. The shells, for example, use numerous environment variables. Some, such as <code>HOME</code> and <code>USER</code>, are set automatically at login; others are left for us to set. We normally set environment variables in a shell start-up file to control the shell’s actions.</p><p>ISO C defines a function that we can use to fetch values from the environment.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">getenv</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p>In addition to fetching the value of an environment variable, sometimes we may want to set an environment variable.</p><h2 id="9-setjmp-and-longjmp-Functions"><a href="#9-setjmp-and-longjmp-Functions" class="headerlink" title="9 setjmp and longjmp Functions"></a>9 <code>setjmp</code> and <code>longjmp</code> Functions</h2><p>In C, we can’t <code>goto</code> a label that’s in another function. Instead, we must use the <code>setjmp</code> and <code>longjmp</code> functions to perform this type of branching.</p><h2 id="10-getrlimit-and-setrlimit-Functions"><a href="#10-getrlimit-and-setrlimit-Functions" class="headerlink" title="10 getrlimit and setrlimit Functions"></a>10 <code>getrlimit</code> and <code>setrlimit</code> Functions</h2><p>Every process has a set of resource limits, some of which can be queried and changed by the <code>getrlimit</code> and <code>setrlimit</code> functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getrlimit</span><span class="params">(<span class="keyword">int</span> resource, struct rlimit *rlptr)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">setrlimit</span><span class="params">(<span class="keyword">int</span> resource, <span class="keyword">const</span> struct rlimit *rlptr)</span></span>;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we’ll see how the &lt;code&gt;main&lt;/code&gt; function is called when the program is executed, how command-line arguments are passed to the new program, what the typical memory layout looks like, how to allocate additional memory, how the process can use environment variables, and various ways for the process to terminate. Additionally, we’ll look at the &lt;code&gt;longjmp&lt;/code&gt; and &lt;code&gt;setjmp&lt;/code&gt; functions and their interaction with the stack. We finish the chapter by examining the resource limits of a process.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
