<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2025-05-18T02:22:58.342Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes about NVMe dataset management</title>
    <link href="http://liujunming.github.io/2025/05/18/Notes-about-NVMe-dataset-management/"/>
    <id>http://liujunming.github.io/2025/05/18/Notes-about-NVMe-dataset-management/</id>
    <published>2025-05-18T01:52:35.000Z</published>
    <updated>2025-05-18T02:22:58.342Z</updated>
    
    <content type="html"><![CDATA[<p>NVMe dataset management就是driver给controller的hints(接下来的访问模式)，为此controller可以做些优化。<a id="more"></a></p><p><img src="/images/2025/05/012.png" alt></p><p><img src="/images/2025/05/013.png" alt></p><p>上图中的potion替换为portion</p><p><img src="/images/2025/05/011.png" alt></p><blockquote><p>The Dataset Management command is used by the host to indicate attributes for ranges of logical blocks. This includes attributes like frequency that data is read or written, access size, and other information that may be used to optimize performance and reliability. This command is advisory; a compliant controller may choose to take no action based on information provided.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://files.futurememorystorage.com/proceedings/2013/20130812_PreConfD_Marks.pdf" target="_blank" rel="noopener">An NVM Express Tutorial</a></li><li>NVMe spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NVMe dataset management就是driver给controller的hints(接下来的访问模式)，为此controller可以做些优化。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PGO &amp;&amp; HW-PGO</title>
    <link href="http://liujunming.github.io/2025/05/17/Notes-about-HW-PGO/"/>
    <id>http://liujunming.github.io/2025/05/17/Notes-about-HW-PGO/</id>
    <published>2025-05-16T23:46:59.000Z</published>
    <updated>2025-05-15T00:27:56.854Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PGO(Profile-Guided Optimization)和HW-PGO(Hardware Profile-Guided Optimization)的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Profile Guided Optimization (PGO) 是一种编译器优化技术，通过收集程序实际运行时的性能数据（Profile），指导编译器生成更高效的代码。其核心思想是“用实际运行行为指导优化”，相比传统的静态分析优化，PGO 能显著提升程序性能并减少代码体积。</p><h2 id="PGO-的工作原理"><a href="#PGO-的工作原理" class="headerlink" title="PGO 的工作原理"></a>PGO 的工作原理</h2><p>PGO 的实现通常分为三个阶段：</p><h3 id="插桩阶段（Instrumentation）"><a href="#插桩阶段（Instrumentation）" class="headerlink" title="插桩阶段（Instrumentation）"></a>插桩阶段（Instrumentation）</h3><p>编译器在代码中插入统计探针（Probes），生成带有数据采集功能的“插桩版本”程序。例如：</p><ul><li>统计函数调用频率</li><li>记录分支（if/else）的执行路径</li><li>跟踪循环迭代次数</li><li>监控热点代码区域（Hot Code）</li></ul><h3 id="数据收集阶段（Profiling）"><a href="#数据收集阶段（Profiling）" class="headerlink" title="数据收集阶段（Profiling）"></a>数据收集阶段（Profiling）</h3><p>运行插桩后的程序，模拟真实场景（如高负载、典型输入数据），生成运行时性能数据文件（如 <code>.profraw</code> 或 <code>.gcda</code> 文件）。</p><h3 id="优化阶段（Optimization）"><a href="#优化阶段（Optimization）" class="headerlink" title="优化阶段（Optimization）"></a>优化阶段（Optimization）</h3><p>编译器基于收集到的 Profile 数据重新编译代码，针对性优化：</p><ul><li>代码布局优化：将高频执行的代码段集中存放，提升指令缓存命中率。</li><li>分支预测优化：根据分支实际执行概率，优化条件判断顺序（如将高概率分支前置）。</li><li>函数内联（Inlining）：对高频调用的小函数进行内联，减少调用开销。</li><li>死代码消除：移除从未执行过的代码路径。</li><li>寄存器分配优化：优先为热点代码分配寄存器。</li></ul><h2 id="PGO-的优势"><a href="#PGO-的优势" class="headerlink" title="PGO 的优势"></a>PGO 的优势</h2><ol><li><p>性能提升<br>通过精准优化热点代码，典型场景下性能提升可达 10%-30%，尤其在分支密集或缓存敏感的代码中效果显著。<br>示例：<br>一个循环若实际运行中迭代次数固定，PGO 可将其展开为确定次数的指令，避免动态判断开销。</p></li><li><p>代码体积减少<br>移除未使用的代码路径，减小可执行文件大小。</p></li><li><p>优化更精准<br>静态优化可能因缺乏运行时信息而保守决策，PGO 则依赖真实数据，避免“过度优化”或“优化错误路径”。</p></li></ol><h2 id="PGO-的局限性"><a href="#PGO-的局限性" class="headerlink" title="PGO 的局限性"></a>PGO 的局限性</h2><ol><li><p>额外步骤与时间成本<br>需经历插桩、运行测试、二次编译，增加开发流程复杂度。</p></li><li><p>依赖场景代表性<br>若测试数据（Profile）不能覆盖真实场景，可能导致优化方向错误。<br>示例：<br>若测试时未触发某个分支，优化后可能错误删除该路径代码。</p></li><li><p>动态负载适应能力有限<br>对运行时行为变化剧烈的程序（如实时系统），静态 Profile 可能无法适应动态变化。</p></li></ol><h2 id="PGO-的应用场景"><a href="#PGO-的应用场景" class="headerlink" title="PGO 的应用场景"></a>PGO 的应用场景</h2><ul><li>高性能计算（HPC）：优化科学计算中的核心算法循环。</li><li>游戏引擎：提升渲染管线和物理模拟的性能。</li><li>数据库系统：加速查询执行计划的热点操作。</li><li>编译器自身优化：如 LLVM、GCC 使用 PGO 优化自身编译速度。</li></ul><h2 id="与传统优化的对比"><a href="#与传统优化的对比" class="headerlink" title="与传统优化的对比"></a>与传统优化的对比</h2><p><img src="/images/2025/05/008.png" alt></p><h2 id="硬件辅助PGO（如Intel-HW-PGO）"><a href="#硬件辅助PGO（如Intel-HW-PGO）" class="headerlink" title="硬件辅助PGO（如Intel HW-PGO）"></a>硬件辅助PGO（如Intel HW-PGO）</h2><p>传统 PGO 依赖软件插桩，而 硬件辅助 PGO 通过处理器内置的性能监控单元（PMU）直接采集硬件事件（如缓存未命中、分支预测失败），无需插桩即可生成更精细的 Profile。优势包括：</p><ul><li>更低开销：硬件级数据采集不影响程序性能。</li><li>更细粒度：可监控底层硬件行为（如指令级并行度）。</li></ul><p><img src="/images/2025/05/009.png" alt><br>这些技术允许在硬件（HW）中以更低的开销采集样本（可能一次性采集多个），并提供其他优势，例如减少采样偏移（Reduced-Skid）、精确分布（Precise Distribution）以及数据地址追踪（Data Address）。</p><p><img src="/images/2025/05/010.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>PGO 通过”用数据驱动优化”，在关键场景中显著提升程序效率，尤其适合性能敏感型应用。硬件辅助 PGO 进一步降低了数据采集成本，代表了编译器与硬件协同优化的未来趋势。</p><hr><p>参考资料:</p><ol><li>deepseek</li><li><a href="https://llvm.org/devmtg/2024-04/slides/TechnicalTalks/Xiao-EnablingHW-BasedPGO.pdf" target="_blank" rel="noopener">Enabling HW-based PGO for both Windows and Linux</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PGO(Profile-Guided Optimization)和HW-PGO(Hardware Profile-Guided Optimization)的相关notes。
    
    </summary>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="计算机系统" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes about IBGDA(InfiniBand GPUDirect Async)</title>
    <link href="http://liujunming.github.io/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/"/>
    <id>http://liujunming.github.io/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/</id>
    <published>2025-05-11T08:08:26.000Z</published>
    <updated>2025-05-11T12:20:09.729Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下IBGDA(InfiniBand GPUDirect Async)的相关notes。<a id="more"></a>内容主要转载自<a href="https://zhuanlan.zhihu.com/p/26082845081" target="_blank" rel="noopener">浅析DeepSeek中提到的IBGDA</a>。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><blockquote><p>Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency.</p></blockquote><p><a href="https://arxiv.org/pdf/2412.19437" target="_blank" rel="noopener">DeepSeek-V3 Technical Report</a></p><h2 id="Without-IBGDA"><a href="#Without-IBGDA" class="headerlink" title="Without IBGDA"></a>Without IBGDA</h2><p>在使用了GPU Direct RDMA的GPU中，网卡是怎么和GPU配合，实现将GPU的HBM的数据发送到远端的呢？</p><p><img src="/images/2025/05/006.png" alt></p><p>在引入InfiniBand GPUDirect Async(IBGDA)之前，是使用CPU上的代理线程来进行网络通信的。</p><p>此时流程是这样的：</p><ol><li>应用程序启动一个CUDA kernel，在GPU内存中产生数据</li><li>kernel function通过往CPU memory中的proxy buffer写入数据的方式，通知CPU要进行网络操作。我们将这个通知称为work descriptor, 它包含源地址、目标地址、数据大小及其他必要的网络信息</li><li>CPU上的proxy thread读取worker descriptor</li><li>CPU上的proxy thread发起相应的网络操作，将请求写入WQ</li><li>CPU会更新host memory中的doorbell record (DBR) buffer。（This buffer is used in the recovery path in case the NIC drops the write to its doorbell. 就是用来记录doorbell的信息，万一硬件来不及及时响应doorbell并把它丢掉，你还能从DBR buffer中恢复doorbell）</li><li>CPU通过写入NIC的 doorbell (DB)通知NIC。DB是NIC硬件中的一个寄存器</li><li>NIC从WQ中读取work descriptor</li><li>NIC使用GPUDirect RDMA直接从GPU内存搬运数据</li><li>NIC将数据传输到远程节点</li><li>NIC通过向主机内存中的CQ写入事件来指示网络操作已完成</li><li>CPU轮询CQ以检测网络操作的完成</li><li>CPU通知GPU操作已完成</li></ol><p>可以发现，这个过程竟然需要GPU, CPU, NIC三方参与。CPU就像是一个中转站，那么显然它有一些缺点：</p><ul><li>proxy thread消耗了CPU cycles</li><li>proxy thread成为瓶颈，导致在细粒度传输（小消息）时无法达到NIC的峰值吞吐。现代NIC每秒可以处理数亿个通信请求。GPU可以按照该速率生成请求，但CPU的处理速率低得多，造成了在细粒度通信时的瓶颈。</li></ul><h2 id="With-IBGDA"><a href="#With-IBGDA" class="headerlink" title="With IBGDA"></a>With IBGDA</h2><p>优化的方法显而易见，能否绕过CPU，让GPU自己来和网卡做交换呢？IBGDA给出了这样的功能。</p><p><img src="/images/2025/05/007.png" alt></p><ol><li>CPU程序启动一个CUDA kernel function，在GPU内存中生成数据</li><li>使用SM创建一个NIC work descriptor，并将其直接写入WQ。与CPU proxy thread不同，该WQ区位于GPU内存中</li><li>SM更新DBR buffer，它也位于GPU内存中</li><li>SM通过写入NIC的DB寄存器通知NIC</li><li>NIC使用GPUDirect RDMA从WQ读取工作描述符</li><li>NIC使用GPUDirect RDMA读取GPU内存中的数据</li><li>NIC将数据传输到远程节点</li><li>NIC通过使用GPUDirect RDMA向CQ缓冲区写入事件，通知GPU网络操作已完成</li></ol><p>可见，IBGDA消除了CPU在通信控制路径中的作用。在使用IBGDA时，GPU和NIC直接交换进行通信所需的信息。WQ和DBR buffer也被移到GPU内存中，以提高SM访问效率。</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/26082845081" target="_blank" rel="noopener">浅析DeepSeek中提到的IBGDA</a></li><li><a href="https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/" target="_blank" rel="noopener">Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async</a></li><li><a href="https://developer.nvidia.com/zh-cn/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/" target="_blank" rel="noopener">使用 NVIDIA Magnum IO NVSHMEM 和 GPUDirect Async 提高 HPC 系统的网络性能</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下IBGDA(InfiniBand GPUDirect Async)的相关notes。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RDMA Doorbell机制</title>
    <link href="http://liujunming.github.io/2025/05/10/Notes-about-RDMA-Doorbell-record%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2025/05/10/Notes-about-RDMA-Doorbell-record机制/</id>
    <published>2025-05-10T08:38:23.000Z</published>
    <updated>2025-05-11T08:50:43.501Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RDMA Doorbell机制的相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>Doorbell是一种软件发起、硬件接收的通知机制。例如软件通过Doorbell告诉硬件：</p><ul><li>开始做某事：例如软件准备好WQE时，通知硬件开始处理</li><li>已经做完某事：例如软件读取CQE后，通知硬件”我已经取走CQE”</li></ul><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p>Doorbell有两种实现机制：</p><h3 id="Doorbell寄存器"><a href="#Doorbell寄存器" class="headerlink" title="Doorbell寄存器"></a>Doorbell寄存器</h3><p>硬件提供的寄存器，来供软件读写。</p><ul><li>优点：实现简单，只需要软件读写寄存器地址</li><li>缺点：读写寄存器行为会抢占总线；硬件需要立即响应，可能会打断硬件正在进行的工作（例如在DMA读取主机的内存），从而影响传输速率</li></ul><h3 id="Doorbell-record"><a href="#Doorbell-record" class="headerlink" title="Doorbell record"></a>Doorbell record</h3><p>使用主机的一段内存作为中介。软件和硬件都知道这段内存的地址。软件直接写这段内存，硬件在必要时读取该内存. This buffer is used in the recovery path in case the NIC drops the write to its doorbell (DB).</p><ul><li>优点：软件通知时不需要抢总线</li><li>缺点：实时性较差</li></ul><p>可以把DB理解成软件通知硬件的方式。</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/26082845081" target="_blank" rel="noopener">浅析DeepSeek中提到的IBGDA</a></li><li><a href="https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/" target="_blank" rel="noopener">Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RDMA Doorbell机制的相关notes。
    
    </summary>
    
      <category term="RDMA" scheme="http://liujunming.github.io/categories/RDMA/"/>
    
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Linux bonded interface</title>
    <link href="http://liujunming.github.io/2025/05/05/Notes-about-Linux-bonded-interface/"/>
    <id>http://liujunming.github.io/2025/05/05/Notes-about-Linux-bonded-interface/</id>
    <published>2025-05-05T13:36:19.000Z</published>
    <updated>2025-05-05T13:42:08.467Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux bonded interface的相关notes。<a id="more"></a></p><p>The Linux bonding driver provides a method for aggregating multiple network interfaces into a single logical “bonded” interface. The behavior of the bonded interface depends on the mode; generally speaking, modes provide either hot standby or load balancing services.</p><p><img src="/images/2025/05/005.png" alt></p><p>Use a bonded interface when you want to increase your link speed or do a failover on your server.</p><p>Here’s how to create a bonded interface:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ip link add bond1 type bond miimon 100 mode active-backup</span><br><span class="line">ip link set eth0 master bond1</span><br><span class="line">ip link set eth1 master bond1</span><br></pre></td></tr></table></figure></p><p>This creates a bonded interface named <code>bond1</code> with mode active-backup. For other modes, please see the <a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" target="_blank" rel="noopener">kernel documentation</a>.</p><p>The Linux bonding driver provides a mechanism for enslaving multiple network interfaces into a single, logical “bonded” interface with the same MAC address.Behavior of the bonded interfaces depends on modes.For instance, the bonding driver has the ability to detect link failure and reroute network traffic around a failed link in a manner transparent to the application, which is active-backup mode. It also has the ability to aggregate network traffic in all working links to achieve higher throughput, which is referred to as trunking.</p><hr><p>参考资料:</p><ol><li><a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#bonded_interface" target="_blank" rel="noopener">Bonded interface</a></li><li><a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" target="_blank" rel="noopener">Documentation/networking/bonding.txt</a></li><li><a href="https://www.kernel.org/doc/ols/2008/ols2008v2-pages-261-267.pdf" target="_blank" rel="noopener">Live Migration with Pass-through Device for Linux VM</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux bonded interface的相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel TLB Topology</title>
    <link href="http://liujunming.github.io/2025/05/05/Notes-about-Intel-TLB-Topology/"/>
    <id>http://liujunming.github.io/2025/05/05/Notes-about-Intel-TLB-Topology/</id>
    <published>2025-05-05T08:15:14.000Z</published>
    <updated>2025-05-05T08:54:38.950Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel TLB Topology的相关notes。<a id="more"></a></p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/images/2025/05/001.png" alt></p><p><img src="/images/2025/05/002.png" alt></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/05/003.png" alt></p><h2 id="L1-TLBs"><a href="#L1-TLBs" class="headerlink" title="L1 TLBs"></a>L1 TLBs</h2><p>Modern processors implement a group of split L1 TLBs for instructions and data, separated by page size.</p><ul><li>Separate I and D TLBs</li><li>Separate L1 TLBs for Different Page Sizes</li></ul><h2 id="L2-TLBs"><a href="#L2-TLBs" class="headerlink" title="L2 TLBs"></a>L2 TLBs</h2><ul><li>Multiple page size support</li><li>Inclusive, mostly-inclusive, or exclusive designs</li></ul><h2 id="SDM"><a href="#SDM" class="headerlink" title="SDM"></a>SDM</h2><p><img src="/images/2025/05/004.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/system/files/sec22_slides-tatar.pdf" target="_blank" rel="noopener">TLB;DR: Enhancing TLB-based Attacks with TLB Desynchronized Reverse Engineering</a></li><li><a href="https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-lipp.pdf" target="_blank" rel="noopener">Meltdown: Reading Kernel Memory from User Space</a></li><li><a href="http://www.cs.yale.edu/homes/abhishek/abhishek-appendix-l.pdf" target="_blank" rel="noopener">Appendix L: Advanced Concepts on Address Translation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel TLB Topology的相关notes。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>硬盘SMART技术</title>
    <link href="http://liujunming.github.io/2025/04/26/%E7%A1%AC%E7%9B%98SMART%E6%8A%80%E6%9C%AF/"/>
    <id>http://liujunming.github.io/2025/04/26/硬盘SMART技术/</id>
    <published>2025-04-26T13:28:53.000Z</published>
    <updated>2025-05-14T23:31:21.192Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下硬盘SMART(Self-Monitoring, Analysis and Reporting Technology)技术的相关notes。<a id="more"></a></p><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>硬盘的<strong>SMART</strong>技术（Self-Monitoring, Analysis, and Reporting Technology，自我监测、分析与报告技术）是一种内置于硬盘的监测系统，旨在实时监控硬盘的健康状态，并提前预警潜在的硬件故障。</p><h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h2><ul><li><p>健康监测<br>持续跟踪硬盘的物理参数（如温度、坏扇区数量、读写错误率等）。</p></li><li><p>故障预警<br>通过分析监测数据，预测硬盘可能发生的故障，并向用户发出警告。</p></li><li><p>数据保护<br>在硬盘完全失效前，为用户提供备份或更换硬盘的窗口期，降低数据丢失风险。</p></li></ul><h2 id="监测的关键参数"><a href="#监测的关键参数" class="headerlink" title="监测的关键参数"></a>监测的关键参数</h2><p>不同硬盘厂商定义的SMART属性可能略有差异，但以下是一些通用核心参数：</p><table><thead><tr><th>参数</th><th>作用</th></tr></thead><tbody><tr><td>Reallocated Sectors</td><td>记录因坏道被替换的扇区数量，值越高风险越大</td></tr><tr><td>Spin-Up Time</td><td>硬盘启动时间，异常值可能预示电机或电路问题</td></tr><tr><td>Temperature</td><td>硬盘工作温度，过高可能加速硬件老化</td></tr><tr><td>Read/Write Error Rate</td><td>读写错误次数，频繁错误可能暗示磁头或盘片问题</td></tr><tr><td>Power-On Hours</td><td>硬盘累计通电时间，寿命预估的重要指标</td></tr><tr><td>SSD Wear Leveling</td><td>(针对SSD)闪存磨损均衡状态，反映剩余寿命</td></tr></tbody></table><h2 id="局限性与注意事项"><a href="#局限性与注意事项" class="headerlink" title="局限性与注意事项"></a>局限性与注意事项</h2><ul><li><p>并非万能<br>SMART主要针对机械故障（如坏道、电机问题）和SSD的闪存磨损，但无法预测突发性电路故障或物理损坏。</p></li><li><p>误报与漏报<br>部分硬盘可能在SMART显示“健康”时突然故障，或因固件问题误报警告。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SMART技术是硬盘健康管理的“守门人”，通过实时监控关键参数帮助用户预防数据灾难。尽管其预测能力有限，但结合定期检查和备份策略，能显著提升数据安全性。对于重要数据，建议将SMART监测纳入日常维护流程。</p><hr><p>参考资料:</p><ol><li><a href="https://www.wikiwand.com/en/articles/Self-Monitoring,_Analysis_and_Reporting_Technology" target="_blank" rel="noopener">https://www.wikiwand.com/en/articles/Self-Monitoring,_Analysis_and_Reporting_Technology</a></li><li>deepseek</li><li><a href="https://mp.weixin.qq.com/s/_atiK380D2wV94kQnfivEw" target="_blank" rel="noopener">OCP NVME SSD规范解读-8.SMART日志要求-1</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下硬盘SMART(Self-Monitoring, Analysis and Reporting Technology)技术的相关notes。
    
    </summary>
    
      <category term="存储" scheme="http://liujunming.github.io/categories/%E5%AD%98%E5%82%A8/"/>
    
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
      <category term="存储" scheme="http://liujunming.github.io/tags/%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Notes about linux KPTI</title>
    <link href="http://liujunming.github.io/2025/04/12/Notes-about-linux-KPTI/"/>
    <id>http://liujunming.github.io/2025/04/12/Notes-about-linux-KPTI/</id>
    <published>2025-04-12T11:43:46.000Z</published>
    <updated>2025-04-12T11:49:39.574Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2025/04/006.png" alt><a id="more"></a> </p><p>KPTI (kernel page table isolation) is proposed to defend against the Meltdown attack. This patch separates user space and kernel space page tables entirely, as shown in the following figure. The one used by kernel is the same as before, while the one used by application contains a copy of user space and a small set of kernel space mapping with only trampoline code to enter the kernel. Since the data of kernel are no longer mapped in the user space, a malicious application cannot directly de-reference kernel’s data address, and thus cannot issue Meltdown attack.</p><p><img src="/images/2025/04/007.png" alt></p><hr><p>参考资料:</p><ol><li>EPTI: Efficient Defense against Meltdown Attack for Unpatched VMs(ATC’18)</li><li><a href="https://www.wikiwand.com/en/articles/Kernel_page-table_isolation" target="_blank" rel="noopener">https://www.wikiwand.com/en/articles/Kernel_page-table_isolation</a></li><li><a href="https://blog.csdn.net/pwl999/article/details/112686914" target="_blank" rel="noopener">Linux mem 2.3 内核页表隔离 (KPTI) 详解</a></li><li><a href="https://www.kernel.org/doc/html/next/x86/pti.html" target="_blank" rel="noopener">Page Table Isolation (PTI)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2025/04/006.png&quot; alt&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel CET</title>
    <link href="http://liujunming.github.io/2025/04/06/Notes-about-Intel-CET/"/>
    <id>http://liujunming.github.io/2025/04/06/Notes-about-Intel-CET/</id>
    <published>2025-04-05T22:27:12.000Z</published>
    <updated>2025-04-12T11:49:09.830Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel CET(Control-flow Enforcement Technology)的相关notes，主要内容转载自<a href="https://mp.weixin.qq.com/s/zs7G5yucR5iANSiS4M1I3A" target="_blank" rel="noopener">Intel CET 安全防御机制深度解析</a>。<a id="more"></a> </p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>CET（Control-flow Enforcement Technology）机制是 Intel提出<strong>基于硬件的</strong>⽤于缓解 <strong>ROP/JOP/COP</strong>的新技术。特别强调下，他是基于硬件⽀持的解决⽅案。从Intel的Tigerlake (11th gen)，Alderlake (12th gen)/Sapphire-Rapid起，粗颗粒度地旨在预防前向（ call/jmp ）和后向（ ret ）控制流指令劫持来御防ROP的攻击。因此针对防御对象不同，CET技术又分为CET-SS用于针对ROP的ret指令和CET-IBT用于针对JOP/COP的jmp/call指令。</p><h2 id="CET为何而生"><a href="#CET为何而生" class="headerlink" title="CET为何而生"></a>CET为何而生</h2><p>说到防御者CET，就不得不提他的进攻者ROP了。什么是ROP? 来看⼀个例⼦，假设程序中正常执行如下代码⽚段，注意此时其中不含<code>ret</code>或<code>call</code>指令。</p><p><img src="/images/2025/04/001.jpg" alt></p><p>但是，如果稍加偏移⼀下解释代码的地址时，就会导致出现完全不⼀样的指令，如下图所示：</p><p><img src="/images/2025/04/002.jpg" alt></p><p>如果按照红框中的顺序解释这些指令的时候，那么将会产⽣⾮代码预期的结果，会出现原代码中未出现过的 ret 指令以及 call 指令，这些指令序列被称为 <strong>gadget</strong> 。通过仔细构造这些由ret指令终⽌的指令集，攻击者可以执⾏原程序中⾮预期的任意恶意代码，这种新产生ret攻击被称为 ROP 攻击，同理还有COP/JOP（call/jmp）攻击。</p><p>为了防御这种此类ROP攻击，Intel在硬件机制上推出了CET缓解机制，针对不同的攻击行为分为CET-SS(影子栈)和CET-IBT(间接跳转跟踪)。</p><h2 id="CP异常"><a href="#CP异常" class="headerlink" title="#CP异常"></a>#CP异常</h2><p>CET-SS和CET-IBT 在实现机制上属于CPU内部异常。当执行启动CET发现执行执行流中没有endbr64或函数返回ret和影子栈中shandow stack保存的ret不一致时，CPU内部出发异常，这里CET占用的中断向量21号，触发#CP并归为陷阱执行中断处理程序<code>exc_control_protection()</code>，对CET-SS CET-IBT分情况进行报错。CET-IBT -&gt; “traps: Missing ENDBR: xxx”， CET-SS-&gt; #CP(control protect)。</p><h2 id="CET-SS（Shadow-Stack）"><a href="#CET-SS（Shadow-Stack）" class="headerlink" title="CET-SS（Shadow Stack）"></a>CET-SS（Shadow Stack）</h2><p>Intel 提出了⼀种基于硬件的 CET 解决⽅案，其中之⼀的 shadow stack 机制⽤于缓解 ROP 攻击。ROP 依赖于 ret 指令，其中要执⾏的后续指令地址从堆栈中获得。因此 ROP 攻击的前提是攻击者能够在堆栈中构造数据。那么再来看 shadow stack 机制是怎么⼯作的。</p><p>CET 使操作系统能够创建⼀个 shadow stack （影⼦栈）。正常情况下，当执⾏ call 指令时，会将 call 指令后⼀条指令地址压栈。当启⽤了 shadow stack 后，会同时在普通数据栈和 shadow stack 中压⼊返回地址，随后在执⾏ ret 返回时，会将 shadow stack 中的返回地址和普通数据栈中的返回地址做对⽐，如匹配，则正常执⾏，如不匹配，则触发#CP(Controlflow Protection) 异常。如下图所示：</p><p><img src="/images/2025/04/004.png" alt></p><h2 id="CET-IBT-Indirect-Branch-Tracking"><a href="#CET-IBT-Indirect-Branch-Tracking" class="headerlink" title="CET-IBT(Indirect Branch Tracking)"></a>CET-IBT(Indirect Branch Tracking)</h2><p>JOP/COP 攻击⼿法与 ROP 类似，只不过是把 ROP 中以 ret 指令做跳板的关键点替换成了 call/jmp 指令。这种不需要 ret 指令的攻击场景下，前⾯所说的 shadow stack 机制就失效了。这种情况下， CET 的第⼆种机制 IBT(Indirect Branch Tracking) 就应运⽽⽣了。</p><p>IBT(Indirect Branch Tracking),间接跳转跟踪”希望能防止攻击者让间接跳转（例如，通过指针变量进行的函数调用）进入一个不应该走到的地方。</p><p>IBT 是为了防御面向跳转编程的（jump-oriented programming）；工作原理是试图确保每个 indirect branch 的目标确实都是适合作为跳转目标的。IBT 的方法有很多，每一种都有自己的优势和劣势。例如，内核在 5.13 开发周期中支持了编译器实现的 IBT 机制。在这种模式下，编译器通过一个 “jump table, 跳转表” 来完成每一个间接跳转，不仅确保目标是要供间接跳转使用的，而且要确保被调用函数的原型与调用者所期望的一致。这种方法是很有效的，但要增加很多编译、运行时的开销。</p><p>CET-IBT方法相当简单，但优点是得到了硬件的支持，因此速度更快.如果 IBT 被启用，那么 CPU 将确保每个间接跳转都落在一条特殊指令（endbr32 或 endbr64）上，该指令执行时跟 no-op 效果一致。如果发现意外，那么处理器将引发一次 control-protection（#CP）exception。</p><p><img src="/images/2025/04/005.png" alt></p><hr><p>参考资料:</p><ol><li>deepseek[介绍下Intel CET技术]</li><li><a href="https://mp.weixin.qq.com/s/zs7G5yucR5iANSiS4M1I3A" target="_blank" rel="noopener">Intel CET 安全防御机制深度解析</a></li><li><a href="https://www.usenix.org/system/files/atc24-chen-xiangdong.pdf" target="_blank" rel="noopener">Limitations and Opportunities of Modern Hardware Isolation Mechanisms</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/technical-look-control-flow-enforcement-technology.html?wapkw=cet" target="_blank" rel="noopener">A Technical Look at Intel’s Control-flow Enforcement Technology</a></li><li><a href="https://i.blackhat.com/asia-19/Thu-March-28/bh-asia-Sun-How-to-Survive-the-Hardware-Assisted-Control-Flow-Integrity-Enforcement.pdf" target="_blank" rel="noopener">How to Survive the Hardware-assisted Controlflow Integrity Enforcement</a></li><li><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3559344" target="_blank" rel="noopener">CETIS: Retrofitting Intel CET for Generic and Efficient Intra-process Memory Isolation</a></li><li><a href="https://lpc.events/event/2/contributions/147/attachments/72/83/CET-LPC-2018.pdf" target="_blank" rel="noopener">Control-flow Enforcement Technology</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel CET(Control-flow Enforcement Technology)的相关notes，主要内容转载自&lt;a href=&quot;https://mp.weixin.qq.com/s/zs7G5yucR5iANSiS4M1I3A&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Intel CET 安全防御机制深度解析&lt;/a&gt;。
    
    </summary>
    
      <category term="Security" scheme="http://liujunming.github.io/categories/Security/"/>
    
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Seccomp filter</title>
    <link href="http://liujunming.github.io/2025/04/05/Notes-about-Seccomp-filter/"/>
    <id>http://liujunming.github.io/2025/04/05/Notes-about-Seccomp-filter/</id>
    <published>2025-04-05T06:08:11.000Z</published>
    <updated>2025-04-05T10:55:23.507Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Seccomp(SECure COMPuting) filtering的相关notes。</p><p><u>Seccomp filtering provides a means for a process to specify a filter for incoming system calls</u>.<a id="more"></a> </p><p>以下内容源于deepseek。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Seccomp filter 是 Linux 内核提供的一种安全机制，用于限制进程能够执行的系统调用（syscall），从而减少潜在的攻击面。它通过自定义过滤规则（基于 BPF 程序）动态允许或拦截系统调用，常用于沙箱、容器等需要严格隔离的场景。</p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Seccomp-模式"><a href="#Seccomp-模式" class="headerlink" title="Seccomp 模式"></a>Seccomp 模式</h3><ul><li>Strict Mode：仅允许<code>read</code>, <code>writ</code>e<code>,</code>exit<code>,</code>sigreturn<code>四个系统调用，其他调用会触发</code>SIGKILL`。</li><li>Filter Mode（Seccomp-BPF）：允许通过 BPF（Berkeley Packet Filter）规则自定义允许或拒绝的系统调用列表，提供更灵活的过滤。</li></ul><h3 id="BPF-规则"><a href="#BPF-规则" class="headerlink" title="BPF 规则"></a>BPF 规则</h3><ul><li>使用类汇编的指令定义过滤逻辑（例如基于系统调用号、参数值等条件）。</li><li>规则在内核中执行，确保高效和安全。</li></ul><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="规则设置"><a href="#规则设置" class="headerlink" title="规则设置"></a>规则设置</h3><ul><li>进程通过 <code>prctl()</code> 或 <code>seccomp()</code> 系统调用加载 BPF 规则。</li><li>规则定义哪些系统调用被允许（<code>ALLOW</code>）、记录（<code>LOG</code>）或拒绝（<code>KILL</code>/<code>ERRNO</code>）。</li></ul><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><ul><li>当进程发起系统调用时，内核触发 Seccomp 过滤器。</li><li>BPF 规则对系统调用号、参数等进行检查。</li><li>根据规则决定放行、终止进程，或返回错误（如 <code>EPERM</code>）。</li></ul><h2 id="主要用途"><a href="#主要用途" class="headerlink" title="主要用途"></a>主要用途</h2><h3 id="沙箱化应用"><a href="#沙箱化应用" class="headerlink" title="沙箱化应用"></a>沙箱化应用</h3><ul><li>浏览器（如 Chrome）用 Seccomp 限制渲染进程的权限。</li><li>游戏反作弊系统防止用户态程序滥用内核功能。</li></ul><h3 id="容器安全"><a href="#容器安全" class="headerlink" title="容器安全"></a>容器安全</h3><p>Docker 和 Kubernetes 默认启用 Seccomp，限制容器内进程的系统调用（如禁止 <code>mount</code> 或 <code>reboot</code>）。</p><h3 id="特权进程降权"><a href="#特权进程降权" class="headerlink" title="特权进程降权"></a>特权进程降权</h3><p>服务启动后通过 Seccomp 丢弃不必要的系统调用权限。</p><hr><p>参考资料:</p><ol><li><a href="https://man7.org/linux/man-pages/man2/seccomp.2.html" target="_blank" rel="noopener">man seccomp</a></li><li><a href="https://www.kernel.org/doc/html/v5.0/userspace-api/seccomp_filter.html" target="_blank" rel="noopener">Seccomp BPF (SECure COMPuting with filters)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Seccomp(SECure COMPuting) filtering的相关notes。&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Seccomp filtering provides a means for a process to specify a filter for incoming system calls&lt;/u&gt;.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ARM MTE technology</title>
    <link href="http://liujunming.github.io/2025/03/29/Notes-about-ARM-MTE-Memory-Tagging-Extension-technology/"/>
    <id>http://liujunming.github.io/2025/03/29/Notes-about-ARM-MTE-Memory-Tagging-Extension-technology/</id>
    <published>2025-03-29T09:50:08.000Z</published>
    <updated>2025-03-29T11:54:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下ARM MTE(Memory Tagging Extension) technology的相关notes。<a id="more"></a> </p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/03/001.jpeg" alt></p><p><img src="/images/2025/03/002.jpeg" alt></p><p><img src="/images/2025/03/003.jpeg" alt></p><p>Starting with ARMv8.3-A, ARM SoCs introduce support for memory tagging extensions (MTE) that <strong>allow partitioning the address space into 16-byte regions that are colored with one of the 16 tags</strong>. <u>The hardware maintains a table that stores the mapping between addresses and tags allowing access to the region only if the tag of the pointer (the tag is stored in the upper bits of the pointer matches the tag of the memory region)</u>.</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="检测内存越界与溢出"><a href="#检测内存越界与溢出" class="headerlink" title="检测内存越界与溢出"></a>检测内存越界与溢出</h3><p>内存越界与溢出是常见的内存安全问题，通常由于错误的索引或指针操作导致访问超出合法的内存范围，可能引发未定义行为、数据损坏，甚至被攻击者利用进行恶意代码执行。MTE 提供了有效的硬件级检测机制，实时防止这些问题的发生。</p><p>假设一个程序中分配了 16 字节的内存，但错误地访问了第 17 个字节：</p><p><img src="/images/2025/03/004.webp" alt></p><p>在此场景中，MTE 通过标签匹配机制发现非法访问，从而阻止越界操作的执行，避免可能的崩溃或安全漏洞。</p><h3 id="防止未初始化内存的使用"><a href="#防止未初始化内存的使用" class="headerlink" title="防止未初始化内存的使用"></a>防止未初始化内存的使用</h3><p>未初始化内存的使用是另一个常见的内存安全问题。当程序访问未初始化的内存时，可能读取到随机数据，导致逻辑错误、崩溃，或被恶意利用泄露敏感信息。MTE 可以有效防止这类问题。</p><p>以下是一个典型的未初始化内存使用的伪代码示例：</p><p><img src="/images/2025/03/005.webp" alt></p><p>在这种情况下，MTE 会阻止对未初始化内存的非法访问，避免出现数据不一致和未定义行为。</p><h3 id="内存释放后的安全保护"><a href="#内存释放后的安全保护" class="headerlink" title="内存释放后的安全保护"></a>内存释放后的安全保护</h3><p>时间局部安全性问题，即内存释放后的访问，是内存管理中的常见隐患。这种问题通常发生在指针仍然试图访问已释放的内存区域，可能导致程序崩溃或被攻击者利用进行数据篡改。MTE 通过标签机制解决了这一问题。</p><p>以下代码展示了释放后访问的典型错误：</p><p><img src="/images/2025/03/006.webp" alt></p><p>在这里，MTE 通过实时检测，阻止了非法访问已释放内存的行为，防止时间局部安全问题带来的潜在风险。</p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/conference/atc24/presentation/chen-xiangdong" target="_blank" rel="noopener">Limitations and Opportunities of Modern Hardware Isolation Mechanisms</a></li><li><a href="https://mp.weixin.qq.com/s/GlXhwNlf4jTs5ndRLCFZjQ" target="_blank" rel="noopener">Android Memory Tagging Extension (MTE) 的深度研究与应用</a></li><li><a href="https://mp.weixin.qq.com/s/H9Pu9i37ogyl05z7NzOStA" target="_blank" rel="noopener">Memory安全和硬件Memory Tagging技术—中篇</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzI5MjYwMTk3OA==&amp;mid=2247484869&amp;idx=1&amp;sn=d7e4faef480a169df8ed79e66c73b58b&amp;chksm=ec7facd0db0825c64e4010f2a92e73d8db5753ce50efbf9f29ea356a7b5277b94a06c1222057&amp;scene=178&amp;cur_album_id=1638614933820997634#rd" target="_blank" rel="noopener">Memory安全和硬件Memory Tagging技术—下篇</a></li><li><a href="https://codasip.com/2023/11/02/fine-grained-memory-protection-cheri/" target="_blank" rel="noopener">Fine-grained memory protection</a></li><li><a href="https://static.linaro.org/connect/yvr18/presentations/yvr18-104.pdf" target="_blank" rel="noopener">https://static.linaro.org/connect/yvr18/presentations/yvr18-104.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下ARM MTE(Memory Tagging Extension) technology的相关notes。
    
    </summary>
    
      <category term="ARM" scheme="http://liujunming.github.io/categories/ARM/"/>
    
    
      <category term="ARM" scheme="http://liujunming.github.io/tags/ARM/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about FPGA concepts</title>
    <link href="http://liujunming.github.io/2025/02/23/Notes-about-FPGA-concepts/"/>
    <id>http://liujunming.github.io/2025/02/23/Notes-about-FPGA-concepts/</id>
    <published>2025-02-23T01:41:23.000Z</published>
    <updated>2025-02-23T02:20:18.357Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下FPGA基本概念的相关notes。<a id="more"></a> </p><p>Field Programmable Gate Arrays (FPGAs) are “a sea” of logic, arithmetic, and memory elements, which users can configure to implement custom compute circuits. FPGA compute capacity is determined by the <em>area</em> available for the circuits.</p><h2 id="FPGA-development"><a href="#FPGA-development" class="headerlink" title="FPGA development"></a>FPGA development</h2><p>FPGAs can be seen as “software defined” hardware. The software definition, <em>a design</em>, is implemented using register transfer languages (RTL) such as Verilog. Additionally, designers can use high-level synthesis (HLS) tools to generate RTL, e.g., from a restricted version of C++. However, HLS C++ programs are different from CPU programs, and must follow certain rules, including explicit exposure of fine-grain pipeline- and task- parallelism to achieve high performance. Implementation tools then compile the design into an FPGA <em>image</em> targeting specific hardware.</p><p>Finally, users can load the image onto an FPGA (slow, up to a few seconds), entirely replacing the previous design. Some FPGAs support <em>partial reconfiguration</em> to replace only a subset of the entire FPGA, a much faster process (milliseconds), which unfortunately incurs significant area overheads.</p><h2 id="FPGA-sharing"><a href="#FPGA-sharing" class="headerlink" title="FPGA sharing"></a>FPGA sharing</h2><p>There are three ways to share an FPGA: space partitioning, coarse-grain, and fine-grain time sharing.</p><h3 id="space-partitioning"><a href="#space-partitioning" class="headerlink" title="space partitioning"></a>space partitioning</h3><p>Space partitioning divides FPGA resources into disjoint sets used by different AFUs(Accelerator Functional Units). If shared I/O interfaces (memory, PCIe bus) are securely isolated and multiplexed, this method enables low-overhead FPGA sharing among mutually distrustful AFUs but requires larger FPGAs to fit them all.</p><p><img src="/images/2025/02/008.png" alt></p><h3 id="Coarse-grain-time-sharing"><a href="#Coarse-grain-time-sharing" class="headerlink" title="Coarse-grain time sharing"></a>Coarse-grain time sharing</h3><p>Coarse-grain time sharing dynamically switches AFUs via full or partial reconfiguration. It incurs high switching latency.</p><p><img src="/images/2025/02/009.png" alt></p><h3 id="Fine-grain-time-sharing"><a href="#Fine-grain-time-sharing" class="headerlink" title="Fine-grain time sharing"></a>Fine-grain time sharing</h3><p>Fine-grain time sharing allows multiple CPU applications to use the same AFU. <u>The AFU implements the context switch internally, in hardware</u>. Packet processing applications such as <a href="https://www.usenix.org/conference/nsdi18/presentation/firestone" target="_blank" rel="noopener">AccelNet</a> use this approach to process each packet in the context of its associated flow. Such AFUs oversee switching between the contexts(此类AFU负责管理不同上下文之间的切换); therefore this type of sharing requires AFUs to be <em>trusted</em> to ensure fair use and state isolation between their users.</p><p><img src="/images/2025/02/010.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/conference/atc19/presentation/eran" target="_blank" rel="noopener">NICA: An Infrastructure for Inline Acceleration of Network Applications</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下FPGA基本概念的相关notes。
    
    </summary>
    
      <category term="Accelerator" scheme="http://liujunming.github.io/categories/Accelerator/"/>
    
    
      <category term="Accelerator" scheme="http://liujunming.github.io/tags/Accelerator/"/>
    
  </entry>
  
  <entry>
    <title>Notes about F-NIC(FPGA-based SmartNIC)</title>
    <link href="http://liujunming.github.io/2025/02/22/Notes-about-F-NIC-FPGA-based-SmartNIC-architecture/"/>
    <id>http://liujunming.github.io/2025/02/22/Notes-about-F-NIC-FPGA-based-SmartNIC-architecture/</id>
    <published>2025-02-22T13:40:52.000Z</published>
    <updated>2025-02-23T01:54:08.692Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下F-NIC(FPGA-based SmartNIC)相关notes。<a id="more"></a> </p><p>We describe bump-in-the-wire(是一种网络技术概念，指在网络通信链路中透明插入的设备或系统，用于增强特定功能而不改变终端设备的配置或通信协议) F-NICs, focusing on Mellanox Innova.</p><p><img src="/images/2025/02/007.png" alt></p><h2 id="Bump-in-the-wire"><a href="#Bump-in-the-wire" class="headerlink" title="Bump-in-the-wire"></a>Bump-in-the-wire</h2><p>A typical F-NIC (Figure 1) combines a commodity network ASIC (e.g., ConnectX-4 Lx NIC) with an FPGA and local DRAM. The FPGA is located <em>between</em> the ASIC and the network port, interposing on all Ethernet traffic in and out of the NIC. The FPGA and the ASIC communicate directly via an internal bus (e.g., 40 Gbps Ethernet), and a PCIe bus connects the ASIC to the host.</p><p>The bump-in-the-wire design reuses the existing data and control planes between the CPU and the NIC ASIC, with its QoS management, and virtualization support (SR-IOV), mature DMA engines, and software stack.</p><h2 id="F-NIC-programming"><a href="#F-NIC-programming" class="headerlink" title="F-NIC programming"></a>F-NIC programming</h2><p>The development of an F-NIC accelerated application involves both hardware logic on FPGA and associated software on the CPU. F-NIC vendors provide a lightweight <em>shell IP</em>: a set of low-level hardware interfaces for basic operations, including link-layer packet exchange with the network and the host, onboard DRAM access, and control register access. However, the vendor SDK leaves it to customers to implement higher level features such as FPGA network stack processing or virtualization support.</p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/system/files/atc19-eran.pdf" target="_blank" rel="noopener">NICA: An Infrastructure for Inline Acceleration of Network Applications</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下F-NIC(FPGA-based SmartNIC)相关notes。
    
    </summary>
    
      <category term="Accelerator" scheme="http://liujunming.github.io/categories/Accelerator/"/>
    
    
      <category term="Accelerator" scheme="http://liujunming.github.io/tags/Accelerator/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PFC(Priority-based Flow Control)</title>
    <link href="http://liujunming.github.io/2025/02/22/Notes-about-PFC-Priority-based-Flow-Control/"/>
    <id>http://liujunming.github.io/2025/02/22/Notes-about-PFC-Priority-based-Flow-Control/</id>
    <published>2025-02-22T12:50:20.000Z</published>
    <updated>2025-02-22T13:10:03.280Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PFC(Priority-based Flow Control)相关notes，内容转载自<a href="https://info.support.huawei.com/info-finder/encyclopedia/en/PFC.html" target="_blank" rel="noopener">What Is PFC?</a><a id="more"></a> </p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p><strong>Disadvantages of Traditional Flow Control Technologies</strong></p><p>The most fundamental flow control technology is the <u>Ethernet Pause mechanism</u> defined in IEEE 802.3. When a downstream device detects that its receive capability is lower than the transmit capability of its upstream device, it sends Pause frames to the upstream device, requesting the upstream device to stop sending traffic for a period of time.</p><p><img src="/images/2025/02/004.png" alt></p><p>The drawback, however, is that the Ethernet Pause mechanism stops all traffic on a link (for the entire interface). This in turn affects link sharing, which is critical to actual services. Link sharing requires:</p><ul><li>Burst traffic of one type cannot affect forwarding of other types of traffic.</li><li>A large amount of one type of traffic in a queue cannot occupy buffer resources of other types of traffic.</li></ul><p><img src="/images/2025/02/005.png" alt></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>The meaning of PFC is Priority-based Flow Control. <u>It is the most widely used flow control technology that can effectively prevent packet loss and serve as the basis for intelligent lossless networks.</u> A PFC-enabled queue is a lossless queue. <em>When congestion occurs in such a queue on a downstream device, the downstream device instructs the upstream device to stop sending traffic in the queue, implementing zero packet loss.</em></p><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p>As shown in the figure, <u>eight priority queues on the transmit interface of DeviceA correspond to eight receive buffers on the receive interface of DeviceB.</u> When a receive buffer on DeviceB is congested, DeviceB sends a backpressure signal “STOP” to DeviceA, requesting DeviceA to stop sending traffic in the corresponding priority queue.</p><p><em>PFC addresses the conflict between the Ethernet Pause mechanisms and link sharing.</em> <u>It controls traffic only in one or several priority queues of an interface, rather than on the entire interface.</u> <em>What’s more, PFC can pause or restart any queue, without interrupting traffic in other queues. This feature enables traffic of various types to share one link.</em></p><p><img src="/images/2025/02/006.png" alt><br><em>PFC working mechanism</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PFC(Priority-based Flow Control)相关notes，内容转载自&lt;a href=&quot;https://info.support.huawei.com/info-finder/encyclopedia/en/PFC.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;What Is PFC?&lt;/a&gt;
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about vCPU MPState</title>
    <link href="http://liujunming.github.io/2025/02/16/Notes-about-vCPU-MPState/"/>
    <id>http://liujunming.github.io/2025/02/16/Notes-about-vCPU-MPState/</id>
    <published>2025-02-16T03:20:47.000Z</published>
    <updated>2025-02-16T03:30:29.868Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下vCPU MPState(MultiProcessing State)的相关notes。<a id="more"></a></p><p>在热迁移时，qemu会执行<code>kvm_get_mp_state</code>函数:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">kvm_get_mp_state</span><span class="params">(X86CPU *cpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    CPUState *cs = CPU(cpu);</span><br><span class="line">    CPUX86State *env = &amp;cpu-&gt;env;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">kvm_mp_state</span> <span class="title">mp_state</span>;</span></span><br><span class="line">    <span class="keyword">int</span> ret;</span><br><span class="line"></span><br><span class="line">    ret = kvm_vcpu_ioctl(cs, KVM_GET_MP_STATE, &amp;mp_state);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    env-&gt;mp_state = mp_state.mp_state;</span><br><span class="line">    <span class="keyword">if</span> (kvm_irqchip_in_kernel()) &#123;</span><br><span class="line">        cs-&gt;halted = (mp_state.mp_state == KVM_MP_STATE_HALTED);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>此时<code>KVM_GET_MP_STATE</code>就是关键线索，搜索<a href="https://www.kernel.org/doc/Documentation/virt/kvm/api.txt" target="_blank" rel="noopener">kvm api Documentation</a>即可找到最终的答案。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">4.38 KVM_GET_MP_STATE</span><br><span class="line"></span><br><span class="line">Capability: KVM_CAP_MP_STATE</span><br><span class="line">Architectures: x86, s390, arm, arm64</span><br><span class="line">Type: vcpu ioctl</span><br><span class="line">Parameters: struct kvm_mp_state (out)</span><br><span class="line">Returns: 0 on success; -1 on error</span><br><span class="line"></span><br><span class="line">struct kvm_mp_state &#123;</span><br><span class="line">__u32 mp_state;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Returns the vcpu&apos;s current &quot;multiprocessing state&quot; (though also valid on</span><br><span class="line">uniprocessor guests).</span><br><span class="line"></span><br><span class="line">Possible values are:</span><br><span class="line"></span><br><span class="line"> - KVM_MP_STATE_RUNNABLE:        the vcpu is currently running [x86,arm/arm64]</span><br><span class="line"> - KVM_MP_STATE_UNINITIALIZED:   the vcpu is an application processor (AP)</span><br><span class="line">                                 which has not yet received an INIT signal [x86]</span><br><span class="line"> - KVM_MP_STATE_INIT_RECEIVED:   the vcpu has received an INIT signal, and is</span><br><span class="line">                                 now ready for a SIPI [x86]</span><br><span class="line"> - KVM_MP_STATE_HALTED:          the vcpu has executed a HLT instruction and</span><br><span class="line">                                 is waiting for an interrupt [x86]</span><br><span class="line"> - KVM_MP_STATE_SIPI_RECEIVED:   the vcpu has just received a SIPI (vector</span><br><span class="line">                                 accessible via KVM_GET_VCPU_EVENTS) [x86]</span><br><span class="line"> - KVM_MP_STATE_STOPPED:         the vcpu is stopped [s390,arm/arm64]</span><br><span class="line"> - KVM_MP_STATE_CHECK_STOP:      the vcpu is in a special error state [s390]</span><br><span class="line"> - KVM_MP_STATE_OPERATING:       the vcpu is operating (running or halted)</span><br><span class="line">                                 [s390]</span><br><span class="line"> - KVM_MP_STATE_LOAD:            the vcpu is in a special load/startup state</span><br><span class="line">                                 [s390]</span><br><span class="line"></span><br><span class="line">On x86, this ioctl is only useful after KVM_CREATE_IRQCHIP. Without an</span><br><span class="line">in-kernel irqchip, the multiprocessing state must be maintained by userspace on</span><br><span class="line">these architectures.</span><br><span class="line"></span><br><span class="line">For arm/arm64:</span><br><span class="line"></span><br><span class="line">The only states that are valid are KVM_MP_STATE_STOPPED and</span><br><span class="line">KVM_MP_STATE_RUNNABLE which reflect if the vcpu is paused or not.</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://crosvm.dev/doc/hypervisor/enum.MPState.html" target="_blank" rel="noopener">Enum hypervisor::MPState</a></li><li><a href="https://www.kernel.org/doc/Documentation/virt/kvm/api.txt" target="_blank" rel="noopener">The Definitive KVM (Kernel-based Virtual Machine) API Documentation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下vCPU MPState(MultiProcessing State)的相关notes。
    
    </summary>
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/categories/QEMU-KVM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/tags/QEMU-KVM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Content-Addressable Memory (CAM)</title>
    <link href="http://liujunming.github.io/2025/02/15/Notes-about-Content-addressable-memory-CAM/"/>
    <id>http://liujunming.github.io/2025/02/15/Notes-about-Content-addressable-memory-CAM/</id>
    <published>2025-02-15T10:50:28.000Z</published>
    <updated>2025-02-15T11:45:04.155Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Content-Addressable Memory(CAM)相关notes，整理下来自deepseek的回答。<a id="more"></a> </p><p>CAM是一种特殊类型的存储器，其核心特点是<strong>通过内容直接检索数据</strong>，而非传统存储器（如RAM）通过地址访问数据的方式。</p><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>传统存储器（如RAM）：输入地址，返回该地址存储的数据；CAM：输入待查找的数据（关键词），返回该数据所在的地址或关联的其他数据。</p><p>工作流程如下所示:</p><ol><li>用户提供待匹配的<strong>关键词</strong>（Search Key）。</li><li>CAM在存储的所有条目中<strong>并行比较</strong>关键词。</li><li>返回匹配条目的地址或关联数据（如优先级最高的匹配结果）。</li></ol><h2 id="CAM的类型"><a href="#CAM的类型" class="headerlink" title="CAM的类型"></a>CAM的类型</h2><ul><li><p>二元CAM（Binary CAM, BCAM）：</p><ul><li>仅支持精确匹配（0或1）</li><li>应用场景：精确查找，如MAC地址表</li></ul></li><li><p>三元CAM（Ternary CAM, TCAM）：</p><ul><li>支持0、1和<strong>“无关”（Don’t Care）</strong>三种状态（通常用掩码表示）</li><li>可实现<strong>部分匹配</strong>，适用于需要通配符的场景</li><li>典型应用：IP路由中的<strong>最长前缀匹配</strong>（Longest Prefix Match）</li></ul></li></ul><h2 id="核心优势"><a href="#核心优势" class="headerlink" title="核心优势"></a>核心优势</h2><ul><li>高速并行搜索：所有存储条目同时比较，搜索时间复杂度为<strong>O(1)</strong>，远快于软件算法（如哈希表、树结构）</li><li>硬件级效率：无需遍历数据或处理冲突，适合实时性要求高的场景</li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>网络设备：</p><ul><li>路由器/交换机：快速查找路由表、MAC地址表</li><li>防火墙：基于规则的实时流量过滤</li></ul><h2 id="缺点与挑战"><a href="#缺点与挑战" class="headerlink" title="缺点与挑战"></a>缺点与挑战</h2><ul><li>高功耗：并行比较所有条目导致大量晶体管同时切换，功耗显著高于传统存储器</li><li>高成本：每个存储单元需集成比较电路，芯片面积增大</li><li>容量限制：受限于功耗和成本，CAM通常用于小规模高速缓存场景</li></ul><h2 id="TCAM的“无关”位示例"><a href="#TCAM的“无关”位示例" class="headerlink" title="TCAM的“无关”位示例"></a>TCAM的“无关”位示例</h2><p>在IP路由中，TCAM允许将子网掩码未覆盖的位设为“无关”，例如：IP地址：<code>192.168.1.0/24</code>（二进制：<code>11000000.10101000.00000001.********</code>），TCAM条目：<code>11000000 10101000 00000001 XXXXXXXX</code>（<code>X</code>表示“无关”位），搜索时，只需匹配前24位，后8位忽略，实现高效的最长前缀匹配。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CAM通过硬件并行性实现了极速数据检索，是网络设备和高速缓存的核心组件，但其功耗和成本限制了大规模应用。</p><hr><p>参考资料:</p><ol><li><a href="https://www.xilinx.com/products/intellectual-property/ef-di-cam.html" target="_blank" rel="noopener">xilinx Content Addressable Memory (CAM)</a></li><li><a href="https://ieeexplore.ieee.org/document/7159147" target="_blank" rel="noopener">Emerging Trends in Design and Applications of Memory-Based Computing and Content-Addressable Memories</a></li><li><a href="https://arxiv.org/pdf/1804.02330" target="_blank" rel="noopener">An Efficient I/O Architecture for RAM-based Content-Addressable Memory on FPGA</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Content-Addressable Memory(CAM)相关notes，整理下来自deepseek的回答。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about linux /proc/kcore</title>
    <link href="http://liujunming.github.io/2025/02/15/Notes-about-linux-proc-kcore/"/>
    <id>http://liujunming.github.io/2025/02/15/Notes-about-linux-proc-kcore/</id>
    <published>2025-02-15T02:18:25.000Z</published>
    <updated>2025-02-15T03:50:51.251Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下linux中<code>/proc/kcore</code>相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p><code>/proc/kcore</code> is a file in the virtual /proc filesystem of a Linux machine. It is created by the kernel in <a href="https://elixir.bootlin.com/linux/v5.0/source/fs/proc/kcore.c" target="_blank" rel="noopener">fs/proc/kcore.c</a> and <strong>allows read access to all the kernels virtual memory space from userland</strong>.</p><p>Internally it has the format of an ELF core dump file (ELF Type 4/ET_CORE). That means it has the same format as a core file from a crashed process; but instead of capturing the (static) state of a single process at the moment of the crash, <strong>it provides a real time view into the state of the whole system</strong>.</p><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><ul><li>The ELF header (<code>Elf64_Ehdr</code>): It’s at the start of every ELF file. We need two pieces of information from it: the location and number of entries of the program header table.</li><li>The Program headers (<code>Elf64_Phdr</code>): An ELF file contains an array of program header structures. There are various subtypes of program headers, but we care only about the ones marked as <code>PT_LOAD</code>. <u>Each of these headers describe a loadable segment - a part of the file that is loaded into memory</u>. In <code>/proc/kcore</code>, <strong>they describe where in the file each portion of the system memory can be found.</strong></li></ul><p>On x86-64 systems, Linux maintains a complete one-to-one map of all physical memory in the kernels virtual address space. So by reading the right ranges of kernel virtual memory, one can get a complete copy of the content of the physical memory of that system.</p><p><img src="/images/2025/02/003.png" alt></p><p>每个<code>PT_LOAD</code> header用来记录一段 memory region，并描述了这段 memory region对应的文件offset、内核虚拟地址和长度。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol><li>使用<code>open</code>系统调用打开<code>/proc/kcore</code>文件</li><li>遍历ELF文件中的程序头表(Program Header Table)，找到包含目标内核虚拟地址的段(每个段描述了内核虚拟地址到文件offset的映射关系)</li><li>根据计算出的文件offset，使用<code>lseek</code>定位到目标位置，使用<code>read</code>读取目标地址的内容</li></ol><hr><p>参考资料:</p><ol><li><a href="https://schlafwandler.github.io/posts/dumping-/proc/kcore/" target="_blank" rel="noopener">Dumping /proc/kcore in 2019</a></li><li><a href="https://blog.csdn.net/weixin_45030965/article/details/124863905" target="_blank" rel="noopener">Linux /proc/kcore详解（一）</a></li><li><a href="https://blog.csdn.net/weixin_45030965/article/details/125164642" target="_blank" rel="noopener">Linux /proc/kcore详解（二）</a></li><li><a href="https://superuser.com/questions/669462/will-applications-running-in-root-mode-be-able-to-overwrite-oss-or-other-progra" target="_blank" rel="noopener">Will applications running in root mode be able to overwrite OS’s or other program’s memory section?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下linux中&lt;code&gt;/proc/kcore&lt;/code&gt;相关notes。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>SystemTap如何改变内核函数返回值</title>
    <link href="http://liujunming.github.io/2025/02/09/SystemTap%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98%E5%86%85%E6%A0%B8%E5%87%BD%E6%95%B0%E8%BF%94%E5%9B%9E%E5%80%BC/"/>
    <id>http://liujunming.github.io/2025/02/09/SystemTap如何改变内核函数返回值/</id>
    <published>2025-02-09T11:41:30.000Z</published>
    <updated>2025-02-09T12:29:55.906Z</updated>
    
    <content type="html"><![CDATA[<p><strong>利用SystemTap，判断内核函数的参数后，才更改返回值?</strong> <a id="more"></a></p><p><img src="/images/2025/02/002.png" alt></p><p>接下来将展示下<a href="https://chatgpt.com/" target="_blank" rel="noopener">chatgpt</a>给出的答案(笔者也没有运行，不保证正确性)。</p><p><img src="/images/2025/02/001.png" alt></p><h2 id="SystemTap-概述"><a href="#SystemTap-概述" class="headerlink" title="SystemTap 概述"></a>SystemTap 概述</h2><p>SystemTap 是一个动态追踪工具，可以用来监控内核和用户空间程序的行为。你可以通过编写脚本来插桩内核函数，并分析函数参数、返回值等信息。</p><p>在这个教程中，我们将演示如何创建一个 SystemTap 脚本，挂钩内核函数、检查参数，并在满足特定条件时修改函数的返回值。</p><h2 id="创建SystemTap-脚本"><a href="#创建SystemTap-脚本" class="headerlink" title="创建SystemTap 脚本"></a>创建SystemTap 脚本</h2><p>我们以<code>sys_open</code>系统调用为例。<code>sys_open</code>用于打开文件，它接受两个主要参数：路径和打开标志。我们将判断路径是否是<code>/etc/passwd</code>，如果是，则修改返回值（模拟无法打开该文件）。</p><p>创建一个新的 SystemTap 脚本文件，比如<code>check_open.stp</code>，并编辑该文件，在文件中输入以下内容:</p><figure class="highlight stp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">probe syscall.open &#123;</span><br><span class="line">    <span class="comment">// 获取传入的路径参数</span></span><br><span class="line">    filename = user_string($filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印文件路径</span></span><br><span class="line">    printf(<span class="string">"Trying to open: %s\n"</span>, filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断路径是否为 /etc/passwd</span></span><br><span class="line">    if (filename == <span class="string">"/etc/passwd"</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果是 /etc/passwd，则返回 -EACCES，表示访问被拒绝</span></span><br><span class="line">        printf(<span class="string">"Blocking access to /etc/passwd\n"</span>)</span><br><span class="line">        return -EACCES</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>probe syscall.open</code>：我们使用 <code>syscall.open</code> 来挂钩 <code>sys_open</code> 系统调用。这将允许我们监视所有尝试打开文件的操作。</li><li><code>user_string($filename)</code>：该函数提取文件路径参数。我们使用 <code>user_string()</code> 来获取用户空间传入的字符串。</li><li><code>if (filename == &quot;/etc/passwd&quot;)</code>：这里检查传入的文件路径是否为 <code>/etc/passwd</code>。</li><li><code>return -EACCES</code>：如果路径是 <code>/etc/passwd</code>，我们通过 <code>return</code> 语句修改返回值，返回 <code>-EACCES</code>，表示访问被拒绝。</li></ul><h2 id="运行-SystemTap-脚本"><a href="#运行-SystemTap-脚本" class="headerlink" title="运行 SystemTap 脚本"></a>运行 SystemTap 脚本</h2><p>在创建了脚本后，需要使用 <code>stap</code> 命令来运行脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo stap check_open.stp</span><br></pre></td></tr></table></figure><p>这将会启动脚本并开始跟踪 <code>sys_open</code> 调用。如果你尝试打开 <code>/etc/passwd</code>，脚本会修改返回值，导致文件无法打开。</p><h2 id="验证脚本效果"><a href="#验证脚本效果" class="headerlink" title="验证脚本效果"></a>验证脚本效果</h2><p>你可以验证脚本的效果，看看它是否按预期工作：</p><p>尝试打开 <code>/etc/passwd</code> 文件，例如使用 <code>cat</code> 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd</span><br></pre></td></tr></table></figure></p><p>由于脚本的作用，<code>cat</code> 命令应该失败，并返回错误信息，如 <code>Permission denied</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>创建脚本：使用<code>probe</code>挂钩内核函数，获取相关参数。</li><li>条件判断：通过<code>if</code>语句判断参数值，执行相应的操作。</li><li>修改返回值：使用<code>return</code>修改函数的返回值。</li><li>运行脚本：使用<code>sudo stap</code>命令来执行脚本并进行监控。</li></ol><hr><p>参考资料:</p><ol><li><a href="https://lrita.github.io/images/posts/systemtap/SystemTap-II.pdf" target="_blank" rel="noopener">SystemTap Tutorial Part-II</a></li><li><a href="https://www.opensourceforu.com/2010/10/systemtap-tutorial-part-2/" target="_blank" rel="noopener">SystemTap Tutorial, Part-2</a></li><li><a href="https://sourceware.org/systemtap/tutorial.pdf" target="_blank" rel="noopener">Systemtap tutorial</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;利用SystemTap，判断内核函数的参数后，才更改返回值?&lt;/strong&gt;
    
    </summary>
    
      <category term="debug" scheme="http://liujunming.github.io/categories/debug/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="debug" scheme="http://liujunming.github.io/tags/debug/"/>
    
  </entry>
  
  <entry>
    <title>Notes about eMCA</title>
    <link href="http://liujunming.github.io/2025/01/05/Notes-about-eMCA-Gen2/"/>
    <id>http://liujunming.github.io/2025/01/05/Notes-about-eMCA-Gen2/</id>
    <published>2025-01-05T07:35:47.000Z</published>
    <updated>2025-01-05T08:28:01.365Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下eMCA(Enhanced Machine Check Architecture)相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>简单来讲，eMCA可以将MCE和CMCI转换成SMI，让Firmware（BIOS）可以先行处理，然后再丢给OS。</p><p>下图以CMCI为例，MCE的流程也是类似:<br><img src="/images/2025/01/020.png" alt></p><h2 id="SDM的描述"><a href="#SDM的描述" class="headerlink" title="SDM的描述"></a>SDM的描述</h2><p><img src="/images/2025/01/019.png" alt></p><ul><li><strong>MCG_EMC_P (Enhanced Machine Check Capability) flag, bit 25</strong> — Indicates (when set) that the processor supports enhanced machine check capabilities for firmware first signaling.</li><li><strong>MCG_ELOG_P (extended error logging) flag, bit 26</strong> — Indicates (when set) that the processor allows platform firmware to be invoked when an error is detected so that it may provide additional platform specific information in an ACPI format “Generic Error Data Entry” that augments the data included in machine check bank registers.</li></ul><h2 id="FFM-Firmware-First-Mode"><a href="#FFM-Firmware-First-Mode" class="headerlink" title="FFM (Firmware First Mode)"></a>FFM (Firmware First Mode)</h2><p><img src="/images/2025/01/021.jpg" alt></p><p>FFM allows firmware to provide additional error information to os, synchronous with MCE or CMCI. </p><p>The hardware generates an SMI upon error. The SMI handler pre-processes the error and constructs Error Log in memory prior to continuing with the MCE or CMCI.<br><img src="/images/2025/01/022.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://peterhu.github.io/posts/2020/12/26/RAS%E7%AE%80%E4%BB%8B.html" target="_blank" rel="noopener">peterhu:RAS简介</a></li><li><a href="https://www.intel.com/content/www/us/en/quality/reliability-availability-serviceability-xeon-paper.html" target="_blank" rel="noopener">4th Gen Intel® Xeon® Scalable Processors: Reliability, Availability, and Serviceability (RAS) Technical Paper</a></li><li><a href="https://uefi.org/sites/default/files/resources/Spike%20Yuan-%20Server%20RAS%20and%20UEFI%20CPER_final.pdf" target="_blank" rel="noopener">Server RAS and UEFI CPER</a></li><li><a href="https://beyond-firmware.blogspot.com/2015/10/mca-machine-check-architecture.html" target="_blank" rel="noopener">Martin’s Coding Note:MCA</a></li><li><a href="https://www.intel.la/content/www/xl/es/content-details/671064/mca-enhancements-in-intel-xeon-processors.html" target="_blank" rel="noopener">MCA Enhancements in Intel® Xeon® Processors</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下eMCA(Enhanced Machine Check Architecture)相关notes。
    
    </summary>
    
      <category term="RAS" scheme="http://liujunming.github.io/categories/RAS/"/>
    
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about DRAM components</title>
    <link href="http://liujunming.github.io/2025/01/05/DRAM-components/"/>
    <id>http://liujunming.github.io/2025/01/05/DRAM-components/</id>
    <published>2025-01-05T00:52:40.000Z</published>
    <updated>2025-01-05T04:26:17.211Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下DRAM components相关notes。<br><img src="/images/2025/01/018.png" alt><br><a id="more"></a><br>In modern cloud servers, a CPU(socket) has several memory controllers. Each controller communicates with DIMMs through high-speed memory channels. Usually, a memory channel is shared by several DIMM slots. A DIMM has several ranks, and each is composed of sevaral DRAM chips. For typical DDR4 DIMMs, a rank is composed of 16 chips for data bits and 2 additional chips for ECC bits. <strong>A chip consists of multiple banks, which enables the access parallesim</strong>. A DRAM bank is structured as a two-dimensional cell array indexed by rows and columns. At the micro-level, a cell can store multiple bits of data, and the number of data bits stored in a cell is called the data width of a chip, which is usually denoted as x4, x8 or x16,etc.</p><ul><li>socket</li><li>memory controller</li><li>channel</li><li>DIMM(Dual In-Line Memory Module)</li><li>rank</li><li>chip</li><li>bank</li><li>cell (row, column)</li></ul><hr><p>参考资料:</p><ol><li>Predicting DRAM-Caused Node Unavailability in Hyper-Scale Clouds(DSN’22)</li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001137649751.html" target="_blank" rel="noopener">内存结构</a></li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001090907934.html" target="_blank" rel="noopener">内存基本概念</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下DRAM components相关notes。&lt;br&gt;&lt;img src=&quot;/images/2025/01/018.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
