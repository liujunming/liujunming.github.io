<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2025-04-12T11:49:39.574Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes about linux KPTI</title>
    <link href="http://liujunming.github.io/2025/04/12/Notes-about-linux-KPTI/"/>
    <id>http://liujunming.github.io/2025/04/12/Notes-about-linux-KPTI/</id>
    <published>2025-04-12T11:43:46.000Z</published>
    <updated>2025-04-12T11:49:39.574Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2025/04/006.png" alt><a id="more"></a> </p><p>KPTI (kernel page table isolation) is proposed to defend against the Meltdown attack. This patch separates user space and kernel space page tables entirely, as shown in the following figure. The one used by kernel is the same as before, while the one used by application contains a copy of user space and a small set of kernel space mapping with only trampoline code to enter the kernel. Since the data of kernel are no longer mapped in the user space, a malicious application cannot directly de-reference kernel’s data address, and thus cannot issue Meltdown attack.</p><p><img src="/images/2025/04/007.png" alt></p><hr><p>参考资料:</p><ol><li>EPTI: Efficient Defense against Meltdown Attack for Unpatched VMs(ATC’18)</li><li><a href="https://www.wikiwand.com/en/articles/Kernel_page-table_isolation" target="_blank" rel="noopener">https://www.wikiwand.com/en/articles/Kernel_page-table_isolation</a></li><li><a href="https://blog.csdn.net/pwl999/article/details/112686914" target="_blank" rel="noopener">Linux mem 2.3 内核页表隔离 (KPTI) 详解</a></li><li><a href="https://www.kernel.org/doc/html/next/x86/pti.html" target="_blank" rel="noopener">Page Table Isolation (PTI)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2025/04/006.png&quot; alt&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel CET</title>
    <link href="http://liujunming.github.io/2025/04/06/Notes-about-Intel-CET/"/>
    <id>http://liujunming.github.io/2025/04/06/Notes-about-Intel-CET/</id>
    <published>2025-04-05T22:27:12.000Z</published>
    <updated>2025-04-12T11:49:09.830Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel CET(Control-flow Enforcement Technology)的相关notes，主要内容转载自<a href="https://mp.weixin.qq.com/s/zs7G5yucR5iANSiS4M1I3A" target="_blank" rel="noopener">Intel CET 安全防御机制深度解析</a>。<a id="more"></a> </p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>CET（Control-flow Enforcement Technology）机制是 Intel提出<strong>基于硬件的</strong>⽤于缓解 <strong>ROP/JOP/COP</strong>的新技术。特别强调下，他是基于硬件⽀持的解决⽅案。从Intel的Tigerlake (11th gen)，Alderlake (12th gen)/Sapphire-Rapid起，粗颗粒度地旨在预防前向（ call/jmp ）和后向（ ret ）控制流指令劫持来御防ROP的攻击。因此针对防御对象不同，CET技术又分为CET-SS用于针对ROP的ret指令和CET-IBT用于针对JOP/COP的jmp/call指令。</p><h2 id="CET为何而生"><a href="#CET为何而生" class="headerlink" title="CET为何而生"></a>CET为何而生</h2><p>说到防御者CET，就不得不提他的进攻者ROP了。什么是ROP? 来看⼀个例⼦，假设程序中正常执行如下代码⽚段，注意此时其中不含<code>ret</code>或<code>call</code>指令。</p><p><img src="/images/2025/04/001.jpg" alt></p><p>但是，如果稍加偏移⼀下解释代码的地址时，就会导致出现完全不⼀样的指令，如下图所示：</p><p><img src="/images/2025/04/002.jpg" alt></p><p>如果按照红框中的顺序解释这些指令的时候，那么将会产⽣⾮代码预期的结果，会出现原代码中未出现过的 ret 指令以及 call 指令，这些指令序列被称为 <strong>gadget</strong> 。通过仔细构造这些由ret指令终⽌的指令集，攻击者可以执⾏原程序中⾮预期的任意恶意代码，这种新产生ret攻击被称为 ROP 攻击，同理还有COP/JOP（call/jmp）攻击。</p><p>为了防御这种此类ROP攻击，Intel在硬件机制上推出了CET缓解机制，针对不同的攻击行为分为CET-SS(影子栈)和CET-IBT(间接跳转跟踪)。</p><h2 id="CP异常"><a href="#CP异常" class="headerlink" title="#CP异常"></a>#CP异常</h2><p>CET-SS和CET-IBT 在实现机制上属于CPU内部异常。当执行启动CET发现执行执行流中没有endbr64或函数返回ret和影子栈中shandow stack保存的ret不一致时，CPU内部出发异常，这里CET占用的中断向量21号，触发#CP并归为陷阱执行中断处理程序<code>exc_control_protection()</code>，对CET-SS CET-IBT分情况进行报错。CET-IBT -&gt; “traps: Missing ENDBR: xxx”， CET-SS-&gt; #CP(control protect)。</p><h2 id="CET-SS（Shadow-Stack）"><a href="#CET-SS（Shadow-Stack）" class="headerlink" title="CET-SS（Shadow Stack）"></a>CET-SS（Shadow Stack）</h2><p>Intel 提出了⼀种基于硬件的 CET 解决⽅案，其中之⼀的 shadow stack 机制⽤于缓解 ROP 攻击。ROP 依赖于 ret 指令，其中要执⾏的后续指令地址从堆栈中获得。因此 ROP 攻击的前提是攻击者能够在堆栈中构造数据。那么再来看 shadow stack 机制是怎么⼯作的。</p><p>CET 使操作系统能够创建⼀个 shadow stack （影⼦栈）。正常情况下，当执⾏ call 指令时，会将 call 指令后⼀条指令地址压栈。当启⽤了 shadow stack 后，会同时在普通数据栈和 shadow stack 中压⼊返回地址，随后在执⾏ ret 返回时，会将 shadow stack 中的返回地址和普通数据栈中的返回地址做对⽐，如匹配，则正常执⾏，如不匹配，则触发#CP(Controlflow Protection) 异常。如下图所示：</p><p><img src="/images/2025/04/004.png" alt></p><h2 id="CET-IBT-Indirect-Branch-Tracking"><a href="#CET-IBT-Indirect-Branch-Tracking" class="headerlink" title="CET-IBT(Indirect Branch Tracking)"></a>CET-IBT(Indirect Branch Tracking)</h2><p>JOP/COP 攻击⼿法与 ROP 类似，只不过是把 ROP 中以 ret 指令做跳板的关键点替换成了 call/jmp 指令。这种不需要 ret 指令的攻击场景下，前⾯所说的 shadow stack 机制就失效了。这种情况下， CET 的第⼆种机制 IBT(Indirect Branch Tracking) 就应运⽽⽣了。</p><p>IBT(Indirect Branch Tracking),间接跳转跟踪”希望能防止攻击者让间接跳转（例如，通过指针变量进行的函数调用）进入一个不应该走到的地方。</p><p>IBT 是为了防御面向跳转编程的（jump-oriented programming）；工作原理是试图确保每个 indirect branch 的目标确实都是适合作为跳转目标的。IBT 的方法有很多，每一种都有自己的优势和劣势。例如，内核在 5.13 开发周期中支持了编译器实现的 IBT 机制。在这种模式下，编译器通过一个 “jump table, 跳转表” 来完成每一个间接跳转，不仅确保目标是要供间接跳转使用的，而且要确保被调用函数的原型与调用者所期望的一致。这种方法是很有效的，但要增加很多编译、运行时的开销。</p><p>CET-IBT方法相当简单，但优点是得到了硬件的支持，因此速度更快.如果 IBT 被启用，那么 CPU 将确保每个间接跳转都落在一条特殊指令（endbr32 或 endbr64）上，该指令执行时跟 no-op 效果一致。如果发现意外，那么处理器将引发一次 control-protection（#CP）exception。</p><p><img src="/images/2025/04/005.png" alt></p><hr><p>参考资料:</p><ol><li>deepseek[介绍下Intel CET技术]</li><li><a href="https://mp.weixin.qq.com/s/zs7G5yucR5iANSiS4M1I3A" target="_blank" rel="noopener">Intel CET 安全防御机制深度解析</a></li><li><a href="https://www.usenix.org/system/files/atc24-chen-xiangdong.pdf" target="_blank" rel="noopener">Limitations and Opportunities of Modern Hardware Isolation Mechanisms</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/technical-look-control-flow-enforcement-technology.html?wapkw=cet" target="_blank" rel="noopener">A Technical Look at Intel’s Control-flow Enforcement Technology</a></li><li><a href="https://i.blackhat.com/asia-19/Thu-March-28/bh-asia-Sun-How-to-Survive-the-Hardware-Assisted-Control-Flow-Integrity-Enforcement.pdf" target="_blank" rel="noopener">How to Survive the Hardware-assisted Controlflow Integrity Enforcement</a></li><li><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3559344" target="_blank" rel="noopener">CETIS: Retrofitting Intel CET for Generic and Efficient Intra-process Memory Isolation</a></li><li><a href="https://lpc.events/event/2/contributions/147/attachments/72/83/CET-LPC-2018.pdf" target="_blank" rel="noopener">Control-flow Enforcement Technology</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel CET(Control-flow Enforcement Technology)的相关notes，主要内容转载自&lt;a href=&quot;https://mp.weixin.qq.com/s/zs7G5yucR5iANSiS4M1I3A&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Intel CET 安全防御机制深度解析&lt;/a&gt;。
    
    </summary>
    
      <category term="Security" scheme="http://liujunming.github.io/categories/Security/"/>
    
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Seccomp filter</title>
    <link href="http://liujunming.github.io/2025/04/05/Notes-about-Seccomp-filter/"/>
    <id>http://liujunming.github.io/2025/04/05/Notes-about-Seccomp-filter/</id>
    <published>2025-04-05T06:08:11.000Z</published>
    <updated>2025-04-05T10:55:23.507Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Seccomp(SECure COMPuting) filtering的相关notes。</p><p><u>Seccomp filtering provides a means for a process to specify a filter for incoming system calls</u>.<a id="more"></a> </p><p>以下内容源于deepseek。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Seccomp filter 是 Linux 内核提供的一种安全机制，用于限制进程能够执行的系统调用（syscall），从而减少潜在的攻击面。它通过自定义过滤规则（基于 BPF 程序）动态允许或拦截系统调用，常用于沙箱、容器等需要严格隔离的场景。</p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Seccomp-模式"><a href="#Seccomp-模式" class="headerlink" title="Seccomp 模式"></a>Seccomp 模式</h3><ul><li>Strict Mode：仅允许<code>read</code>, <code>writ</code>e<code>,</code>exit<code>,</code>sigreturn<code>四个系统调用，其他调用会触发</code>SIGKILL`。</li><li>Filter Mode（Seccomp-BPF）：允许通过 BPF（Berkeley Packet Filter）规则自定义允许或拒绝的系统调用列表，提供更灵活的过滤。</li></ul><h3 id="BPF-规则"><a href="#BPF-规则" class="headerlink" title="BPF 规则"></a>BPF 规则</h3><ul><li>使用类汇编的指令定义过滤逻辑（例如基于系统调用号、参数值等条件）。</li><li>规则在内核中执行，确保高效和安全。</li></ul><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="规则设置"><a href="#规则设置" class="headerlink" title="规则设置"></a>规则设置</h3><ul><li>进程通过 <code>prctl()</code> 或 <code>seccomp()</code> 系统调用加载 BPF 规则。</li><li>规则定义哪些系统调用被允许（<code>ALLOW</code>）、记录（<code>LOG</code>）或拒绝（<code>KILL</code>/<code>ERRNO</code>）。</li></ul><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><ul><li>当进程发起系统调用时，内核触发 Seccomp 过滤器。</li><li>BPF 规则对系统调用号、参数等进行检查。</li><li>根据规则决定放行、终止进程，或返回错误（如 <code>EPERM</code>）。</li></ul><h2 id="主要用途"><a href="#主要用途" class="headerlink" title="主要用途"></a>主要用途</h2><h3 id="沙箱化应用"><a href="#沙箱化应用" class="headerlink" title="沙箱化应用"></a>沙箱化应用</h3><ul><li>浏览器（如 Chrome）用 Seccomp 限制渲染进程的权限。</li><li>游戏反作弊系统防止用户态程序滥用内核功能。</li></ul><h3 id="容器安全"><a href="#容器安全" class="headerlink" title="容器安全"></a>容器安全</h3><p>Docker 和 Kubernetes 默认启用 Seccomp，限制容器内进程的系统调用（如禁止 <code>mount</code> 或 <code>reboot</code>）。</p><h3 id="特权进程降权"><a href="#特权进程降权" class="headerlink" title="特权进程降权"></a>特权进程降权</h3><p>服务启动后通过 Seccomp 丢弃不必要的系统调用权限。</p><hr><p>参考资料:</p><ol><li><a href="https://man7.org/linux/man-pages/man2/seccomp.2.html" target="_blank" rel="noopener">man seccomp</a></li><li><a href="https://www.kernel.org/doc/html/v5.0/userspace-api/seccomp_filter.html" target="_blank" rel="noopener">Seccomp BPF (SECure COMPuting with filters)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Seccomp(SECure COMPuting) filtering的相关notes。&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Seccomp filtering provides a means for a process to specify a filter for incoming system calls&lt;/u&gt;.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ARM MTE technology</title>
    <link href="http://liujunming.github.io/2025/03/29/Notes-about-ARM-MTE-Memory-Tagging-Extension-technology/"/>
    <id>http://liujunming.github.io/2025/03/29/Notes-about-ARM-MTE-Memory-Tagging-Extension-technology/</id>
    <published>2025-03-29T09:50:08.000Z</published>
    <updated>2025-03-29T11:54:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下ARM MTE(Memory Tagging Extension) technology的相关notes。<a id="more"></a> </p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/03/001.jpeg" alt></p><p><img src="/images/2025/03/002.jpeg" alt></p><p><img src="/images/2025/03/003.jpeg" alt></p><p>Starting with ARMv8.3-A, ARM SoCs introduce support for memory tagging extensions (MTE) that <strong>allow partitioning the address space into 16-byte regions that are colored with one of the 16 tags</strong>. <u>The hardware maintains a table that stores the mapping between addresses and tags allowing access to the region only if the tag of the pointer (the tag is stored in the upper bits of the pointer matches the tag of the memory region)</u>.</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="检测内存越界与溢出"><a href="#检测内存越界与溢出" class="headerlink" title="检测内存越界与溢出"></a>检测内存越界与溢出</h3><p>内存越界与溢出是常见的内存安全问题，通常由于错误的索引或指针操作导致访问超出合法的内存范围，可能引发未定义行为、数据损坏，甚至被攻击者利用进行恶意代码执行。MTE 提供了有效的硬件级检测机制，实时防止这些问题的发生。</p><p>假设一个程序中分配了 16 字节的内存，但错误地访问了第 17 个字节：</p><p><img src="/images/2025/03/004.webp" alt></p><p>在此场景中，MTE 通过标签匹配机制发现非法访问，从而阻止越界操作的执行，避免可能的崩溃或安全漏洞。</p><h3 id="防止未初始化内存的使用"><a href="#防止未初始化内存的使用" class="headerlink" title="防止未初始化内存的使用"></a>防止未初始化内存的使用</h3><p>未初始化内存的使用是另一个常见的内存安全问题。当程序访问未初始化的内存时，可能读取到随机数据，导致逻辑错误、崩溃，或被恶意利用泄露敏感信息。MTE 可以有效防止这类问题。</p><p>以下是一个典型的未初始化内存使用的伪代码示例：</p><p><img src="/images/2025/03/005.webp" alt></p><p>在这种情况下，MTE 会阻止对未初始化内存的非法访问，避免出现数据不一致和未定义行为。</p><h3 id="内存释放后的安全保护"><a href="#内存释放后的安全保护" class="headerlink" title="内存释放后的安全保护"></a>内存释放后的安全保护</h3><p>时间局部安全性问题，即内存释放后的访问，是内存管理中的常见隐患。这种问题通常发生在指针仍然试图访问已释放的内存区域，可能导致程序崩溃或被攻击者利用进行数据篡改。MTE 通过标签机制解决了这一问题。</p><p>以下代码展示了释放后访问的典型错误：</p><p><img src="/images/2025/03/006.webp" alt></p><p>在这里，MTE 通过实时检测，阻止了非法访问已释放内存的行为，防止时间局部安全问题带来的潜在风险。</p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/conference/atc24/presentation/chen-xiangdong" target="_blank" rel="noopener">Limitations and Opportunities of Modern Hardware Isolation Mechanisms</a></li><li><a href="https://mp.weixin.qq.com/s/GlXhwNlf4jTs5ndRLCFZjQ" target="_blank" rel="noopener">Android Memory Tagging Extension (MTE) 的深度研究与应用</a></li><li><a href="https://mp.weixin.qq.com/s/H9Pu9i37ogyl05z7NzOStA" target="_blank" rel="noopener">Memory安全和硬件Memory Tagging技术—中篇</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzI5MjYwMTk3OA==&amp;mid=2247484869&amp;idx=1&amp;sn=d7e4faef480a169df8ed79e66c73b58b&amp;chksm=ec7facd0db0825c64e4010f2a92e73d8db5753ce50efbf9f29ea356a7b5277b94a06c1222057&amp;scene=178&amp;cur_album_id=1638614933820997634#rd" target="_blank" rel="noopener">Memory安全和硬件Memory Tagging技术—下篇</a></li><li><a href="https://codasip.com/2023/11/02/fine-grained-memory-protection-cheri/" target="_blank" rel="noopener">Fine-grained memory protection</a></li><li><a href="https://static.linaro.org/connect/yvr18/presentations/yvr18-104.pdf" target="_blank" rel="noopener">https://static.linaro.org/connect/yvr18/presentations/yvr18-104.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下ARM MTE(Memory Tagging Extension) technology的相关notes。
    
    </summary>
    
      <category term="ARM" scheme="http://liujunming.github.io/categories/ARM/"/>
    
    
      <category term="ARM" scheme="http://liujunming.github.io/tags/ARM/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about FPGA concepts</title>
    <link href="http://liujunming.github.io/2025/02/23/Notes-about-FPGA-concepts/"/>
    <id>http://liujunming.github.io/2025/02/23/Notes-about-FPGA-concepts/</id>
    <published>2025-02-23T01:41:23.000Z</published>
    <updated>2025-02-23T02:20:18.357Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下FPGA基本概念的相关notes。<a id="more"></a> </p><p>Field Programmable Gate Arrays (FPGAs) are “a sea” of logic, arithmetic, and memory elements, which users can configure to implement custom compute circuits. FPGA compute capacity is determined by the <em>area</em> available for the circuits.</p><h2 id="FPGA-development"><a href="#FPGA-development" class="headerlink" title="FPGA development"></a>FPGA development</h2><p>FPGAs can be seen as “software defined” hardware. The software definition, <em>a design</em>, is implemented using register transfer languages (RTL) such as Verilog. Additionally, designers can use high-level synthesis (HLS) tools to generate RTL, e.g., from a restricted version of C++. However, HLS C++ programs are different from CPU programs, and must follow certain rules, including explicit exposure of fine-grain pipeline- and task- parallelism to achieve high performance. Implementation tools then compile the design into an FPGA <em>image</em> targeting specific hardware.</p><p>Finally, users can load the image onto an FPGA (slow, up to a few seconds), entirely replacing the previous design. Some FPGAs support <em>partial reconfiguration</em> to replace only a subset of the entire FPGA, a much faster process (milliseconds), which unfortunately incurs significant area overheads.</p><h2 id="FPGA-sharing"><a href="#FPGA-sharing" class="headerlink" title="FPGA sharing"></a>FPGA sharing</h2><p>There are three ways to share an FPGA: space partitioning, coarse-grain, and fine-grain time sharing.</p><h3 id="space-partitioning"><a href="#space-partitioning" class="headerlink" title="space partitioning"></a>space partitioning</h3><p>Space partitioning divides FPGA resources into disjoint sets used by different AFUs(Accelerator Functional Units). If shared I/O interfaces (memory, PCIe bus) are securely isolated and multiplexed, this method enables low-overhead FPGA sharing among mutually distrustful AFUs but requires larger FPGAs to fit them all.</p><p><img src="/images/2025/02/008.png" alt></p><h3 id="Coarse-grain-time-sharing"><a href="#Coarse-grain-time-sharing" class="headerlink" title="Coarse-grain time sharing"></a>Coarse-grain time sharing</h3><p>Coarse-grain time sharing dynamically switches AFUs via full or partial reconfiguration. It incurs high switching latency.</p><p><img src="/images/2025/02/009.png" alt></p><h3 id="Fine-grain-time-sharing"><a href="#Fine-grain-time-sharing" class="headerlink" title="Fine-grain time sharing"></a>Fine-grain time sharing</h3><p>Fine-grain time sharing allows multiple CPU applications to use the same AFU. <u>The AFU implements the context switch internally, in hardware</u>. Packet processing applications such as <a href="https://www.usenix.org/conference/nsdi18/presentation/firestone" target="_blank" rel="noopener">AccelNet</a> use this approach to process each packet in the context of its associated flow. Such AFUs oversee switching between the contexts(此类AFU负责管理不同上下文之间的切换); therefore this type of sharing requires AFUs to be <em>trusted</em> to ensure fair use and state isolation between their users.</p><p><img src="/images/2025/02/010.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/conference/atc19/presentation/eran" target="_blank" rel="noopener">NICA: An Infrastructure for Inline Acceleration of Network Applications</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下FPGA基本概念的相关notes。
    
    </summary>
    
      <category term="Accelerator" scheme="http://liujunming.github.io/categories/Accelerator/"/>
    
    
      <category term="Accelerator" scheme="http://liujunming.github.io/tags/Accelerator/"/>
    
  </entry>
  
  <entry>
    <title>Notes about F-NIC(FPGA-based SmartNIC)</title>
    <link href="http://liujunming.github.io/2025/02/22/Notes-about-F-NIC-FPGA-based-SmartNIC-architecture/"/>
    <id>http://liujunming.github.io/2025/02/22/Notes-about-F-NIC-FPGA-based-SmartNIC-architecture/</id>
    <published>2025-02-22T13:40:52.000Z</published>
    <updated>2025-02-23T01:54:08.692Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下F-NIC(FPGA-based SmartNIC)相关notes。<a id="more"></a> </p><p>We describe bump-in-the-wire(是一种网络技术概念，指在网络通信链路中透明插入的设备或系统，用于增强特定功能而不改变终端设备的配置或通信协议) F-NICs, focusing on Mellanox Innova.</p><p><img src="/images/2025/02/007.png" alt></p><h2 id="Bump-in-the-wire"><a href="#Bump-in-the-wire" class="headerlink" title="Bump-in-the-wire"></a>Bump-in-the-wire</h2><p>A typical F-NIC (Figure 1) combines a commodity network ASIC (e.g., ConnectX-4 Lx NIC) with an FPGA and local DRAM. The FPGA is located <em>between</em> the ASIC and the network port, interposing on all Ethernet traffic in and out of the NIC. The FPGA and the ASIC communicate directly via an internal bus (e.g., 40 Gbps Ethernet), and a PCIe bus connects the ASIC to the host.</p><p>The bump-in-the-wire design reuses the existing data and control planes between the CPU and the NIC ASIC, with its QoS management, and virtualization support (SR-IOV), mature DMA engines, and software stack.</p><h2 id="F-NIC-programming"><a href="#F-NIC-programming" class="headerlink" title="F-NIC programming"></a>F-NIC programming</h2><p>The development of an F-NIC accelerated application involves both hardware logic on FPGA and associated software on the CPU. F-NIC vendors provide a lightweight <em>shell IP</em>: a set of low-level hardware interfaces for basic operations, including link-layer packet exchange with the network and the host, onboard DRAM access, and control register access. However, the vendor SDK leaves it to customers to implement higher level features such as FPGA network stack processing or virtualization support.</p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/system/files/atc19-eran.pdf" target="_blank" rel="noopener">NICA: An Infrastructure for Inline Acceleration of Network Applications</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下F-NIC(FPGA-based SmartNIC)相关notes。
    
    </summary>
    
      <category term="Accelerator" scheme="http://liujunming.github.io/categories/Accelerator/"/>
    
    
      <category term="Accelerator" scheme="http://liujunming.github.io/tags/Accelerator/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PFC(Priority-based Flow Control)</title>
    <link href="http://liujunming.github.io/2025/02/22/Notes-about-PFC-Priority-based-Flow-Control/"/>
    <id>http://liujunming.github.io/2025/02/22/Notes-about-PFC-Priority-based-Flow-Control/</id>
    <published>2025-02-22T12:50:20.000Z</published>
    <updated>2025-02-22T13:10:03.280Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PFC(Priority-based Flow Control)相关notes，内容转载自<a href="https://info.support.huawei.com/info-finder/encyclopedia/en/PFC.html" target="_blank" rel="noopener">What Is PFC?</a><a id="more"></a> </p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p><strong>Disadvantages of Traditional Flow Control Technologies</strong></p><p>The most fundamental flow control technology is the <u>Ethernet Pause mechanism</u> defined in IEEE 802.3. When a downstream device detects that its receive capability is lower than the transmit capability of its upstream device, it sends Pause frames to the upstream device, requesting the upstream device to stop sending traffic for a period of time.</p><p><img src="/images/2025/02/004.png" alt></p><p>The drawback, however, is that the Ethernet Pause mechanism stops all traffic on a link (for the entire interface). This in turn affects link sharing, which is critical to actual services. Link sharing requires:</p><ul><li>Burst traffic of one type cannot affect forwarding of other types of traffic.</li><li>A large amount of one type of traffic in a queue cannot occupy buffer resources of other types of traffic.</li></ul><p><img src="/images/2025/02/005.png" alt></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>The meaning of PFC is Priority-based Flow Control. <u>It is the most widely used flow control technology that can effectively prevent packet loss and serve as the basis for intelligent lossless networks.</u> A PFC-enabled queue is a lossless queue. <em>When congestion occurs in such a queue on a downstream device, the downstream device instructs the upstream device to stop sending traffic in the queue, implementing zero packet loss.</em></p><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p>As shown in the figure, <u>eight priority queues on the transmit interface of DeviceA correspond to eight receive buffers on the receive interface of DeviceB.</u> When a receive buffer on DeviceB is congested, DeviceB sends a backpressure signal “STOP” to DeviceA, requesting DeviceA to stop sending traffic in the corresponding priority queue.</p><p><em>PFC addresses the conflict between the Ethernet Pause mechanisms and link sharing.</em> <u>It controls traffic only in one or several priority queues of an interface, rather than on the entire interface.</u> <em>What’s more, PFC can pause or restart any queue, without interrupting traffic in other queues. This feature enables traffic of various types to share one link.</em></p><p><img src="/images/2025/02/006.png" alt><br><em>PFC working mechanism</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PFC(Priority-based Flow Control)相关notes，内容转载自&lt;a href=&quot;https://info.support.huawei.com/info-finder/encyclopedia/en/PFC.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;What Is PFC?&lt;/a&gt;
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about vCPU MPState</title>
    <link href="http://liujunming.github.io/2025/02/16/Notes-about-vCPU-MPState/"/>
    <id>http://liujunming.github.io/2025/02/16/Notes-about-vCPU-MPState/</id>
    <published>2025-02-16T03:20:47.000Z</published>
    <updated>2025-02-16T03:30:29.868Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下vCPU MPState(MultiProcessing State)的相关notes。<a id="more"></a></p><p>在热迁移时，qemu会执行<code>kvm_get_mp_state</code>函数:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">kvm_get_mp_state</span><span class="params">(X86CPU *cpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    CPUState *cs = CPU(cpu);</span><br><span class="line">    CPUX86State *env = &amp;cpu-&gt;env;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">kvm_mp_state</span> <span class="title">mp_state</span>;</span></span><br><span class="line">    <span class="keyword">int</span> ret;</span><br><span class="line"></span><br><span class="line">    ret = kvm_vcpu_ioctl(cs, KVM_GET_MP_STATE, &amp;mp_state);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    env-&gt;mp_state = mp_state.mp_state;</span><br><span class="line">    <span class="keyword">if</span> (kvm_irqchip_in_kernel()) &#123;</span><br><span class="line">        cs-&gt;halted = (mp_state.mp_state == KVM_MP_STATE_HALTED);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>此时<code>KVM_GET_MP_STATE</code>就是关键线索，搜索<a href="https://www.kernel.org/doc/Documentation/virt/kvm/api.txt" target="_blank" rel="noopener">kvm api Documentation</a>即可找到最终的答案。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">4.38 KVM_GET_MP_STATE</span><br><span class="line"></span><br><span class="line">Capability: KVM_CAP_MP_STATE</span><br><span class="line">Architectures: x86, s390, arm, arm64</span><br><span class="line">Type: vcpu ioctl</span><br><span class="line">Parameters: struct kvm_mp_state (out)</span><br><span class="line">Returns: 0 on success; -1 on error</span><br><span class="line"></span><br><span class="line">struct kvm_mp_state &#123;</span><br><span class="line">__u32 mp_state;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Returns the vcpu&apos;s current &quot;multiprocessing state&quot; (though also valid on</span><br><span class="line">uniprocessor guests).</span><br><span class="line"></span><br><span class="line">Possible values are:</span><br><span class="line"></span><br><span class="line"> - KVM_MP_STATE_RUNNABLE:        the vcpu is currently running [x86,arm/arm64]</span><br><span class="line"> - KVM_MP_STATE_UNINITIALIZED:   the vcpu is an application processor (AP)</span><br><span class="line">                                 which has not yet received an INIT signal [x86]</span><br><span class="line"> - KVM_MP_STATE_INIT_RECEIVED:   the vcpu has received an INIT signal, and is</span><br><span class="line">                                 now ready for a SIPI [x86]</span><br><span class="line"> - KVM_MP_STATE_HALTED:          the vcpu has executed a HLT instruction and</span><br><span class="line">                                 is waiting for an interrupt [x86]</span><br><span class="line"> - KVM_MP_STATE_SIPI_RECEIVED:   the vcpu has just received a SIPI (vector</span><br><span class="line">                                 accessible via KVM_GET_VCPU_EVENTS) [x86]</span><br><span class="line"> - KVM_MP_STATE_STOPPED:         the vcpu is stopped [s390,arm/arm64]</span><br><span class="line"> - KVM_MP_STATE_CHECK_STOP:      the vcpu is in a special error state [s390]</span><br><span class="line"> - KVM_MP_STATE_OPERATING:       the vcpu is operating (running or halted)</span><br><span class="line">                                 [s390]</span><br><span class="line"> - KVM_MP_STATE_LOAD:            the vcpu is in a special load/startup state</span><br><span class="line">                                 [s390]</span><br><span class="line"></span><br><span class="line">On x86, this ioctl is only useful after KVM_CREATE_IRQCHIP. Without an</span><br><span class="line">in-kernel irqchip, the multiprocessing state must be maintained by userspace on</span><br><span class="line">these architectures.</span><br><span class="line"></span><br><span class="line">For arm/arm64:</span><br><span class="line"></span><br><span class="line">The only states that are valid are KVM_MP_STATE_STOPPED and</span><br><span class="line">KVM_MP_STATE_RUNNABLE which reflect if the vcpu is paused or not.</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://crosvm.dev/doc/hypervisor/enum.MPState.html" target="_blank" rel="noopener">Enum hypervisor::MPState</a></li><li><a href="https://www.kernel.org/doc/Documentation/virt/kvm/api.txt" target="_blank" rel="noopener">The Definitive KVM (Kernel-based Virtual Machine) API Documentation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下vCPU MPState(MultiProcessing State)的相关notes。
    
    </summary>
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/categories/QEMU-KVM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/tags/QEMU-KVM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Content-Addressable Memory (CAM)</title>
    <link href="http://liujunming.github.io/2025/02/15/Notes-about-Content-addressable-memory-CAM/"/>
    <id>http://liujunming.github.io/2025/02/15/Notes-about-Content-addressable-memory-CAM/</id>
    <published>2025-02-15T10:50:28.000Z</published>
    <updated>2025-02-15T11:45:04.155Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Content-Addressable Memory(CAM)相关notes，整理下来自deepseek的回答。<a id="more"></a> </p><p>CAM是一种特殊类型的存储器，其核心特点是<strong>通过内容直接检索数据</strong>，而非传统存储器（如RAM）通过地址访问数据的方式。</p><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>传统存储器（如RAM）：输入地址，返回该地址存储的数据；CAM：输入待查找的数据（关键词），返回该数据所在的地址或关联的其他数据。</p><p>工作流程如下所示:</p><ol><li>用户提供待匹配的<strong>关键词</strong>（Search Key）。</li><li>CAM在存储的所有条目中<strong>并行比较</strong>关键词。</li><li>返回匹配条目的地址或关联数据（如优先级最高的匹配结果）。</li></ol><h2 id="CAM的类型"><a href="#CAM的类型" class="headerlink" title="CAM的类型"></a>CAM的类型</h2><ul><li><p>二元CAM（Binary CAM, BCAM）：</p><ul><li>仅支持精确匹配（0或1）</li><li>应用场景：精确查找，如MAC地址表</li></ul></li><li><p>三元CAM（Ternary CAM, TCAM）：</p><ul><li>支持0、1和<strong>“无关”（Don’t Care）</strong>三种状态（通常用掩码表示）</li><li>可实现<strong>部分匹配</strong>，适用于需要通配符的场景</li><li>典型应用：IP路由中的<strong>最长前缀匹配</strong>（Longest Prefix Match）</li></ul></li></ul><h2 id="核心优势"><a href="#核心优势" class="headerlink" title="核心优势"></a>核心优势</h2><ul><li>高速并行搜索：所有存储条目同时比较，搜索时间复杂度为<strong>O(1)</strong>，远快于软件算法（如哈希表、树结构）</li><li>硬件级效率：无需遍历数据或处理冲突，适合实时性要求高的场景</li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>网络设备：</p><ul><li>路由器/交换机：快速查找路由表、MAC地址表</li><li>防火墙：基于规则的实时流量过滤</li></ul><h2 id="缺点与挑战"><a href="#缺点与挑战" class="headerlink" title="缺点与挑战"></a>缺点与挑战</h2><ul><li>高功耗：并行比较所有条目导致大量晶体管同时切换，功耗显著高于传统存储器</li><li>高成本：每个存储单元需集成比较电路，芯片面积增大</li><li>容量限制：受限于功耗和成本，CAM通常用于小规模高速缓存场景</li></ul><h2 id="TCAM的“无关”位示例"><a href="#TCAM的“无关”位示例" class="headerlink" title="TCAM的“无关”位示例"></a>TCAM的“无关”位示例</h2><p>在IP路由中，TCAM允许将子网掩码未覆盖的位设为“无关”，例如：IP地址：<code>192.168.1.0/24</code>（二进制：<code>11000000.10101000.00000001.********</code>），TCAM条目：<code>11000000 10101000 00000001 XXXXXXXX</code>（<code>X</code>表示“无关”位），搜索时，只需匹配前24位，后8位忽略，实现高效的最长前缀匹配。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CAM通过硬件并行性实现了极速数据检索，是网络设备和高速缓存的核心组件，但其功耗和成本限制了大规模应用。</p><hr><p>参考资料:</p><ol><li><a href="https://www.xilinx.com/products/intellectual-property/ef-di-cam.html" target="_blank" rel="noopener">xilinx Content Addressable Memory (CAM)</a></li><li><a href="https://ieeexplore.ieee.org/document/7159147" target="_blank" rel="noopener">Emerging Trends in Design and Applications of Memory-Based Computing and Content-Addressable Memories</a></li><li><a href="https://arxiv.org/pdf/1804.02330" target="_blank" rel="noopener">An Efficient I/O Architecture for RAM-based Content-Addressable Memory on FPGA</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Content-Addressable Memory(CAM)相关notes，整理下来自deepseek的回答。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about linux /proc/kcore</title>
    <link href="http://liujunming.github.io/2025/02/15/Notes-about-linux-proc-kcore/"/>
    <id>http://liujunming.github.io/2025/02/15/Notes-about-linux-proc-kcore/</id>
    <published>2025-02-15T02:18:25.000Z</published>
    <updated>2025-02-15T03:50:51.251Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下linux中<code>/proc/kcore</code>相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p><code>/proc/kcore</code> is a file in the virtual /proc filesystem of a Linux machine. It is created by the kernel in <a href="https://elixir.bootlin.com/linux/v5.0/source/fs/proc/kcore.c" target="_blank" rel="noopener">fs/proc/kcore.c</a> and <strong>allows read access to all the kernels virtual memory space from userland</strong>.</p><p>Internally it has the format of an ELF core dump file (ELF Type 4/ET_CORE). That means it has the same format as a core file from a crashed process; but instead of capturing the (static) state of a single process at the moment of the crash, <strong>it provides a real time view into the state of the whole system</strong>.</p><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><ul><li>The ELF header (<code>Elf64_Ehdr</code>): It’s at the start of every ELF file. We need two pieces of information from it: the location and number of entries of the program header table.</li><li>The Program headers (<code>Elf64_Phdr</code>): An ELF file contains an array of program header structures. There are various subtypes of program headers, but we care only about the ones marked as <code>PT_LOAD</code>. <u>Each of these headers describe a loadable segment - a part of the file that is loaded into memory</u>. In <code>/proc/kcore</code>, <strong>they describe where in the file each portion of the system memory can be found.</strong></li></ul><p>On x86-64 systems, Linux maintains a complete one-to-one map of all physical memory in the kernels virtual address space. So by reading the right ranges of kernel virtual memory, one can get a complete copy of the content of the physical memory of that system.</p><p><img src="/images/2025/02/003.png" alt></p><p>每个<code>PT_LOAD</code> header用来记录一段 memory region，并描述了这段 memory region对应的文件offset、内核虚拟地址和长度。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol><li>使用<code>open</code>系统调用打开<code>/proc/kcore</code>文件</li><li>遍历ELF文件中的程序头表(Program Header Table)，找到包含目标内核虚拟地址的段(每个段描述了内核虚拟地址到文件offset的映射关系)</li><li>根据计算出的文件offset，使用<code>lseek</code>定位到目标位置，使用<code>read</code>读取目标地址的内容</li></ol><hr><p>参考资料:</p><ol><li><a href="https://schlafwandler.github.io/posts/dumping-/proc/kcore/" target="_blank" rel="noopener">Dumping /proc/kcore in 2019</a></li><li><a href="https://blog.csdn.net/weixin_45030965/article/details/124863905" target="_blank" rel="noopener">Linux /proc/kcore详解（一）</a></li><li><a href="https://blog.csdn.net/weixin_45030965/article/details/125164642" target="_blank" rel="noopener">Linux /proc/kcore详解（二）</a></li><li><a href="https://superuser.com/questions/669462/will-applications-running-in-root-mode-be-able-to-overwrite-oss-or-other-progra" target="_blank" rel="noopener">Will applications running in root mode be able to overwrite OS’s or other program’s memory section?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下linux中&lt;code&gt;/proc/kcore&lt;/code&gt;相关notes。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>SystemTap如何改变内核函数返回值</title>
    <link href="http://liujunming.github.io/2025/02/09/SystemTap%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98%E5%86%85%E6%A0%B8%E5%87%BD%E6%95%B0%E8%BF%94%E5%9B%9E%E5%80%BC/"/>
    <id>http://liujunming.github.io/2025/02/09/SystemTap如何改变内核函数返回值/</id>
    <published>2025-02-09T11:41:30.000Z</published>
    <updated>2025-02-09T12:29:55.906Z</updated>
    
    <content type="html"><![CDATA[<p><strong>利用SystemTap，判断内核函数的参数后，才更改返回值?</strong> <a id="more"></a></p><p><img src="/images/2025/02/002.png" alt></p><p>接下来将展示下<a href="https://chatgpt.com/" target="_blank" rel="noopener">chatgpt</a>给出的答案(笔者也没有运行，不保证正确性)。</p><p><img src="/images/2025/02/001.png" alt></p><h2 id="SystemTap-概述"><a href="#SystemTap-概述" class="headerlink" title="SystemTap 概述"></a>SystemTap 概述</h2><p>SystemTap 是一个动态追踪工具，可以用来监控内核和用户空间程序的行为。你可以通过编写脚本来插桩内核函数，并分析函数参数、返回值等信息。</p><p>在这个教程中，我们将演示如何创建一个 SystemTap 脚本，挂钩内核函数、检查参数，并在满足特定条件时修改函数的返回值。</p><h2 id="创建SystemTap-脚本"><a href="#创建SystemTap-脚本" class="headerlink" title="创建SystemTap 脚本"></a>创建SystemTap 脚本</h2><p>我们以<code>sys_open</code>系统调用为例。<code>sys_open</code>用于打开文件，它接受两个主要参数：路径和打开标志。我们将判断路径是否是<code>/etc/passwd</code>，如果是，则修改返回值（模拟无法打开该文件）。</p><p>创建一个新的 SystemTap 脚本文件，比如<code>check_open.stp</code>，并编辑该文件，在文件中输入以下内容:</p><figure class="highlight stp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">probe syscall.open &#123;</span><br><span class="line">    <span class="comment">// 获取传入的路径参数</span></span><br><span class="line">    filename = user_string($filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印文件路径</span></span><br><span class="line">    printf(<span class="string">"Trying to open: %s\n"</span>, filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断路径是否为 /etc/passwd</span></span><br><span class="line">    if (filename == <span class="string">"/etc/passwd"</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果是 /etc/passwd，则返回 -EACCES，表示访问被拒绝</span></span><br><span class="line">        printf(<span class="string">"Blocking access to /etc/passwd\n"</span>)</span><br><span class="line">        return -EACCES</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>probe syscall.open</code>：我们使用 <code>syscall.open</code> 来挂钩 <code>sys_open</code> 系统调用。这将允许我们监视所有尝试打开文件的操作。</li><li><code>user_string($filename)</code>：该函数提取文件路径参数。我们使用 <code>user_string()</code> 来获取用户空间传入的字符串。</li><li><code>if (filename == &quot;/etc/passwd&quot;)</code>：这里检查传入的文件路径是否为 <code>/etc/passwd</code>。</li><li><code>return -EACCES</code>：如果路径是 <code>/etc/passwd</code>，我们通过 <code>return</code> 语句修改返回值，返回 <code>-EACCES</code>，表示访问被拒绝。</li></ul><h2 id="运行-SystemTap-脚本"><a href="#运行-SystemTap-脚本" class="headerlink" title="运行 SystemTap 脚本"></a>运行 SystemTap 脚本</h2><p>在创建了脚本后，需要使用 <code>stap</code> 命令来运行脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo stap check_open.stp</span><br></pre></td></tr></table></figure><p>这将会启动脚本并开始跟踪 <code>sys_open</code> 调用。如果你尝试打开 <code>/etc/passwd</code>，脚本会修改返回值，导致文件无法打开。</p><h2 id="验证脚本效果"><a href="#验证脚本效果" class="headerlink" title="验证脚本效果"></a>验证脚本效果</h2><p>你可以验证脚本的效果，看看它是否按预期工作：</p><p>尝试打开 <code>/etc/passwd</code> 文件，例如使用 <code>cat</code> 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd</span><br></pre></td></tr></table></figure></p><p>由于脚本的作用，<code>cat</code> 命令应该失败，并返回错误信息，如 <code>Permission denied</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>创建脚本：使用<code>probe</code>挂钩内核函数，获取相关参数。</li><li>条件判断：通过<code>if</code>语句判断参数值，执行相应的操作。</li><li>修改返回值：使用<code>return</code>修改函数的返回值。</li><li>运行脚本：使用<code>sudo stap</code>命令来执行脚本并进行监控。</li></ol><hr><p>参考资料:</p><ol><li><a href="https://lrita.github.io/images/posts/systemtap/SystemTap-II.pdf" target="_blank" rel="noopener">SystemTap Tutorial Part-II</a></li><li><a href="https://www.opensourceforu.com/2010/10/systemtap-tutorial-part-2/" target="_blank" rel="noopener">SystemTap Tutorial, Part-2</a></li><li><a href="https://sourceware.org/systemtap/tutorial.pdf" target="_blank" rel="noopener">Systemtap tutorial</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;利用SystemTap，判断内核函数的参数后，才更改返回值?&lt;/strong&gt;
    
    </summary>
    
      <category term="debug" scheme="http://liujunming.github.io/categories/debug/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="debug" scheme="http://liujunming.github.io/tags/debug/"/>
    
  </entry>
  
  <entry>
    <title>Notes about eMCA</title>
    <link href="http://liujunming.github.io/2025/01/05/Notes-about-eMCA-Gen2/"/>
    <id>http://liujunming.github.io/2025/01/05/Notes-about-eMCA-Gen2/</id>
    <published>2025-01-05T07:35:47.000Z</published>
    <updated>2025-01-05T08:28:01.365Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下eMCA(Enhanced Machine Check Architecture)相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>简单来讲，eMCA可以将MCE和CMCI转换成SMI，让Firmware（BIOS）可以先行处理，然后再丢给OS。</p><p>下图以CMCI为例，MCE的流程也是类似:<br><img src="/images/2025/01/020.png" alt></p><h2 id="SDM的描述"><a href="#SDM的描述" class="headerlink" title="SDM的描述"></a>SDM的描述</h2><p><img src="/images/2025/01/019.png" alt></p><ul><li><strong>MCG_EMC_P (Enhanced Machine Check Capability) flag, bit 25</strong> — Indicates (when set) that the processor supports enhanced machine check capabilities for firmware first signaling.</li><li><strong>MCG_ELOG_P (extended error logging) flag, bit 26</strong> — Indicates (when set) that the processor allows platform firmware to be invoked when an error is detected so that it may provide additional platform specific information in an ACPI format “Generic Error Data Entry” that augments the data included in machine check bank registers.</li></ul><h2 id="FFM-Firmware-First-Mode"><a href="#FFM-Firmware-First-Mode" class="headerlink" title="FFM (Firmware First Mode)"></a>FFM (Firmware First Mode)</h2><p><img src="/images/2025/01/021.jpg" alt></p><p>FFM allows firmware to provide additional error information to os, synchronous with MCE or CMCI. </p><p>The hardware generates an SMI upon error. The SMI handler pre-processes the error and constructs Error Log in memory prior to continuing with the MCE or CMCI.<br><img src="/images/2025/01/022.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://peterhu.github.io/posts/2020/12/26/RAS%E7%AE%80%E4%BB%8B.html" target="_blank" rel="noopener">peterhu:RAS简介</a></li><li><a href="https://www.intel.com/content/www/us/en/quality/reliability-availability-serviceability-xeon-paper.html" target="_blank" rel="noopener">4th Gen Intel® Xeon® Scalable Processors: Reliability, Availability, and Serviceability (RAS) Technical Paper</a></li><li><a href="https://uefi.org/sites/default/files/resources/Spike%20Yuan-%20Server%20RAS%20and%20UEFI%20CPER_final.pdf" target="_blank" rel="noopener">Server RAS and UEFI CPER</a></li><li><a href="https://beyond-firmware.blogspot.com/2015/10/mca-machine-check-architecture.html" target="_blank" rel="noopener">Martin’s Coding Note:MCA</a></li><li><a href="https://www.intel.la/content/www/xl/es/content-details/671064/mca-enhancements-in-intel-xeon-processors.html" target="_blank" rel="noopener">MCA Enhancements in Intel® Xeon® Processors</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下eMCA(Enhanced Machine Check Architecture)相关notes。
    
    </summary>
    
      <category term="RAS" scheme="http://liujunming.github.io/categories/RAS/"/>
    
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about DRAM components</title>
    <link href="http://liujunming.github.io/2025/01/05/DRAM-components/"/>
    <id>http://liujunming.github.io/2025/01/05/DRAM-components/</id>
    <published>2025-01-05T00:52:40.000Z</published>
    <updated>2025-01-05T04:26:17.211Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下DRAM components相关notes。<br><img src="/images/2025/01/018.png" alt><br><a id="more"></a><br>In modern cloud servers, a CPU(socket) has several memory controllers. Each controller communicates with DIMMs through high-speed memory channels. Usually, a memory channel is shared by several DIMM slots. A DIMM has several ranks, and each is composed of sevaral DRAM chips. For typical DDR4 DIMMs, a rank is composed of 16 chips for data bits and 2 additional chips for ECC bits. <strong>A chip consists of multiple banks, which enables the access parallesim</strong>. A DRAM bank is structured as a two-dimensional cell array indexed by rows and columns. At the micro-level, a cell can store multiple bits of data, and the number of data bits stored in a cell is called the data width of a chip, which is usually denoted as x4, x8 or x16,etc.</p><ul><li>socket</li><li>memory controller</li><li>channel</li><li>DIMM(Dual In-Line Memory Module)</li><li>rank</li><li>chip</li><li>bank</li><li>cell (row, column)</li></ul><hr><p>参考资料:</p><ol><li>Predicting DRAM-Caused Node Unavailability in Hyper-Scale Clouds(DSN’22)</li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001137649751.html" target="_blank" rel="noopener">内存结构</a></li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001090907934.html" target="_blank" rel="noopener">内存基本概念</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下DRAM components相关notes。&lt;br&gt;&lt;img src=&quot;/images/2025/01/018.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Memory Scrubbing</title>
    <link href="http://liujunming.github.io/2025/01/04/Notes-about-Memory-Scrubbing/"/>
    <id>http://liujunming.github.io/2025/01/04/Notes-about-Memory-Scrubbing/</id>
    <published>2025-01-04T11:38:43.000Z</published>
    <updated>2025-01-04T13:26:49.114Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Memory Scrubbing相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>Memory scrubbing refers to the process of correcting or ‘scrubbing’ erroneously flipped bits in memory as a result of transient faults such as those caused by physical phenomena. Scrubbing is considered a RAS feature.</p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>Memory scrubbing将错误纠正后回写，可以防止内存条上的单bit错误逐渐累积形成多bit不可纠正错误。These actions provide software with early visibility for possible preventive measures such as page off-lining based on the rate of error corrections.</p><h2 id="When"><a href="#When" class="headerlink" title="When"></a>When</h2><ul><li>Patrol scrubbing proactively searches the system memory(由硬件而非软件来做), repairing correctable errors. It prevents accumulation of single-bit errors.</li><li>Demand scrubbing is the ability to write corrected data back to the memory once a correctable error is detected on a read transaction.</li></ul><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p><strong>Patrol Scrubbing consists in reading memory, checking it against ECC for errors, and overwriting with the corrected memory words when an error is discovered</strong>. </p><p>Patrol scrubbing is done using a hardware engine, on either the platform or on the memory device, which generates requests to memory addresses on the memory device. The engine generates memory requests at a predefined frequency. Given enough time, it will eventually access every memory address. The frequency in which patrol scrub generates requests produces no noticeable impact on the memory device’s quality of service.</p><p>By generating read requests to memory addresses, the patrol scrubber allows the hardware an opportunity to run ECC on a memory address and correct any correctable errors before they can become uncorrectable errors. <u>Optionally, if an uncorrectable error is discovered, the patrol scrubber can trigger a hardware interrupt and notify the software layer of its memory address</u>.</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/quality/reliability-availability-serviceability-xeon-paper.html" target="_blank" rel="noopener">4th Gen Intel® Xeon® Scalable Processors: Reliability, Availability, and Serviceability (RAS) Technical Paper</a></li><li><a href="https://en.wikichip.org/wiki/memory_scrubbing" target="_blank" rel="noopener">https://en.wikichip.org/wiki/memory_scrubbing</a></li><li><a href="https://community.intel.com/t5/Server-Products/Uncorrectable-Memory-Error-amp-Patrol-Scrub/m-p/545123" target="_blank" rel="noopener">Uncorrectable Memory Error &amp; Patrol Scrub</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/pmem-RAS.html" target="_blank" rel="noopener">Reliability, Availability, and Serviceability (RAS)</a></li><li><a href="https://uefi.org/htmlspecs/ACPI_Spec_6_4_html/05_ACPI_Software_Programming_Model/ACPI_Software_Programming_Model.html" target="_blank" rel="noopener">ACPI Software Programming Model</a></li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001108627660.html" target="_blank" rel="noopener">Demand Scrubbing/Patrol Scrubbing（内存巡检）</a></li><li><a href="https://www.cnblogs.com/xyjk1002-rejuvenation/p/16479297.html" target="_blank" rel="noopener">内存错误和服务器内存RAS功能-DELL篇-1</a></li><li><a href="https://superuser.com/questions/372422/can-linux-scrub-memory" target="_blank" rel="noopener">Can Linux scrub memory?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Memory Scrubbing相关notes。
    
    </summary>
    
      <category term="RAS" scheme="http://liujunming.github.io/categories/RAS/"/>
    
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about SDM MCA</title>
    <link href="http://liujunming.github.io/2025/01/04/Notes-about-SDM-MCA/"/>
    <id>http://liujunming.github.io/2025/01/04/Notes-about-SDM-MCA/</id>
    <published>2025-01-04T03:35:29.000Z</published>
    <updated>2025-01-04T06:38:01.401Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下SDM中MCA相关notes。<a id="more"></a></p><h2 id="1-Architecture"><a href="#1-Architecture" class="headerlink" title="1. Architecture"></a>1. Architecture</h2><ul><li>processors implement a machine-check architecture that provides a mechanism for detecting and reporting hardware (machine) errors, such as: system bus errors, ECC errors, parity errors, cache errors, and TLB errors. It consists of a set of model-specific registers (MSRs) that are used to set up machine checking and additional banks of MSRs used for recording errors that are detected.</li><li>the processor can report information on corrected machine-check errors and deliver a programmable interrupt for software to respond to MC errors, referred to as corrected machine-check error interrupt (CMCI).</li><li>Intel 64 processors support for software recovery from certain uncorrected recoverable machine check errors.</li></ul><h2 id="2-MSRs"><a href="#2-MSRs" class="headerlink" title="2. MSRs"></a>2. MSRs</h2><p><img src="/images/2025/01/005.png" alt></p><h3 id="2-1-Machine-Check-Global-Control-MSRs"><a href="#2-1-Machine-Check-Global-Control-MSRs" class="headerlink" title="2.1 Machine-Check Global Control MSRs"></a>2.1 Machine-Check Global Control MSRs</h3><h4 id="2-1-1-IA32-MCG-CAP-MSR"><a href="#2-1-1-IA32-MCG-CAP-MSR" class="headerlink" title="2.1.1 IA32_MCG_CAP MSR"></a>2.1.1 IA32_MCG_CAP MSR</h4><p>The IA32_MCG_CAP MSR is a read-only register that provides information about the machine-check architecture of the processor.</p><p><img src="/images/2025/01/006.png" alt></p><p>深入了解各个field后，其实就可以对MCA整体架构有个全局的认识了。</p><h4 id="2-1-2-IA32-MCG-STATUS-MSR"><a href="#2-1-2-IA32-MCG-STATUS-MSR" class="headerlink" title="2.1.2 IA32_MCG_STATUS MSR"></a>2.1.2 IA32_MCG_STATUS MSR</h4><p>The IA32_MCG_STATUS MSR describes the current state of the processor after a machine-check exception has occurred.</p><p><img src="/images/2025/01/007.png" alt></p><h4 id="2-1-3-IA32-MCG-CTL-MSR"><a href="#2-1-3-IA32-MCG-CTL-MSR" class="headerlink" title="2.1.3 IA32_MCG_CTL MSR"></a>2.1.3 IA32_MCG_CTL MSR</h4><p>IA32_MCG_CTL controls the reporting of machine-check exceptions.</p><h4 id="2-1-4-IA32-MCG-EXT-CTL-MSR"><a href="#2-1-4-IA32-MCG-EXT-CTL-MSR" class="headerlink" title="2.1.4 IA32_MCG_EXT_CTL MSR"></a>2.1.4 IA32_MCG_EXT_CTL MSR</h4><p>IA32_MCG_EXT_CTL.LMCE_EN (bit 0) allows the processor to signal some MCEs to only a single logical processor in the system.</p><p><img src="/images/2025/01/008.png" alt></p><h4 id="2-1-5-Enabling-Local-Machine-Check"><a href="#2-1-5-Enabling-Local-Machine-Check" class="headerlink" title="2.1.5 Enabling Local Machine Check"></a>2.1.5 Enabling Local Machine Check</h4><p>When system software has enabled LMCE, then hardware will determine if a particular error can be delivered only to a single logical processor. Software should make no assumptions about the type of error that hardware can choose to deliver as LMCE.</p><h3 id="2-2-Error-Reporting-Register-Banks"><a href="#2-2-Error-Reporting-Register-Banks" class="headerlink" title="2.2 Error-Reporting Register Banks"></a>2.2 Error-Reporting Register Banks</h3><h4 id="2-2-1-IA32-MCi-CTL-MSRs"><a href="#2-2-1-IA32-MCi-CTL-MSRs" class="headerlink" title="2.2.1 IA32_MCi_CTL MSRs"></a>2.2.1 IA32_MCi_CTL MSRs</h4><p><img src="/images/2025/01/009.png" alt></p><h4 id="2-2-2-IA32-MCi-STATUS-MSRs"><a href="#2-2-2-IA32-MCi-STATUS-MSRs" class="headerlink" title="2.2.2 IA32_MCi_STATUS MSRs"></a>2.2.2 IA32_MCi_STATUS MSRs</h4><p><img src="/images/2025/01/010.png" alt></p><p><img src="/images/2025/01/011.png" alt></p><h4 id="2-2-3-IA32-MCi-ADDR-MSRs"><a href="#2-2-3-IA32-MCi-ADDR-MSRs" class="headerlink" title="2.2.3 IA32_MCi_ADDR MSRs"></a>2.2.3 IA32_MCi_ADDR MSRs</h4><p><img src="/images/2025/01/012.png" alt></p><h4 id="2-2-4-IA32-MCi-MISC-MSRs"><a href="#2-2-4-IA32-MCi-MISC-MSRs" class="headerlink" title="2.2.4 IA32_MCi_MISC MSRs"></a>2.2.4 IA32_MCi_MISC MSRs</h4><p><img src="/images/2025/01/013.png" alt></p><h4 id="2-2-5-IA32-MCi-CTL2-MSRs"><a href="#2-2-5-IA32-MCi-CTL2-MSRs" class="headerlink" title="2.2.5 IA32_MCi_CTL2 MSRs"></a>2.2.5 IA32_MCi_CTL2 MSRs</h4><p><img src="/images/2025/01/014.png" alt></p><h2 id="3-Enhanced-Cache-Error-reporting"><a href="#3-Enhanced-Cache-Error-reporting" class="headerlink" title="3. Enhanced Cache Error reporting"></a>3. Enhanced Cache Error reporting</h2><ul><li>In earlier Intel processors, cache status was based on the number of correction events that occurred in a cache.</li><li>In “threshold-based error status”, cache status is based on the number of lines (ECC blocks) in a cache that incur repeated corrections.</li><li>A processor that supports enhanced cache error reporting contains hardware that tracks the operating status of certain caches and provides an indicator of their “health”.<ul><li>The hardware reports a “green” status when the number of lines that incur repeated corrections is at or below a pre-defined threshold</li><li>a “yellow” status when the number of affected lines exceeds the threshold. Yellow status means that the cache reporting the event is operating correctly, but you should schedule the system for servicing within a few weeks.</li></ul></li></ul><h2 id="4-Corrected-Machine-Check-Error-Interrupt"><a href="#4-Corrected-Machine-Check-Error-Interrupt" class="headerlink" title="4. Corrected Machine Check Error Interrupt"></a>4. Corrected Machine Check Error Interrupt</h2><p>待另择篇幅整理</p><h2 id="5-Recovery-of-Uncorrected-Recoverable-UCR-Errors"><a href="#5-Recovery-of-Uncorrected-Recoverable-UCR-Errors" class="headerlink" title="5. Recovery of Uncorrected Recoverable(UCR) Errors"></a>5. Recovery of Uncorrected Recoverable(UCR) Errors</h2><p>Recovery of uncorrected recoverable machine check errors is an enhancement in machine-check architecture. <strong>This allow system software to perform recovery action on certain class of uncorrected errors and continue execution.</strong></p><h3 id="5-1-Detection-of-Software-Error-Recovery-Support"><a href="#5-1-Detection-of-Software-Error-Recovery-Support" class="headerlink" title="5.1 Detection of Software Error Recovery Support"></a>5.1 Detection of Software Error Recovery Support</h3><p>The new class of architectural MCA errors from which system software can attempt recovery is called <u><strong>Uncorrected Recoverable (UCR)</strong></u> Errors. <u>UCR errors are uncorrected errors that have been detected and signaled but have not corrupted the processor context</u>. For certain UCR errors, this means that once system software has performed a certain recovery action, it is possible to continue execution on this processor. UCR error reporting provides an error containment mechanism for data poisoning. The machine check handler will use the error log information from the error reporting registers to analyze and implement specific error recovery actions for UCR errors.</p><h3 id="5-2-UCR-Error-Reporting-and-Logging"><a href="#5-2-UCR-Error-Reporting-and-Logging" class="headerlink" title="5.2 UCR Error Reporting and Logging"></a>5.2 UCR Error Reporting and Logging</h3><p>IA32_MCi_STATUS MSR is used for reporting UCR errors and existing corrected or uncorrected errors.<br>When IA32_MCG_CAP[24] is set, a UCR error is indicated by the following bit settings in the IA32_MCi_STATUS register:</p><ul><li>Valid (bit 63) = 1</li><li>UC(bit61)=1</li><li>PCC(bit57)=0</li></ul><p>In addition, the IA32_MCi_STATUS register bit fields, bits 56:55, are defined (see Figure 16-6) to provide additional information to help system software to properly identify the necessary recovery action for the UCR error:</p><ul><li>S (Signaling) flag, bit 56</li><li>AR (Action Required) flag, bit 55 </li></ul><h3 id="5-3-UCR-Error-Classification"><a href="#5-3-UCR-Error-Classification" class="headerlink" title="5.3 UCR Error Classification"></a>5.3 UCR Error Classification</h3><ul><li><u>Uncorrected no action required (UCNA)</u> - is a UCR error that is not signaled via a machine check exception and, instead, is reported to system software as a corrected machine check error. </li><li><u>Software recoverable action optional (SRAO)</u> - a UCR error is signaled either via a machine check exception or CMCI. System software recovery action is optional and not required to continue execution from this machine check exception.</li><li><u>Software recoverable action required (SRAR)</u> - a UCR error that requires system software to take a recovery action on this processor before scheduling another stream of execution on this processor. </li></ul><p><img src="/images/2025/01/015.png" alt></p><h3 id="5-4-UCR-Error-Overwrite-Rules"><a href="#5-4-UCR-Error-Overwrite-Rules" class="headerlink" title="5.4 UCR Error Overwrite Rules"></a>5.4 UCR Error Overwrite Rules</h3><p>In general, the overwrite rules are as follows:</p><ul><li>UCR errors will overwrite corrected errors.</li><li>Uncorrected (PCC=1) errors overwrite UCR (PCC=0) errors.</li><li>UCR errors are not written over previous UCR errors.</li><li>Corrected errors do not write over previous UCR errors.</li></ul><h2 id="6-Interpreting-the-MCA-Error-Codes"><a href="#6-Interpreting-the-MCA-Error-Codes" class="headerlink" title="6. Interpreting the MCA Error Codes"></a>6. Interpreting the MCA Error Codes</h2><p>When the processor detects a machine-check error condition, it writes a 16-bit error code to the MCA error code field of one of the IA32_MCi_STATUS registers and sets the VAL (valid) flag in that register. The processor may also write a 16-bit model-specific error code in the IA32_MCi_STATUS register depending on the implementation of the machine-check architecture of the processor.</p><h3 id="6-1-Simple-Error-Codes"><a href="#6-1-Simple-Error-Codes" class="headerlink" title="6.1 Simple Error Codes"></a>6.1 Simple Error Codes</h3><p>Simple error codes indicate global error information.</p><p><img src="/images/2025/01/016.png" alt></p><h3 id="6-2-Compound-Error-Codes"><a href="#6-2-Compound-Error-Codes" class="headerlink" title="6.2 Compound Error Codes"></a>6.2 Compound Error Codes</h3><p>Compound error codes describe errors related to the TLBs, memory, caches, bus and interconnect logic, and internal timer. <strong>A set of sub-fields is common to all of compound errors. These sub-fields describe the type of access, level in the cache hierarchy, and type of request</strong>.</p><p><img src="/images/2025/01/017.png" alt></p><ul><li>Transaction Type (TT) Sub-Field</li><li>Level (LL) Sub-Field</li><li>Request (RRRR) Sub-Field</li><li>Bus and Interconnect Errors</li><li>Memory Controller and Extended Memory Errors</li></ul><h3 id="6-3-Architecturally-Defined-UCR-Errors"><a href="#6-3-Architecturally-Defined-UCR-Errors" class="headerlink" title="6.3 Architecturally Defined UCR Errors"></a>6.3 Architecturally Defined UCR Errors</h3><ul><li>Architecturally Defined SRAO Errors</li><li>Architecturally Defined SRAR Errors</li></ul><h3 id="6-4-Multiple-MCA-Errors"><a href="#6-4-Multiple-MCA-Errors" class="headerlink" title="6.4 Multiple MCA Errors"></a>6.4 Multiple MCA Errors</h3><p>When multiple MCA errors are detected within a certain detection window, the processor may aggregate the reporting of these errors together as a single event, i.e., a single machine exception condition. If this occurs, system software <u>may find multiple MCA errors logged in different MC banks on one logical processor</u> or <u>find multiple MCA errors logged across different processors for a single machine check broadcast event</u>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下SDM中MCA相关notes。
    
    </summary>
    
      <category term="RAS" scheme="http://liujunming.github.io/categories/RAS/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Device Memory TCP</title>
    <link href="http://liujunming.github.io/2025/01/01/Notes-about-Device-Memory-TCP/"/>
    <id>http://liujunming.github.io/2025/01/01/Notes-about-Device-Memory-TCP/</id>
    <published>2025-01-01T11:10:46.000Z</published>
    <updated>2025-01-01T12:04:42.997Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Device Memory TCP相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Device memory TCP is a proposal for transferring data to and/or from device memory efficiently, without bouncing the data to a host memory buffer.</p><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p><img src="/images/2025/01/001.png" alt></p><p><img src="/images/2025/01/002.png" alt></p><p><img src="/images/2025/01/003.png" alt></p><p>A large amount of data transfers have device memory as the source and/or destination. Accelerators drastically increased the volume of such transfers. Some examples include:</p><ul><li>ML accelerators transferring large amounts of training data from storage into GPU/TPU memory. In some cases ML training setup time can be as long as 50% of TPU compute time, improving data transfer throughput &amp; efficiency can help improving GPU/TPU utilization.</li><li>Distributed training, where ML accelerators, such as GPUs on different hosts, exchange data among them.</li><li>Distributed raw block storage applications transfer large amounts of data with remote SSDs, much of this data does not require host processing.</li></ul><p>Today, the majority of the Device-to-Device data transfers the network are implemented as the following low level operations: Device-to-Host copy, Host-to-Host network transfer, and Host-to-Device copy.</p><p>The implementation is suboptimal, especially for bulk data transfers, and can put significant strains on system resources, such as host memory bandwidth, PCIe bandwidth, etc.</p><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><p><img src="/images/2025/01/004.png" alt></p><p>We attempt to optimize this use case by implementing socket APIs that enable the user to:</p><ol><li>send device memory across the network directly, and</li><li>receive incoming network packets directly into device memory.</li></ol><p>Packet payloads go directly from the NIC to device memory for receive and from device memory to NIC for transmit. Packet headers go to/from host memory and are processed by the TCP/IP stack normally.</p><p>The NIC must support header split to achieve this. i.e. the capability to split incoming packets into a header + payload and to put each into a separate buffer. Device Memory works by using device memory for the packet payload, and host memory for the packet headers.</p><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>Alleviate host memory bandwidth pressure, compared to existing network-transfer + device-copy semantics.</li><li>Alleviate PCIe BW pressure, by limiting data transfer to the lowest level of the PCIe tree, compared to traditional path which sends data through the root complex.</li></ul><hr><p>参考资料:</p><ol><li><a href="https://docs.kernel.org/networking/devmem.html" target="_blank" rel="noopener">docs.kernel:Device Memory TCP</a></li><li><a href="https://netdevconf.org/0x17/sessions/talk/device-memory-tcp.html" target="_blank" rel="noopener">netdevconf:Device Memory TCP</a></li><li><a href="https://lore.kernel.org/netdev/20240831004313.3713467-1-almasrymina@google.com/" target="_blank" rel="noopener">[PATCH net-next v24 00/13] Device Memory TCP</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Device Memory TCP相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about AMD Processor Hierarchy</title>
    <link href="http://liujunming.github.io/2024/12/22/Notes-about-AMD-Processor-Hierarchy/"/>
    <id>http://liujunming.github.io/2024/12/22/Notes-about-AMD-Processor-Hierarchy/</id>
    <published>2024-12-22T10:14:50.000Z</published>
    <updated>2024-12-22T11:05:21.491Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2024/12/008.jpg" alt><a id="more"></a></p><p>This is a simplified configuration of an EPYC Rome node with two sockets. Each socket contains eight <strong>Core Complex Dies</strong> (CCDs, each enclosed in a green box) and one I/O die (IOD, enclosed in a yellow box). The infinity sign (♾) represents the Infinity Fabric. Each CCD contains two <strong>Core Complexes</strong> (CCXs). Each CCX has 4 cores and 16 MB of L3 cache. Thus, there are 64 cores per socket and 128 cores per node.</p><p>The Rome processor hierarchy is as follows:</p><ul><li>Core: A CPU core has private L1I, L1D, and L2 caches, which are shared by two hyperthreads on the core.</li><li>CCX: A core complex includes four cores and a common L3 cache of 16 MB. Different CCXs do not share L3.</li><li>CCD: A core complex die includes two CCXs and an Infinity Link to the I/O die (IOD). The CCDs connect to memory, I/O, and each other through the IOD.</li><li>Socket: A socket includes eight CCDs (total of 64 cores), a common centralized I/O die (includes eight unified memory controllers and eight IO x16 PCIe 4.0 lanes—total of 128 lanes), and a link to the network interface controller (NIC).</li><li>Node: A node includes two sockets and a network interface controller (NIC).</li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.nas.nasa.gov/hecc/support/kb/amd-rome-processors_658.html" target="_blank" rel="noopener">AMD Rome Processors</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2024/12/008.jpg&quot; alt&gt;
    
    </summary>
    
      <category term="AMD" scheme="http://liujunming.github.io/categories/AMD/"/>
    
    
      <category term="AMD" scheme="http://liujunming.github.io/tags/AMD/"/>
    
  </entry>
  
  <entry>
    <title>Notes about FUSE filesystem</title>
    <link href="http://liujunming.github.io/2024/12/21/Notes-about-FUSE-filesystem/"/>
    <id>http://liujunming.github.io/2024/12/21/Notes-about-FUSE-filesystem/</id>
    <published>2024-12-21T11:07:07.000Z</published>
    <updated>2025-01-12T12:30:54.361Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2024/12/005.png" alt><a id="more"></a></p><h2 id="tutorial"><a href="#tutorial" class="headerlink" title="tutorial"></a>tutorial</h2><p><img src="/images/2024/12/007.png" alt></p><p>very nice tutorial: <a href="https://maastaar.net/fuse/linux/filesystem/c/2016/05/21/writing-a-simple-filesystem-using-fuse/" target="_blank" rel="noopener">Writing a Simple Filesystem Using FUSE in C</a></p><h3 id="Making-a-call-into-a-FUSE-file-system"><a href="#Making-a-call-into-a-FUSE-file-system" class="headerlink" title="Making a call into a FUSE file system"></a>Making a call into a FUSE file system</h3><p><img src="/images/2024/12/006.gif" alt></p><ol><li>A program, such as ls, mkdir makes a call to a file system routine. For example, open(“/test/fuse/file1.txt”). This call gets sent to the kernel.</li><li>If this file is in a FUSE volume, the kernel passes it on to the FUSE kernel module, which then passes it on to the implementation of that file system.</li><li>The implementation of open then refers to the actual data structures that represent the file system and returns a file handle. It is open’s job to take a concrete view of data (bits stored on a hard drive) and present an abstract view (a hierarchically organized file system).</li><li>The kernel returns the result of the open function to the program that originally made the call.</li></ol><p>Cited From <a href="https://www.cs.cmu.edu/~fp/courses/15213-s07/lectures/15-filesys/index.html" target="_blank" rel="noopener">File Systems and FUSE</a>.</p><h3 id="simple-fuse-example"><a href="#simple-fuse-example" class="headerlink" title="simple fuse example"></a>simple fuse example</h3><ul><li><a href="https://github.com/JulesWang/helloworld-fuse/tree/master" target="_blank" rel="noopener">helloworld-fuse</a></li><li><a href="https://github.com/libfuse/libfuse/blob/master/example/hello.c" target="_blank" rel="noopener">libfuse/example/hello.c</a></li></ul><h3 id="辅助资料"><a href="#辅助资料" class="headerlink" title="辅助资料"></a>辅助资料</h3><ul><li><a href="https://www.cs.hmc.edu/~geoff/classes/hmc.cs135.201109/homework/fuse/fuse_doc.html" target="_blank" rel="noopener">CS135 FUSE Documentation</a></li><li><a href="https://www.cs.nmsu.edu/~pfeiffer/fuse-tutorial/" target="_blank" rel="noopener">Writing a FUSE Filesystem: a Tutorial</a></li><li><a href="https://github.com/osxfuse/fuse/blob/master/doc/how-fuse-works" target="_blank" rel="noopener">How Fuse-1.3 Works</a></li><li><a href="https://www.bilibili.com/video/BV1NS4y1L7Me/" target="_blank" rel="noopener">linux内核开发第38讲：linux基于fuse实现自定义文件系统整体架构</a></li></ul><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>FAST’17 paper: <a href="https://www.usenix.org/system/files/conference/fast17/fast17-vangoor.pdf" target="_blank" rel="noopener">To FUSE or Not to FUSE: Performance of User-Space File Systems</a>，<a href="https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_vangoor.pdf" target="_blank" rel="noopener">slides</a>也非常硬核!</p><h3 id="辅助资料-1"><a href="#辅助资料-1" class="headerlink" title="辅助资料"></a>辅助资料</h3><ul><li><a href="https://georgesims21.github.io/fuse/" target="_blank" rel="noopener">George’s Blog FUSE</a></li><li><a href="https://www.bilibili.com/video/BV1r24y157gm/" target="_blank" rel="noopener">FUSE 文件系统浅析 - 张老师</a></li><li><a href="https://zhuanlan.zhihu.com/p/143256077" target="_blank" rel="noopener">用户态文件系统 - FUSE</a></li><li><a href="https://www.kernel.org/doc/html/next/filesystems/fuse.html" target="_blank" rel="noopener">kernel doc fuse</a></li><li><a href="https://zhuanlan.zhihu.com/p/17059519212" target="_blank" rel="noopener">FUSE读写流程梳理</a></li></ul><h2 id="Manual"><a href="#Manual" class="headerlink" title="Manual"></a>Manual</h2><ul><li><a href="https://man7.org/linux/man-pages/man4/fuse.4.html" target="_blank" rel="noopener">fuse(4)</a></li><li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/uapi/linux/fuse.h" target="_blank" rel="noopener">include/uapi/linux/fuse.h</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/2024/12/005.png&quot; alt&gt;
    
    </summary>
    
      <category term="文件系统" scheme="http://liujunming.github.io/categories/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Linux Transmit Packet Steering(XPS) technology</title>
    <link href="http://liujunming.github.io/2024/12/21/Notes-about-Linux-Transmit-Packet-Steering-XPS-technology/"/>
    <id>http://liujunming.github.io/2024/12/21/Notes-about-Linux-Transmit-Packet-Steering-XPS-technology/</id>
    <published>2024-12-21T00:07:27.000Z</published>
    <updated>2024-12-21T05:01:50.936Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux XPS(Transmit Packet Steering)相关notes。<a id="more"></a></p><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><blockquote><p>The Linux network stack maps each core C to a different Tx queue Q, such that Q’s memory is allocated from C’s node. Additionally, memory allocations of packets transmitted via Q are likewise fulfilled using the same node. Cores can then transmit simultaneously through their individual queues in an uncoordinated, NU(D)MA-friendly manner while avoiding synchronization overheads. When a thread T that executes on C issues a system call to open a socket file descriptor S, the network stack associates Q with S, saving Q’s identifier in the socket data structure. After that, whenever T transmits through S, the network stack checks that T still runs on C. If it does not, the network stack updates S to point to the queue of T ’s new core. (The actual modification happens after Q is drained from any outstanding packets that originated from S, to avoid out-of-order transmissions.)</p></blockquote><h2 id="2-Optimization"><a href="#2-Optimization" class="headerlink" title="2. Optimization"></a>2. Optimization</h2><h3 id="2-1-reduce-contention"><a href="#2-1-reduce-contention" class="headerlink" title="2.1 reduce contention"></a>2.1 reduce contention</h3><p>contention on the device queue lock is significantly reduced since fewer CPUs contend for the same queue(contention can be eliminated completely if each CPU has its own transmit queue).</p><h3 id="2-2-reduce-cache-miss-rate-on-transmit-completion"><a href="#2-2-reduce-cache-miss-rate-on-transmit-completion" class="headerlink" title="2.2 reduce cache miss rate on transmit completion"></a>2.2 reduce cache miss rate on transmit completion</h3><p>cache miss rate on transmit completion is reduced, in particular for data cache lines that hold the <code>sk_buff</code> structures.</p><p>网卡发完包后，会给CPU发送中断；接着linux内核协议栈就会调用<code>kfree_skb</code>，此时就会访问到<code>sk_buff</code> structures。如果发送数据包的core与调用<code>kfree_skb</code>的core一样，那么<code>sk_buff</code> structures的cache miss rate就会降低。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kfree_skb</span><br><span class="line">└── kfree_skb_reason</span><br><span class="line">    └── skb_unref</span><br><span class="line">        └── skb-&gt;users</span><br></pre></td></tr></table></figure><h3 id="2-3-DMA-Buffer-NUMA-Affinity"><a href="#2-3-DMA-Buffer-NUMA-Affinity" class="headerlink" title="2.3 DMA Buffer NUMA Affinity"></a>2.3 DMA Buffer NUMA Affinity</h3><p>网卡发包时，DMA本node的内存即可，无需跨numa node，可以提升DMA的性能。详情可以参考<a href="/2024/11/17/%E8%BD%AC%E8%BD%BD-Linux-NUMA-Optimization-1/#3-2-1-DMA-Buffer-NUMA-Affinity">DMA Buffer NUMA Affinity</a>。</p><hr><p>参考资料:</p><ol><li>IOctopus: Outsmarting Nonuniform DMA(ASPLOS’20)</li><li><a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt" target="_blank" rel="noopener">Scaling in the Linux Networking Stack</a></li><li><a href="https://zhuanlan.zhihu.com/p/148756667" target="_blank" rel="noopener">Linux网络栈的性能缩放</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux XPS(Transmit Packet Steering)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about network RFS and aRFS</title>
    <link href="http://liujunming.github.io/2024/12/08/Notes-about-network-RFS-and-aRFS/"/>
    <id>http://liujunming.github.io/2024/12/08/Notes-about-network-RFS-and-aRFS/</id>
    <published>2024-12-08T04:49:12.000Z</published>
    <updated>2024-12-08T13:24:24.082Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下network的RFS(Receive Flow Steering)和aRFS(Accelerated Receive Flow Steering)相关notes。<a id="more"></a></p><h3 id="RFS"><a href="#RFS" class="headerlink" title="RFS"></a>RFS</h3><p><a href="/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/">RSS</a>/<a href="/2024/12/01/Notes-about-Linux-Receive-Packet-Steering-RPS-technology/">RPS</a>很好地保证了数据包处理的负载均衡，可以将处理任务合理的分配到所有的 CPU 上。</p><p>但是，有时候我们还需要考虑其他的因素。比如，网卡收到了属于一个运行在 CPU0 上的进程的数据包，那么这些数据包被 CPU0 处理会比其他 CPU 处理更高效。</p><p>原因很简单直观，数据在 CPU 内传递比跨 CPU 传递要更节省时间。因此，我们<strong>希望数据包尽量能够被其所属的进程所在的CPU处理，至少能够被同属一个 NUMA域的CPU处理</strong>。</p><p>RFS 机制就是为了实现这一点。使用 hash 函数根据包头信息计算得到一个 hash 值，然后作为索引查表。RFS 所查找的匹配表（<code>rps_sock_flow_table</code>）中存储的是数据包所属进程所在的 CPU。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hash value] : [CPU id]</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>如果能查找到有效的CPU，就将数据包入队到CPU对应的backlog队列中；如果查找不到，那么就直接按照 RPS 机制转发。</p><p>与 RPS 预先配置好的 CPU 列表不同，<code>rps_sock_flow</code>表是动态更新的。如果有数据包的收发操作，如 <code>inet_recvmsg()</code>, <code>inet_sendmsg()</code>, <code>inet_sendpage()</code>, <code>tcp_splice_read()</code> 等操作，则会插入新的值。类似的情况还发生在进程被调度到新的 CPU 的时候。这时候就需要更新匹配表中的值。如果原来的 CPU 队列上还有未处理完的数据包，那么就会发生乱序。</p><p>为了避免乱序，RFS 使用了另一个表 —— <code>rps_dev_flow</code> 表，每个网卡队列对应一个该表。该表的索引依旧是包头的 hash 值，每个表项对应两个字段：1) 现在的 CPU（也就是该数据包所属流已经把数据包放在其队列上等待其内核处理的 CPU）号。2) 当该流最后一个数据包到达后，该 CPU 的 backlog 队列的尾计数器值(用来判断原CPU 队列上有没有未处理完的包)。</p><p>当进程切换 CPU 时，先判断原 CPU 队列上有没有未处理完的包，如果有就不切换；如果没有，就切换。</p><p><img src="/images/2024/12/003.png" alt></p><h3 id="aRFS"><a href="#aRFS" class="headerlink" title="aRFS"></a>aRFS</h3><p>Accelerated RFS 之于 RFS 相当于 RSS 之于 RPS。Accelerated RFS 在硬件上就可以选择正确的队列，随后触发该数据包所属流所在的 CPU 的中断。由此可见，如果想要在硬件上实现队列选择，我们需要一个从流到硬件队列的对应关系。</p><p><strong>aRFS允许网卡在选择队列时，直接将数据包放入应用程序所在CPU对应的队列</strong>。</p><ol><li>维护规则: 内核自动维护socket五元组、socket应用程序所在的CPU和CPU对应的接收队列的映射关系<ul><li>socket五元组 -&gt; CPU(从流到 CPU 的映射关系，记录在 <code>rps_dev_flow</code> 表中)</li><li>CPU -&gt; RX queue(CPU 和硬件队列的关系，通过 <code>/proc/irq/&lt;irq_num&gt;/smp_affinity</code> 进行配置)</li></ul></li><li>下发规则: 内核根据上述映射关系，在用户态接收数据时，将五元组与CPU对应的规则下发到网卡 </li><li>匹配规则: 网卡接收到的所有报文会根据aRFS规则进行匹配并转发到相应的接收队列</li></ol><p><img src="/images/2024/12/004.png" alt></p><p>每当<code>rps_dev_flow</code>表中的条目被更新，网络协议栈就会调用驱动中的<code>ndo_rx_flow_steer</code>函数来更新流到硬件队列的对应关系。</p><p>Modern NICs support aRFS by (1) providing the OS with an API that allows it to associate networking flows with Rx queues, and by (2) steering incoming packets accordingly. When the OS migrates thread T away from  core C, the OS updates the NIC regarding thread T ’s new queue using the aRFS API. The actual update is delayed until the original queue is drained from packets of socket file descriptor S(flow在old core上的排空), to avoid out-of-order receives.</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>理解aRFS或许更为关键</li><li>aRFS允许网卡在选择队列时，直接将数据包放入应用程序所在CPU对应的队列</li></ul><hr><p>参考资料:</p><ol><li><a href="https://borispis.github.io/files/2020-ioctopus.pdf" target="_blank" rel="noopener">IOctopus: Outsmarting Nonuniform DMA</a></li><li><a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt" target="_blank" rel="noopener">Scaling in the Linux Networking Stack</a></li><li><a href="https://blog.csdn.net/dog250/article/details/80025959" target="_blank" rel="noopener">Linux RPS/RFS 实现原理浅析</a></li><li><a href="https://garycplin.blogspot.com/2017/06/linux-network-scaling-receives-packets.html" target="_blank" rel="noopener">Linux Network Scaling: Receiving Packets</a></li><li><a href="https://blog.csdn.net/weixin_45485072/article/details/133248630" target="_blank" rel="noopener">Understanding Host Network Stack Overheads论文阅读笔记</a></li><li><a href="https://blog.luckyoung.org/2023/23-02-13_network-parameters/" target="_blank" rel="noopener">网络参数 RSS、RPS、RFS、aRFS 学习总结</a></li><li><a href="https://arthurchiao.art/blog/linux-net-stack-implementation-rx-zh/#684-arfs-hardware-accelerated-rfs" target="_blank" rel="noopener">Linux 网络栈接收数据（RX）：原理及内核实现（2022）</a></li><li><a href="https://zhuanlan.zhihu.com/p/148756667" target="_blank" rel="noopener">Linux网络栈的性能缩放</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下network的RFS(Receive Flow Steering)和aRFS(Accelerated Receive Flow Steering)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
