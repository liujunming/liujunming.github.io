<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2022-03-29T12:21:10.541Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Introduction to Intel I/OAT</title>
    <link href="http://liujunming.github.io/2022/03/29/Introduction-to-Intel-I-OAT/"/>
    <id>http://liujunming.github.io/2022/03/29/Introduction-to-Intel-I-OAT/</id>
    <published>2022-03-29T05:34:04.000Z</published>
    <updated>2022-03-29T12:21:10.541Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel I/OAT(I/O Acceleration Technology)相关知识点。<a id="more"></a></p><h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>Intel I/OAT is actually a set of technologies that each contributes to increased performance.</p><p>The features of Intel I/OAT enhance data acceleration across the computing platform.</p><ul><li><strong>Intel® QuickData Technology</strong> enables data copy by the chipset instead of the CPU, to move data more efficiently through the server and provide fast, scalable, and reliable throughput.</li><li><strong>Direct Cache Access (DCA)</strong> allows a capable I/O device, such as a network controller, to place data directly into CPU cache, reducing cache misses and improving application response times.</li><li><strong>Extended Message Signaled Interrupts (MSI-X)</strong> distributes I/O interrupts to multiple CPUs and cores, for higher efficiency, better CPU utilization, and higher application performance.</li><li><strong>Receive Side Coalescing (RSC)</strong> aggregates packets from the same TCP/IP flow into one larger packet, reducing per-packet processing costs for faster TCP/IP processing.</li><li><strong>Low latency interrupts</strong> tune interrupt interval times depending on the latency sensitivity of the data, using criteria such as port number or packet size, for higher processing efficiency.</li></ul><p><img src="/images/2022/03/16.PNG" alt></p><p>本文只介绍QuickData Technology和DCA。</p><h3 id="2-Intel®-QuickData-Technology"><a href="#2-Intel®-QuickData-Technology" class="headerlink" title="2. Intel® QuickData Technology"></a>2. Intel® QuickData Technology</h3><p><img src="/images/2022/03/17.PNG" alt></p><p><img src="/images/2022/03/15.PNG" alt><br><img src="/images/2022/03/19.PNG" alt></p><p><img src="/images/2022/03/18.PNG" alt></p><p><a href="https://01.org/blogs/2019/introducing-intel-data-streaming-accelerator" target="_blank" rel="noopener">Intel® DSA</a> replaces the Intel® QuickData Technology.</p><h3 id="3-Direct-Cache-Access-DCA"><a href="#3-Direct-Cache-Access-DCA" class="headerlink" title="3. Direct Cache Access (DCA)"></a>3. Direct Cache Access (DCA)</h3><p><img src="/images/2022/03/13.PNG" alt><br><img src="/images/2022/03/14.PNG" alt></p><p><a href="/2022/03/28/Introduction-to-Intel-DDIO-technology/">Introduction to Intel DDIO technology</a></p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/io/i-o-acceleration-technology-paper.html" target="_blank" rel="noopener">White Paper</a></li><li><a href="https://www.intel.com/content/www/us/en/wireless-network/accel-technology.html" target="_blank" rel="noopener">Intel® I/O Acceleration Technology</a></li><li><a href="https://www.usenix.org/system/files/atc20-farshin.pdf" target="_blank" rel="noopener">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi hundred-gigabit Networks</a></li><li><a href="https://insujang.github.io/2021-04-26/using-intel-ioat-dma/" target="_blank" rel="noopener">Using Intel IOAT DMA</a></li><li><a href="https://www.snia.org/sites/default/files/SDC/2016/presentations/persistent_memory/Tanveer_Alam_Enterprise_Storage_RAS_Augmented_native_Intel_Platform_Storage_Extensions.pdf" target="_blank" rel="noopener">Tanveer_Alam_Enterprise_Storage_RAS_Augmented_native_Intel_Platform_Storage_Extensions.pdf</a></li><li><a href="https://landley.net/kdocs/ols/2005/ols2005v1-pages-289-296.pdf" target="_blank" rel="noopener">Accelerating Network Receive Processing</a></li><li><a href="https://0xffff.one/d/934" target="_blank" rel="noopener">关于intel的IOAT技术</a></li><li><a href="http://nowlab.cse.ohio-state.edu/static/media/publications/abstract/vaidyana-cluster07.pdf" target="_blank" rel="noopener">Efficient Asynchronous Memory Copy Operations on Multi-Core Systems and I/OAT</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel I/OAT(I/O Acceleration Technology)相关知识点。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Compute Express Link (CXL)</title>
    <link href="http://liujunming.github.io/2022/03/28/Introduction-to-Compute-Express-Link-CXL/"/>
    <id>http://liujunming.github.io/2022/03/28/Introduction-to-Compute-Express-Link-CXL/</id>
    <published>2022-03-28T11:15:28.000Z</published>
    <updated>2022-03-28T15:59:00.766Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Compute Express Link (CXL)相关知识点。<a id="more"></a></p><h3 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h3><p>Compute Express Link™ (CXL™) is an industry-supported <strong>Cache-Coherent Interconnect</strong> for <em>Processors</em>, <em>Memory Expansion</em> and <em>Accelerators</em>. CXL technology maintains <strong>memory coherency</strong> between the CPU memory space and memory on attached devices, which allows resource sharing for higher performance, reduced software stack complexity, and lower overall system cost. This permits users to simply focus on target workloads as opposed to the redundant memory management hardware in their accelerators. </p><p>CXL is designed to be an industry open standard interface for <strong>high-speed communications</strong>, as accelerators are increasingly used to complement CPUs in support of emerging applications such as Artificial Intelligence and Machine Learning.</p><h3 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h3><p>通俗地说，有了CXL，Host在访问Device Memory时，可以得到像访问本地Memory一样的体验；同样的，Device访问host Memory时，也像是在访问Device Memory一样。</p><p>我们已经有了PCIe这样的高速串行总线，为什么还要再搞出一个新的CXL呢 ？主要是因为<strong>PCIe不支持cache的一致性</strong>，这会导致每次Device去访问Host上的内存时，即便已经访问了多次而且内存也没有变化的情况下，都要重新访问，这样导致性能很差。另外因为人工智能和机器学习的兴起，FPGA/GPU 卡上会有大量的内存，在不进行运算时闲置的话，会造成资源浪费。而因为PCIe不支持Cache的一致性，Host访问设备上的内存也会非常的慢(CPU访问设备的内存是不cache的，意味着这次访问完而且设备内存也没有变化的情况下，下次还要重新访问。为什么不cache 呢?因为设备的内存不能汇报自己的改变)。所以Intel就发明了CXL，它在PCIe的基础上加入了<strong>Cache一致性</strong>的支持以用来<strong>提高设备和主机之间内存互相访问的速度</strong>。</p><h3 id="3-Components"><a href="#3-Components" class="headerlink" title="3. Components"></a>3. Components</h3><p><img src="/images/2022/03/11.PNG" alt><br>CXL在PCIe 5.0的基础上复用三种类型的协议:</p><ul><li>CXL.io: Provides discovery, configuration, register access, interrupts, etc.</li><li>CXL.cache: Provides the CXL device access to the processor memory</li><li>CXL.memory: Provides the Processor access to the CXL device attached memory</li></ul><p>Each of these are illustrated in the block below.<br><img src="/images/2022/03/09.webp" alt></p><h3 id="4-Usage"><a href="#4-Usage" class="headerlink" title="4. Usage"></a>4. Usage</h3><p><img src="/images/2022/03/10.PNG" alt><br>HBM(High Bandwidth Memory)</p><h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><p><img src="/images/2022/03/12.PNG" alt></p><p>In short, CXL is an open industry standard interconnect offering <strong>high-bandwidth</strong>, <strong>low-latency</strong> connectivity between the host processor and devices including accelerators, memory expansion, and smart I/O devices. CXL utilizes the PCI Express® (PCIe®) 5.0 physical layer infrastructure and the PCIe alternate protocol to address the demanding needs of high-performance computational workloads in Artificial Intelligence, Machine Learning, communication systems, and HPC through the enablement of <strong>coherency and memory semantics</strong> across heterogeneous processing and memory systems.</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/65435956" target="_blank" rel="noopener">基于PCIe 5.0的CXL是什么？</a></li><li><a href="https://zhuanlan.zhihu.com/p/383878879" target="_blank" rel="noopener">CXL简介</a></li><li><a href="http://www.360doc.com/content/20/0519/10/36367108_913231044.shtml" target="_blank" rel="noopener">强力科普PCIe/CXL</a></li><li><a href="https://www.computeexpresslink.org/about-cxl" target="_blank" rel="noopener">About CXL™</a></li><li><a href="https://www.computeexpresslink.org/post/introduction-to-compute-express-link-cxl-the-cpu-to-device-interconnect-breakthrough" target="_blank" rel="noopener">Introduction to Compute Express Link (CXL): The CPU-To-Device Interconnect Breakthrough</a></li><li><a href="https://docs.wixstatic.com/ugd/0c1418_27f700c09d4648c4bede5dec99a20824.pdf" target="_blank" rel="noopener">Compute Express Link® (CXL):A Coherent Interface for Ultra-High-Speed Transfers</a></li><li><a href="https://docs.wixstatic.com/ugd/0c1418_d9878707bbb7427786b70c3c91d5fbd1.pdf" target="_blank" rel="noopener">Compute Express Link</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Compute Express Link (CXL)相关知识点。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel DDIO technology</title>
    <link href="http://liujunming.github.io/2022/03/28/Introduction-to-Intel-DDIO-technology/"/>
    <id>http://liujunming.github.io/2022/03/28/Introduction-to-Intel-DDIO-technology/</id>
    <published>2022-03-28T01:29:57.000Z</published>
    <updated>2022-03-28T06:24:15.604Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel DDIO(Data Direct I/O) technology。<a id="more"></a></p><h2 id="1-What-is-Intel®-Data-Direct-I-O"><a href="#1-What-is-Intel®-Data-Direct-I-O" class="headerlink" title="1. What is  Intel® Data Direct I/O?"></a>1. What is  Intel® Data Direct I/O?</h2><p>Intel® Data Direct I/O (Intel® DDIO) is a feature to be introduced on the Intel® E5 Xeon® processors. Intel’s LAN Access Division (LAD) worked for the incorporation of Intel DDIO into the Xeon E5 processor because of its benefits for LAN I/O in terms of performance and system power consumption. With Intel DDIO, Intel’s Ethernet server NICs and controllers talk directly to the processor cache without a detour via system memory. Intel DDIO makes the processor cache the primary destination and source of I/O data rather than main memory. By avoiding system memory, Intel DDIO reduces latency, increases system I/O bandwidth, and reduces power consumption due to memory reads and writes.<br><img src="/images/2022/03/06.PNG" alt></p><h2 id="2-How-does-it-work"><a href="#2-How-does-it-work" class="headerlink" title="2. How does it work?"></a>2. How does it work?</h2><p>Read and Writes的视角是NIC。<br>详情可以阅读<a href="https://blog.csdn.net/qq_40500045/article/details/109272627" target="_blank" rel="noopener">谈谈DDIO你该知道的事</a>。</p><h3 id="2-1-NIC-Reads"><a href="#2-1-NIC-Reads" class="headerlink" title="2.1 NIC Reads"></a>2.1 NIC Reads</h3><p><img src="/images/2022/03/07.PNG" alt></p><h4 id="2-1-1-Without-DDIO"><a href="#2-1-1-Without-DDIO" class="headerlink" title="2.1.1 Without DDIO"></a>2.1.1 Without DDIO</h4><ol><li>处理器更新报文和控制结构体。由于分配的缓冲区在内存中， 因此会触发一次Cache不命中，处理器把内存读取到Cache中，然后更新控制结构体和报文信息。之后通知NIC来读取报文。</li><li>NIC收到有报文需要传递到网络上的通知后，它首先需要读取控制结构体进而知道从哪里获取报文。由于之前处理器刚把该缓冲区从内存读到Cache中并且做了更新，很有可能Cache还没有来得及把更新的内容写回到内存中。因此，当NIC发起一个对内存的读请求时，很有可能这个请求会发送到Cache系统中，Cache系统会把数据写回到内存中，然后内存控制器再把数据写到PCI总线上去。因此，一个读内存的操作会产生多次内存的读写。</li></ol><h4 id="2-1-2-With-DDIO"><a href="#2-1-2-With-DDIO" class="headerlink" title="2.1.2 With DDIO"></a>2.1.2 With DDIO</h4><ol><li>处理器更新报文和控制结构体。这个步骤和没有DDIO的技术类似，但是由于DDIO的引入，处理器会开始就把内存中的缓冲区和控制结构体预取到Cache，因此减少了内存读的时间。</li><li>NIC收到有报文需要传递到网络上的通知后，通过PCI总线把控制结构体和报文送到NIC内部。利用DDIO技术，I/O访问可以直接将Cache的内容送到PCI总线上。这样，就减少了Cache写回时等待的时间。</li></ol><p>由此可以看出，由于DDIO技术的引入，网卡的读操作减少了访问内存的次数，因而提高了访问效率，减少了报文转发的延迟。在理想状况下，NIC和处理器无需访问内存，直接通过访问Cache就可以完成更新数据，把数据送到NIC内部，进而送到网络上的所有操作。</p><h3 id="2-2-NIC-Writes"><a href="#2-2-NIC-Writes" class="headerlink" title="2.2 NIC Writes"></a>2.2 NIC Writes</h3><p><img src="/images/2022/03/08.PNG" alt></p><h4 id="2-2-1-Without-DDIO"><a href="#2-2-1-Without-DDIO" class="headerlink" title="2.2.1 Without DDIO"></a>2.2.1 Without DDIO</h4><ol><li>报文和控制结构体通过PCI总线送到指定的内存中。如果该内存恰好缓存在Cache中(有可能之前处理器有对该内存进行过读写操作)，则需要等待Cache把内容先写回到内存中，然后才能把报文和控制结构体写到内存中。</li><li>运行在处理器上的驱动程序或者软件得到通知收到新报文，去内存中读取控制结构体和相应的报文，Cache不命中。之所以Cache一定不会命中，是因为即使该内存地址在Cache中，在步骤1中也被强制写回到内存中。因此，只能从内存中读取控制结构体和报文。</li></ol><h4 id="2-2-2-With-DDIO"><a href="#2-2-2-With-DDIO" class="headerlink" title="2.2.2 With DDIO"></a>2.2.2 With DDIO</h4><p>这时，报文和控制结构体通过PCI总线直接送到Cache中。这时有两种情形:</p><ol><li>a) 如果该内存恰好缓存在Cache中(有可能之前处理器有对该内存进行过读写操作)，则直接在Cache中更新内容，覆盖原有内容。<br>b) 如果该内存没有缓存在Cache中，则在最后一级Cache中分配一块区域，并相应更新Cache表，表明该内容是对应于内存中的某个地址的。</li><li>运行在处理器上的驱动或者软件被通知到有报文到达，其产生一个内存读操作，由于该内容已经在Cache中，因此直接从Cache中读。</li></ol><p>由此可以看出，DDIO技术在处理器和外设之间交换数据时，减少了处理器和外设访问内存的次数，也减少了Cache写回的等待，提高了系统的吞吐率和数据的交换延迟。</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology-brief.html" target="_blank" rel="noopener">Intel® Data Direct I/O Technology (Intel® DDIO): A Primer</a></li><li><a href="https://www.intel.com/content/www/us/en/io/data-direct-i-o-faq.html" target="_blank" rel="noopener">Intel Data Direct I/O (Intel DDIO): Frequently Asked Questions</a></li><li><a href="https://blog.csdn.net/qq_40500045/article/details/109272627" target="_blank" rel="noopener">谈谈DDIO你该知道的事</a></li><li><a href="https://www.usenix.org/system/files/atc20-farshin.pdf" target="_blank" rel="noopener">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel DDIO(Data Direct I/O) technology。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>thread core module die package</title>
    <link href="http://liujunming.github.io/2022/03/16/thread-core-module-die-package/"/>
    <id>http://liujunming.github.io/2022/03/16/thread-core-module-die-package/</id>
    <published>2022-03-16T06:42:02.000Z</published>
    <updated>2022-03-16T11:58:04.885Z</updated>
    
    <content type="html"><![CDATA[<p>Terms: thread core module die package.<a id="more"></a><br><img src="/images/2022/03/03.PNG" alt></p><p>Per SDM Vol4:<br><img src="/images/2022/03/02.PNG" alt></p><h3 id="thread"><a href="#thread" class="headerlink" title="thread"></a>thread</h3><p>Individual cores can support multiple <em>hardware threads</em> of execution. These are also known as <em>logical processors</em>. This technique has multiple names, including <em>simultaneous multithreading</em> (SMT) and <em>Hyper-Threading Technology </em>(HT).</p><h3 id="core"><a href="#core" class="headerlink" title="core"></a>core</h3><p><strong>(Physical) processor core</strong> is an independent execution unit that can run one program thread at a time in parallel with other cores.</p><h3 id="module"><a href="#module" class="headerlink" title="module"></a>module</h3><p>Intel Atom processors also have the concept of CPU modules. In these processors, two cores share a large L2 cache. The modules interface with the CPU fabric rather than the cores interfacing directly.</p><h3 id="die"><a href="#die" class="headerlink" title="die"></a>die</h3><p><strong>Processor die</strong> is a single continuous piece of semiconductor material (usually silicon). A die can contain any number of cores. Up to 15 are available on the Intel product line. Processor die is where the transistors making up the CPU actually reside.</p><p><img src="/images/2022/03/05.jpg" alt><br>One die with multiple cores</p><h3 id="package"><a href="#package" class="headerlink" title="package"></a>package</h3><p><strong>Processor package</strong> is what you get when you buy a single processor. It contains one or more dies, plastic/ceramic housing for dies and gold-plated contacts that match those on your motherboard.</p><p><img src="/images/2022/03/04.jpg" alt><br>CPU Package containing 2 separate DIEs</p><hr><p>参考资料:</p><ol><li><a href="https://link.springer.com/chapter/10.1007/978-1-4302-6638-9_2" target="_blank" rel="noopener">CPU Power Management</a></li><li><a href="https://superuser.com/questions/324284/what-is-meant-by-the-terms-cpu-core-die-and-package" target="_blank" rel="noopener">What is meant by the terms CPU, Core, Die and Package?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Terms: thread core module die package.
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>How to setup proxy for docker</title>
    <link href="http://liujunming.github.io/2022/03/15/How-to-setup-proxy-for-docker/"/>
    <id>http://liujunming.github.io/2022/03/15/How-to-setup-proxy-for-docker/</id>
    <published>2022-03-15T06:02:49.000Z</published>
    <updated>2022-03-15T08:23:12.113Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍docker中的代理设置。<a id="more"></a></p><h3 id="1-Docker-daemon"><a href="#1-Docker-daemon" class="headerlink" title="1. Docker daemon"></a>1. Docker daemon</h3><p><a href="https://stackoverflow.com/questions/23111631/cannot-download-docker-images-behind-a-proxy" target="_blank" rel="noopener">Cannot download Docker images behind a proxy</a></p><p>In order to download container images from the outside world when running commands like <code>docker pull busybox</code> ,set the proxies by:</p><ul><li>Create directory</li></ul><p><code>mkdir /etc/systemd/system/docker.service.d</code></p><ul><li>Create file</li></ul><p><code>vi /etc/systemd/system/docker.service.d/http-proxy.conf</code></p><ul><li>File content</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=&quot;HTTP_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:80/&quot;</span><br></pre></td></tr></table></figure><p>If you have internal Docker registries that you need to contact without proxying you can specify them via the <code>NO_PROXY</code> environment variable:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Environment=&quot;HTTP_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;NO_PROXY=localhost,127.0.0.0/8,docker-registry.somecorporation.com&quot;</span><br></pre></td></tr></table></figure></p><ul><li>Flush changes</li></ul><p><code>systemctl daemon-reload</code></p><ul><li>Verify that the configuration has been loaded</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show --property Environment docker</span><br><span class="line">Environment=HTTP_PROXY=http://proxy.example.com:80/</span><br><span class="line">Environment=HTTPS_PROXY=http://proxy.example.com:80/</span><br></pre></td></tr></table></figure><ul><li>Restart docker</li></ul><p><code>systemctl restart docker</code></p><h3 id="2-Running-container"><a href="#2-Running-container" class="headerlink" title="2. Running container"></a>2. Running container</h3><p>In order for a running container to access the internet you need to fix the dns names since Google’s are the default and they don’t work. We need to update the dns names by using <a href="https://docs.docker.com/engine/reference/commandline/dockerd/" target="_blank" rel="noopener">daemon configuration file</a>.</p><ul><li>Create file</li></ul><p><code>/etc/docker/daemon.json</code></p><ul><li>File content</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;dns&quot;:[</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reload daemon</li></ul><p><code>systemctl daemon-reload</code></p><ul><li>Restart docker</li></ul><p><code>systemctl restart docker</code></p><ul><li>Now you can run a container and pass the proxies like this</li></ul><p><code>docker run --env http_proxy=http://proxy.example.com:80/ --env https_proxy=http://proxy.example.com:80/ -ti ubuntu bash</code></p><h3 id="3-Building-the-container"><a href="#3-Building-the-container" class="headerlink" title="3. Building the container"></a>3. Building the container</h3><ul><li>You can use ENV in the docker file:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">FROM ubuntu:16.04</span><br><span class="line"> </span><br><span class="line">ENV http_proxy http://proxy.example.com:80/</span><br><span class="line">ENV https_proxy http://proxy.example.com:80/</span><br><span class="line">...</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://elegantinfrastructure.com/docker/ultimate-guide-to-docker-http-proxy-configuration/" target="_blank" rel="noopener">Ultimate Guide to Docker HTTP Proxy Configuration</a></li><li><a href="https://docs.docker.com/network/proxy/" target="_blank" rel="noopener">Configure Docker to use a proxy server</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍docker中的代理设置。
    
    </summary>
    
      <category term="工具" scheme="http://liujunming.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to VT-x Page-Modification Logging</title>
    <link href="http://liujunming.github.io/2022/03/02/Introduction-to-VT-x-Page-Modification-Logging/"/>
    <id>http://liujunming.github.io/2022/03/02/Introduction-to-VT-x-Page-Modification-Logging/</id>
    <published>2022-03-02T07:41:04.000Z</published>
    <updated>2022-03-02T10:39:30.417Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍VT-x中的Page-Modification Logging(PML)技术。<a id="more"></a></p><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h3><p>在没有PML前，VMM要监控GPA的修改时，需要将EPT的页面结构设置为not-present或者read-only，这样会触发许多EPT violations,开销非常大。</p><p>PML建立在CPU对EPT中的accessed与dirty标志位支持上。<br>当启用PML时，对EPT中设置了dirty标志位的写操作都会产生一条in-memory记录，报告写操作的GPA，当记录写满时，触发一次VM Exit，然后VMM就可以监控被修改的页面。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h3><p>PML is a new feature on Intel’s Boardwell server platfrom targeted to reduce overhead of dirty logging mechanism.</p><p>The specification can be found at: <a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/page-modification-logging-vmm-white-paper.pdf" target="_blank" rel="noopener">Page Modification Logging for Virtual Machine Monitor White Paper</a></p><p>Currently, dirty logging is done by write protection, which write protects guest memory, and mark dirty GFN to dirty_bitmap in subsequent write fault. This works fine, except with overhead of additional write fault for logging each dirty GFN. The overhead can be large if the write operations from guest is intensive.</p><p>PML is a hardware-assisted efficient way for dirty logging. PML logs dirty GPA automatically to a 4K PML memory buffer <strong>when CPU changes EPT table’s D-bit from 0 to 1</strong>. To do this, A new 4K PML buffer base address, and a PML index were added to VMCS. Initially PML index is set to 512 (8 bytes for each GPA), and CPU decreases PML index after logging one GPA, and eventually a PML buffer full VMEXIT happens when PML buffer is fully logged.</p><p><img src="/images/2022/03/01.PNG" alt></p><p>With PML, we don’t have to use write protection so the intensive write fault EPT violation can be avoided, with an additional PML buffer full VMEXIT for 512 dirty GPAs. Theoretically, this can reduce hypervisor overhead when guest is in dirty logging mode, and therefore more CPU cycles can be allocated to guest, so it’s expected benchmarks in guest will have better performance comparing to non-PML.</p><h3 id="3-KVM-design"><a href="#3-KVM-design" class="headerlink" title="3. KVM design"></a>3. KVM design</h3><h4 id="3-1-Enable-Disable-PML"><a href="#3-1-Enable-Disable-PML" class="headerlink" title="3.1 Enable/Disable PML"></a>3.1 Enable/Disable PML</h4><p>PML is per-vcpu (per-VMCS), while EPT table can be shared by vcpus, so we need to enable/disable PML for all vcpus of guest. A dedicated 4K page will be allocated for each vcpu when PML is enabled for that vcpu.</p><p>Currently, we choose to always enable PML for guest, which means we enables PML when creating VCPU, and never disable it during guest’s life time. This avoids the complicated logic to enable PML by demand when guest is running. And to eliminate potential unnecessary GPA logging in non-dirty logging mode, we set D-bit manually for the slots with dirty logging disabled(<a href="https://lore.kernel.org/kvm/1422413668-3509-4-git-send-email-kai.huang@linux.intel.com/" target="_blank" rel="noopener">KVM: MMU: Explicitly set D-bit for writable spte.</a>).</p><h4 id="3-2-Flush-PML-buffer"><a href="#3-2-Flush-PML-buffer" class="headerlink" title="3.2 Flush PML buffer"></a>3.2 Flush PML buffer</h4><p>When userspace querys dirty_bitmap, it’s possible that there are GPAs logged in vcpu’s PML buffer, but as PML buffer is not full, so no VMEXIT happens. In this case, we’d better to manually flush PML buffer for all vcpus and update the dirty GPAs to dirty_bitmap.</p><p>We do PML buffer flush at the beginning of each VMEXIT, this makes dirty_bitmap more updated, and also makes logic of flushing PML buffer for all vcpus easier– we only need to kick all vcpus out of guest and PML buffer for each vcpu will be flushed automatically.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.</span></span><br><span class="line"><span class="comment"> * Called before reporting dirty_bitmap to userspace.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">kvm_flush_pml_buffers</span><span class="params">(struct kvm *kvm)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_vcpu</span> *<span class="title">vcpu</span>;</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * We only need to kick vcpu out of guest mode here, as PML buffer</span></span><br><span class="line"><span class="comment"> * is flushed at beginning of all VMEXITs, and it's obvious that only</span></span><br><span class="line"><span class="comment"> * vcpus running in guest are possible to have unflushed GPAs in PML</span></span><br><span class="line"><span class="comment"> * buffer.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">kvm_for_each_vcpu(i, vcpu, kvm)</span><br><span class="line">kvm_vcpu_kick(vcpu);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/page-modification-logging-vmm-white-paper.pdf" target="_blank" rel="noopener">Page Modification Logging for Virtual Machine Monitor White Paper</a></li><li><a href="https://lore.kernel.org/kvm/1422413668-3509-1-git-send-email-kai.huang@linux.intel.com/" target="_blank" rel="noopener">KVM: VMX: Page Modification Logging (PML) support</a></li><li><a href="https://diting0x.github.io/20170821/intel-pml/" target="_blank" rel="noopener">Intel VT 页面修改记录(PML)</a></li><li><a href="https://arxiv.org/pdf/2001.09991.pdf" target="_blank" rel="noopener">Intel Page Modification Logging, a hardware virtualization feature: study and improvement for virtual machine working set estimation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍VT-x中的Page-Modification Logging(PML)技术。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to umonitor/umwait/tpause instructions</title>
    <link href="http://liujunming.github.io/2022/03/01/Introduction-to-umonitor-umwait-tpause-instructions/"/>
    <id>http://liujunming.github.io/2022/03/01/Introduction-to-umonitor-umwait-tpause-instructions/</id>
    <published>2022-03-01T01:36:39.000Z</published>
    <updated>2022-03-01T10:42:53.651Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍<code>umonitor</code>、<code>umwait</code>和<code>tpause</code> 这三个指令的相关知识点。<a id="more"></a></p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>最权威的描述当然来自于SDM。</p><h4 id="1-1-UMONITOR—User-Level-Set-Up-Monitor-Address"><a href="#1-1-UMONITOR—User-Level-Set-Up-Monitor-Address" class="headerlink" title="1.1 UMONITOR—User Level Set Up Monitor Address"></a>1.1 UMONITOR—User Level Set Up Monitor Address</h4><p>The UMONITOR instruction arms address <strong>monitoring hardware</strong> using an address specified in the source register(the address range that the monitoring hardware checks for store operations can be determined by using the CPUID monitor leaf function, EAX=05H). A store to an address within the specified address range triggers the monitoring hardware. The state of monitor hardware is used by <strong>UMWAIT</strong>.</p><p>UMONITOR sets up an address range for the monitor hardware using the content of source register as an effective address and puts the monitor hardware in armed state. A store to the specified address range will <strong>trigger</strong> the monitor hardware.</p><h4 id="1-2-UMWAIT—User-Level-Monitor-Wait"><a href="#1-2-UMWAIT—User-Level-Monitor-Wait" class="headerlink" title="1.2 UMWAIT—User Level Monitor Wait"></a>1.2 UMWAIT—User Level Monitor Wait</h4><p>A hint that allows the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events.</p><p>a class of events:</p><ul><li>the monitoring hardware is triggered</li><li>when the time-stamp counter reaches or exceeds the implicit EDX:EAX 64-bit input value(if the monitoring hardware did not trigger beforehand)</li></ul><p>UMWAIT instructs the processor to enter an implementation-dependent optimized state while monitoring a range of addresses. The optimized state may be either a light-weight power/performance optimized state or an improved power/performance optimized state. The selection between the two states is governed by the explicit input register bit[0] source operand.</p><h4 id="1-3-Timed-PAUSE"><a href="#1-3-Timed-PAUSE" class="headerlink" title="1.3 Timed PAUSE"></a>1.3 Timed PAUSE</h4><p>Directs the processor to enter an implementation dependent optimized state until the TSC reaches the value in EDX:EAX.</p><p>TPAUSE instructs the processor to enter an implementation-dependent optimized state. There are two such optimized states to choose from: light-weight power/performance optimized state, and improved power/performance optimized state. The selection between the two is governed by the explicit input register bit[0] source operand.</p><h3 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2. Usage"></a>2. Usage</h3><h4 id="2-1-spin-lock"><a href="#2-1-spin-lock" class="headerlink" title="2.1 spin-lock"></a>2.1 spin-lock</h4><p>Today, if an application needs to wait for a very short duration they have to have spinloops. Spinloops consume more power and continue to use execution resources that could hurt its thread siblings in a core with hyperthreads(HT). New instructions <code>umonitor</code>, <code>umwait</code> and <code>tpause</code> allow a low power alternative waiting at the same time could improve the HT sibling perform while giving it any power headroom. These instructions can be used in both user space and kernel space.</p><p>A new MSR IA32_UMWAIT_CONTROL allows kernel to set a time limit(how long the <code>umwait</code> and <code>tpause</code> instructions can wait before normal execution continues) in TSC-quanta that prevents user applications from waiting for a long time.</p><p>The processor supports two levels of optimized states: a light-weight power/performance optimized state (C0.1 state) or an improved power/performance optimized state (C0.2 state with deeper power saving and higher exit latency). It is conceivable that system administrators might not want to allow the system to go into C0.2 if, for example, it is handling workloads with realtime response requirements. </p><h4 id="2-2-DPDK"><a href="#2-2-DPDK" class="headerlink" title="2.2 DPDK"></a>2.2 DPDK</h4><p><a href="https://lore.kernel.org/dpdk-dev/1639360.KZQzHLtdKU@thomas/T/#m1cfc1a8d08b92f52416ac81fc0559dfebf59f55d" target="_blank" rel="noopener">Power-optimized RX for Ethernet devices</a></p><p>This patchset proposes a simple API for Ethernet drivers to cause the CPU to enter a power-optimized state while waiting for packets to arrive, along with a set of(hopefully generic) intrinsics that facilitate that. This is achieved through cooperation with the NIC driver that will allow us to know address of the next NIC RX(Receive) ring packet descriptor, and wait for writes on it.</p><hr><p>参考资料:</p><ol><li><a href="https://lwn.net/Articles/790920/" target="_blank" rel="noopener">Short waits with umwait</a></li><li><a href="https://lore.kernel.org/lkml/1560994438-235698-1-git-send-email-fenghua.yu@intel.com/" target="_blank" rel="noopener">x86/umwait: Enable user wait instructions</a></li><li><a href="https://lore.kernel.org/lkml/20190716065551.27264-1-tao3.xu@intel.com/" target="_blank" rel="noopener">KVM: x86: Enable user wait instructions</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍&lt;code&gt;umonitor&lt;/code&gt;、&lt;code&gt;umwait&lt;/code&gt;和&lt;code&gt;tpause&lt;/code&gt; 这三个指令的相关知识点。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Time-Stamp Counter Virtualization</title>
    <link href="http://liujunming.github.io/2022/02/28/Time-Stamp-Counter-virtualization/"/>
    <id>http://liujunming.github.io/2022/02/28/Time-Stamp-Counter-virtualization/</id>
    <published>2022-02-28T06:56:25.000Z</published>
    <updated>2022-02-28T10:17:23.634Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录Time-Stamp Counter Virtualization相关内容。<a id="more"></a></p><h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><p><img src="/images/2022/02/28.PNG" alt></p><p>The product means <em>the result obtained by multiplying two or more quantities together</em>.</p><h3 id="TSC-offsetting"><a href="#TSC-offsetting" class="headerlink" title="TSC offsetting"></a>TSC offsetting</h3><p><strong>Timestamp-counter offsetting</strong> (<strong>TSC offsetting</strong>) is an existing feature that allows VMM software to specify a value (the <strong>TSC offset</strong>) that is added to the TSC when it is read by guest software. A VMM can use this feature to provide guest software with the illusion that it is operating at a time later or earlier than that represented by the current TSC value.</p><h3 id="TSC-scaling"><a href="#TSC-scaling" class="headerlink" title="TSC scaling"></a>TSC scaling</h3><p>With TSC offsetting, guest software perceives a TSC that is offset from the real hardware, but which advances at the same rate. That may be adequate for usages in which the offset is used to account for execution time before virtual machine was created. But it might not suffice if the VMM migrates a virtual machine between platforms on which the TSC moves at different rates.</p><p>TSC scaling provides VMM software with a mechanism by which is it can adjust the TSC rate perceived by guest software. When TSC scaling and TSC offsetting are both enabled, reads from the TSC in VMX nonroot operation multiply the actual TSC value by a new <strong>TSC multiplier</strong>, add the TSC offset to the product, and return the sum to guest software.</p><p>With both TSC offsetting and TSC scaling, a VMM that migrates a virtual machine from one platform to another can configure the TSC offset and the TSC multiplier on the new platform so that the TSC (as perceived by the guest) appears to proceed from the same value that it had before the migration <strong>and at the same rate</strong>.</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/timestamp-counter-scaling-virtualization-white-paper.pdf" target="_blank" rel="noopener">Timestamp-Counter Scaling (TSC scaling) for Virtualization</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录Time-Stamp Counter Virtualization相关内容。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
  </entry>
  
  <entry>
    <title>Dive into Time-Stamp Counter</title>
    <link href="http://liujunming.github.io/2022/02/28/Dive-into-Time-Stamp-Counter/"/>
    <id>http://liujunming.github.io/2022/02/28/Dive-into-Time-Stamp-Counter/</id>
    <published>2022-02-28T06:23:39.000Z</published>
    <updated>2022-02-28T10:09:54.798Z</updated>
    
    <content type="html"><![CDATA[<p>本文将深入研究Time-Stamp Counter。<a id="more"></a></p><p><img src="/images/2022/02/24.PNG" alt></p><p><img src="/images/2022/02/25.PNG" alt></p><p><img src="/images/2022/02/26.PNG" alt></p><p><img src="/images/2022/02/27.PNG" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将深入研究Time-Stamp Counter。
    
    </summary>
    
      <category term="Time" scheme="http://liujunming.github.io/categories/Time/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
  </entry>
  
  <entry>
    <title>What&#39;s serializing instruction in Intel terminology</title>
    <link href="http://liujunming.github.io/2022/02/28/What-s-serializing-instruction-in-Intel-terminology/"/>
    <id>http://liujunming.github.io/2022/02/28/What-s-serializing-instruction-in-Intel-terminology/</id>
    <published>2022-02-28T05:53:10.000Z</published>
    <updated>2022-02-28T10:09:54.799Z</updated>
    
    <content type="html"><![CDATA[<p>What’s serializing instruction in Intel terminology?<a id="more"></a></p><p><img src="/images/2022/02/23.PNG" alt></p><p>Serializing instructions <strong>force the processor to complete all modifications to flags, registers, and memory by previous instructions and to drain all buffered writes to memory before the next instruction is fetched and executed</strong>. For example, when a MOV to control register instruction is used to load a new value into control register CR0 to enable protected mode, the processor must perform a serializing operation before it enters protected mode. This serializing operation ensures that all operations that were started while the processor was in real-address mode are completed before the switch to protected mode is made.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;What’s serializing instruction in Intel terminology?
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to PKS</title>
    <link href="http://liujunming.github.io/2022/02/27/Introduction-to-PKS/"/>
    <id>http://liujunming.github.io/2022/02/27/Introduction-to-PKS/</id>
    <published>2022-02-27T05:54:21.000Z</published>
    <updated>2022-02-27T06:58:14.794Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍PKS(<strong>P</strong>rotection <strong>K</strong>eys for <strong>S</strong>upervisor Pages).<a id="more"></a></p><h3 id="1-Prerequisite"><a href="#1-Prerequisite" class="headerlink" title="1. Prerequisite"></a>1. Prerequisite</h3><p><a href="/2020/03/07/Introduction-to-pkeys/">Introduction to PKU</a></p><h3 id="2-SPEC"><a href="#2-SPEC" class="headerlink" title="2. SPEC"></a>2. SPEC</h3><p><img src="/images/2022/02/21.PNG" alt></p><p><img src="/images/2022/02/22.PNG" alt></p><h3 id="3-Description"><a href="#3-Description" class="headerlink" title="3. Description"></a>3. Description</h3><p>参见<a href="https://lpc.events/event/11/contributions/907/attachments/787/1699/lpc-2021-PKS-22-Sept-2021.pdf" target="_blank" rel="noopener">Protection Keys, Supervisor (PKS)</a>中的<strong>PKS Hardware Overview</strong>一节。</p><p>Protection Keys for Supervisor Pages(PKS) is a feature that extends the Protection Keys architecture to support thread-specific permission restrictions on supervisor pages.</p><p>PKS works similar to an existing feature named PKU(protecting user pages). They both perform an additional check after normal paging permission checks are done. Access or Writes can be disabled via a MSR update without TLB flushes when permissions changes. If violating this addional check, #PF occurs and PFEC.PK bit will be set.</p><p>PKS introduces MSR IA32_PKRS to manage supervisor protection key rights. The MSR contains 16 pairs of ADi and WDi bits. Each pair advertises on a group of pages with the same key which is set in the leaf paging-structure entries(bits[62:59]). Currently, IA32_PKRS is not supported by XSAVES architecture.</p><hr><p>参考资料:</p><ol><li><a href="https://lpc.events/event/11/contributions/907/attachments/787/1699/lpc-2021-PKS-22-Sept-2021.pdf" target="_blank" rel="noopener">Protection Keys, Supervisor (PKS)</a></li><li><a href="https://lore.kernel.org/lkml/20210505003032.489164-1-rick.p.edgecombe@intel.com/" target="_blank" rel="noopener">PKS write protected page tables</a></li><li><a href="https://lore.kernel.org/lkml/20220127175505.851391-1-ira.weiny@intel.com/" target="_blank" rel="noopener">PKS/PMEM: Add Stray Write Protection</a></li><li><a href="https://lore.kernel.org/lkml/20220221080840.7369-1-chenyi.qiang@intel.com/" target="_blank" rel="noopener">KVM: PKS Virtualization support</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍PKS(&lt;strong&gt;P&lt;/strong&gt;rotection &lt;strong&gt;K&lt;/strong&gt;eys for &lt;strong&gt;S&lt;/strong&gt;upervisor Pages).
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel VT-rp</title>
    <link href="http://liujunming.github.io/2022/02/16/Introduction-to-Intel-VT-rp/"/>
    <id>http://liujunming.github.io/2022/02/16/Introduction-to-Intel-VT-rp/</id>
    <published>2022-02-16T08:50:20.000Z</published>
    <updated>2022-02-16T13:01:30.095Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel的VT-rp技术。<a id="more"></a></p><h3 id="1-Material"><a href="#1-Material" class="headerlink" title="1. Material"></a>1. Material</h3><ul><li>SPEC:<a href="https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html" target="_blank" rel="noopener">ISE</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2020/34/kvm2020_hypervisor-managed%20linear%20address%20translation_v3.pdf" target="_blank" rel="noopener">Hypervisor-managed Linear Address Translation by Chao Gao slides</a></li><li><a href="https://www.youtube.com/watch?v=j2T90htYSko" target="_blank" rel="noopener">Hypervisor-managed Linear Address Translation by Chao Gao video</a></li></ul><h3 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h3><p><img src="/images/2022/02/08.png" alt><br>对于write-protecting CR3 page table leads to high performance penalty的解释如下：<br>VMMs could enforce the integrity of these specific guest linear to guest physical mappings (paging structures) by using legacy EPT permissions to mark the guest physical memory containing the relevant guest paging structures as read-only. The intent of marking these guest paging structures as read-only is to ensure an invalid mapping is not created by guest software. However, such page-table edit control techniques are known to cause very high overheads(EPT violation VM Exit)  due to the requirement that the VMM must monitor all paging contexts created by the (Guest) operating system.<br><img src="/images/2022/02/09.png" alt></p><h3 id="3-Terms"><a href="#3-Terms" class="headerlink" title="3. Terms"></a>3. Terms</h3><ul><li>Intel VT-rp(Intel Virtualization Technology - Redirect Protection)</li><li>HLAT(Hypervisor-managed Linear Address Translation)</li><li>PLR(Protected Linear Range)</li><li>PW(Paging Write)</li><li>VPW(Verify Paging-Write)</li></ul><h3 id="4-VT-rp"><a href="#4-VT-rp" class="headerlink" title="4. VT-rp"></a>4. VT-rp</h3><p><img src="/images/2022/02/10.png" alt></p><h4 id="4-1-HLAT"><a href="#4-1-HLAT" class="headerlink" title="4.1 HLAT"></a>4.1 HLAT</h4><p><img src="/images/2022/02/11.png" alt><br><img src="/images/2022/02/12.png" alt><br><img src="/images/2022/02/13.png" alt></p><h4 id="4-2-EPT-Control-Bit-“Paging-Write”"><a href="#4-2-EPT-Control-Bit-“Paging-Write”" class="headerlink" title="4.2 EPT Control Bit “Paging-Write”"></a>4.2 EPT Control Bit “Paging-Write”</h4><p>A new EPT control bit called <strong>“Paging-Write”</strong> specified in EPT leaf entries. </p><p><img src="/images/2022/02/16.png" alt></p><p><img src="/images/2022/02/14.png" alt></p><p>硬件会更新guest paging structure pages的A/D bits without EPT violation VM exits</p><h4 id="4-3-EPT-Control-Bit-“Verify-Paging-write”"><a href="#4-3-EPT-Control-Bit-“Verify-Paging-write”" class="headerlink" title="4.3 EPT Control Bit “Verify Paging-write”"></a>4.3 EPT Control Bit “Verify Paging-write”</h4><p>A new EPT control bit called <strong>“Verify Paging-Write”</strong> specified in EPT leaf entries (that refer to the final host physical page in the translation).</p><p><img src="/images/2022/02/17.png" alt></p><p><img src="/images/2022/02/15.png" alt></p><h4 id="4-4-Prevent-Alias-Mapping-with-PW-amp-VPW"><a href="#4-4-Prevent-Alias-Mapping-with-PW-amp-VPW" class="headerlink" title="4.4 Prevent Alias Mapping with PW &amp; VPW"></a>4.4 Prevent Alias Mapping with PW &amp; VPW</h4><p><img src="/images/2022/02/19.png" alt></p><h3 id="5-Implementaion"><a href="#5-Implementaion" class="headerlink" title="5. Implementaion"></a>5. Implementaion</h3><p><img src="/images/2022/02/18.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel的VT-rp技术。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to IPIv</title>
    <link href="http://liujunming.github.io/2022/02/15/Introduction-to-IPIv/"/>
    <id>http://liujunming.github.io/2022/02/15/Introduction-to-IPIv/</id>
    <published>2022-02-15T07:17:34.000Z</published>
    <updated>2022-02-16T13:01:30.095Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel的IPIv(IPI Virtualization)硬件技术。<a id="more"></a></p><h3 id="1-Previous-software-technology"><a href="#1-Previous-software-technology" class="headerlink" title="1. Previous software technology"></a>1. Previous software technology</h3><ul><li><a href="/2022/02/12/Introduction-to-PV-IPI/">PV IPI</a></li><li><a href="/2022/02/13/Introduction-to-Passthrough-IPI/">Passthrough IPI</a></li></ul><p>软件方案终究不够完美，还是需要Intel推出硬件方案来解决该问题。</p><h3 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h3><p>Currently, issuing an IPI except self-ipi in guest on Intel CPU always causes a VM-exit. It can lead to non-negligible overhead to some workloads involving frequent IPIs when running in VMs.</p><p>IPI virtualization is a new VT-x feature, targeting to eliminate VM-exits on source vCPUs <strong>when issuing unicast, physical-addressing IPIs</strong>. Once it is enabled, the processor virtualizes following kinds of operations that send IPIs without causing VM-exits:</p><ul><li>Memory-mapped ICR writes</li><li>MSR-mapped ICR writes</li><li>SENDUIPI execution</li></ul><h3 id="3-Spec"><a href="#3-Spec" class="headerlink" title="3. Spec"></a>3. Spec</h3><p>latest <a href="https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html" target="_blank" rel="noopener">Intel Architecture Instruction Set Extensions Programming Reference</a><br><img src="/images/2022/02/05.png" alt></p><p>Idea:</p><ul><li>The processor uses a data structure called the PID-pointer table. Each entry in the PID-pointer table contains the 64-bit physical address of a PID.</li><li>The processor indexes into a PID-pointer table using a virtual APIC ID<br><img src="/images/2022/02/06.png" alt></li></ul><p>不是所有类型的IPI都可以利用IPIv的，只有满足一定条件时，硬件的IPIv才能生效，否则，依然需要发生VM Exit。<br><img src="/images/2022/02/07.png" alt><br>这也是为什么 <a href="https://lore.kernel.org/kvm/20211231142849.611-1-guang.zeng@intel.com/" target="_blank" rel="noopener">IPI virtualization support for VM</a>的cover letter中有这样的描述:<em>when issuing unicast, physical-addressing IPIs</em>. 像SIPI/NMI/INIT等IPI就不能使用IPIv。</p><h3 id="4-Implementation"><a href="#4-Implementation" class="headerlink" title="4. Implementation"></a>4. Implementation</h3><p>mailing patch: <a href="https://lore.kernel.org/kvm/20211231142849.611-1-guang.zeng@intel.com/" target="_blank" rel="noopener">IPI virtualization support for VM</a><br>等以后IPIv feature upstream了再更新吧，说白了，patch的功能就是让硬件happy。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel的IPIv(IPI Virtualization)硬件技术。
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Passthrough IPI</title>
    <link href="http://liujunming.github.io/2022/02/13/Introduction-to-Passthrough-IPI/"/>
    <id>http://liujunming.github.io/2022/02/13/Introduction-to-Passthrough-IPI/</id>
    <published>2022-02-13T06:32:42.000Z</published>
    <updated>2022-02-15T09:42:45.804Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Passthrough IPI技术。<a id="more"></a></p><p>了解这项技术之前，需要读者对posted interrupt有深刻的理解。</p><h3 id="1-Idea"><a href="#1-Idea" class="headerlink" title="1. Idea"></a>1. Idea</h3><blockquote><p>Exposing the PI_DESC  and msr.icr to guest. When sending a IPI, set  the PIR of destination VCPU’s PI_DESC from guest directly and write the ICR with notification vector and destination PCPU which are got  from hypervisor.<br>This mechanism only handle the normal IPI. For SIPI/NMI/INIT, still  goes to legacy way but which write a new msr instead msr.icr.</p></blockquote><h3 id="2-Steps"><a href="#2-Steps" class="headerlink" title="2. Steps"></a>2. Steps</h3><p><img src="/images/2022/02/03.png" alt></p><p><img src="/images/2022/02/04.png" alt></p><h3 id="3-Pros-and-cons"><a href="#3-Pros-and-cons" class="headerlink" title="3. Pros and cons"></a>3. Pros and cons</h3><p>It can achieve huge performance improvement.</p><p>However it may increase the risk in the system since the guest could decide to send IPI to any processor. It’s OK in private cloud only.</p><h3 id="4-Implementation"><a href="#4-Implementation" class="headerlink" title="4. Implementation"></a>4. Implementation</h3><p>代码的解析可以参考<a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/09/10/kvm-performance-1" target="_blank" rel="noopener">kvm performance optimization technologies, part one</a>。</p><hr><p>参考资料:</p><ol><li><a href="https://lore.kernel.org/kvm/0C23CC2D-B770-43D0-8215-20CE591F2E8F@bytedance.com/" target="_blank" rel="noopener">KVM: X86: implement Passthrough IPI</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2020/1f/Minimizing%20VMExits%20in%20Private%20Cloud%20by%20%20Aggressive%20PV%20IPI%20%20and%20Passthrough%20Timer.pdf" target="_blank" rel="noopener">Minimizing VMExits in Private Cloud by Aggressive PV IPI and Passthrough Timer</a></li><li><a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/09/10/kvm-performance-1" target="_blank" rel="noopener">kvm performance optimization technologies, part one</a></li><li><a href="https://dl.acm.org/doi/abs/10.1145/3381052.3381317" target="_blank" rel="noopener">Directvisor: virtualization for bare-metal cloud</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Passthrough IPI技术。
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to PV IPI</title>
    <link href="http://liujunming.github.io/2022/02/12/Introduction-to-PV-IPI/"/>
    <id>http://liujunming.github.io/2022/02/12/Introduction-to-PV-IPI/</id>
    <published>2022-02-12T03:32:02.000Z</published>
    <updated>2022-02-15T09:42:45.803Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍PV IPI技术。<a id="more"></a>部分内容转载自:<a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/09/10/kvm-performance-1" target="_blank" rel="noopener">kvm performance optimization technologies, part one</a>。</p><h3 id="1-Idea"><a href="#1-Idea" class="headerlink" title="1. Idea"></a>1. Idea</h3><p><img src="/images/2022/02/02.png" alt></p><p>Instead of sending the IPI to vcpu one by one, the pv ipi send uses a bitmap to to record the IPI vcpu and then make a hypercall thus reduce the VM-exit. The patchset is <a href="https://lore.kernel.org/kvm/1532327996-17619-1-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">here</a>. </p><h3 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2. Usage"></a>2. Usage</h3><p><a href="https://www.kernel.org/doc/Documentation/virtual/kvm/hypercalls.txt" target="_blank" rel="noopener">Doc</a>:</p><pre><code>6. KVM_HC_SEND_IPI------------------------Architecture: x86Status: activePurpose: Send IPIs to multiple vCPUs.a0: lower part of the bitmap of destination APIC IDsa1: higher part of the bitmap of destination APIC IDsa2: the lowest APIC ID in bitmapa3: APIC ICRThe hypercall lets a guest send multicast IPIs, with at most 128128 destinations per hypercall in 64-bit mode and 64 vCPUs perhypercall in 32-bit mode.  The destinations are represented by abitmap contained in the first two arguments (a0 and a1). Bit 0 ofa0 corresponds to the APIC ID in the third argument (a2), bit 1corresponds to the APIC ID a2+1, and so on.Returns the number of CPUs to which the IPIs were delivered successfully.</code></pre><p>The test code in KVM unit test:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test_pv_ipi</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> a0 = <span class="number">0xFFFFFFFF</span>, a1 = <span class="number">0</span>, a2 = <span class="number">0xFFFFFFFF</span>, a3 = <span class="number">0x0</span>;</span><br><span class="line"></span><br><span class="line">    asm volatile("vmcall" : "=a"(ret) :"a"(KVM_HC_SEND_IPI), "b"(a0), "c"(a1), "d"(a2), "S"(a3));</span><br><span class="line">    report(!ret, <span class="string">"PV IPIs testing"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>a3</code>就是<code>kvm_pv_send_ipi</code>函数中的<code>icr</code>参数。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_pv_send_ipi</span><span class="params">(struct kvm *kvm, <span class="keyword">unsigned</span> <span class="keyword">long</span> ipi_bitmap_low,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">unsigned</span> <span class="keyword">long</span> ipi_bitmap_high, u32 min,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">unsigned</span> <span class="keyword">long</span> icr, <span class="keyword">int</span> op_64_bit)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> (icr &amp; (APIC_DEST_MASK | APIC_SHORT_MASK))</span><br><span class="line"><span class="keyword">return</span> -KVM_EINVAL;</span><br><span class="line"></span><br><span class="line">irq.<span class="built_in">vector</span> = icr &amp; APIC_VECTOR_MASK;</span><br><span class="line">irq.delivery_mode = icr &amp; APIC_MODE_MASK;</span><br><span class="line">irq.level = (icr &amp; APIC_INT_ASSERT) != <span class="number">0</span>;</span><br><span class="line">irq.trig_mode = icr &amp; APIC_INT_LEVELTRIG;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h3><p>源码基于Kernel v5.17.0-rc1。</p><h4 id="3-1-kvm-side"><a href="#3-1-kvm-side" class="headerlink" title="3.1 kvm side"></a>3.1 kvm side</h4><ul><li><p>Expose PV_SEND_IPI CPUID feature bit to guest<br><code>KVM_FEATURE_PV_SEND_IPI</code></p></li><li><p>Implement PV IPIs send hypercall<br><code>KVM_HC_SEND_IPI</code></p></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_pv_send_ipi</span><span class="params">(struct kvm *kvm, <span class="keyword">unsigned</span> <span class="keyword">long</span> ipi_bitmap_low,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">unsigned</span> <span class="keyword">long</span> ipi_bitmap_high, u32 min,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">unsigned</span> <span class="keyword">long</span> icr, <span class="keyword">int</span> op_64_bit)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_apic_map</span> *<span class="title">map</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_lapic_irq</span> <span class="title">irq</span> = &#123;</span><span class="number">0</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> cluster_size = op_64_bit ? <span class="number">64</span> : <span class="number">32</span>;</span><br><span class="line"><span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (icr &amp; (APIC_DEST_MASK | APIC_SHORT_MASK))</span><br><span class="line"><span class="keyword">return</span> -KVM_EINVAL;</span><br><span class="line"></span><br><span class="line">irq.<span class="built_in">vector</span> = icr &amp; APIC_VECTOR_MASK;</span><br><span class="line">irq.delivery_mode = icr &amp; APIC_MODE_MASK;</span><br><span class="line">irq.level = (icr &amp; APIC_INT_ASSERT) != <span class="number">0</span>;</span><br><span class="line">irq.trig_mode = icr &amp; APIC_INT_LEVELTRIG;</span><br><span class="line"></span><br><span class="line">rcu_read_lock();</span><br><span class="line"><span class="built_in">map</span> = rcu_dereference(kvm-&gt;arch.apic_map);</span><br><span class="line"></span><br><span class="line">count = -EOPNOTSUPP;</span><br><span class="line"><span class="keyword">if</span> (likely(<span class="built_in">map</span>)) &#123;</span><br><span class="line">count = __pv_send_ipi(&amp;ipi_bitmap_low, <span class="built_in">map</span>, &amp;irq, min);</span><br><span class="line">min += cluster_size;</span><br><span class="line">count += __pv_send_ipi(&amp;ipi_bitmap_high, <span class="built_in">map</span>, &amp;irq, min);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rcu_read_unlock();</span><br><span class="line"><span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __pv_send_ipi(<span class="keyword">unsigned</span> <span class="keyword">long</span> *ipi_bitmap, struct kvm_apic_map *<span class="built_in">map</span>,</span><br><span class="line"> struct kvm_lapic_irq *irq, u32 min)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> i, count = <span class="number">0</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_vcpu</span> *<span class="title">vcpu</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (min &gt; <span class="built_in">map</span>-&gt;max_apic_id)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">for_each_set_bit(i, ipi_bitmap,</span><br><span class="line">min((u32)BITS_PER_LONG, (<span class="built_in">map</span>-&gt;max_apic_id - min + <span class="number">1</span>))) &#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">map</span>-&gt;phys_map[min + i]) &#123;</span><br><span class="line">vcpu = <span class="built_in">map</span>-&gt;phys_map[min + i]-&gt;vcpu;</span><br><span class="line">count += kvm_apic_set_irq(vcpu, irq, <span class="literal">NULL</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-2-guest-side"><a href="#3-2-guest-side" class="headerlink" title="3.2 guest side"></a>3.2 guest side</h4><ul><li><p>Set the IPI entry points</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">kvm_setup_pv_ipi</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">apic-&gt;send_IPI_mask = kvm_send_ipi_mask;</span><br><span class="line">apic-&gt;send_IPI_mask_allbutself = kvm_send_ipi_mask_allbutself;</span><br><span class="line">pr_info(<span class="string">"setup PV IPIs\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Guest trigger IPI</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __send_ipi_mask(<span class="keyword">const</span> struct cpumask *mask, <span class="keyword">int</span> <span class="built_in">vector</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> flags;</span><br><span class="line"><span class="keyword">int</span> cpu, apic_id, icr;</span><br><span class="line"><span class="keyword">int</span> min = <span class="number">0</span>, max = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_X86_64</span></span><br><span class="line"><span class="keyword">__uint128_t</span> ipi_bitmap = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">u64 ipi_bitmap = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="keyword">long</span> ret;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (cpumask_empty(mask))</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">local_irq_save(flags);</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> (<span class="built_in">vector</span>) &#123;</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">icr = APIC_DM_FIXED | <span class="built_in">vector</span>;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> NMI_VECTOR:</span><br><span class="line">icr = APIC_DM_NMI;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">for_each_cpu(cpu, mask) &#123;</span><br><span class="line">apic_id = per_cpu(x86_cpu_to_apicid, cpu);</span><br><span class="line"><span class="keyword">if</span> (!ipi_bitmap) &#123;</span><br><span class="line">min = max = apic_id;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (apic_id &lt; min &amp;&amp; max - apic_id &lt; KVM_IPI_CLUSTER_SIZE) &#123;</span><br><span class="line">ipi_bitmap &lt;&lt;= min - apic_id;</span><br><span class="line">min = apic_id;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (apic_id &lt; min + KVM_IPI_CLUSTER_SIZE) &#123;</span><br><span class="line">max = apic_id &lt; max ? max : apic_id;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">ret = kvm_hypercall4(KVM_HC_SEND_IPI, (<span class="keyword">unsigned</span> <span class="keyword">long</span>)ipi_bitmap,</span><br><span class="line">(<span class="keyword">unsigned</span> <span class="keyword">long</span>)(ipi_bitmap &gt;&gt; BITS_PER_LONG), min, icr);</span><br><span class="line">WARN_ONCE(ret &lt; <span class="number">0</span>, <span class="string">"kvm-guest: failed to send PV IPI: %ld"</span>,</span><br><span class="line">  ret);</span><br><span class="line">min = max = apic_id;</span><br><span class="line">ipi_bitmap = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">__set_bit(apic_id - min, (<span class="keyword">unsigned</span> <span class="keyword">long</span> *)&amp;ipi_bitmap);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (ipi_bitmap) &#123;</span><br><span class="line">ret = kvm_hypercall4(KVM_HC_SEND_IPI, (<span class="keyword">unsigned</span> <span class="keyword">long</span>)ipi_bitmap,</span><br><span class="line">(<span class="keyword">unsigned</span> <span class="keyword">long</span>)(ipi_bitmap &gt;&gt; BITS_PER_LONG), min, icr);</span><br><span class="line">WARN_ONCE(ret &lt; <span class="number">0</span>, <span class="string">"kvm-guest: failed to send PV IPI: %ld"</span>,</span><br><span class="line">  ret);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">local_irq_restore(flags);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>It will set the bitmap accross the IPI target vcpu and finally call the <code>kvm_hypercall4(KVM_HC_SEND_IPI)</code>.</p><hr><p>参考资料:</p><ol><li><a href="https://lore.kernel.org/kvm/1532327996-17619-1-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">KVM: X86: Implement Exit-less IPIs support</a></li><li><a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/09/10/kvm-performance-1" target="_blank" rel="noopener">kvm performance optimization technologies, part one</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2019/e3/Boosting%20Dedicated%20Instances%20by%20KVM%20Tax%20Cut.pdf" target="_blank" rel="noopener">Boosting Dedicated InstanceviaKVMTaxCut</a></li><li><a href="https://zhuanlan.zhihu.com/p/372958922" target="_blank" rel="noopener">IOMMU(六)-post interrupt</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍PV IPI技术。
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to QEMU-KVM Live Migration</title>
    <link href="http://liujunming.github.io/2022/02/10/Introduction-to-QEMU-KVM-Live-Migration/"/>
    <id>http://liujunming.github.io/2022/02/10/Introduction-to-QEMU-KVM-Live-Migration/</id>
    <published>2022-02-10T07:28:03.000Z</published>
    <updated>2022-02-11T11:00:12.981Z</updated>
    
    <content type="html"><![CDATA[<p>Introduction to QEMU-KVM Live Migration.<a id="more"></a></p><p>本文只介绍<a href="https://en.wikipedia.org/wiki/Live_migration#Pre-copy_memory_migration" target="_blank" rel="noopener">Pre-copy memory migration</a>，大部分内容转载自:<a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/03/01/qemu-live-migration" target="_blank" rel="noopener">qemu热迁移简介</a>.</p><p>文中的代码解析基于QEMU 5.1.0。</p><h3 id="1-Usage"><a href="#1-Usage" class="headerlink" title="1. Usage"></a>1. Usage</h3><p><a href="/2021/12/04/The-usage-of-QEMU-KVM-live-migration/">The usage of QEMU&amp;&amp;KVM live migration</a></p><h3 id="2-基本原理"><a href="#2-基本原理" class="headerlink" title="2. 基本原理"></a>2. 基本原理</h3><p>推荐读下<a href="https://github.com/liujunming/paper_reading_notes/issues/9" target="_blank" rel="noopener">NSDI‘05 Live Migration of Virtual Machines</a>。</p><p><img src="/images/2022/02/01.png" alt></p><p>首先看看热迁移过程中qemu的哪些部分会包含进来。上图中间的灰色部分是虚拟机的内存，它对于qemu来说是黑盒，qemu不会做任何假设，而只是一股脑儿的发送到dst(destination host)。左边的区域是表示设备的状态，这部分是虚拟机可见的，qemu使用自己的协议来发送这部分。右边的是不会迁移的部分，但是还是需要dst和src(source host)保持一致，一般来说，src和dst的虚拟机使用相同的qemu command line能够保证这部分一致。</p><p>需要满足很多条件才能进行热迁:</p><ol><li>使用共享存储，如NFS</li><li>host的时间要一致</li><li>网络配置要一致，不能说src能访问某个网络，dst不能</li><li>host CPU类型要一致，毕竟host导出指令集给guest</li><li>虚拟机的机器类型，QEMU版本，rom版本等</li></ol><p>热迁移主要包括三个步骤：</p><ol><li>将虚拟机所有RAM pages设置成dirty，主要函数:<code>ram_save_setup</code></li><li>持续迭代将虚拟机的dirty pages发送到dst，直到达到一定条件，比如dirty pages数量比较少, 主要函数:<code>ram_save_iterate</code></li><li>停止src上面的guest，把剩下的dirty pages发送到dst，之后发送设备状态，主要函数: <code>qemu_savevm_state_complete_precopy</code></li></ol><p>其中步骤1和步骤2是上图中的灰色区域，步骤3是灰色和左边的区域。</p><p>之后就可以在dst上面继续运行qemu程序了。</p><h3 id="3-Algorithm"><a href="#3-Algorithm" class="headerlink" title="3. Algorithm"></a>3. Algorithm</h3><ol><li>Setup</li></ol><ul><li>Start guest on destination, connect, enable dirty page logging and more</li></ul><ol start="2"><li>Transfer Memory</li></ol><ul><li>Guest continues to run</li><li>Bandwidth limitation (controlled by the user)</li><li>First transfer the whole memory</li><li>Iteratively transfer all dirty pages (pages that were written to by the guest).</li></ul><ol start="3"><li>Stop the guest</li></ol><ul><li>And sync VM image(s) (guest’s hard drives).</li></ul><ol start="4"><li>Transfer State</li></ol><ul><li>As fast as possible (no bandwidth limitation)</li><li>All VM devices’ state and dirty pages yet to be transferred</li></ul><ol start="5"><li>Continue the guest</li></ol><ul><li>On destination upon success<ul><li>Broadcast “I’m over here” Ethernet packet to announce new location of NIC(s).</li></ul></li><li>On source upon failure (with one exception).</li></ul><h3 id="4-发送端源码分析"><a href="#4-发送端源码分析" class="headerlink" title="4. 发送端源码分析"></a>4. 发送端源码分析</h3><p>在qemu的monitor输入migrate命令后，经过的一些函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hmp_migrate</span><br><span class="line">qmp_migrate</span><br><span class="line">tcp_start_outgoing_migration</span><br><span class="line">socket_start_outgoing_migration</span><br><span class="line">socket_outgoing_migration</span><br><span class="line">migration_channel_connect</span><br><span class="line">qemu_fopen_channel_output</span><br><span class="line">migrate_fd_connect</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">migrate_fd_connect</span><span class="params">(MigrationState *s, Error *error_in)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    qemu_thread_create(&amp;s-&gt;thread, <span class="string">"live_migration"</span>, migration_thread, s,</span><br><span class="line">                       QEMU_THREAD_JOINABLE);</span><br><span class="line">    s-&gt;migration_thread_running = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>migrate_fd_connect</code>函数创建了一个迁移线程，线程函数为<code>migration_thread</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">migration_thread</span><br><span class="line">qemu_savevm_state_setup</span><br><span class="line">ram_save_setup[save_setup]</span><br><span class="line">ram_init_all</span><br><span class="line">ram_init_bitmaps</span><br><span class="line">ram_list_init_bitmaps</span><br><span class="line">bitmap_new</span><br><span class="line">bitmap_set</span><br><span class="line">migration_iteration_run</span><br><span class="line">qemu_savevm_state_pending</span><br><span class="line">ram_save_pending[save_live_pending]</span><br><span class="line">qemu_savevm_state_iterate</span><br><span class="line">ram_save_iterate[save_live_iterate]</span><br><span class="line">ram_find_and_save_block</span><br><span class="line">migration_completion</span><br><span class="line">vm_stop_force_state</span><br><span class="line">qemu_savevm_state_complete_precopy</span><br><span class="line">qemu_savevm_state_complete_precopy_iterable</span><br><span class="line">ram_save_complete[save_live_complete_precopy]</span><br><span class="line">ram_find_and_save_block</span><br></pre></td></tr></table></figure><p><code>migration_thread</code>主要就是用来完成之前提到的热迁移的三个步骤。<br>首先来看第一个步骤，<code>qemu_savevm_state_setup</code>标记所有RAM pages为dirty。</p><p>接着看第二个步骤，由while循环中的两个函数完成: <code>qemu_savevm_state_pending</code>和<code>qemu_savevm_state_iterate</code>。</p><p>第一个函数通过调用回调函数<code>ram_save_pending</code>确定还要传输的字节数，比较简单。 第二个函数通过调用回调函数<code>ram_save_iterate</code>用来把dirty  pages传到dst上面。</p><p><code>ram_find_and_save_block</code>–&gt;<code>find_dirty_block</code>–&gt;<code>ram_save_host_page</code>–&gt;<code>migration_bitmap_clear_dirty</code>–&gt;<code>ram_save_target_page</code>–&gt;<code>ram_save_page</code>–&gt;<code>save_normal_page</code>-&gt;<code>qemu_put_buffer_async</code> –&gt;…-&gt;<code>qemu_fflush</code> –&gt;…-&gt;<code>send</code></p><p>在while循环中反复调用<code>ram_save_pending</code>和<code>ram_save_iterate</code>不停向dst发送虚拟机脏页，直到达到一定的条件，然后进入第三个步骤。</p><p>第三个步骤就是调用<code>migration_completion</code>，在这一步中会停止src虚拟机，然后把最后剩的一点脏页拷贝到dst去。</p><h3 id="5-接收端源码分析"><a href="#5-接收端源码分析" class="headerlink" title="5. 接收端源码分析"></a>5. 接收端源码分析</h3><p>接收端的qemu运行参数跟发送端的一样，但是多了一个参数<code>-incoming tcp:0:6666</code>, qemu在解析到<code>-incoming</code>后，就会等待src迁移过来，我们来看看这个流程。</p><p><code>main</code> –&gt;<code>qemu_init</code> –&gt;<code>qemu_start_incoming_migration</code> –&gt;<code>tcp_start_incoming_migration</code> –&gt;<code>socket_start_incoming_migration</code> –&gt;<code>socket_accept_incoming_migration</code> –&gt;<code>migration_channel_process_incoming</code> -&gt;<code>migration_ioc_process_incoming</code> -&gt;<code>migration_incoming_process</code> -&gt;<code>process_incoming_migration_co</code> -&gt;<code>qemu_loadvm_state</code> -&gt;<code>qemu_loadvm_state_main</code></p><p><code>process_incoming_migration_co</code>函数用来完成数据接收，恢复虚拟机的运行。最重要的是<code>qemu_loadvm_state</code>，用于接收数据，在dst重构虚拟机。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">qemu_loadvm_state</span><span class="params">(QEMUFile *f)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    ret = qemu_loadvm_state_main(f, mis);</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>显然，<code>qemu_loadvm_state_main</code>是构建虚拟机的主要函数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">qemu_loadvm_state_main</span><span class="params">(QEMUFile *f, MigrationIncomingState *mis)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">uint8_t</span> section_type;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">retry:</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        section_type = qemu_get_byte(f);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (qemu_file_get_error(f)) &#123;</span><br><span class="line">            ret = qemu_file_get_error(f);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        trace_qemu_loadvm_state_section(section_type);</span><br><span class="line">        <span class="keyword">switch</span> (section_type) &#123;</span><br><span class="line">        <span class="keyword">case</span> QEMU_VM_SECTION_START:</span><br><span class="line">        <span class="keyword">case</span> QEMU_VM_SECTION_FULL:</span><br><span class="line">            ret = qemu_loadvm_section_start_full(f, mis);</span><br><span class="line">            <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">goto</span> out;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> QEMU_VM_SECTION_PART:</span><br><span class="line">        <span class="keyword">case</span> QEMU_VM_SECTION_END:</span><br><span class="line">            ret = qemu_loadvm_section_part_end(f, mis);</span><br><span class="line">            <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">goto</span> out;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> QEMU_VM_COMMAND:</span><br><span class="line">            ret = loadvm_process_command(f);</span><br><span class="line">            trace_qemu_loadvm_state_section_command(ret);</span><br><span class="line">            <span class="keyword">if</span> ((ret &lt; <span class="number">0</span>) || (ret == LOADVM_QUIT)) &#123;</span><br><span class="line">                <span class="keyword">goto</span> out;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> QEMU_VM_EOF:</span><br><span class="line">            <span class="comment">/* This is the end of migration */</span></span><br><span class="line">            <span class="keyword">goto</span> out;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            error_report(<span class="string">"Unknown savevm section type %d"</span>, section_type);</span><br><span class="line">            ret = -EINVAL;</span><br><span class="line">            <span class="keyword">goto</span> out;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        qemu_file_set_error(f, ret);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Cancel bitmaps incoming regardless of recovery */</span></span><br><span class="line">        dirty_bitmap_mig_cancel_incoming();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * If we are during an active postcopy, then we pause instead</span></span><br><span class="line"><span class="comment">         * of bail out to at least keep the VM's dirty data.  Note</span></span><br><span class="line"><span class="comment">         * that POSTCOPY_INCOMING_LISTENING stage is still not enough,</span></span><br><span class="line"><span class="comment">         * during which we're still receiving device states and we</span></span><br><span class="line"><span class="comment">         * still haven't yet started the VM on destination.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * Only RAM postcopy supports recovery. Still, if RAM postcopy is</span></span><br><span class="line"><span class="comment">         * enabled, canceled bitmaps postcopy will not affect RAM postcopy</span></span><br><span class="line"><span class="comment">         * recovering.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">if</span> (postcopy_state_get() == POSTCOPY_INCOMING_RUNNING &amp;&amp;</span><br><span class="line">            migrate_postcopy_ram() &amp;&amp; postcopy_pause_incoming(mis)) &#123;</span><br><span class="line">            <span class="comment">/* Reset f to point to the newly created channel */</span></span><br><span class="line">            f = mis-&gt;from_src_file;</span><br><span class="line">            <span class="keyword">goto</span> retry;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>qemu_loadvm_state_main</code>分别处理各个section, src会把<code>QEMU_VM_SECTION_START</code>等标志放到流中。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">qemu_loadvm_section_start_full</span><br><span class="line">find_se</span><br><span class="line">vmstate_load</span><br><span class="line">ram_load[load_state]</span><br><span class="line">ram_load_precopy</span><br><span class="line">qemu_get_buffer</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p><code>ram_load</code>负责把接收到的数据拷贝到dst这端虚拟机的内存上。 </p><h3 id="6-MISC"><a href="#6-MISC" class="headerlink" title="6. MISC"></a>6. MISC</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">SaveVMHandlers</span> &#123;</span></span><br><span class="line">    <span class="comment">/* This runs inside the iothread lock.  */</span></span><br><span class="line">    SaveStateHandler *save_state;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> (*save_cleanup)(<span class="keyword">void</span> *opaque);</span><br><span class="line">    <span class="keyword">int</span> (*save_live_complete_postcopy)(QEMUFile *f, <span class="keyword">void</span> *opaque);</span><br><span class="line">    <span class="keyword">int</span> (*save_live_complete_precopy)(QEMUFile *f, <span class="keyword">void</span> *opaque);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* This runs both outside and inside the iothread lock.  */</span></span><br><span class="line">    <span class="keyword">bool</span> (*is_active)(<span class="keyword">void</span> *opaque);</span><br><span class="line">    <span class="keyword">bool</span> (*has_postcopy)(<span class="keyword">void</span> *opaque);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* is_active_iterate</span></span><br><span class="line"><span class="comment">     * If it is not NULL then qemu_savevm_state_iterate will skip iteration if</span></span><br><span class="line"><span class="comment">     * it returns false. For example, it is needed for only-postcopy-states,</span></span><br><span class="line"><span class="comment">     * which needs to be handled by qemu_savevm_state_setup and</span></span><br><span class="line"><span class="comment">     * qemu_savevm_state_pending, but do not need iterations until not in</span></span><br><span class="line"><span class="comment">     * postcopy stage.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">bool</span> (*is_active_iterate)(<span class="keyword">void</span> *opaque);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* This runs outside the iothread lock in the migration case, and</span></span><br><span class="line"><span class="comment">     * within the lock in the savevm case.  The callback had better only</span></span><br><span class="line"><span class="comment">     * use data that is local to the migration thread or protected</span></span><br><span class="line"><span class="comment">     * by other locks.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">int</span> (*save_live_iterate)(QEMUFile *f, <span class="keyword">void</span> *opaque);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* This runs outside the iothread lock!  */</span></span><br><span class="line">    <span class="keyword">int</span> (*save_setup)(QEMUFile *f, <span class="keyword">void</span> *opaque);</span><br><span class="line">    <span class="keyword">void</span> (*save_live_pending)(QEMUFile *f, <span class="keyword">void</span> *opaque,</span><br><span class="line">                              <span class="keyword">uint64_t</span> threshold_size,</span><br><span class="line">                              <span class="keyword">uint64_t</span> *res_precopy_only,</span><br><span class="line">                              <span class="keyword">uint64_t</span> *res_compatible,</span><br><span class="line">                              <span class="keyword">uint64_t</span> *res_postcopy_only);</span><br><span class="line">    <span class="comment">/* Note for save_live_pending:</span></span><br><span class="line"><span class="comment">     * - res_precopy_only is for data which must be migrated in precopy phase</span></span><br><span class="line"><span class="comment">     *     or in stopped state, in other words - before target vm start</span></span><br><span class="line"><span class="comment">     * - res_compatible is for data which may be migrated in any phase</span></span><br><span class="line"><span class="comment">     * - res_postcopy_only is for data which must be migrated in postcopy phase</span></span><br><span class="line"><span class="comment">     *     or in stopped state, in other words - after source vm stop</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * Sum of res_postcopy_only, res_compatible and res_postcopy_only is the</span></span><br><span class="line"><span class="comment">     * whole amount of pending data.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    LoadStateHandler *load_state;</span><br><span class="line">    <span class="keyword">int</span> (*load_setup)(QEMUFile *f, <span class="keyword">void</span> *opaque);</span><br><span class="line">    <span class="keyword">int</span> (*load_cleanup)(<span class="keyword">void</span> *opaque);</span><br><span class="line">    <span class="comment">/* Called when postcopy migration wants to resume from failure */</span></span><br><span class="line">    <span class="keyword">int</span> (*resume_prepare)(MigrationState *s, <span class="keyword">void</span> *opaque);</span><br><span class="line">&#125; SaveVMHandlers;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> SaveVMHandlers savevm_ram_handlers = &#123;</span><br><span class="line">    .save_setup = ram_save_setup,</span><br><span class="line">    .save_live_iterate = ram_save_iterate,</span><br><span class="line">    .save_live_complete_postcopy = ram_save_complete,</span><br><span class="line">    .save_live_complete_precopy = ram_save_complete,</span><br><span class="line">    .has_postcopy = ram_has_postcopy,</span><br><span class="line">    .save_live_pending = ram_save_pending,</span><br><span class="line">    .load_state = ram_load,</span><br><span class="line">    .save_cleanup = ram_save_cleanup,</span><br><span class="line">    .load_setup = ram_load_setup,</span><br><span class="line">    .load_cleanup = ram_load_cleanup,</span><br><span class="line">    .resume_prepare = ram_resume_prepare,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>以这些callback函数为接口来研究Live Migration，也是学习源码的一个极佳途径，能掌握全局。</p><hr><p>参考资料:</p><ol><li><a href="https://www.linux-kvm.org/page/Migration" target="_blank" rel="noopener">Migration | KVM Docs</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Introduction to QEMU-KVM Live Migration.
    
    </summary>
    
      <category term="live migration" scheme="http://liujunming.github.io/categories/live-migration/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
  </entry>
  
  <entry>
    <title>GDB Conditional Breakpoints</title>
    <link href="http://liujunming.github.io/2022/02/08/GDB-Conditional-Breakpoints/"/>
    <id>http://liujunming.github.io/2022/02/08/GDB-Conditional-Breakpoints/</id>
    <published>2022-02-08T06:01:15.000Z</published>
    <updated>2022-02-08T10:00:14.938Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自<a href="https://www.fayewilliams.com/2011/07/13/gdb-conditional-breakpoints/" target="_blank" rel="noopener">GDB Conditional Breakpoints</a>。<a id="more"></a></p><p>What if you have a large loop running and you only want to look at what’s happening as it nears the end? Do you really have to step through 99 iterations in a 100 item loop?</p><p>Of course you don’t. Step forward the conditional breakpoint.</p><h3 id="Set-a-conditional-breakpoint-using-a-condition"><a href="#Set-a-conditional-breakpoint-using-a-condition" class="headerlink" title="Set a conditional breakpoint using a condition"></a>Set a conditional breakpoint using a condition</h3><p>In GDB you can specify a condition in the programming language you are debugging and apply it to any breakpoint. Let’s stop a loop at the 99th iteration (I’m debugging C/C++, so my conditions are written in C/C++):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(gdb) b Message.cpp:112 if i == 99</span><br></pre></td></tr></table></figure><p>That’s all there is to it.</p><p>To ensure gdb stops execution, use <em>the first line of code inside the loop</em> as the stopping point, not the loop itself.</p><p>You can also specify a condition on an existing breakpoint by using the breakpoint number as a reference:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(gdb) cond 3 i == 99</span><br></pre></td></tr></table></figure><p>And remove a condition from a breakpoint using:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) cond 3</span><br><span class="line">Breakpoint 3 now unconditional.</span><br></pre></td></tr></table></figure><h3 id="That’s-great-What-else-can-you-do"><a href="#That’s-great-What-else-can-you-do" class="headerlink" title="That’s great! What else can you do?"></a>That’s great! What else can you do?</h3><p>Pretty much anything you like! Just write the condition exactly as if you were testing for it in your code, e.g.:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(gdb) cond 1 strcmp(message,&quot;earthquake&quot;) == 0</span><br><span class="line">//stop if the array message is equal to &apos;earthquake&apos;</span><br><span class="line"></span><br><span class="line">(gdb) cond 2 *p == &apos;r&apos;</span><br><span class="line">//stop if the char* pointer p points to the letter &apos;r&apos;</span><br><span class="line"></span><br><span class="line">(gdb) cond 3 num &lt; 0.75</span><br><span class="line">//stop while the float num is less than 0.75</span><br></pre></td></tr></table></figure><p>Conditional breakpoints covered and your efficiency increased, all in less than 5 minutes!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自&lt;a href=&quot;https://www.fayewilliams.com/2011/07/13/gdb-conditional-breakpoints/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GDB Conditional Breakpoints&lt;/a&gt;。
    
    </summary>
    
      <category term="debug" scheme="http://liujunming.github.io/categories/debug/"/>
    
    
      <category term="debug" scheme="http://liujunming.github.io/tags/debug/"/>
    
  </entry>
  
  <entry>
    <title>Interrupt and Interrupt Virtualization</title>
    <link href="http://liujunming.github.io/2022/02/06/Interrupt-and-Interrupt-Virtualization/"/>
    <id>http://liujunming.github.io/2022/02/06/Interrupt-and-Interrupt-Virtualization/</id>
    <published>2022-02-06T11:08:31.000Z</published>
    <updated>2022-02-06T11:28:46.357Z</updated>
    
    <content type="html"><![CDATA[<p>个人在学习中断及其虚拟化过程中的笔记。<a id="more"></a></p><p><a href="/pdf/Interrupt_and_interrupt_virtualization.pdf">Interrupt and Interrupt Virtualization</a>。若有疑问，可邮箱联系作者：<a href="mailto:ljm0910@mail.ustc.edu.cn" target="_blank" rel="noopener">ljm0910@mail.ustc.edu.cn</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;个人在学习中断及其虚拟化过程中的笔记。
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>Linux kernel uapi header file</title>
    <link href="http://liujunming.github.io/2022/02/05/Linux-kernel-uapi-header-file/"/>
    <id>http://liujunming.github.io/2022/02/05/Linux-kernel-uapi-header-file/</id>
    <published>2022-02-05T02:04:08.000Z</published>
    <updated>2022-02-05T07:13:39.423Z</updated>
    
    <content type="html"><![CDATA[<p>What’s in include/uapi of kernel source project?<a id="more"></a></p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>The <code>uapi</code> folder is supposed to contain the user space API of the kernel. Then upon kernel installation, the <code>uapi</code> include files become the top level <code>/usr/include/linux/</code> files. </p><p>The other headers in theory are then private to the kernel. This allow clean separation of the user-visible and kernel-only structures which previously were intermingled in a single header file.</p><blockquote><p>the “uapi” include file cleanups. The idea is that the stuff exported to user space should now be found under include/uapi and arch/$(ARCH)/include/uapi.</p></blockquote><h3 id="2-Where"><a href="#2-Where" class="headerlink" title="2. Where"></a>2. Where</h3><p><a href="https://elixir.bootlin.com/linux/v5.17-rc1/source/include/uapi" target="_blank" rel="noopener">include/uapi</a> corresponding to <code>/user/include/linux/</code> files.</p><p><a href="https://elixir.bootlin.com/linux/v5.17-rc1/source/arch/x86/include/uapi/asm" target="_blank" rel="noopener">arch/x86/include/uapi/asm</a> corresponding to <code>/usr/include/x86_64-linux-gnu/asm</code> files.</p><h3 id="3-What"><a href="#3-What" class="headerlink" title="3. What"></a>3. What</h3><p>Split out the user-space API content of the kernel header files in the <code>include</code> and <code>arch/xxxxxx/include</code> directories, placing that content into corresponding headers created in new uapi/ subdirectories that reside under each of the original directories. </p><h3 id="4-Benefit"><a href="#4-Benefit" class="headerlink" title="4. Benefit"></a>4. Benefit</h3><ul><li>simplifies and reduces the size of the kernel-only headers</li><li>simplifies the complex interdependencies between headers that are [currently] partly exported to userspace</li><li>There is one other benefit of the UAPI split that may be of particular interest to the wider Linux ecosystem. By placing all of the user-space API-related definitions into files dedicated solely to that task, it becomes easier to track changes to the APIs that the kernel presents to user space. In the first instance, these changes can be discovered by scanning the git logs for changes in files under the <code>uapi/</code> subdirectories. Easing the task of tracking user-space APIs would help many other parts of the ecosystem, for example, C library maintainers, scripting language projects that maintain language bindings for the user-space API, testing projects such as <a href="http://ltp.sourceforge.net/" target="_blank" rel="noopener">LTP</a>, documentation projects such as <a href="http://www.kernel.org/doc/man-pages/" target="_blank" rel="noopener">man-pages</a>, and perhaps even LWN editors preparing summaries of changes in the merge window that occurs at the start of each kernel release cycle.</li></ul><h3 id="5-How"><a href="#5-How" class="headerlink" title="5. How"></a>5. How</h3><p>The task of disintegrating each of the header files into two pieces is in principle straightforward. In the general case, each header file has the following form:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Header comments (copyright, etc.) */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> _XXXXXX_H     <span class="comment">/* Guard macro preventing double inclusion */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _XXXXXX_H</span></span><br><span class="line"></span><br><span class="line">[User-space definitions]</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> __KERNEL__</span></span><br><span class="line"></span><br><span class="line">[Kernel-space definitions]</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">/* __KERNEL__ */</span></span></span><br><span class="line"></span><br><span class="line">[User-space definitions]</span><br><span class="line">  </span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">/* End prevent double inclusion */</span></span></span><br></pre></td></tr></table></figure><p>Each of the above parts may or may not be present in individual header files, and there may be multiple blocks governed by <code>#ifdef __KERNEL__</code> preprocessor directives.</p><p>The part of this file that is of most interest is the code that falls inside the outermost <code>#ifndef</code> block that prevents double inclusion of the header file. <em>[User-space definitions]</em> should move to the corresponding <code>uapi/</code> header file. The content inside the <code>#ifdef __KERNEL__</code> block remains in the original header file, but the <code>#ifdef __KERNEL__</code> and its accompanying <code>#endif</code> are removed.</p><p>A copy of the header comments remains in the original header file, and is duplicated in the new <code>uapi/</code> header file. In addition, a <code>#include</code> directive needs to be added to the original header file so that it includes the new <code>uapi/</code> header file, and of course a suitable <code>git commit</code> message needs to be supplied for the change.</p><p>The goal is to modify the original header file to look like this:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Header comments (copyright, etc.) */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> _XXXXXX_H     <span class="comment">/* Guard macro preventing double inclusion */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _XXXXXX_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;include/uapi/path/to/header.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">[Kernel-space definitions]</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">/* End prevent double inclusion */</span></span></span><br></pre></td></tr></table></figure><p>The corresponding <code>uapi/</code> header file will look like this:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Header comments (copyright, etc.) */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> _UAPI__XXXXXX_H     <span class="comment">/* Guard macro preventing double inclusion */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _UAPI__XXXXXX_H</span></span><br><span class="line"></span><br><span class="line">[User-space definitions]</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">/* End prevent double inclusion */</span></span></span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://lwn.net/Articles/507794/" target="_blank" rel="noopener">The UAPI header file split</a></li><li><a href="https://lwn.net/Articles/507832/" target="_blank" rel="noopener">UAPI header file split</a></li><li><a href="https://stackoverflow.com/questions/18858190/whats-in-include-uapi-of-kernel-source-project" target="_blank" rel="noopener">What’s in include/uapi of kernel source project</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;What’s in include/uapi of kernel source project?
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>FPU virtualization in KVM</title>
    <link href="http://liujunming.github.io/2022/01/24/FPU-virtualization-in-KVM/"/>
    <id>http://liujunming.github.io/2022/01/24/FPU-virtualization-in-KVM/</id>
    <published>2022-01-24T06:06:08.000Z</published>
    <updated>2022-01-26T11:37:44.753Z</updated>
    
    <content type="html"><![CDATA[<p>本文将以<a href="https://elixir.bootlin.com/linux/v5.16-rc1/source" target="_blank" rel="noopener">v5.16-rc1</a>源码为例，介绍下KVM中FPU virtualization的实现。<a id="more"></a></p><h3 id="1-Prerequisite"><a href="#1-Prerequisite" class="headerlink" title="1. Prerequisite"></a>1. Prerequisite</h3><ul><li><a href="/2021/11/12/Notes-about-XSAVE-feature-set/">Notes about XSAVE feature set</a></li><li><a href="/2022/01/08/Notes-about-FPU-implementation-in-Linux-kernel/">Notes about FPU implementation in Linux kernel</a></li></ul><h3 id="2-xsave-state"><a href="#2-xsave-state" class="headerlink" title="2. xsave state"></a>2. xsave state</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vmx_vcpu_run</span><br><span class="line">kvm_load_guest_xsave_state</span><br><span class="line">vmx_vcpu_enter_exit(vcpu, vmx); <span class="comment">/* The actual VMENTER/EXIT is in the .noinstr.text section. */</span></span><br><span class="line">kvm_load_host_xsave_state</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">kvm_load_guest_xsave_state</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vcpu-&gt;arch.xcr0 != host_xcr0)</span><br><span class="line">xsetbv(XCR_XFEATURE_ENABLED_MASK, vcpu-&gt;arch.xcr0);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vcpu-&gt;arch.xsaves_enabled &amp;&amp;</span><br><span class="line">    vcpu-&gt;arch.ia32_xss != host_xss)</span><br><span class="line">wrmsrl(MSR_IA32_XSS, vcpu-&gt;arch.ia32_xss);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">kvm_load_host_xsave_state</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vcpu-&gt;arch.xcr0 != host_xcr0)</span><br><span class="line">xsetbv(XCR_XFEATURE_ENABLED_MASK, host_xcr0);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vcpu-&gt;arch.xsaves_enabled &amp;&amp;</span><br><span class="line">    vcpu-&gt;arch.ia32_xss != host_xss)</span><br><span class="line">wrmsrl(MSR_IA32_XSS, host_xss);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-kvm-load-guest-fpu-and-kvm-put-guest-fpu"><a href="#3-kvm-load-guest-fpu-and-kvm-put-guest-fpu" class="headerlink" title="3. kvm_load_guest_fpu and kvm_put_guest_fpu"></a>3. kvm_load_guest_fpu and kvm_put_guest_fpu</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_vcpu_arch</span> &#123;</span></span><br><span class="line">...</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * QEMU userspace and the guest each have their own FPU state.</span></span><br><span class="line"><span class="comment"> * In vcpu_run, we switch between the user and guest FPU contexts.</span></span><br><span class="line"><span class="comment"> * While running a VCPU, the VCPU thread will have the guest FPU</span></span><br><span class="line"><span class="comment"> * context.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note that while the PKRU state lives inside the fpu registers,</span></span><br><span class="line"><span class="comment"> * it is switched out separately at VMENTER and VMEXIT time. The</span></span><br><span class="line"><span class="comment"> * "guest_fpstate" state here contains the guest FPU context, with the</span></span><br><span class="line"><span class="comment"> * host PRKU bits.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fpu_guest</span> <span class="title">guest_fpu</span>;</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Swap (qemu) user FPU context for the guest FPU context. */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">kvm_load_guest_fpu</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Exclude PKRU from restore as restored separately in</span></span><br><span class="line"><span class="comment"> * kvm_x86_ops.run().</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">fpu_swap_kvm_fpstate(&amp;vcpu-&gt;arch.guest_fpu, <span class="literal">true</span>);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* When vcpu_run ends, restore user space FPU context. */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">kvm_put_guest_fpu</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">fpu_swap_kvm_fpstate(&amp;vcpu-&gt;arch.guest_fpu, <span class="literal">false</span>);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fpu</span> &#123;</span></span><br><span class="line">...</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * @fpstate:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Pointer to the active struct fpstate. Initialized to</span></span><br><span class="line"><span class="comment"> * point at @__fpstate below.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fpstate</span>*<span class="title">fpstate</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * @__task_fpstate:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Pointer to an inactive struct fpstate. Initialized to NULL. Is</span></span><br><span class="line"><span class="comment"> * used only for KVM support to swap out the regular task fpstate.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fpstate</span>*__<span class="title">task_fpstate</span>;</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fpu_swap_kvm_fpstate</span><span class="params">(struct fpu_guest *guest_fpu, <span class="keyword">bool</span> enter_guest)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fpstate</span> *<span class="title">guest_fps</span> = <span class="title">guest_fpu</span>-&gt;<span class="title">fpstate</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fpu</span> *<span class="title">fpu</span> = &amp;<span class="title">current</span>-&gt;<span class="title">thread</span>.<span class="title">fpu</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fpstate</span> *<span class="title">cur_fps</span> = <span class="title">fpu</span>-&gt;<span class="title">fpstate</span>;</span></span><br><span class="line"></span><br><span class="line">fpregs_lock();</span><br><span class="line"><span class="keyword">if</span> (!cur_fps-&gt;is_confidential &amp;&amp; !test_thread_flag(TIF_NEED_FPU_LOAD))</span><br><span class="line">save_fpregs_to_fpstate(fpu);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Swap fpstate */</span></span><br><span class="line"><span class="keyword">if</span> (enter_guest) &#123;</span><br><span class="line">fpu-&gt;__task_fpstate = cur_fps;</span><br><span class="line">fpu-&gt;fpstate = guest_fps;</span><br><span class="line">guest_fps-&gt;in_use = <span class="literal">true</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">guest_fps-&gt;in_use = <span class="literal">false</span>;</span><br><span class="line">fpu-&gt;fpstate = fpu-&gt;__task_fpstate;</span><br><span class="line">fpu-&gt;__task_fpstate = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cur_fps = fpu-&gt;fpstate;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!cur_fps-&gt;is_confidential) &#123;</span><br><span class="line"><span class="comment">/* Includes XFD update */</span></span><br><span class="line">restore_fpregs_from_fpstate(cur_fps, XFEATURE_MASK_FPSTATE);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * XSTATE is restored by firmware from encrypted</span></span><br><span class="line"><span class="comment"> * memory. Make sure XFD state is correct while</span></span><br><span class="line"><span class="comment"> * running with guest fpstate</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">xfd_update_state(cur_fps);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fpregs_mark_activate();</span><br><span class="line">fpregs_unlock();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-KVM-GET-XSAVE-and-KVM-SET-XSAVE-ioctl"><a href="#4-KVM-GET-XSAVE-and-KVM-SET-XSAVE-ioctl" class="headerlink" title="4. KVM_GET_XSAVE and KVM_SET_XSAVE ioctl"></a>4. KVM_GET_XSAVE and KVM_SET_XSAVE ioctl</h3><p><a href="https://lore.kernel.org/lkml/20211017152048.666354328@linutronix.de/" target="_blank" rel="noopener">x86/kvm: Convert FPU handling to a single swap buffer</a></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_arch_vcpu_ioctl</span><br><span class="line">kvm_vcpu_ioctl_x86_get_xsave[KVM_GET_XSAVE]</span><br><span class="line">fpu_copy_guest_fpstate_to_uabi</span><br><span class="line">__copy_xstate_to_uabi_buf</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * __copy_xstate_to_uabi_buf - Copy kernel saved xstate to a UABI buffer</span></span><br><span class="line"><span class="comment"> * @to:membuf descriptor</span></span><br><span class="line"><span class="comment"> * @fpstate:The fpstate buffer from which to copy</span></span><br><span class="line"><span class="comment"> * @pkru_val:The PKRU value to store in the PKRU component</span></span><br><span class="line"><span class="comment"> * @copy_mode:The requested copy mode</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Converts from kernel XSAVE or XSAVES compacted format to UABI conforming</span></span><br><span class="line"><span class="comment"> * format, i.e. from the kernel internal hardware dependent storage format</span></span><br><span class="line"><span class="comment"> * to the requested @mode. UABI XSTATE is always uncompacted!</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * It supports partial copy but @to.pos always starts from zero.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,</span><br><span class="line">       u32 pkru_val, <span class="keyword">enum</span> xstate_copy_mode copy_mode)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://elixir.bootlin.com/linux/v5.16-rc1/source/arch/x86/kernel/fpu/xstate.c#L1132" target="_blank" rel="noopener">__copy_xstate_to_uabi_buf</a></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kvm_arch_vcpu_ioctl</span><br><span class="line">kvm_vcpu_ioctl_x86_set_xsave[KVM_SET_XSAVE]</span><br><span class="line">fpu_copy_uabi_to_guest_fpstate</span><br><span class="line">copy_uabi_from_kernel_to_xstate</span><br><span class="line">copy_uabi_to_xstate</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将以&lt;a href=&quot;https://elixir.bootlin.com/linux/v5.16-rc1/source&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;v5.16-rc1&lt;/a&gt;源码为例，介绍下KVM中FPU virtualization的实现。
    
    </summary>
    
      <category term="KVM" scheme="http://liujunming.github.io/categories/KVM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
</feed>
