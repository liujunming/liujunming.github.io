<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2018-12-21T10:08:25.988Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title> apue 读书笔记-System Data Files and Information</title>
    <link href="http://liujunming.github.io/2018/12/21/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-System-Data-Files-and-Information/"/>
    <id>http://liujunming.github.io/2018/12/21/apue-读书笔记-System-Data-Files-and-Information/</id>
    <published>2018-12-21T02:52:39.000Z</published>
    <updated>2018-12-21T10:08:25.988Z</updated>
    
    <content type="html"><![CDATA[<p>A UNIX system requires numerous data files for normal operation: the password file <code>/etc/passwd</code> and the group file <code>/etc/group</code> are two files that are frequently used by various programs. <a id="more"></a> </p><h2 id="1-Password-File"><a href="#1-Password-File" class="headerlink" title="1 Password File"></a>1 Password File</h2><p>The UNIX System’s password file contains the fields shown in Figure 6.1. These fields are contained in a <code>passwd</code> structure that is defined in <code>&lt;pwd.h&gt;</code>.<br><img src="/images/2018/12/35.png" alt=""><br>Historically, the password file has been stored in /etc/passwd and has been an ASCII file. Each line contains the fields described in Figure 6.1, separated by colons. For example, four lines from the <code>/etc/passwd</code> file on Linux could be:<br><img src="/images/2018/12/36.png" alt=""></p><ul><li>There is usually an entry with the user name <code>root</code>. This entry has a user ID of 0 (the superuser).</li><li>The encrypted password field contains a single character as a placeholder.</li><li>Some fields in a password file entry can be empty. If the encrypted password field is empty, it usually means that the user does not have a password. (This is not recommended.)</li><li>The shell field contains the name of the executable program to be used as the login shell for the user.</li><li>The <code>nobody</code> user name can be used to allow people to log in to a system, but with a user ID (65534) and group ID (65534) that provide no privileges. </li></ul><p>POSIX.1 defines two functions to fetch entries from the password file. These functions allow us to look up an entry given a user’s login name or numerical user ID.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct passwd *<span class="title">getpwuid</span><span class="params">(<span class="keyword">uid_t</span> uid)</span></span>;</span><br><span class="line"><span class="function">struct passwd *<span class="title">getpwnam</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>getpwuid</code> function is used by the <code>ls</code> program to map the numerical user ID contained in an i-node into a user’s login name. The <code>getpwnam</code> function is used by the <code>login</code> program when we enter our login name.</p><p>These two POSIX.1 functions are fine if we want to look up either a login name or a user ID, but some programs need to go through the entire password file. Three functions can be used for this purpose: <code>getpwent</code>, <code>setpwent</code>, and <code>endpwent</code>.</p><h2 id="2-Shadow-Passwords"><a href="#2-Shadow-Passwords" class="headerlink" title="2 Shadow Passwords"></a>2 Shadow Passwords</h2><p>The encrypted password is a copy of the user’s password that has been put through a one-way encryption algorithm. Because this algorithm is one-way, we can’t guess the original password from the encrypted version.</p><p>Linux store the encrypted password in another file, often called the <em>shadow password file</em>. Minimally, this file has to contain the user name and the encrypted password. Other information relating to the password is also stored here.<br><img src="/images/2018/12/37.png" alt=""></p><h2 id="3-Group-File"><a href="#3-Group-File" class="headerlink" title="3 Group File"></a>3 Group File</h2><p>The UNIX System’s group file contains the fields shown in Figure 6.4. These fields are contained in a <code>group</code> structure that is defined in <code>&lt;grp.h&gt;</code>.<br><img src="/images/2018/12/38.png" alt="">.</p><p>The field <code>gr_mem</code> is an array of pointers to the user names that belong to this group. This array is terminated by a null pointer.</p><p>We can look up either a group name or a numerical group ID with the following two functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct group *<span class="title">getgrgid</span><span class="params">(<span class="keyword">gid_t</span> gid)</span></span>;</span><br><span class="line"><span class="function">struct group *<span class="title">getgrnam</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p>If we want to search the entire group file, we need some additional functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct group *<span class="title">getgrent</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setgrent</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">endgrent</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="4-Supplementary-Group-IDs"><a href="#4-Supplementary-Group-IDs" class="headerlink" title="4 Supplementary Group IDs"></a>4 Supplementary Group IDs</h2><p>Not only did we belong to the group corresponding to the group ID in our password file entry, but we could also belong to as many as 16 additional groups. The file access permission checks were modified so that in addition to comparing the the file’s group ID to the process effective group ID, it was also compared to all the supplementary group IDs.<br>The advantage of using supplementary group IDs is that we no longer have to change groups explicitly. It is common to belong to multiple groups (i.e., participate in multiple projects) at the same time.</p><p>Three functions are provided to fetch and set the supplementary group IDs.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getgroups</span><span class="params">(<span class="keyword">int</span> gidsetsize, <span class="keyword">gid_t</span> grouplist[])</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">setgroups</span><span class="params">(<span class="keyword">int</span> ngroups, <span class="keyword">const</span> <span class="keyword">gid_t</span> grouplist[])</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">initgroups</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *username, <span class="keyword">gid_t</span> basegid)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="5-Other-Data-Files"><a href="#5-Other-Data-Files" class="headerlink" title="5 Other Data Files"></a>5 Other Data Files</h2><p>We’ve discussed only two of the system’s data files so far: the password file and the group file. Numerous other files are used by UNIX systems in normal day-to-day operation. </p><p>Figure 6.6 shows some of these routines, which are common to UNIX systems. In this figure, we show the functions for the password files and group file, which we discussed earlier in this chapter, and some of the networking functions. There are <code>get</code>, <code>set</code>, and <code>end</code> functions for all the data files in this figure.<br><img src="/images/2018/12/39.png" alt=""></p><h2 id="6-Login-Accounting"><a href="#6-Login-Accounting" class="headerlink" title="6 Login Accounting"></a>6 Login Accounting</h2><p>Two data files provided with most UNIX systems are the <code>utmp</code> file, which keeps track of all the users currently logged in, and the <code>wtmp</code> file, which keeps track of all logins and logouts.</p><h2 id="7-System-Identification"><a href="#7-System-Identification" class="headerlink" title="7 System Identification"></a>7 System Identification</h2><p>POSIX.1 defines the <code>uname</code> function to return information on the current host and operating system.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">uname</span><span class="params">(struct utsname *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p><code>gethostname</code> function only return the name of the host.</p><h2 id="8-Time-and-Date-Routines"><a href="#8-Time-and-Date-Routines" class="headerlink" title="8 Time and Date Routines"></a>8 Time and Date Routines</h2><p>The <code>time</code> function returns the current time and date.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">time_t</span> time(<span class="keyword">time_t</span> *calptr);</span><br></pre></td></tr></table></figure></p><p>The real-time extensions to POSIX.1 added support for multiple system clocks.A clock is identified by the <code>clockid_t</code> type. Standard values are summarized in Figure 6.8.<br><img src="/images/2018/12/41.png" alt=""><br>The <code>clock_gettime</code> function can be used to get the time of the specified clock. The time is returned in a <code>timespec</code> structure which expresses time values in terms of seconds and nanoseconds.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">clock_gettime</span><span class="params">(<span class="keyword">clockid_t</span> clock_id, struct timespec *tsp)</span></span>;</span><br></pre></td></tr></table></figure></p><p>When the clock ID is set to <code>CLOCK_REALTIME</code>, the <code>clock_gettime</code> function provides similar functionality to the <code>time</code> function, except with <code>clock_gettime</code>, we might be able to get a higher-resolution time value if the system supports it.</p><p>Once we have the integer value that counts the number of seconds since the Epoch, we normally call a function to convert it to a broken-down time structure, and then call another function to generate a human-readable time and date. Figure 6.9 shows the relationships between the various time functions. (The three functions in this figure that are shown with dashed lines—<code>localtime</code>, <code>mktime</code>, and <code>strftime</code>—are all affected by the <code>TZ</code> environment variable. The dotted lines show how the calendar time is obtained from time-related structures.)<br><img src="/images/2018/12/40.png" alt=""><br>The two functions <code>localtime</code> and <code>gmtime</code> convert a calendar time into what’s called a broken-down time, a tm structure.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">tm</span> &#123;</span>      <span class="comment">/* a broken-down time */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_sec;   <span class="comment">/* seconds after the minute: [0 - 60] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_min;   <span class="comment">/* minutes after the hour: [0 - 59] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_hour;  <span class="comment">/* hours after midnight: [0 - 23] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_mday;  <span class="comment">/* day of the month: [1 - 31] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_mon;   <span class="comment">/* months since January: [0 - 11] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_year;  <span class="comment">/* years since 1900 */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_wday;  <span class="comment">/* days since Sunday: [0 - 6] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_yday;  <span class="comment">/* days since January 1: [0 - 365] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_isdst; <span class="comment">/* daylight saving time flag: &lt;0, 0, &gt;0 */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The difference between <code>localtime</code> and <code>gmtime</code> is that the first converts the calendar time to the local time, taking into account the local time zone and daylight saving time flag, whereas the latter converts the calendar time into a broken-down time expressed as UTC.</p><p>The function <code>mktime</code> takes a broken-down time, expressed as a local time, and converts it into a <code>time_t</code> value.</p><p>The <code>strftime</code> function is a <code>printf-like</code> function for time values. It is complicated by the multitude of arguments available to customize the string it produces.</p><p>We mentioned that the three functions in Figure 6.9 with dashed lines were affected by the <code>TZ</code> environment variable:<code>localtime</code>,<code>mktime</code>,and <code>strftime</code>. If defined,the value of this environment variable is used by these functions instead of the default time zone. If the variable is defined to be a null string, such as <code>TZ=</code>, then UTC is normally used.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A UNIX system requires numerous data files for normal operation: the password file &lt;code&gt;/etc/passwd&lt;/code&gt; and the group file &lt;code&gt;/etc/group&lt;/code&gt; are two files that are frequently used by various programs.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Standard I/O Library</title>
    <link href="http://liujunming.github.io/2018/12/19/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Standard-I-O-Library/"/>
    <id>http://liujunming.github.io/2018/12/19/apue-读书笔记-Standard-I-O-Library/</id>
    <published>2018-12-19T03:50:28.000Z</published>
    <updated>2018-12-21T04:04:53.370Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we describe the standard I/O library. This library is specified by the ISO C standard because it has been implemented on many operating systems other than the UNIX System.<a id="more"></a> </p><p>The standard I/O library handles such details as buffer allocation and performing I/O in optimal-sized chunks, obviating our need to worry about using the correct block size.</p><h2 id="1-Streams-and-FILE-Objects"><a href="#1-Streams-and-FILE-Objects" class="headerlink" title="1 Streams and FILE Objects"></a>1 Streams and <code>FILE</code> Objects</h2><p>In <a href="http://liujunming.top/2018/12/17/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-File-I-O/" target="_blank" rel="noopener">File I/O</a>, all the I/O routines centered on file descriptors. When a file is opened, a file descriptor is returned, and that descriptor is then used for all subsequent I/O operations. With the standard I/O library, the discussion centers on <em>streams</em>. When we open or create a file with the standard I/O library, we say that we have associated a stream with the file.</p><p>With the ASCII character set, a single character is represented by a single byte. With international character sets, a character can be represented by more than one byte. Standard I/O file streams can be used with both single-byte and multibyte (‘‘wide’’) character sets. A stream’s orientation determines whether the characters that are read and written are single byte or multibyte. Initially, when a stream is created, it has no orientation. If a multibyte I/O function is used on a stream without orientation, the stream’s orientation is set to wide oriented. If a byte I/O function is used on a stream without orientation, the stream’s orientation is set to byte oriented. Only two functions can change the orientation once set. The <code>freopen</code> function will clear a stream’s orientation; the <code>fwide</code> function can be used to set a stream’s orientation.</p><p>When we open a stream, the standard I/O function <code>fopen</code> returns a pointer to a <code>FILE</code> object. This object is normally a structure that contains all the information required by the standard I/O library to manage the stream: the file descriptor used for actual I/O, a pointer to a buffer for the stream, the size of the buffer, a count of the number of characters currently in the buffer, an error flag, and the like.</p><h2 id="2-Standard-Input-Standard-Output-and-Standard-Error"><a href="#2-Standard-Input-Standard-Output-and-Standard-Error" class="headerlink" title="2 Standard Input, Standard Output, and Standard Error"></a>2 Standard Input, Standard Output, and Standard Error</h2><p>Three streams are predefined and automatically available to a process: standard input, standard output, and standard error. These streams refer to the same files as the file descriptors <code>STDIN_FILENO</code>, <code>STDOUT_FILENO</code>, and <code>STDERR_FILENO</code>, respectively.<br>These three standard I/O streams are referenced through the predefined file pointers <code>stdin</code>,<code>stdout</code>,and <code>stderr</code>. The file pointers are defined in the<code>&lt;stdio.h&gt;</code> header.</p><h2 id="3-Buffering"><a href="#3-Buffering" class="headerlink" title="3 Buffering"></a>3 Buffering</h2><p>The goal of the buffering provided by the standard I/O library is to use the minimum number of <code>read</code> and <code>write</code> calls.Also, this library tries to do its buffering automatically for each I/O stream, obviating the need for the application to worry about it.<br>Three types of buffering are provided:</p><ol><li>Fully buffered. In this case, actual I/O takes place when the standard I/O buffer is filled.</li><li>Line buffered. </li><li>Unbuffered.</li></ol><p>Linux buffering characteristics:</p><ul><li>Standard error is always unbuffered.</li><li>All other streams are line buffered if they refer to a terminal device; otherwise, they are fully buffered.</li></ul><p>If we don’t like these defaults for any given stream, we can change the buffering by calling either the <code>setbuf</code> or <code>setvbuf</code> function.</p><p>At any time, we can force a stream to be flushed.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fflush</span><span class="params">(FILE *fp)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>fflush</code> function causes any unwritten data for the stream to be passed to the kernel.</p><h2 id="4-Opening-a-Stream"><a href="#4-Opening-a-Stream" class="headerlink" title="4 Opening a Stream"></a>4 Opening a Stream</h2><p>The <code>fopen</code>, <code>freopen</code>, and <code>fdopen</code> functions open a standard I/O stream.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">FILE *<span class="title">fopen</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> type)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">freopen</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> type, FILE *<span class="keyword">restrict</span> fp)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">fdopen</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> *type)</span></span>;</span><br></pre></td></tr></table></figure></p><p>An open stream is closed by calling <code>fclose</code>.</p><h2 id="5-Reading-and-Writing-a-Stream"><a href="#5-Reading-and-Writing-a-Stream" class="headerlink" title="5 Reading and Writing a Stream"></a>5 Reading and Writing a Stream</h2><p>Once we open a stream, we can choose from among three types of unformatted I/O:</p><ol><li>Character-at-a-time I/O. We can read or write one character at a time, with the standard I/O functions handling all the buffering, if the stream is buffered.</li><li>Line-at-a-time I/O. If we want to read or write a line at a time, we use <code>fgets</code> and <code>fputs</code>.</li><li>Binary I/O. This type of I/O is supported by the <code>fread</code> and <code>fwrite</code> functions. For each I/O operation, we read or write some number of objects, where each object is of a specified size. These two functions are often used for binary files where we read or write a structure with each operation. </li></ol><h3 id="5-1-Character-at-a-time-I-O"><a href="#5-1-Character-at-a-time-I-O" class="headerlink" title="5.1 Character-at-a-time I/O"></a>5.1 Character-at-a-time I/O</h3><p>Three functions allow us to read one character at a time.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getc</span><span class="params">(FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fgetc</span><span class="params">(FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getchar</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><p>In most implementations, two flags are maintained for each stream in the <code>FILE</code> object:</p><ul><li>An error flag</li><li>An end-of-file flag</li></ul><p>Output functions are available that correspond to each of the input functions we’ve already described.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">putc</span><span class="params">(<span class="keyword">int</span> c, FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fputc</span><span class="params">(<span class="keyword">int</span> c, FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">putchar</span><span class="params">(<span class="keyword">int</span> c)</span></span>;</span><br></pre></td></tr></table></figure></p><h3 id="5-2-Line-at-a-Time-I-O"><a href="#5-2-Line-at-a-Time-I-O" class="headerlink" title="5.2 Line-at-a-Time I/O"></a>5.2 Line-at-a-Time I/O</h3><p>Line-at-a-time input is provided by the two functions, <code>fgets</code> and <code>gets</code>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">fgets</span><span class="params">(<span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">int</span> n, FILE *<span class="keyword">restrict</span> fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">gets</span><span class="params">(<span class="keyword">char</span> *buf)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>gets</code> function should never be used. The problem is that it doesn’t allow the caller to specify the buffer size. This allows the buffer to overflow if the line is longer than the buffer, writing over whatever happens to follow the buffer in memory.</p><p>Line-at-a-time output is provided by <code>fputs</code> and <code>puts</code>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fputs</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> str, FILE *<span class="keyword">restrict</span> fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">puts</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *str)</span></span>;</span><br></pre></td></tr></table></figure></p><h3 id="5-3-Binary-I-O"><a href="#5-3-Binary-I-O" class="headerlink" title="5.3 Binary I/O"></a>5.3 Binary I/O</h3><p>If we’re doing binary I/O, we often would like to read or write an entire structure at a time.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">size_t</span> fread(<span class="keyword">void</span> *<span class="keyword">restrict</span> ptr, <span class="keyword">size_t</span> size, <span class="keyword">size_t</span> nobj,</span><br><span class="line">FILE *<span class="keyword">restrict</span> fp);</span><br><span class="line"><span class="keyword">size_t</span> fwrite(<span class="keyword">const</span> <span class="keyword">void</span> *<span class="keyword">restrict</span> ptr, <span class="keyword">size_t</span> size, <span class="keyword">size_t</span> nobj,</span><br><span class="line">FILE *<span class="keyword">restrict</span> fp);</span><br></pre></td></tr></table></figure></p><h2 id="6-Positioning-a-Stream"><a href="#6-Positioning-a-Stream" class="headerlink" title="6 Positioning a Stream"></a>6 Positioning a Stream</h2><p>There are three ways to position a standard I/O stream:</p><ol><li>The two functions <code>ftell</code> and <code>fseek</code>.</li><li>The two functions <code>ftello</code> and <code>fseeko</code>.</li><li>The two functions <code>fgetpos</code> and <code>fsetpos</code>.</li></ol><h2 id="7-Formatted-I-O"><a href="#7-Formatted-I-O" class="headerlink" title="7 Formatted I/O"></a>7 Formatted I/O</h2><h3 id="7-1-Formatted-Output"><a href="#7-1-Formatted-Output" class="headerlink" title="7.1 Formatted Output"></a>7.1 Formatted Output</h3><p>Formatted output is handled by the five <code>printf</code> functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">printf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fprintf</span><span class="params">(FILE *<span class="keyword">restrict</span> fp, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dprintf</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sprintf</span><span class="params">(<span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">snprintf</span><span class="params">(<span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">size_t</span> n, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br></pre></td></tr></table></figure></p><h3 id="7-2-Formatted-Input"><a href="#7-2-Formatted-Input" class="headerlink" title="7.2 Formatted Input"></a>7.2 Formatted Input</h3><p>Formatted input is handled by the three <code>scanf</code> functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">scanf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fscanf</span><span class="params">(FILE *<span class="keyword">restrict</span> fp, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sscanf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="8-Implementation-Details"><a href="#8-Implementation-Details" class="headerlink" title="8 Implementation Details"></a>8 Implementation Details</h2><p>Under the UNIX System, the standard I/O library ends up calling the I/O routines that we described in <a href="http://liujunming.top/2018/12/17/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-File-I-O/" target="_blank" rel="noopener">File I/O</a>. Each standard I/O stream has an associated file descriptor, and we can obtain the descriptor for a stream by calling <code>fileno</code>.</p><h2 id="9-Temporary-Files"><a href="#9-Temporary-Files" class="headerlink" title="9 Temporary Files"></a>9 Temporary Files</h2><p>The ISO C standard defines two functions that are provided by the standard I/O library to assist in creating temporary files.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">tmpnam</span><span class="params">(<span class="keyword">char</span> *ptr)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">tmpfile</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="10-Memory-Streams"><a href="#10-Memory-Streams" class="headerlink" title="10 Memory Streams"></a>10 Memory Streams</h2><p>The standard I/O library buffers data in memory, so operations such as character-at-a-time I/O and line-at-a-time I/O are more efficient. We can provide our own buffer for the library to use by calling <code>setbuf</code> or <code>setvbuf</code>.  <em>Memory streams</em> are standard I/O streams for which there are no underlying files, although they are still accessed with FILE pointers. All I/O is done by transferring bytes to and from buffers in main memory.</p><p>Three functions are available to create memory streams.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">FILE *<span class="title">fmemopen</span><span class="params">(<span class="keyword">void</span> *<span class="keyword">restrict</span> buf, <span class="keyword">size_t</span> size, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> type)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">open_memstream</span><span class="params">(<span class="keyword">char</span> **bufp, <span class="keyword">size_t</span> *sizep)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">open_wmemstream</span><span class="params">(<span class="keyword">wchar_t</span> **bufp, <span class="keyword">size_t</span> *sizep)</span></span>;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we describe the standard I/O library. This library is specified by the ISO C standard because it has been implemented on many operating systems other than the UNIX System.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Files and Directories</title>
    <link href="http://liujunming.github.io/2018/12/18/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Files-and-Directories/"/>
    <id>http://liujunming.github.io/2018/12/18/apue-读书笔记-Files-and-Directories/</id>
    <published>2018-12-18T01:36:45.000Z</published>
    <updated>2018-12-18T08:57:30.141Z</updated>
    
    <content type="html"><![CDATA[<p>We’ll now look at additional features of the file system and the properties of a file.<br><a id="more"></a> </p><h2 id="1-stat-fstat-fstatat-and-lstat-Functions"><a href="#1-stat-fstat-fstatat-and-lstat-Functions" class="headerlink" title="1 stat, fstat, fstatat, and lstat Functions"></a>1 <code>stat</code>, <code>fstat</code>, <code>fstatat</code>, and <code>lstat</code> Functions</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">stat</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, struct stat *<span class="keyword">restrict</span> buf )</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fstat</span><span class="params">(<span class="keyword">int</span> fd, struct stat *buf)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">lstat</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, struct stat *<span class="keyword">restrict</span> buf )</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fstatat</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, struct stat *<span class="keyword">restrict</span> buf, <span class="keyword">int</span> flag)</span></span>;</span><br></pre></td></tr></table></figure><p>Given a <code>pathname</code>, the <code>stat</code> function returns a structure of information about the named file.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">stat</span> &#123;</span></span><br><span class="line">    <span class="keyword">mode_t</span>    st_mode;          <span class="comment">/* file type &amp; mode (permissions) */</span></span><br><span class="line">    <span class="keyword">ino_t</span>    st_ino;            <span class="comment">/* i-node number (serial number) */</span></span><br><span class="line">    <span class="keyword">dev_t</span>    st_dev;            <span class="comment">/* device number (file system) */</span></span><br><span class="line">    <span class="keyword">dev_t</span>    st_rdev;           <span class="comment">/* device number for special files */</span></span><br><span class="line">    <span class="keyword">nlink_t</span>    st_nlink;        <span class="comment">/* number of links */</span></span><br><span class="line">    <span class="keyword">uid_t</span>    st_uid;            <span class="comment">/* user ID of the owner */</span></span><br><span class="line">    <span class="keyword">gid_t</span>    st_gid;            <span class="comment">/* group ID of the owner */</span></span><br><span class="line">    <span class="keyword">off_t</span>    st_size;           <span class="comment">/* size in bytes, for regular files */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>    <span class="title">st_atim</span>;</span> <span class="comment">/* time of last access */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>    <span class="title">st_mtim</span>;</span> <span class="comment">/* time of last modification */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>    <span class="title">st_ctim</span>;</span> <span class="comment">/* time of last file status change */</span></span><br><span class="line">    <span class="keyword">blksize_t</span>    st_blksize;    <span class="comment">/* best I/O block size*/</span></span><br><span class="line">    <span class="keyword">blkcnt_t</span>    st_blocks;      <span class="comment">/* number of disk blocks allocated */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-File-Types"><a href="#2-File-Types" class="headerlink" title="2 File Types"></a>2 File Types</h2><p>The types are:</p><ol><li>Regular file.</li><li>Directory file.</li><li>Block special file. A type of file providing buffered I/O access in fixed-size units to devices such as disk drives.</li><li>Character special file. A type of file providing unbuffered I/O access in variable-sized units to devices. All devices on a system are either block special files or character special files.</li><li>FIFO. A type of file used for communication between processes. </li><li>Socket.</li><li>Symbolic link. A type of file that points to another file.</li></ol><p>The type of a file is encoded in the <code>st_mode</code> member of the <code>stat</code> structure.</p><h2 id="3-Set-User-ID-and-Set-Group-ID"><a href="#3-Set-User-ID-and-Set-Group-ID" class="headerlink" title="3 Set-User-ID and Set-Group-ID"></a>3 Set-User-ID and Set-Group-ID</h2><p>Every process has six or more IDs associated with it. These are shown in Figure 4.5.<br><img src="/images/2018/12/29.png" alt=""></p><ul><li>The real user ID and real group ID identify who we really are. These two fields are taken from our entry in the password file when we log in. Normally, these values don’t change during a login session, although there are ways for a superuser process to change them.</li><li>The effective user ID, effective group ID, and supplementary group IDs determine our file access permissions.</li><li>The saved set-user-ID and saved set-group-ID contain copies of the effective user ID and the effective group ID, respectively, when a program is executed.</li></ul><p>Normally, the effective user ID equals the real user ID, and the effective group ID equals the real group ID.</p><p>When we execute a program file, the effective user ID of the process is usually the real user ID, and the effective group ID is usually the real group ID. However, we can also set a special flag in the file’s mode word (<code>st_mode</code>) that says, ‘‘When this file is executed, set the effective user ID of the process to be the owner of the file (<code>st_uid</code>).’’ Similarly, we can set another bit in the file’s mode word that causes the effective group ID to be the group owner of the file (<code>st_gid</code>). These two bits in the file’s mode word are called the <em>set-user-ID</em> bit and the <em>set-group-ID</em> bit.</p><h2 id="4-File-Access-Permissions"><a href="#4-File-Access-Permissions" class="headerlink" title="4 File Access Permissions"></a>4 File Access Permissions</h2><p>The <code>st_mode</code> value also encodes the access permission bits for the file.</p><p>There are nine permission bits for each file, divided into three categories. They are shown in Figure 4.6.<br><img src="/images/2018/12/30.png" alt=""><br>The term <code>user</code> in the first three rows in Figure 4.6 refers to the owner of the file. The <code>chmod</code> command, which is typically used to modify these nine permission bit.</p><h2 id="5-Ownership-of-New-Files-and-Directories"><a href="#5-Ownership-of-New-Files-and-Directories" class="headerlink" title="5 Ownership of New Files and Directories"></a>5 Ownership of New Files and Directories</h2><p>The user ID of a new file is set to the effective user ID of the process.</p><h2 id="6-access-and-faccessat-Functions"><a href="#6-access-and-faccessat-Functions" class="headerlink" title="6 access and faccessat Functions"></a>6 <code>access</code> and <code>faccessat</code> Functions</h2><p>As we described earlier, when we open a file, the kernel performs its access tests based on the effective user and group IDs. Sometimes, however, a process wants to test accessibility based on the real user and group IDs.</p><h2 id="7-umask-Function"><a href="#7-umask-Function" class="headerlink" title="7 umask Function"></a>7 <code>umask</code> Function</h2><p>Now that we’ve described the nine permission bits associated with every file, we can describe the <em>file mode creation mask</em> that is associated with every process.<br>The <code>umask</code> function sets the file mode creation mask for the process and returns the previous value.</p><p>For example, if we want to ensure that anyone can read a file, we should set the <code>umask</code> to 0.</p><h2 id="8-chmod-fchmod-and-fchmodat-Functions"><a href="#8-chmod-fchmod-and-fchmodat-Functions" class="headerlink" title="8 chmod, fchmod, and fchmodat Functions"></a>8 <code>chmod</code>, <code>fchmod</code>, and <code>fchmodat</code> Functions</h2><p>The <code>chmod</code>, <code>fchmod</code>, and <code>fchmodat</code> functions allow us to change the file access permissions for an existing file.</p><h2 id="9-chown-fchown-fchownat-and-lchown-Functions"><a href="#9-chown-fchown-fchownat-and-lchown-Functions" class="headerlink" title="9 chown, fchown, fchownat, and lchown Functions"></a>9 <code>chown</code>, <code>fchown</code>, <code>fchownat</code>, and <code>lchown</code> Functions</h2><p>The <code>chown</code> functions allow us to change a file’s user ID and group ID.</p><h2 id="10-File-Size"><a href="#10-File-Size" class="headerlink" title="10 File Size"></a>10 File Size</h2><p>The <code>st_size</code> member of the <code>stat</code> structure contains the size of the file in bytes. This field is meaningful only for regular files, directories, and symbolic links.</p><h2 id="11-File-Truncation"><a href="#11-File-Truncation" class="headerlink" title="11 File Truncation"></a>11 File Truncation</h2><p>Sometimes we would like to truncate a file by chopping off data at the end of the file.</p><h2 id="12-File-Systems"><a href="#12-File-Systems" class="headerlink" title="12 File Systems"></a>12 File Systems</h2><p>We can think of a disk drive being divided into one or more partitions. Each partition can contain a file system, as shown in Figure 4.13. The i-nodes are fixed-length entries that contain most of the information about a file.<br><img src="/images/2018/12/31.png" alt=""><br>If we examine the i-node and data block portion of a cylinder group in more detail, we could have the arrangement shown in Figure 4.14.<br><img src="/images/2018/12/32.png" alt=""></p><p>Assume that we make a new directory in the working directory, as in:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir testdir</span><br></pre></td></tr></table></figure></p><p>Figure 4.15 shows the result. Note that in this figure, we explicitly show the entries for dot and dot-dot.<br><img src="/images/2018/12/33.png" alt=""></p><h2 id="13-link-linkat-unlink-unlinkat-and-remove-Functions"><a href="#13-link-linkat-unlink-unlinkat-and-remove-Functions" class="headerlink" title="13 link, linkat, unlink, unlinkat, and remove Functions"></a>13 <code>link</code>, <code>linkat</code>, <code>unlink</code>, <code>unlinkat</code>, and <code>remove</code> Functions</h2><p>A file can have multiple directory entries pointing to its i-node. We can use either the <code>link</code> function or the <code>linkat</code> function to create a link to an existing file. To remove an existing directory entry, we call the <code>unlink</code> function.</p><h2 id="14-rename-and-renameat-Functions"><a href="#14-rename-and-renameat-Functions" class="headerlink" title="14 rename and renameat Functions"></a>14 <code>rename</code> and <code>renameat</code> Functions</h2><p>A file or a directory is renamed with either the <code>rename</code> or <code>renameat</code> function.</p><h2 id="15-Symbolic-Links"><a href="#15-Symbolic-Links" class="headerlink" title="15 Symbolic Links"></a>15 Symbolic Links</h2><p>A symbolic link is an indirect pointer to a file, unlike the hard links, which pointed directly to the i-node of the file.</p><h2 id="16-Creating-and-Reading-Symbolic-Links"><a href="#16-Creating-and-Reading-Symbolic-Links" class="headerlink" title="16 Creating and Reading Symbolic Links"></a>16 Creating and Reading Symbolic Links</h2><h2 id="17-File-Times"><a href="#17-File-Times" class="headerlink" title="17 File Times"></a>17 File Times</h2><h2 id="18-futimens-utimensat-and-utimes-Functions"><a href="#18-futimens-utimensat-and-utimes-Functions" class="headerlink" title="18 futimens, utimensat, and utimes Functions"></a>18 <code>futimens</code>, <code>utimensat</code>, and <code>utimes</code> Functions</h2><p>Several functions are available to change the access time and the modification time of a file.</p><h2 id="19-mkdir-mkdirat-and-rmdir-Functions"><a href="#19-mkdir-mkdirat-and-rmdir-Functions" class="headerlink" title="19 mkdir, mkdirat, and rmdir Functions"></a>19 <code>mkdir</code>, <code>mkdirat</code>, and <code>rmdir</code> Functions</h2><p>Directories are created with the <code>mkdir</code> and <code>mkdirat</code> functions, and deleted with the <code>rmdir</code> function.</p><h2 id="20-Reading-Directories"><a href="#20-Reading-Directories" class="headerlink" title="20 Reading Directories"></a>20 Reading Directories</h2><h2 id="21-chdir-fchdir-and-getcwd-Functions"><a href="#21-chdir-fchdir-and-getcwd-Functions" class="headerlink" title="21 chdir, fchdir, and getcwd Functions"></a>21 <code>chdir</code>, <code>fchdir</code>, and <code>getcwd</code> Functions</h2><p>Every process has a current working directory. This directory is where the search for all relative pathnames starts (i.e., with all pathnames that do not begin with a slash). When a user logs in to a UNIX system, the current working directory normally starts at the directory specified by the sixth field in the /etc/passwd file—the user’s home directory. The current working directory is an attribute of a process; the home directory is an attribute of a login name.</p><p>We can change the current working directory of the calling process by calling the <code>chdir</code> or <code>fchdir</code> function.</p><h2 id="22-Device-Special-Files"><a href="#22-Device-Special-Files" class="headerlink" title="22 Device Special Files"></a>22 Device Special Files</h2><p>The two fields <code>st_dev</code> and <code>st_rdev</code> are often confused.The rules for their use are simple.</p><ul><li>Every file system is known by its major and minor device numbers, which are encoded in the primitive system data type <code>dev_t</code>. The major number identifies the device driver and sometimes encodes which peripheral board to communicate with; the minor number identifies the specific subdevice. Recall from Figure 4.13 that a disk drive often contains several file systems. Each file system on the same disk drive would usually have the same major number, but a different minor number.</li><li>We can usually access the major and minor device numbers through two macros defined by most implementations: <code>major</code> and <code>minor</code>.</li><li>The <code>st_dev</code> value for every filename on a system is the device number of the file system containing that filename and its corresponding i-node.</li><li>Only character special files and block special files have an <code>st_rdev</code> value. This value contains the device number for the actual device.</li></ul><h2 id="23-Summary-of-File-Access-Permission-Bits"><a href="#23-Summary-of-File-Access-Permission-Bits" class="headerlink" title="23 Summary of File Access Permission Bits"></a>23 Summary of File Access Permission Bits</h2><p>We’ve covered all the file access permission bits, some of which serve multiple purposes. Figure 4.26 summarizes these permission bits and their interpretation when applied to a regular file and a directory.<br><img src="/images/2018/12/34.png" alt=""><br>The final nine constants can also be grouped into threes, as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">S_IRWXU = S_IRUSR | S_IWUSR | S_IXUSR</span><br><span class="line">S_IRWXG = S_IRGRP | S_IWGRP | S_IXGRP</span><br><span class="line">S_IRWXO = S_IROTH | S_IWOTH | S_IXOTH</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We’ll now look at additional features of the file system and the properties of a file.&lt;br&gt;
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-File I/O</title>
    <link href="http://liujunming.github.io/2018/12/17/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-File-I-O/"/>
    <id>http://liujunming.github.io/2018/12/17/apue-读书笔记-File-I-O/</id>
    <published>2018-12-17T01:15:52.000Z</published>
    <updated>2018-12-20T08:40:04.267Z</updated>
    
    <content type="html"><![CDATA[<p>The functions described in this chapter are often referred to as <em>unbuffered I/O</em>.The term <em>unbuffered</em> means that each <code>read</code> or <code>write</code> invokes a system call in the kernel.<a id="more"></a> </p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><h3 id="1-1-File-Descriptors"><a href="#1-1-File-Descriptors" class="headerlink" title="1.1 File Descriptors"></a>1.1 File Descriptors</h3><p>To the kernel, all open files are referred to by file descriptors. A file descriptor is a non-negative integer. When we open an existing file or create a new file, the kernel returns a file descriptor to the process.</p><p>By convention, UNIX System shells associate file descriptor 0 with the standard input of a process, file descriptor 1 with the standard output, and file descriptor 2 with the standard error.</p><p>Although their values are standardized by POSIX.1, the magic numbers 0, 1, and 2 should be replaced in POSIX-compliant applications with the symbolic constants <code>STDIN_FILENO</code>, <code>STDOUT_FILENO</code>, and <code>STDERR_FILENO</code> to improve readability.</p><h3 id="1-2-open-and-openat-Functions"><a href="#1-2-open-and-openat-Functions" class="headerlink" title="1.2 open and openat Functions"></a>1.2 <code>open</code> and <code>openat</code> Functions</h3><p>A file is opened or created by calling either the <code>open</code> function or the <code>openat</code> function.</p><h3 id="1-3-creat-Function"><a href="#1-3-creat-Function" class="headerlink" title="1.3 creat Function"></a>1.3 creat Function</h3><p>A new file can also be created by calling the <code>creat</code> function.</p><h3 id="1-4-close-Function"><a href="#1-4-close-Function" class="headerlink" title="1.4 close Function"></a>1.4 close Function</h3><p>An open file is closed by calling the close function.<br>When a process terminates, all of its open files are closed automatically by the kernel. Many programs take advantage of this fact and don’t explicitly close open files.</p><h3 id="1-5-lseek-Function"><a href="#1-5-lseek-Function" class="headerlink" title="1.5 lseek Function"></a>1.5 <code>lseek</code> Function</h3><p>Every open file has an associated <strong>current file offset</strong>, normally a non-negative integer that measures the number of bytes from the beginning of the file.<br>An open file’s offset can be set explicitly by calling <code>lseek</code>.</p><p><code>lseek</code> only records the current file offset within the kernel — it does not cause any I/O to take place. This offset is then used by the next read or write operation.<br>The file’s offset can be greater than the file’s current size, in which case the next write to the file will extend the file. This is referred to as creating a <em>hole</em> in a file and is allowed. Any bytes in a file that have not been written are read back as 0. A hole in a file isn’t required to have storage backing it on disk.</p><p>We use the <code>od</code> command to look at the contents of the file.</p><h3 id="1-6-read-Function"><a href="#1-6-read-Function" class="headerlink" title="1.6 read Function"></a>1.6 <code>read</code> Function</h3><p>Data is read from an open file with the <code>read</code> function.</p><h3 id="1-7-write-Function"><a href="#1-7-write-Function" class="headerlink" title="1.7 write Function"></a>1.7 <code>write</code> Function</h3><p>Data is written to an open file with the <code>write</code> function.</p><h2 id="2-I-O-Efficiency"><a href="#2-I-O-Efficiency" class="headerlink" title="2 I/O Efficiency"></a>2 I/O Efficiency</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"apue.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFSIZE 4096</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">char</span> buf[BUFFSIZE];</span><br><span class="line">    <span class="keyword">while</span> ((n = read(STDIN_FILENO, buf, BUFFSIZE)) &gt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (write(STDOUT_FILENO, buf, n) != n)</span><br><span class="line">            err_sys(<span class="string">"write error"</span>);</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">0</span>)</span><br><span class="line">            err_sys(<span class="string">"read error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Figure 3.6 shows the results for reading a 516,581,760-byte file, using 20 different buffer sizes.</p><center><img src="/images/2018/12/25.png" alt=""></center><p>This accounts for the minimum in the system time occurring at the few timing measurements starting around a BUFFSIZE of 4,096. Increasing the buffer size beyond this limit has little positive effect.</p><p>Most file systems support some kind of read-ahead to improve performance. When sequential reads are detected, the system tries to read in more data than an application requests, assuming that the application will read it shortly. The effect of read-ahead can be seen in Figure 3.6, where the elapsed time for buffer sizes as small as 32 bytes is as good as the elapsed time for larger buffer sizes.</p><h2 id="3-File-Sharing"><a href="#3-File-Sharing" class="headerlink" title="3 File Sharing"></a>3 File Sharing</h2><p>The UNIX System supports the sharing of open files among different processes.</p><ol><li>Every process has an entry in the process table. Within each process table entry is a table of open file descriptors, which we can think of as a vector, with one entry per descriptor. Associated with each file descriptor are:</li></ol><ul><li>The file descriptor flags </li><li>A pointer to a file table entry</li></ul><ol start="2"><li>The kernel maintains a file table for all open files. Each file table entry contains:</li></ol><ul><li>The file status flags for the file, such as read, write, append, sync, and nonblocking</li><li>The current file offset</li><li>A pointer to the v-node table entry for the file</li></ul><ol start="3"><li>Each open file (or device) has a v-node structure that contains information about the type of file and pointers to functions that operate on the file. For most files, the v-node also contains the i-node for the file. This information is read from disk when the file is opened, so that all the pertinent information about the file is readily available. For example, the i-node contains the owner of the file, the size of the file, pointers to where the actual data blocks for the file are located on disk, and so on.</li></ol><p><em>Linux has no v-node. Instead, a generic i-node structure is used</em>.</p><p>Figure 3.7 shows a pictorial arrangement of these three tables for a single process that has two different files open: one file is open on standard input (file descriptor 0), and the other is open on standard output (file descriptor 1).<br><img src="/images/2018/12/26.png" alt=""></p><p>If two independent processes have the same file open, we could have the arrangement shown in Figure 3.8.<br><img src="/images/2018/12/27.png" alt=""><br>We assume here that the first process has the file open on descriptor 3 and that the second process has that same file open on descriptor 4. Each process that opens the file gets its own file table entry, but only a single v-node table entry is required for a given file. One reason each process gets its own file table entry is so that each process has its own current offset for the file.</p><h2 id="4-Atomic-Operations"><a href="#4-Atomic-Operations" class="headerlink" title="4 Atomic Operations"></a>4 Atomic Operations</h2><h3 id="4-1-Appending-to-a-File"><a href="#4-1-Appending-to-a-File" class="headerlink" title="4.1 Appending to a File"></a>4.1 Appending to a File</h3><p>Consider a single process that wants to append to the end of a file.This works fine for a single process, but problems arise if multiple processes use this technique to append to the same file. (This scenario can arise if multiple instances of the same program are appending messages to a log file, for example.)</p><p>Assume that two independent processes, A and B, are appending to the same file. Each has opened the file but without the O_APPEND flag. This gives us the same picture as Figure 3.8. Each process has its own file table entry, but they share a single v-node table entry. Assume that process A does the lseek and that this sets the current offset for the file for process A to byte offset 1,500 (the current end of file). Then the kernel switches processes, and B continues running. Process B then does the lseek, which sets the current offset for the file for process B to byte offset 1,500 also (the current end of file). Then B calls write, which increments B’s current file offset for the file to 1,600. Because the file’s size has been extended, the kernel also updates the current file size in the v-node to 1,600. Then the kernel switches processes and A resumes. When A calls write, the data is written starting at the current file offset for A, which is byte offset 1,500. This overwrites the data that B wrote to the file.</p><p>The problem here is that our logical operation of ‘‘position to the end of file and write’’ requires two separate function calls (as we’ve shown it). The solution is to have the positioning to the current end of file and the write be an atomic operation with regard to other processes. Any operation that requires more than one function call cannot be atomic, as there is always the possibility that the kernel might temporarily suspend the process between the two function calls (as we assumed previously).</p><p>The UNIX System provides an atomic way to do this operation if we set the O_APPEND flag when a file is opened. As we described in the previous section, this causes the kernel to position the file to its current end of file before each write. We no longer have to call lseek before each write.</p><h3 id="4-2-pread-and-pwrite-Functions"><a href="#4-2-pread-and-pwrite-Functions" class="headerlink" title="4.2 pread and pwrite Functions"></a>4.2 <code>pread</code> and <code>pwrite</code> Functions</h3><p>Calling <code>pread</code> is equivalent to calling <code>lseek</code> followed by a call to <code>read</code>, with the following exceptions.</p><ul><li>There is no way to interrupt the two operations that occur when we call <code>pread</code>.</li><li>The current file offset is not updated.</li></ul><h3 id="4-3-Creating-a-File"><a href="#4-3-Creating-a-File" class="headerlink" title="4.3 Creating a File"></a>4.3 Creating a File</h3><p>When <code>O_CREAT</code> and <code>O_EXCL</code> options for the open function are specified, the open will fail if the file already exists. We also said that the check for the existence of the file and the creation of the file was performed as an atomic operation. If we didn’t have this atomic operation, we might try:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">if</span> ((fd = open(path, O_WRONLY)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (errno == ENOENT) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((fd = creat(path, mode)) &lt; <span class="number">0</span>)</span><br><span class="line">            err_sys(<span class="string">"creat error"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        err_sys(<span class="string">"open error"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>The problem occurs if the file is created by another process between the <code>open</code> and the <code>creat</code>. If the file is created by another process between these two function calls, and if that other process writes something to the file, that data is erased when this <code>creat</code> is executed. Combining the test for existence and the creation into a single atomic operation avoids this problem.</p><p>In general, the term <em>atomic operation</em> refers to an operation that might be composed of multiple steps. If the operation is performed atomically, either all the steps are performed (on success) or none are performed (on failure). It must not be possible for only a subset of the steps to be performed.</p><h2 id="5-dup-and-dup2-Functions"><a href="#5-dup-and-dup2-Functions" class="headerlink" title="5 dup and dup2 Functions"></a>5 <code>dup</code> and <code>dup2</code> Functions</h2><p>An existing file descriptor is duplicated by either of the following functions:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dup</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dup2</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">int</span> fd2)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The new file descriptor returned by <code>dup</code> is guaranteed to be the lowest-numbered available file descriptor.</p><p>The new file descriptor that is returned as the value of the functions shares the same file table entry as the <code>fd</code> argument. We show this in Figure 3.9.</p><center><img src="/images/2018/12/28.png" alt=""></center><p>Each descriptor has its own set of file descriptor flags.</p><h2 id="6-sync-fsync-and-fdatasync-Functions"><a href="#6-sync-fsync-and-fdatasync-Functions" class="headerlink" title="6 sync, fsync, and fdatasync Functions"></a>6 <code>sync</code>, <code>fsync</code>, and <code>fdatasync</code> Functions</h2><p>Traditional implementations of the UNIX System have a buffer cache or page cache in the kernel through which most disk I/O passes. When we write data to a file, the data is normally copied by the kernel into one of its buffers and queued for writing to disk at some later time. This is called <em>delayed write</em>.</p><p>The kernel eventually writes all the delayed-write blocks to disk.<br>To ensure consistency of the file system on disk with the contents of the buffer cache, the <code>sync</code>, <code>fsync</code>, and <code>fdatasync</code> functions are provided.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fsync</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fdatasync</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sync</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>sync</code> function simply queues all the modified block buffers for writing and returns; it does not wait for the disk writes to take place.</p><p>The function <code>sync</code> is normally called periodically (usually every 30 seconds) from a system daemon, often called <code>update</code>. This guarantees regular flushing of the kernel’s block buffers.</p><p>The function <code>fsync</code> refers only to a single file, specified by the file descriptor <code>fd</code>, and waits for the disk writes to complete before returning. This function is used when an application, such as a database, needs to be sure that the modified blocks have been written to the disk.</p><p>The <code>fdatasync</code> function is similar to <code>fsync</code>, but it affects only the data portions of a file. With <code>fsync</code>, the file’s attributes are also updated synchronously.</p><h2 id="7-fcntl-Function"><a href="#7-fcntl-Function" class="headerlink" title="7 fcntl Function"></a>7 <code>fcntl</code> Function</h2><p>The <code>fcntl</code> function can change the properties of a file that is already open.</p><p>The <code>fcntl</code> function is used for five different purposes.</p><ol><li>Duplicate an existing descriptor (cmd = <code>F_DUPFD</code> or <code>F_DUPFD_CLOEXEC</code>)</li><li>Get/set file descriptor flags (cmd = <code>F_GETFD</code> or <code>F_SETFD</code>)</li><li>Get/set file status flags (cmd = <code>F_GETFL</code> or <code>F_SETFL</code>)</li><li>Get/set asynchronous I/O ownership (cmd = <code>F_GETOWN</code> or <code>F_SETOWN</code>)</li><li>Get/set record locks (cmd = <code>F_GETLK</code>, <code>F_SETLK</code>, or <code>F_SETLKW</code>)</li></ol><h2 id="8-ioctl-Function"><a href="#8-ioctl-Function" class="headerlink" title="8 ioctl Function"></a>8 <code>ioctl</code> Function</h2><p>The <code>ioctl</code> function has always been the catchall for I/O operations.<br>Terminal I/O was the biggest user of this function.Each device driver can define its own set of <code>ioctl</code> commands. The system, however, provides generic <code>ioctl</code> commands for different classes of devices.</p><h2 id="9-dev-fd"><a href="#9-dev-fd" class="headerlink" title="9 /dev/fd"></a>9 <code>/dev/fd</code></h2><p>Newer systems provide a directory named <code>/dev/fd</code> whose entries are files named 0, 1, 2, and so on.</p><p>The main use of the <code>/dev/fd</code> files is from the shell. It allows programs that use pathname arguments to handle standard input and standard output in the same manner as other pathnames.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The functions described in this chapter are often referred to as &lt;em&gt;unbuffered I/O&lt;/em&gt;.The term &lt;em&gt;unbuffered&lt;/em&gt; means that each &lt;code&gt;read&lt;/code&gt; or &lt;code&gt;write&lt;/code&gt; invokes a system call in the kernel.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Timing Measurements</title>
    <link href="http://liujunming.github.io/2018/12/15/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Timing-Measurements/"/>
    <id>http://liujunming.github.io/2018/12/15/Understanding-the-Linux-Kernel-读书笔记-Timing-Measurements/</id>
    <published>2018-12-15T00:57:28.000Z</published>
    <updated>2018-12-15T06:53:14.598Z</updated>
    
    <content type="html"><![CDATA[<p>We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel:</p><ul><li>Keeping the current time and date so they can be returned to user programs through the time(), ftime(), and gettimeofday() APIs and used by the kernel itself as timestamps for files and network packets</li><li>Maintaining timers—mechanisms that are able to notify the kernel or a user program that a certain interval of time has elapsed<a id="more"></a></li></ul><h2 id="1-Clock-and-Timer-Circuits"><a href="#1-Clock-and-Timer-Circuits" class="headerlink" title="1 Clock and Timer Circuits"></a>1 Clock and Timer Circuits</h2><p>The <em>clock circuits</em> are used both to keep track of the current time of day and to make precise time measurements. The <em>timer circuits</em> are programmed by the kernel, so that they issue interrupts at a fixed, predefined frequency.</p><h3 id="1-1-Real-Time-Clock-RTC"><a href="#1-1-Real-Time-Clock-RTC" class="headerlink" title="1.1 Real Time Clock (RTC)"></a>1.1 Real Time Clock (RTC)</h3><p>All PCs include a clock called <em>Real Time Clock</em> (RTC), which is independent of the CPU and all other chips.<br>Linux uses the RTC only to derive the time and date.</p><h3 id="1-2-Time-Stamp-Counter-TSC"><a href="#1-2-Time-Stamp-Counter-TSC" class="headerlink" title="1.2 Time Stamp Counter (TSC)"></a>1.2 Time Stamp Counter (TSC)</h3><p>Starting with the Pentium, 80×86 microprocessors sport a counter that is increased at each clock signal. The counter is accessible through the 64-bit <em>Time Stamp Counter</em> (TSC) register. When using this register, the kernel has to take into consideration the frequency of the clock signal: if, for instance, the clock ticks at 1 GHz, the Time Stamp Counter is increased once every nanosecond.<br>Linux may take advantage of this register to get much more accurate time measurements than those delivered by the Programmable Interval Timer.</p><h3 id="1-3-Programmable-Interval-Timer-PIT"><a href="#1-3-Programmable-Interval-Timer-PIT" class="headerlink" title="1.3 Programmable Interval Timer (PIT)"></a>1.3 Programmable Interval Timer (PIT)</h3><p>Besides the Real Time Clock and the Time Stamp Counter, IBM-compatible PCs include another type of time-measuring device called <em>Programmable Interval Timer</em> (PIT ). The role of a PIT is similar to the alarm clock of a microwave oven: it makes the user aware that the cooking time interval has elapsed. Instead of ringing a bell, this device issues a special interrupt called timer interrupt, which notifies the kernel that one more time interval has elapsed. Another difference from the alarm clock is that the PIT goes on issuing interrupts forever at some fixed frequency established by the kernel. </p><h3 id="1-4-CPU-Local-Timer"><a href="#1-4-CPU-Local-Timer" class="headerlink" title="1.4 CPU Local Timer"></a>1.4 CPU Local Timer</h3><p>The local APIC present in recent 80×86 microprocessors provides yet another time-measuring device: the <em>CPU local timer</em>.<br>The CPU local timer is a device similar to the Programmable Interval Timer just described that can issue one-shot or periodic interrupts. There are, however, a few differences:</p><ul><li>The APIC’s timer counter is 32bits long,while the PIT’s timer counter is 16 bits long;</li><li>The local APIC timer sends an interrupt only to its processor, while the PIT raises a global interrupt, which may be handled by any CPU in the system.</li><li>The APIC’s timer is based on the bus clock signal,the PIT, which makes use of its own clock signals, can be programmed in a more flexible way.</li></ul><h3 id="1-5-High-Precision-Event-Timer-HPET"><a href="#1-5-High-Precision-Event-Timer-HPET" class="headerlink" title="1.5 High Precision Event Timer (HPET)"></a>1.5 High Precision Event Timer (HPET)</h3><p>The HPET provides a number of hardware timers that can be exploited by the kernel.<br>The next generation of motherboards will likely support both the HPET and the 8254 PIT; in some future time, however, the HPET is expected to completely replace the PIT.</p><h3 id="1-6-ACPI-Power-Management-Timer"><a href="#1-6-ACPI-Power-Management-Timer" class="headerlink" title="1.6 ACPI Power Management Timer"></a>1.6 ACPI Power Management Timer</h3><p>The device is actually a simple counter increased at each clock tick.Its clock signal has a fixed frequency of roughly 3.58 MHz. </p><p>The ACPI Power Management Timer is preferable to the TSC if the operating system or the BIOS may dynamically lower the frequency or voltage of the CPU to save battery power. On the other hand, the high-frequency of the TSC counter is quite handy for measuring very small time intervals.</p><p>However, if an HPET device is present, it should always be preferred to the other circuits because of its richer architecture. </p><h2 id="2-The-Linux-Timekeeping-Architecture"><a href="#2-The-Linux-Timekeeping-Architecture" class="headerlink" title="2 The Linux Timekeeping Architecture"></a>2 The Linux Timekeeping Architecture</h2><p>Linux’s <em>timekeeping architecture</em> is the set of kernel data structures and functions related to the flow of time.</p><p>Linux’s timekeeping architecture depends also on the availability of the Time Stamp Counter (TSC), of the ACPI Power Management Timer, and of the High Precision Event Timer (HPET). The kernel uses two basic timekeeping functions: one to keep the current time up-to-date and another to count the number of nanoseconds that have elapsed within the current second. There are different ways to get the last value. Some methods are more precise and are available if the CPU has a Time Stamp Counter or a HPET; a less-precise method is used in the opposite case.</p><h3 id="2-1-Data-Structures-of-the-Timekeeping-Architecture"><a href="#2-1-Data-Structures-of-the-Timekeeping-Architecture" class="headerlink" title="2.1 Data Structures of the Timekeeping Architecture"></a>2.1 Data Structures of the Timekeeping Architecture</h3><h4 id="2-1-1-The-timer-object"><a href="#2-1-1-The-timer-object" class="headerlink" title="2.1.1 The timer object"></a>2.1.1 The timer object</h4><p>In order to handle the possible timer sources in a uniform way, the kernel makes use of a “timer object,” which is a descriptor of type <code>timer_opts</code>consisting of the timer name and of four standard methods.</p><p>The <code>cur_timer</code> variable stores the address of the timer object corresponding to the “best” timer source available in the system. </p><h4 id="2-1-2-The-jiffies-variable"><a href="#2-1-2-The-jiffies-variable" class="headerlink" title="2.1.2 The jiffies variable"></a>2.1.2 The jiffies variable</h4><p>The <code>jiffies</code> variable is a counter that stores the number of elapsed ticks since the system was started. It is increased by one when a timer interrupt occurs—that is, on every tick. </p><h4 id="2-1-3-The-xtime-variable"><a href="#2-1-3-The-xtime-variable" class="headerlink" title="2.1.3 The xtime variable"></a>2.1.3 The xtime variable</h4><p>The <code>xtime</code> variable stores the current time and date; it is a structure of type <code>timespec</code>.</p><h3 id="2-2-Timekeeping-Architecture-in-Uniprocessor-Systems"><a href="#2-2-Timekeeping-Architecture-in-Uniprocessor-Systems" class="headerlink" title="2.2 Timekeeping Architecture in Uniprocessor Systems"></a>2.2 Timekeeping Architecture in Uniprocessor Systems</h3><h3 id="2-3-Timekeeping-Architecture-in-Multiprocessor-Systems"><a href="#2-3-Timekeeping-Architecture-in-Multiprocessor-Systems" class="headerlink" title="2.3 Timekeeping Architecture in Multiprocessor Systems"></a>2.3 Timekeeping Architecture in Multiprocessor Systems</h3><h2 id="3-Updating-the-Time-and-Date"><a href="#3-Updating-the-Time-and-Date" class="headerlink" title="3 Updating the Time and Date"></a>3 Updating the Time and Date</h2><p>User programs get the current time and date from the xtime variable. The kernel must periodically update this variable, so that its value is always reasonably accurate.<br>The <code>update_times()</code> function, which is invoked by the global timer interrupt handler, updates the value of the <code>xtime</code> variable.</p><h2 id="4-Updating-System-Statistics"><a href="#4-Updating-System-Statistics" class="headerlink" title="4 Updating System Statistics"></a>4 Updating System Statistics</h2><p>The kernel, among the other time-related duties, must periodically collect various data used for:</p><ul><li>Checking the CPU resource limit of the running processes</li><li>Updating statistics about the local CPU workload</li><li>Computing the average system load</li><li>Profiling the kernel code</li></ul><h2 id="5-Software-Timers-and-Delay-Functions"><a href="#5-Software-Timers-and-Delay-Functions" class="headerlink" title="5 Software Timers and Delay Functions"></a>5 Software Timers and Delay Functions</h2><p>A <em>timer</em> is a software facility that allows functions to be invoked at some future moment, after a given time interval has elapsed; a <em>time-out</em> denotes a moment at which the time interval associated with a timer has elapsed.</p><p>Linux considers two types of timers called <em>dynamic timers</em> and <em>interval timers</em>. The first type is used by the kernel, while interval timers may be created by processes in User Mode.</p><p>Besides software timers, the kernel also makes use of <em>delay functions</em>, which execute a tight instruction loop until a given time interval elapses. </p><h3 id="5-1-Dynamic-Timers"><a href="#5-1-Dynamic-Timers" class="headerlink" title="5.1 Dynamic Timers"></a>5.1 Dynamic Timers</h3><h3 id="5-2-An-Application-of-Dynamic-Timers-the-nanosleep-System-Call"><a href="#5-2-An-Application-of-Dynamic-Timers-the-nanosleep-System-Call" class="headerlink" title="5.2 An Application of Dynamic Timers: the nanosleep( ) System Call"></a>5.2 An Application of Dynamic Timers: the nanosleep( ) System Call</h3><h3 id="5-3-Delay-Functions"><a href="#5-3-Delay-Functions" class="headerlink" title="5.3 Delay Functions"></a>5.3 Delay Functions</h3><p>Software timers are useless when the kernel must wait for a short time interval—let’s say, less than a few milliseconds. For instance, often a device driver has to wait for a predefined number of microseconds until the hardware completes some operation. Because a dynamic timer has a significant setup overhead and a rather large minimum wait time (1 millisecond), the device driver cannot conveniently use it.</p><h2 id="6-System-Calls-Related-to-Timing-Measurements"><a href="#6-System-Calls-Related-to-Timing-Measurements" class="headerlink" title="6 System Calls Related to Timing Measurements"></a>6 System Calls Related to Timing Measurements</h2><p>Several system calls allow User Mode processes to read and modify the time and date and to create timers. Let’s briefly review these and discuss how the kernel handles them.</p><h3 id="6-1-The-time-and-gettimeofday-System-Calls"><a href="#6-1-The-time-and-gettimeofday-System-Calls" class="headerlink" title="6.1 The time( ) and gettimeofday( ) System Calls"></a>6.1 The time( ) and gettimeofday( ) System Calls</h3><p>Processes in User Mode can get the current time and date by means of several system calls:</p><ul><li><code>time()</code><br>Returns the number of elapsed seconds since midnight at the start of January 1, 1970 (UTC).</li><li><code>gettimeofday()</code><br>Returns, in a data structure named timeval, the number of elapsed seconds since midnight of January 1, 1970 (UTC) and the number of elapsed microseconds in the last second.</li></ul><h3 id="6-2-The-adjtimex-System-Call"><a href="#6-2-The-adjtimex-System-Call" class="headerlink" title="6.2 The adjtimex( ) System Call"></a>6.2 The adjtimex( ) System Call</h3><h3 id="6-3-The-setitimer-and-alarm-System-Calls"><a href="#6-3-The-setitimer-and-alarm-System-Calls" class="headerlink" title="6.3 The setitimer( ) and alarm( ) System Calls"></a>6.3 The setitimer( ) and alarm( ) System Calls</h3><h3 id="6-4-System-Calls-for-POSIX-Timers"><a href="#6-4-System-Calls-for-POSIX-Timers" class="headerlink" title="6.4 System Calls for POSIX Timers"></a>6.4 System Calls for POSIX Timers</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keeping the current time and date so they can be returned to user programs through the time(), ftime(), and gettimeofday() APIs and used by the kernel itself as timestamps for files and network packets&lt;/li&gt;
&lt;li&gt;Maintaining timers—mechanisms that are able to notify the kernel or a user program that a certain interval of time has elapsed
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Kernel Synchronization</title>
    <link href="http://liujunming.github.io/2018/12/14/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Kernel-Synchronization/"/>
    <id>http://liujunming.github.io/2018/12/14/Understanding-the-Linux-Kernel-读书笔记-Kernel-Synchronization/</id>
    <published>2018-12-14T04:18:06.000Z</published>
    <updated>2018-12-14T10:38:57.246Z</updated>
    
    <content type="html"><![CDATA[<p>You could think of the kernel as a server that answers requests; these requests can come either from a process running on a CPU or an external device issuing an interrupt request. We make this analogy to underscore that parts of the kernel are not run serially, but in an interleaved way. Thus, they can give rise to race conditions, which must be controlled through proper synchronization techniques.<a id="more"></a> </p><h2 id="1-How-the-Kernel-Services-Requests"><a href="#1-How-the-Kernel-Services-Requests" class="headerlink" title="1 How the Kernel Services Requests"></a>1 How the Kernel Services Requests</h2><p>In the rest of this chapter, we will generally denote as “exceptions” both the system calls and the usual exceptions.</p><h3 id="1-1-Kernel-Preemption"><a href="#1-1-Kernel-Preemption" class="headerlink" title="1.1 Kernel Preemption"></a>1.1 Kernel Preemption</h3><p>内核抢占<br>In nonpreemptive kernels, the current process cannot be replaced unless it is about to switch to User Mode. Therefore, the main characteristic of a preemptive kernel is that a process running in Kernel Mode can be replaced by another process while in the middle of a kernel function.</p><p>The main motivation for making a kernel preemptive is to reduce the dispatch latency of the User Mode processes, that is, the delay between the time they become runnable and the time they actually begin running. </p><p>The kernel can be preempted only when it is executing an exception handler (in particular a system call) and the kernel preemption has not been explicitly disabled. The local CPU must have local interrupts enabled, otherwise kernel preemption is not performed.</p><p>Kernel preemption may happen either when a kernel control path (usually, an interrupt handler) is terminated, or when an exception handler reenables kernel preemption by means of <code>preempt_enable()</code>.</p><h2 id="2-Synchronization-Primitives"><a href="#2-Synchronization-Primitives" class="headerlink" title="2 Synchronization Primitives"></a>2 Synchronization Primitives</h2><center><img src="/images/2018/12/21.png" alt=""></center><h3 id="2-1-Per-CPU-Variables"><a href="#2-1-Per-CPU-Variables" class="headerlink" title="2.1 Per-CPU Variables"></a>2.1 Per-CPU Variables</h3><p>The simplest and most efficient synchronization technique consists of declaring kernel variables as <em>per-CPU variables</em>. Basically, a per-CPU variable is an array of data structures, one element per each CPU in the system. However, that the per-CPU variables can be used only in particular cases—basically, when it makes sense to logically split the data across the CPUs of the system.</p><p>Furthermore, per-CPU variables are prone to race conditions caused by kernel preemption, both in uniprocessor and multiprocessor systems. As a general rule, a kernel control path should access a per-CPU variable with kernel preemption disabled.</p><h3 id="2-2-Atomic-Operations"><a href="#2-2-Atomic-Operations" class="headerlink" title="2.2 Atomic Operations"></a>2.2 Atomic Operations</h3><p>Several assembly language instructions are of type “read-modify-write”—that is, they access a memory location twice, the first time to read the old value and the second time to write a new value.</p><p>The easiest way to prevent race conditions due to “read-modify-write” instructions is by ensuring that such operations are atomic at the chip level. Every such operation must be executed in a single instruction without being interrupted in the middle and avoiding accesses to the same memory location by other CPUs.</p><h3 id="2-3-Optimization-and-Memory-Barriers"><a href="#2-3-Optimization-and-Memory-Barriers" class="headerlink" title="2.3 Optimization and Memory Barriers"></a>2.3 Optimization and Memory Barriers</h3><p>When using optimizing compilers, you should never take for granted that instructions will be performed in the exact order in which they appear in the source code. For example, a compiler might reorder the assembly language instructions in such a way to optimize how registers are used. Moreover, modern CPUs usually execute several instructions in parallel and might reorder memory accesses. These kinds of reordering can greatly speed up the program.</p><p>When dealing with synchronization, however, reordering instructions must be avoided. Things would quickly become hairy if an instruction placed after a synchronization primitive is executed before the synchronization primitive itself. Therefore, all synchronization primitives act as optimization and memory barriers.</p><p>An <em>optimization barrier</em> primitive ensures that the assembly language instructions corresponding to C statements placed before the primitive are not mixed by the compiler with assembly language instructions corresponding to C statements placed after the primitive. In Linux the <code>barrier()</code> macro acts as an optimization barrier.</p><p>A <em>memory barrier</em> primitive ensures that the operations placed before the primitive are finished before starting the operations placed after the primitive. </p><p><img src="/images/2018/12/22.png" alt=""></p><p>Notice that in multiprocessor systems, all atomic operations described in the earlier section “Atomic Operations” act as memory barriers.</p><h3 id="2-4-Spin-Locks"><a href="#2-4-Spin-Locks" class="headerlink" title="2.4 Spin Locks"></a>2.4 Spin Locks</h3><p><em>Spin locks</em> are a special kind of lock designed to work in a multiprocessor environment. If the kernel control path finds the spin lock “open,” it acquires the lock and continues its execution. Conversely, if the kernel control path finds the lock “closed” by a kernel control path running on another CPU, it “spins” around, repeatedly executing a tight instruction loop, until the lock is released.</p><p>The instruction loop of spin locks represents a “busy wait.” The waiting kernel control path keeps running on the CPU, even if it has nothing to do besides waste time. Nevertheless, spin locks are usually convenient, because many kernel resources are locked for a fraction of a millisecond only; therefore, it would be far more time-consuming to release the CPU and reacquire it later.</p><p>As a general rule, <strong>kernel preemption is disabled in every critical region protected by spin locks</strong>. In the case of a uniprocessor system, the locks themselves are useless, and the spin lock primitives just disable or enable the kernel preemption. Please notice that kernel preemption is still enabled during the busy wait phase, thus a process waiting for a spin lock to be released could be replaced by a higher priority process.<br><img src="/images/2018/12/23.png" alt=""></p><h3 id="2-5-Read-Write-Spin-Locks"><a href="#2-5-Read-Write-Spin-Locks" class="headerlink" title="2.5 Read/Write Spin Locks"></a>2.5 Read/Write Spin Locks</h3><p><em>Read/write spin locks</em> have been introduced to increase the amount of concurrency inside the kernel. They allow several kernel control paths to simultaneously read the same data structure, as long as no kernel control path modifies it. If a kernel control path wishes to write to the structure, it must acquire the write version of the read/write lock, which grants exclusive access to the resource. Of course, allowing concurrent reads on data structures improves system performance.</p><p><strong>Getting and releasing a lock for reading</strong></p><ul><li><code>read_lock</code></li><li><code>read_unlock</code></li></ul><p><strong>Getting and releasing a lock for writing</strong></p><ul><li><code>write_lock</code></li><li><code>write_unlock</code></li></ul><h3 id="2-6-Seqlocks"><a href="#2-6-Seqlocks" class="headerlink" title="2.6 Seqlocks"></a>2.6 Seqlocks</h3><p>When using read/write spin locks, requests issued by kernel control paths to perform a <code>read_lock</code> or a <code>write_lock</code> operation have the same priority: readers must wait until the writer has finished and, similarly, a writer must wait until all readers have finished.</p><p><em>Seqlocks</em> are similar to read/write spin locks, except that they give a much higher priority to writers: in fact a writer is allowed to proceed even when readers are active. The good part of this strategy is that a writer never waits (unless another writer is active); the bad part is that a reader may sometimes be forced to read the same data several times until it gets a valid copy.</p><p>The critical regions of the readers should be short and writers should seldom acquire the seqlock</p><h3 id="2-7-Read-Copy-Update-RCU"><a href="#2-7-Read-Copy-Update-RCU" class="headerlink" title="2.7 Read-Copy Update (RCU)"></a>2.7 Read-Copy Update (RCU)</h3><p><a href="http://liujunming.top/2018/12/13/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Linux-%E7%9A%84-RCU-%E6%9C%BA%E5%88%B6/" target="_blank" rel="noopener">深入理解 Linux 的 RCU 机制</a><br><em>Read-copy update (RCU)</em> is yet another synchronization technique designed to protect data structures that are mostly accessed for reading by several CPUs. RCU is lock-free, that is, it uses no lock or counter shared by all CPUs; this is a great advantage over read/write spin locks and seqlocks, which have a high overhead due to cache line-snooping and invalidation.</p><p>How does RCU obtain the surprising result of synchronizing several CPUs without shared data structures? The key idea consists of limiting the scope of RCU as follows:</p><ol><li>Only data structures that are dynamically allocated and referenced by means of pointers can be protected by RCU.</li><li>No kernel control path can sleep inside a critical region protected by RCU.</li></ol><h3 id="2-8-Semaphores"><a href="#2-8-Semaphores" class="headerlink" title="2.8 Semaphores"></a>2.8 Semaphores</h3><p>Essentially, Semaphores implement a locking primitive that allows waiters to sleep until the desired resource becomes free.</p><p>Actually, Linux offers two kinds of semaphores:</p><ul><li>Kernel semaphores, which are used by kernel control paths</li><li>System V IPC semaphores, which are used by User Mode processes</li></ul><p>In this section, we focus on kernel semaphores.</p><p>A kernel semaphore is similar to a spin lock, in that it doesn’t allow a kernel control path to proceed unless the lock is open. However, whenever a kernel control path tries to acquire a busy resource protected by a kernel semaphore, the corresponding process is suspended. It becomes runnable again when the resource is released. Therefore, kernel semaphores can be acquired only by functions that are allowed to sleep; interrupt handlers and deferrable functions cannot use them.</p><p>The <code>init_MUTEX()</code> and <code>init_MUTEX_LOCKED()</code> functions may be used to initialize a semaphore for exclusive access.</p><p><strong>Getting and releasing semaphores</strong></p><ul><li><code>up()</code></li><li><code>down()</code></li></ul><h3 id="2-9-Read-Write-Semaphores"><a href="#2-9-Read-Write-Semaphores" class="headerlink" title="2.9 Read/Write Semaphores"></a>2.9 Read/Write Semaphores</h3><p>Read/write semaphores are similar to the read/write spin locks described earlier in the section “Read/Write Spin Locks,” except that waiting processes are suspended instead of spinning until the semaphore becomes open again.</p><p>Many kernel control paths may concurrently acquire a read/write semaphore for reading; however, every writer kernel control path must have exclusive access to the protected resource. Therefore, the semaphore can be acquired for writing only if no other kernel control path is holding it for either read or write access. Read/write semaphores improve the amount of concurrency inside the kernel and improve overall system performance.</p><p>The <code>down_read()</code> and <code>down_write()</code> functions acquire the read/write semaphore for reading and writing, respectively. Similarly, the <code>up_read()</code> and <code>up_write()</code> functions release a read/write semaphore previously acquired for reading and for writing.</p><h3 id="2-10-Completions"><a href="#2-10-Completions" class="headerlink" title="2.10 Completions"></a>2.10 Completions</h3><p>Linux 2.6 also makes use of another synchronization primitive similar to semaphores: <em>completions</em>. They have been introduced to solve a subtle race condition that occurs in multiprocessor systems when process A allocates a temporary semaphore variable, initializes it as closed MUTEX, passes its address to process B, and then invokes <code>down()</code> on it. Process A plans to destroy the semaphore as soon as it awakens. Later on, process B running on a different CPU invokes <code>up()</code> on the semaphore. However, in the current implementation <code>up()</code> and <code>down()</code> can execute concurrently on the same semaphore. Thus, process A can be woken up and destroy the temporary semaphore while process B is still executing the <code>up()</code> function. As a result, <code>up()</code>might attempt to access a data structure that no longer exists.</p><h3 id="2-11-Local-Interrupt-Disabling"><a href="#2-11-Local-Interrupt-Disabling" class="headerlink" title="2.11 Local Interrupt Disabling"></a>2.11 Local Interrupt Disabling</h3><p>Interrupt disabling is one of the key mechanisms used to ensure that a sequence of kernel statements is treated as a critical section. It allows a kernel control path to continue executing even when hardware devices issue IRQ signals, thus providing an effective way to protect data structures that are also accessed by interrupt handlers. By itself, however, local interrupt disabling does not protect against concurrent accesses to data structures by interrupt handlers running on other CPUs, so in multi-processor systems, local interrupt disabling is often coupled with spin locks.</p><p>The <code>local_irq_disable()</code> macro disables interrupts on the local CPU. The <code>local_irq_enable()</code> macro enables them.</p><h3 id="2-12-Disabling-and-Enabling-Deferrable-Functions"><a href="#2-12-Disabling-and-Enabling-Deferrable-Functions" class="headerlink" title="2.12 Disabling and Enabling Deferrable Functions"></a>2.12 Disabling and Enabling Deferrable Functions</h3><p>Deferrable functions can be executed at unpredictable times (essentially, on termination of hardware interrupt handlers). Therefore, data structures accessed by deferrable functions must be protected against race conditions.<br>The kernel sometimes needs to disable deferrable functions without disabling interrupts. Local deferrable functions can be enabled or disabled on the local CPU by acting on the softirq counter stored in the <code>preempt_count</code> field of the current’s <code>thread_info</code> descriptor.</p><h2 id="3-Synchronizing-Accesses-to-Kernel-Data-Structures"><a href="#3-Synchronizing-Accesses-to-Kernel-Data-Structures" class="headerlink" title="3 Synchronizing Accesses to Kernel Data Structures"></a>3 Synchronizing Accesses to Kernel Data Structures</h2><p>Usually, the following rule of thumb is adopted by kernel developers: <em>always keep the concurrency level as high as possible in the system</em>.</p><p>In turn, the concurrency level in the system depends on two main factors:</p><ul><li>The number of I/O devices that operate concurrently</li><li>The number of CPUs that do productive work</li></ul><p>To maximize I/O throughput, interrupts should be disabled for very short periods of time. To use CPUs efficiently, synchronization primitives based on spin locks should be avoided whenever possible.</p><h3 id="3-1-Choosing-Among-Spin-Locks-Semaphores-and-Interrupt-Disabling"><a href="#3-1-Choosing-Among-Spin-Locks-Semaphores-and-Interrupt-Disabling" class="headerlink" title="3.1 Choosing Among Spin Locks, Semaphores, and Interrupt Disabling"></a>3.1 Choosing Among Spin Locks, Semaphores, and Interrupt Disabling</h3><p>Generally speaking, choosing the synchronization primitives depends on what kinds of kernel control paths access the data structure, as shown in Table 5-8. Remember that whenever a kernel control path acquires a spin lock (as well as a read/write lock, a seqlock, or a RCU “read lock”), disables the local interrupts, or disables the local softirqs, kernel preemption is automatically disabled.</p><p><img src="/images/2018/12/24.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;You could think of the kernel as a server that answers requests; these requests can come either from a process running on a CPU or an external device issuing an interrupt request. We make this analogy to underscore that parts of the kernel are not run serially, but in an interleaved way. Thus, they can give rise to race conditions, which must be controlled through proper synchronization techniques.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/tags/Concurrency/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 Linux 的 RCU 机制</title>
    <link href="http://liujunming.github.io/2018/12/13/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Linux-%E7%9A%84-RCU-%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2018/12/13/深入理解-Linux-的-RCU-机制/</id>
    <published>2018-12-13T10:59:18.000Z</published>
    <updated>2018-12-14T02:37:43.017Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Read-copy update (RCU) is a synchronization mechanism. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, <strong>RCU supports concurrency between a single updater and multiple readers</strong>. <a id="more"></a>RCU ensures that reads are coherent by maintaining multiple versions of objects and ensuring that they are not freed up until all pre-existing read-side critical sections complete. RCU defines and uses efficient and scalable mechanisms for publishing and reading new versions of an object, and also for deferring the collection of old versions. These mechanisms distribute the work among read and update paths in such a way as to make read paths extremely fast.</p><p>RCU is made up of three fundamental mechanisms, the first being used for insertion, the second being used for deletion, and the third being used to allow readers to tolerate concurrent insertions and deletions. These mechanisms are described in the following sections, which focus on applying RCU to linked lists:</p><ol><li>Publish-Subscribe Mechanism (for insertion)</li><li>Wait For Pre-Existing RCU Readers to Complete (for deletion)</li><li>Maintain Multiple Versions of Recently Updated Objects (for readers)</li></ol><h3 id="1-1-Publish-Subscribe-Mechanism"><a href="#1-1-Publish-Subscribe-Mechanism" class="headerlink" title="1.1 Publish-Subscribe Mechanism"></a>1.1 Publish-Subscribe Mechanism</h3><p>One key attribute of RCU is the ability to safely scan data, even though that data is being modified concurrently. To provide this ability for concurrent insertion, RCU uses what can be thought of as a publish-subscribe mechanism. For example, consider an initially <code>NULL</code> global pointer <code>gp</code> that is to be modified to point to a newly allocated and initialized data structure. The following code fragment (with the addition of appropriate locking) might be used for this purpose:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> *<span class="title">gp</span> = <span class="title">NULL</span>;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line">   </span><br><span class="line">p = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">gp = p;</span><br></pre></td></tr></table></figure></p><p>Unfortunately, there is nothing forcing the compiler and CPU to execute the last four assignment statements in order. If the assignment to <code>gp</code> happens before the initialization of <code>p</code>‘s fields, then concurrent readers could see the uninitialized values. Memory barriers are required to keep things ordered, but memory barriers are notoriously difficult to use. We therefore encapsulate them into a primitive <code>rcu_assign_pointer()</code> that has publication semantics. The last four lines would then be as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">rcu_assign_pointer(gp, p);</span><br></pre></td></tr></table></figure></p><p>The <code>rcu_assign_pointer()</code> would <em>publish</em> the new structure, forcing both the compiler and the CPU to execute the assignment to <code>gp</code> after the assignments to the fields referenced by <code>p</code>.</p><p>However, it is not sufficient to only enforce ordering at the updater, as the reader must enforce proper ordering as well. Consider for example the following code fragment:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p = gp;</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Although this code fragment might well seem immune to misordering, unfortunately, the <a href="http://www.rdrop.com/users/paulmck/scalability/paper/ordering.2007.09.19a.pdf" target="_blank" rel="noopener">DEC Alpha CPU</a>  and value-speculation compiler optimizations can cause the values of <code>p-&gt;a</code>, <code>p-&gt;b</code>, and <code>p-&gt;c</code> to be fetched before the value of <code>p</code>!</p><p>Clearly, we need to prevent this sort of skullduggery on the part of both the compiler and the CPU. The <code>rcu_dereference()</code> primitive uses whatever memory-barrier instructions and compiler directives are required for this purpose:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rcu_read_lock();</span><br><span class="line">p = rcu_dereference(gp);</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br></pre></td></tr></table></figure></p><p>The <code>rcu_dereference()</code> primitive can thus be thought of as <em>subscribing</em> to a given value of the specified pointer, guaranteeing that subsequent dereference operations will see any initialization that occurred before the corresponding publish (<code>rcu_assign_pointer()</code>) operation. The <code>rcu_read_lock()</code> and <code>rcu_read_unlock()</code> calls are absolutely required: they define the extent of the RCU read-side critical section. Their purpose is explained in the next section, however, they never spin or block, nor do they prevent the <code>list_add_rcu()</code> from executing concurrently. </p><p>Although <code>rcu_assign_pointer()</code> and <code>rcu_dereference()</code> can in theory be used to construct any conceivable RCU-protected data structure, in practice it is often better to use higher-level constructs. Therefore, the <code>rcu_assign_pointer()</code> and <code>rcu_dereference()</code> primitives have been embedded in special RCU variants of Linux’s list-manipulation API. Linux has two variants of doubly linked list, the circular <code>struct list_head</code> and the linear <code>struct hlist_head</code>/<code>struct hlist_node</code> pair. The former is laid out as follows, where the green boxes represent the list header and the blue boxes represent the elements in the list.</p><center><img src="/images/2018/12/6.jpg" alt=""></center><p>Adapting the pointer-publish example for the linked list gives the following:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">list</span>;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line">LIST_HEAD(head);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line"> </span><br><span class="line">p = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_add_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;head);</span><br></pre></td></tr></table></figure></p><p>Line 15 must be protected by some synchronization mechanism (most commonly some sort of lock) to prevent multiple <code>list_add()</code> instances from executing concurrently. However, such synchronization does not prevent this <code>list_add()</code> from executing concurrently with RCU readers.</p><p>Subscribing to an RCU-protected list is straightforward:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rcu_read_lock();</span><br><span class="line">list_for_each_entry_rcu(p, head, <span class="built_in">list</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br></pre></td></tr></table></figure></p><p>The <code>list_add_rcu()</code> primitive publishes an entry into the specified list, guaranteeing that the corresponding <code>list_for_each_entry_rcu()</code> invocation will properly subscribe to this same entry.</p><p>The set of RCU publish and subscribe primitives are shown in the following table, along with additional primitives to “unpublish”, or retract:</p><center><img src="/images/2018/12/7.png" alt=""></center><p>Note that the <code>list_replace_rcu()</code>, <code>list_del_rcu()</code>, <code>hlist_replace_rcu()</code>, and <code>hlist_del_rcu()</code> APIs add a complication. When is it safe to free up the data element that was replaced or removed? In particular, how can we possibly know when all the readers have released their references to that data element?</p><p>These questions are addressed in the following section.</p><h3 id="1-2-Wait-For-Pre-Existing-RCU-Readers-to-Complete"><a href="#1-2-Wait-For-Pre-Existing-RCU-Readers-to-Complete" class="headerlink" title="1.2 Wait For Pre-Existing RCU Readers to Complete"></a>1.2 Wait For Pre-Existing RCU Readers to Complete</h3><p>In its most basic form, RCU is a way of waiting for things to finish. Of course, there are a great many other ways of waiting for things to finish, including reference counts, reader-writer locks, events, and so on. The great advantage of RCU is that it can wait for each of (say) 20,000 different things without having to explicitly track each and every one of them, and without having to worry about the performance degradation, scalability limitations, complex deadlock scenarios, and memory-leak hazards that are inherent in schemes using explicit tracking.</p><p>In RCU’s case, the things waited on are called “RCU read-side critical sections”. An RCU read-side critical section starts with an <code>rcu_read_lock()</code> primitive, and ends with a corresponding <code>rcu_read_unlock()</code> primitive. RCU read-side critical sections can be nested, and may contain pretty much any code, as long as that code does not explicitly block or sleep (although a special form of RCU called “<a href="https://lwn.net/Articles/202847/" target="_blank" rel="noopener">SRCU</a>“ does permit general sleeping in SRCU read-side critical sections). If you abide by these conventions, you can use RCU to wait for <em>any</em> desired piece of code to complete.</p><p>RCU accomplishes this feat by indirectly determining when these other things have finished, as has been described elsewhere for <a href="http://www.rdrop.com/users/paulmck/RCU/whatisRCU.html" target="_blank" rel="noopener">RCU Classic</a> and <a href="https://lwn.net/Articles/253651/" target="_blank" rel="noopener">realtime RCU</a>.</p><p>In particular, as shown in the following figure, RCU is a way of waiting for pre-existing RCU read-side critical sections to completely finish, including memory operations executed by those critical sections.</p><p><center><img src="/images/2018/12/8.png" alt=""></center><br>However, note that RCU read-side critical sections that begin after the beginning of a given grace period can and will extend beyond the end of that grace period.</p><p>The following pseudocode shows the basic form of algorithms that use RCU to wait for readers:</p><ol><li>Make a change, for example, replace an element in a linked list.</li><li>Wait for all pre-existing RCU read-side critical sections to completely finish (for example, by using the <code>synchronize_rcu()</code> primitive). The key observation here is that subsequent RCU read-side critical sections have no way to gain a reference to the newly removed element.</li><li>Clean up, for example, free the element that was replaced above.</li></ol><p>The following code fragment, adapted from those in the previous section, demonstrates this process, with field <code>a</code> being the search key:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">list</span>;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line">LIST_HEAD(head);</span><br><span class="line"> </span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line"> </span><br><span class="line">p = search(head, key);</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span>) &#123;</span><br><span class="line">    <span class="comment">/* Take appropriate action, unlock, and return. */</span></span><br><span class="line">&#125;</span><br><span class="line">q = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">*q = *p;</span><br><span class="line">q-&gt;b = <span class="number">2</span>;</span><br><span class="line">q-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_replace_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;q-&gt;<span class="built_in">list</span>);</span><br><span class="line">synchronize_rcu();</span><br><span class="line">kfree(p);</span><br></pre></td></tr></table></figure></p><p>Lines 19, 20, and 21 implement the three steps called out above. Lines 16-19 gives RCU (“read-copy update”) its name: while permitting concurrent reads, line 16 <em>copies</em> and lines 17-19 do an <em>update</em>.</p><p>The <code>synchronize_rcu()</code>must wait for all RCU read-side critical sections to complete.</p><p>RCU Classic read-side critical sections delimited by <code>rcu_read_lock()</code> and <code>rcu_read_unlock()</code> are not permitted to block or sleep.</p><p>What exactly do RCU readers see when traversing a concurrently updated list? This question is addressed in the following section.</p><h3 id="1-3-Maintain-Multiple-Versions-of-Recently-Updated-Objects"><a href="#1-3-Maintain-Multiple-Versions-of-Recently-Updated-Objects" class="headerlink" title="1.3 Maintain Multiple Versions of Recently Updated Objects"></a>1.3 Maintain Multiple Versions of Recently Updated Objects</h3><p>This section demonstrates how RCU maintains multiple versions of lists to accommodate synchronization-free readers. Two examples are presented showing how an element that might be referenced by a given reader must remain intact while that reader remains in its RCU read-side critical section. The first example demonstrates deletion of a list element, and the second example demonstrates replacement of an element.</p><h4 id="Example-1-Maintaining-Multiple-Versions-During-Deletion"><a href="#Example-1-Maintaining-Multiple-Versions-During-Deletion" class="headerlink" title="Example 1: Maintaining Multiple Versions During Deletion"></a>Example 1: Maintaining Multiple Versions During Deletion</h4><p>To start the “deletion” example, we will modify lines 11-21 in the example in the previous section as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">p = search(head, key);</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    list_del_rcu(&amp;p-&gt;<span class="built_in">list</span>);</span><br><span class="line">    synchronize_rcu();</span><br><span class="line">    kfree(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>The initial state of the list, including the pointer <code>p</code>, is as follows.</p><p><center><img src="/images/2018/12/9.jpg" alt=""></center><br>The triples in each element represent the values of fields <code>a</code>, <code>b</code>, and <code>c</code>, respectively. The red borders on each element indicate that readers might be holding references to them, and because readers do not synchronize directly with updaters, readers might run concurrently with this entire replacement process. Please note that we have omitted the backwards pointers and the link from the tail of the list to the head for clarity.</p><p>After the <code>list_del_rcu()</code> on line 3 has completed, the <code>5</code>,<code>6</code>,<code>7</code> element has been removed from the list, as shown below. Since readers do not synchronize directly with updaters, readers might be concurrently scanning this list. These concurrent readers might or might not see the newly removed element, depending on timing. However, readers that were delayed just after fetching a pointer to the newly removed element might see the old version of the list for quite some time after the removal. Therefore, we now have two versions of the list, one with element <code>5</code>,<code>6</code>,<code>7</code> and one without. The border of the <code>5</code>,<code>6</code>,<code>7</code> element is still red, indicating that readers might be referencing it.</p><p><center><img src="/images/2018/12/10.jpg" alt=""></center><br>Please note that readers are not permitted to maintain references to element ,<code>5</code>,<code>6</code>,<code>7</code> after exiting from their RCU read-side critical sections. Therefore, once the <code>synchronize_rcu()</code> on line 4 completes, so that all pre-existing readers are guaranteed to have completed, there can be no more readers referencing this element, as indicated by its black border below. We are thus back to a single version of the list.</p><p><center><img src="/images/2018/12/11.jpg" alt=""></center><br>At this point, the <code>5</code>,<code>6</code>,<code>7</code> element may safely be freed, as shown below:</p><p><center><img src="/images/2018/12/12.jpg" alt=""></center><br>At this point, we have completed the deletion of element <code>5</code>,<code>6</code>,<code>7</code>. The following section covers replacement.</p><h4 id="Example-2-Maintaining-Multiple-Versions-During-Replacement"><a href="#Example-2-Maintaining-Multiple-Versions-During-Replacement" class="headerlink" title="Example 2: Maintaining Multiple Versions During Replacement"></a>Example 2: Maintaining Multiple Versions During Replacement</h4><p>To start the replacement example, here are the last few lines of the example in the previous section:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">*q = *p;</span><br><span class="line">q-&gt;b = <span class="number">2</span>;</span><br><span class="line">q-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_replace_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;q-&gt;<span class="built_in">list</span>);</span><br><span class="line">synchronize_rcu();</span><br><span class="line">kfree(p);</span><br></pre></td></tr></table></figure></p><p>The initial state of the list, including the pointer <code>p</code>, is the same as for the deletion example:</p><p><center><img src="/images/2018/12/13.jpg" alt=""></center><br>As before, the triples in each element represent the values of fields <code>a</code>, <code>b</code>, and <code>c</code>, respectively. The red borders on each element indicate that readers might be holding references to them, and because readers do not synchronize directly with updaters, readers might run concurrently with this entire replacement process. Please note that we again omit the backwards pointers and the link from the tail of the list to the head for clarity.<br>Line 1 <code>kmalloc()</code>s a replacement element, as follows:</p><p><center><img src="/images/2018/12/14.jpg" alt=""></center><br>Line 2 copies the old element to the new one:</p><p><center><img src="/images/2018/12/15.jpg" alt=""></center><br>Line 3 updates <code>q-&gt;b</code> to the value “2”:</p><p><center><img src="/images/2018/12/16.jpg" alt=""></center><br>Line 4 updates q-&gt;c to the value “3”:</p><p><center><img src="/images/2018/12/17.jpg" alt=""></center><br>Now, line 5 does the replacement, so that the new element is finally visible to readers. At this point, as shown below, we have two versions of the list. Pre-existing readers might see the <code>5</code>,<code>6</code>,<code>7</code> element, but new readers will instead see the <code>5</code>,<code>2</code>,<code>3</code> element. But any given reader is guaranteed to see some well-defined list.</p><p><center><img src="/images/2018/12/18.jpg" alt=""></center><br>After the <code>synchronize_rcu()</code> on line 6 returns, a grace period will have elapsed, and so all reads that started before the <code>list_replace_rcu()</code> will have completed. In particular, any readers that might have been holding references to the <code>5</code>,<code>6</code>,<code>7</code> element are guaranteed to have exited their RCU read-side critical sections, and are thus prohibited from continuing to hold a reference. Therefore, there can no longer be any readers holding references to the old element, as indicated by the thin black border around the <code>5</code>,<code>6</code>,<code>7</code> element below. As far as the readers are concerned, we are back to having a single version of the list, but with the new element in place of the old.</p><p><center><img src="/images/2018/12/19.jpg" alt=""></center><br>After the <code>kfree()</code> on line 7 completes, the list will appear as follows:</p><p><center><img src="/images/2018/12/20.jpg" alt=""></center></p><h2 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2 Usage"></a>2 Usage</h2><p><a href="http://lwn.net/Articles/263130/" target="_blank" rel="noopener">What is RCU? Part 2: Usage</a><br><a href="https://lwn.net/Articles/609973/" target="_blank" rel="noopener">The RCU API tables</a><br><a href="https://github.com/jinb-park/rcu_example/blob/master/list_rcu_example.c" target="_blank" rel="noopener">list_rcu_example</a>是一个具体实例，可以仔细研究下代码。</p><hr><p>参考资料：</p><ol><li><a href="https://www.kernel.org/doc/Documentation/RCU/whatisRCU.txt" target="_blank" rel="noopener">whatisRCU</a></li><li><a href="https://www.wikiwand.com/en/Read-copy-update" target="_blank" rel="noopener">wikiwand Read-copy-update</a></li><li><a href="https://lwn.net/Articles/262464/" target="_blank" rel="noopener">What is RCU, Fundamentally?</a></li><li><a href="https://pdos.csail.mit.edu/6.828/2018/readings/rcu-decade-later.pdf" target="_blank" rel="noopener">RCU Usage In the Linux Kernel: One Decade Later</a></li><li><a href="https://github.com/jinb-park/rcu_example" target="_blank" rel="noopener">rcu_example</a></li><li><a href="https://www.cnblogs.com/qcloud1001/p/7755331.html" target="_blank" rel="noopener">深入理解 Linux 的 RCU 机制</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1 Introduction&quot;&gt;&lt;/a&gt;1 Introduction&lt;/h2&gt;&lt;p&gt;Read-copy update (RCU) is a synchronization mechanism. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, &lt;strong&gt;RCU supports concurrency between a single updater and multiple readers&lt;/strong&gt;.
    
    </summary>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/categories/Concurrency/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/tags/Concurrency/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Interrupts and Exceptions</title>
    <link href="http://liujunming.github.io/2018/12/04/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Interrupts-and-Exceptions/"/>
    <id>http://liujunming.github.io/2018/12/04/Understanding-the-Linux-Kernel-读书笔记-Interrupts-and-Exceptions/</id>
    <published>2018-12-04T09:01:41.000Z</published>
    <updated>2018-12-14T05:56:02.319Z</updated>
    
    <content type="html"><![CDATA[<p>An <em>interrupt</em> is usually defined as an event that alters the sequence of instructions executed by a processor.</p><p>Intel microprocessor manuals designate synchronous and asynchronous interrupts as <em>exceptions</em> and <em>interrupts</em>.<a id="more"></a> We’ll occasionally use the term “interrupt signal” to designate both types together (synchronous as well as asynchronous).</p><p>Interrupts are issued by interval timers and I/O devices.</p><p>Exceptions, on the other hand, are caused either by programming errors or by anomalous conditions that must be handled by the kernel.</p><h2 id="1-The-Role-of-Interrupt-Signals"><a href="#1-The-Role-of-Interrupt-Signals" class="headerlink" title="1 The Role of Interrupt Signals"></a>1 The Role of Interrupt Signals</h2><p>When an interrupt signal arrives, the CPU must stop what it’s currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the <code>eip</code> and <code>cs</code> registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter.</p><p>There is a key difference between interrupt handling and process switching: the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred. As a kernel control path, the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down).</p><h2 id="2-Interrupts-and-Exceptions"><a href="#2-Interrupts-and-Exceptions" class="headerlink" title="2 Interrupts and Exceptions"></a>2 Interrupts and Exceptions</h2><ul><li>Interrupts</li><li>Exceptions<ul><li>Processor-detected exceptions<ul><li>Faults</li><li>Traps</li><li>Aborts</li></ul></li><li>Programmed exceptions</li></ul></li></ul><p><strong>Processor-detected exceptions:</strong> These are further divided into three groups, depending on the value of the eip register that is saved on the Kernel Mode stack when the CPU control unit raises the exception.</p><p><strong>Traps:</strong>The saved value of eip is the address of the instruction that should be executed after the one that caused the trap.</p><p><strong>Aborts:</strong>A serious error occurred; the control unit is in trouble, and it may be unable to store in the eip register the precise location of the instruction causing the exception. Aborts are used to report severe errors, such as hardware failures and invalid or inconsistent values in system tables.</p><p><strong>Programmed exceptions:</strong>Occur at the request of the programmer. Programmed exceptions are handled by the control unit as traps; they are often called <code>software interrupts</code>. Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event.</p><p>Each interrupt or exception is identified by a number ranging from 0 to 255; Intel calls this 8-bit unsigned number a <em>vector</em>. The vectors of nonmaskable interrupts and exceptions are fixed, while those of maskable interrupts can be altered by programming the Interrupt Controller.</p><h3 id="2-1-IRQs-and-Interrupts"><a href="#2-1-IRQs-and-Interrupts" class="headerlink" title="2.1 IRQs and Interrupts"></a>2.1 IRQs and Interrupts</h3><p>Each hardware device controller capable of issuing interrupt requests usually has a single output line designated as the <code>Interrupt ReQuest (IRQ)</code> line.All existing IRQ lines are connected to the input pins of a hardware circuit called the <code>Programmable Interrupt Controller(PIC)</code>.</p><center><img src="/images/2018/12/1.JPG" alt=""></center><p><strong>The Advanced Programmable Interrupt Controller (APIC)</strong><br> However, if the system includes two or more CPUs, this approach is no longer valid and more sophisticated PICs are needed.<br> <center><img src="/images/2018/12/2.png" alt=""></center></p><p> Besides distributing interrupts among processors, the multi-APIC system allows CPUs to generate <code>interprocessor interrupts(IPI)</code>. </p><h3 id="2-2-Exceptions"><a href="#2-2-Exceptions" class="headerlink" title="2.2 Exceptions"></a>2.2 Exceptions</h3><p>Each exception is handled by a specific exception handler, which usually sends a Unix signal to the process that caused the exception.</p><h3 id="2-3-Interrupt-Descriptor-Table"><a href="#2-3-Interrupt-Descriptor-Table" class="headerlink" title="2.3 Interrupt Descriptor Table"></a>2.3 Interrupt Descriptor Table</h3><p>A system table called <code>Interrupt Descriptor Table (IDT)</code> associates each interrupt or exception vector with the address of the corresponding interrupt or exception handler. </p><p>The IDT may include three types of descriptors;<br> <center><img src="/images/2018/12/3.png" alt=""></center><br>Linux uses interrupt gates to handle interrupts and trap gates to handle exceptions.</p><h3 id="2-4-Hardware-Handling-of-Interrupts-and-Exceptions"><a href="#2-4-Hardware-Handling-of-Interrupts-and-Exceptions" class="headerlink" title="2.4 Hardware Handling of Interrupts and Exceptions"></a>2.4 Hardware Handling of Interrupts and Exceptions</h3><p>After the interrupt or exception is processed, the corresponding handler must relinquish control to the interrupted process.</p><h2 id="3-Nested-Execution-of-Exception-and-Interrupt-Handlers"><a href="#3-Nested-Execution-of-Exception-and-Interrupt-Handlers" class="headerlink" title="3 Nested Execution of Exception and Interrupt Handlers"></a>3 Nested Execution of Exception and Interrupt Handlers</h2><p>Every interrupt or exception gives rise to a kernel control path or separate sequence of instructions that execute in Kernel Mode on behalf of the current process.<br> <center><img src="/images/2018/12/4.png" alt=""></center><br>The price to pay for allowing nested kernel control paths is that an interrupt handler must never block, that is, 中断处理程序运行期间不能发生进程切换.</p><p>An interrupt handler may preempt both other interrupt handlers and exception handlers. Conversely, an exception handler never preempts an interrupt handler. Interrupt handlers never perform operations that can induce page faults, and thus, potentially, a process switch.</p><p>On multiprocessor systems, several kernel control paths may execute concurrently. Moreover, a kernel control path associated with an exception may start executing on a CPU and, due to a process switch, migrate to another CPU.</p><h2 id="4-Initializing-the-Interrupt-Descriptor-Table"><a href="#4-Initializing-the-Interrupt-Descriptor-Table" class="headerlink" title="4 Initializing the Interrupt Descriptor Table"></a>4 Initializing the Interrupt Descriptor Table</h2><h2 id="5-Exception-Handling"><a href="#5-Exception-Handling" class="headerlink" title="5 Exception Handling"></a>5 Exception Handling</h2><p>When one of them occurs, the kernel sends a signal to the process that caused the exception to notify it of an anomalous condition. </p><p>Exception handlers have a standard structure consisting of three steps:</p><ol><li>Save the contents of most registers in the Kernel Mode stack (this part is coded in assembly language).</li><li>Handle the exception by means of a high-level C function.</li><li>Exit from the handler by means of the <code>ret_from_exception()</code> function.</li></ol><h3 id="5-1-Saving-the-Registers-for-the-Exception-Handler"><a href="#5-1-Saving-the-Registers-for-the-Exception-Handler" class="headerlink" title="5.1 Saving the Registers for the Exception Handler"></a>5.1 Saving the Registers for the Exception Handler</h3><h3 id="5-2-Entering-and-Leaving-the-Exception-Handler"><a href="#5-2-Entering-and-Leaving-the-Exception-Handler" class="headerlink" title="5.2 Entering and Leaving the Exception Handler"></a>5.2 Entering and Leaving the Exception Handler</h3><p>Store the hardware error code and the exception vector in the process descriptor of current, and then send a suitable signal to that process:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">current-&gt;thread.error_code = error_code;</span><br><span class="line">current-&gt;thread.trap_no = <span class="built_in">vector</span>;</span><br><span class="line">force_sig(sig_number, current);</span><br></pre></td></tr></table></figure></p><p>The current process takes care of the signal right after the termination of the exception handler. The signal will be handled either in User Mode by the process’s own signal handler (if it exists) or in Kernel Mode. In the latter case, the kernel usually kills the process.</p><h2 id="6-Interrupt-Handling"><a href="#6-Interrupt-Handling" class="headerlink" title="6 Interrupt Handling"></a>6 Interrupt Handling</h2><p>Most exceptions are handled simply by sending a Unix signal to the process that caused the exception. The action to be taken is thus deferred until the process receives the signal; as a result, the kernel is able to process the exception quickly.</p><p>Interrupt handling depends on the type of interrupt. </p><ul><li>I/O interrupts</li><li>Timer interrupts</li><li>Interprocessor interrupts</li></ul><h3 id="6-1-I-O-Interrupt-Handling"><a href="#6-1-I-O-Interrupt-Handling" class="headerlink" title="6.1 I/O Interrupt Handling"></a>6.1 I/O Interrupt Handling</h3><p>In the PCI bus architecture, for instance, several devices may share the same IRQ line. This means that the interrupt vector alone does not tell the whole story. </p><ul><li>IRQ sharing</li><li>IRQ dynamic allocation</li></ul><p>The interrupt handler executes several <code>interrupt service routines (ISRs)</code>.</p><p>Not all actions to be performed when an interrupt occurs have the same urgency.Long noncritical operations should be deferred, because while an interrupt handler is running, the signals on the corresponding IRQ line are temporarily ignored. Most important, the process on behalf of which an interrupt handler is executed must always stay in the TASK_RUNNING state, or a system freeze can occur.Therefore, interrupt handlers cannot perform any blocking procedure such as an I/O disk operation. Linux divides the actions to be performed following an interrupt into three classes:</p><ul><li>Critical</li><li>Noncritical</li><li>Noncritical deferrable</li></ul><p>Regardless of the kind of circuit that caused the interrupt, all I/O interrupt handlers perform the same four basic actions:</p><ol><li>Save the IRQ value and the register’s contents on the Kernel Mode stack.</li><li>Send an acknowledgment to the PIC that is servicing the IRQ line, thus allowing it to issue further interrupts.</li><li>Execute the interrupt service routines (ISRs) associated with all the devices that share the IRQ.</li><li>Terminate by jumping to the <code>ret_from_intr()</code> address.</li></ol><center><img src="/images/2018/12/5.png" alt=""></center><h2 id="7-Softirqs-and-Tasklets"><a href="#7-Softirqs-and-Tasklets" class="headerlink" title="7 Softirqs and Tasklets"></a>7 Softirqs and Tasklets</h2><p>We mentioned earlier in the section “Interrupt Handling” that several tasks among those executed by the kernel are not critical: they can be deferred for a long period of time, if necessary.</p><p>The deferrable tasks can execute with all interrupts enabled. Taking them out of the interrupt handler helps keep kernel response time small. This is a very important property for many time-critical applications that expect their interrupt requests to be serviced in a few milliseconds.</p><p>Linux 2.6 answers such a challenge by using two kinds of non-urgent interruptible kernel functions: the so-called deferrable functions (softirqs and tasklets), and those executed by means of some work queues.</p><p>Softirqs and tasklets are strictly correlated, because tasklets are implemented on top of softirqs. As a matter of fact, the term “softirq,” which appears in the kernel source code, often denotes both kinds of deferrable functions.</p><h2 id="8-Work-Queues"><a href="#8-Work-Queues" class="headerlink" title="8 Work Queues"></a>8 Work Queues</h2><p>The <em>work queues</em> allow kernel functions to be activated (much like deferrable functions) and later executed by special kernel threads called <em>worker threads</em>.</p><p>Despite their similarities, deferrable functions and work queues are quite different. The main difference is that deferrable functions run in interrupt context while functions in work queues run in process context. Running in process context is the only way to execute functions that can block. No process switch can take place in interrupt context. A function in a work queue is executed by a kernel thread,</p><hr><p>参考资料：</p><ol><li><a href="http://home.ustc.edu.cn/~boj/courses/linux_kernel/2_int.html" target="_blank" rel="noopener">Linux源代码阅读——中断</a></li><li><a href="http://www.wowotech.net/irq_subsystem/interrupt_subsystem_architecture.html" target="_blank" rel="noopener">Linux kernel的中断子系统</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-linuxkernelint/index.html" target="_blank" rel="noopener">Linux 内核中断内幕</a></li><li><a href="https://my.oschina.net/fileoptions/blog/918164" target="_blank" rel="noopener">linux内核之中断实现原理</a></li><li><a href="https://www.tldp.org/LDP/lkmpg/2.6/html/x1256.html" target="_blank" rel="noopener">Interrupt Handlers</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;An &lt;em&gt;interrupt&lt;/em&gt; is usually defined as an event that alters the sequence of instructions executed by a processor.&lt;/p&gt;
&lt;p&gt;Intel microprocessor manuals designate synchronous and asynchronous interrupts as &lt;em&gt;exceptions&lt;/em&gt; and &lt;em&gt;interrupts&lt;/em&gt;.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Fix-Mapped Linear Addresses</title>
    <link href="http://liujunming.github.io/2018/11/28/Fix-Mapped-Linear-Addresses/"/>
    <id>http://liujunming.github.io/2018/11/28/Fix-Mapped-Linear-Addresses/</id>
    <published>2018-11-28T02:15:51.000Z</published>
    <updated>2018-11-28T03:59:22.179Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>fixmap是一段固定地址映射，kernel预留的一段虚拟地址空间，虚拟地址是在编译的时候确定。fixmap可以用来做什么？kernel启动初期，由于此时的kernel已经运行在虚拟地址上。因此我们访问具体的物理地址是不行的，必须建立虚拟地址和物理地址的映射，然后通过虚拟地址访问才可以。例如：dtb中包含bootloader传递过来的内存信息，我们需要解析dtb，但是我们得到的是dtb的物理地址。因此访问之前必须创建映射，创建映射又需要内存系统。但是由于所有的内存管理子系统还没有ready，因此我们不能使用ioremap接口创建映射，为此kernel提出fixmap的解决方案。<br><a id="more"></a></p><h2 id="fixmap空间分配"><a href="#fixmap空间分配" class="headerlink" title="fixmap空间分配"></a>fixmap空间分配</h2><p>fixmap虚拟地址空间又被平均分成两个部分permanent fixed addresses和temporary fixed addresses。permanent fixed addresses是永久映射，temporary fixed addresses是临时映射。永久映射是指在建立的映射关系在kernel阶段不会改变，仅供特定模块一直使用。临时映射就是模块使用前创建映射，使用后解除映射。</p><p>With respect to variable pointers, fix-mapped linear addresses are more efficient. In fact, dereferencing a variable pointer requires one memory access more than dereferencing an immediate constant address. Moreover, checking the value of a variable pointer before dereferencing it is a good programming practice; conversely, the check is never required for a constant linear address.</p><p>具体函数可以参考Understanding the Linux Kernel p72.</p><hr><p>参考资料：</p><ol><li><a href="https://zohead.com/archives/linux-kernel-learning-memory-addressing/" target="_blank" rel="noopener">zohead</a></li><li><a href="https://www.spinics.net/lists/newbies/msg31797.html" target="_blank" rel="noopener">What is fixmaps?</a></li><li><a href="http://www.wowotech.net/memory_management/440.html" target="_blank" rel="noopener">fixmap addresses原理</a></li><li><a href="http://students.mimuw.edu.pl/ZSO/Wyklady/04_pamiec/4_pamiec_en.html#highmem" target="_blank" rel="noopener">Mapping frames from highmem</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;fixmap是一段固定地址映射，kernel预留的一段虚拟地址空间，虚拟地址是在编译的时候确定。fixmap可以用来做什么？kernel启动初期，由于此时的kernel已经运行在虚拟地址上。因此我们访问具体的物理地址是不行的，必须建立虚拟地址和物理地址的映射，然后通过虚拟地址访问才可以。例如：dtb中包含bootloader传递过来的内存信息，我们需要解析dtb，但是我们得到的是dtb的物理地址。因此访问之前必须创建映射，创建映射又需要内存系统。但是由于所有的内存管理子系统还没有ready，因此我们不能使用ioremap接口创建映射，为此kernel提出fixmap的解决方案。&lt;br&gt;
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>内核页表和进程页表</title>
    <link href="http://liujunming.github.io/2018/11/27/%E5%86%85%E6%A0%B8%E9%A1%B5%E8%A1%A8%E5%92%8C%E8%BF%9B%E7%A8%8B%E9%A1%B5%E8%A1%A8/"/>
    <id>http://liujunming.github.io/2018/11/27/内核页表和进程页表/</id>
    <published>2018-11-27T14:00:35.000Z</published>
    <updated>2018-11-28T02:41:18.710Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自：<a href="http://blog.chinaunix.net/uid-14528823-id-4334619.html" target="_blank" rel="noopener">chinaunix</a></p><p>初学内核时，经常被“内核页表”和“进程页表”搞晕，不知道这到底是个啥东东，跟我们平时理解的页表有和关系。</p><ul><li><p>内核页表：即书上说的<strong>主内核页表</strong>，在内核中其实就是一段内存，存放在主内核页全局目录init_mm.pgd(swapper_pg_dir)中，硬件并不直接使用。</p></li><li><p>进程页表：每个进程自己的页表，放在进程自身的页目录task_struct.pgd中。</p></li></ul><a id="more"></a><p>在保护模式下，从硬件角度看，其运行的基本对象为“进程”(或线程)，而寻址则依赖于“进程页表”，在进程调度而进行上下文切换时，会进行页表的切换：即将新进程的pgd(页目录)加载到CR3寄存器中。从这个角度看，其实是完全没有用到“内核页表”的，那么“内核页表”有什么用呢？跟“进程页表”有什么关系呢？</p><p>1、内核页表中的内容为所有进程共享，每个进程都有自己的“进程页表”，“进程页表”中映射的线性地址包括两部分：</p><ul><li>用户态</li><li>内核态</li></ul><p>其中，内核态地址对应的相关页表项，对于所有进程来说都是相同的(因为内核空间对所有进程来说都是共享的)，而这部分页表内容其实就来源于“内核页表”，即每个进程的“进程页表”中内核态地址相关的页表项都是“内核页表”的一个拷贝。<br>2、“内核页表”由内核自己维护并更新，在vmalloc区发生page fault时，将“内核页表”同步到“进程页表”中。以32位系统为例，内核页表主要包含两部分：</p><ul><li>线性映射区</li><li>vmalloc区</li></ul><p>其中，线性映射区即通过<code>TASK_SIZE</code>偏移进行映射的区域，对32系统来说就是0-896M这部分区域，映射对应的虚拟地址区域为<code>TASK_SIZE~TASK_SIZE+896M</code>。这部分区域在内核初始化时就已经完成映射，并创建好相应的页表，即这部分虚拟内存区域不会发生page fault。</p><p>vmalloc区，为<code>896M~896M+128M</code>，这部分区域用于映射高端内存，有三种映射方式：vmalloc、固定、临时，这里就不详述了。。<br>以vmalloc为例(最常使用)，这部分区域对应的线性地址在内核使用vmalloc分配内存时，其实就已经分配了相应的物理内存，并做了相应的映射，建立了相应的页表项，但相关页表项仅写入了“内核页表”，并没有实时更新到“进程页表中”，内核在这里使用了“延迟更新”的策略，将“进程页表”真正更新推迟到第一次访问相关线性地址，发生page fault时，此时在<a href="https://elixir.bootlin.com/linux/v4.19.4/source/arch/x86/mm/fault.c#L1244" target="_blank" rel="noopener">page fault</a>的处理流程中进行“进程页表”的更新：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 缺页地址位于内核空间。并不代表异常发生于内核空间，有可能是用户</span></span><br><span class="line"><span class="comment"> * 态访问了内核空间的地址。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (unlikely(fault_in_kernel_space(address))) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!(error_code &amp; (PF_RSVD | PF_USER | PF_PROT))) &#123;</span><br><span class="line">        <span class="comment">//检查发生缺页的地址是否在vmalloc区，是则进行相应的处理</span></span><br><span class="line">        <span class="keyword">if</span> (vmalloc_fault(address) &gt;= <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 对于发生缺页异常的指针位于vmalloc区情况的处理，主要是将</span></span><br><span class="line"><span class="comment">  * 主内核页表向当前进程的内核页表同步。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">static</span> noinline __<span class="function">kprobes <span class="keyword">int</span> <span class="title">vmalloc_fault</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> address)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> pgd_paddr;</span><br><span class="line">    <span class="keyword">pmd_t</span> *pmd_k;</span><br><span class="line">    <span class="keyword">pte_t</span> *pte_k;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Make sure we are in vmalloc area: */</span></span><br><span class="line">    <span class="comment">/* 区域检查 */</span></span><br><span class="line">    <span class="keyword">if</span> (!(address &gt;= VMALLOC_START &amp;&amp; address &lt; VMALLOC_END))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    WARN_ON_ONCE(in_nmi());</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Synchronize this task's top level page-table</span></span><br><span class="line"><span class="comment">     * with the 'reference' page table.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * Do _not_ use "current" here. We might be inside</span></span><br><span class="line"><span class="comment">     * an interrupt in the middle of a task switch..</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">     <span class="comment">/*获取pgd(最顶级页目录)地址，直接从CR3寄存器中读取。</span></span><br><span class="line"><span class="comment">     *不要通过current获取，因为缺页异常可能在上下文切换的过程中发生，</span></span><br><span class="line"><span class="comment">     *此时如果通过current获取，则可能会出问题*/</span></span><br><span class="line">    pgd_paddr = read_cr3();</span><br><span class="line">    <span class="comment">//从主内核页表中，同步vmalloc区发生缺页异常地址对应的页表</span></span><br><span class="line">    pmd_k = vmalloc_sync_one(__va(pgd_paddr), address);</span><br><span class="line">    <span class="keyword">if</span> (!pmd_k)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">//如果同步后，相应的PTE还不存在，则说明该地址有问题了</span></span><br><span class="line">    pte_k = pte_offset_kernel(pmd_k, address);</span><br><span class="line">    <span class="keyword">if</span> (!pte_present(*pte_k))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时，问题来了，为什么需要内核页表呢？<br>详情可以参考<a href="http://bbs.chinaunix.net/thread-4190879-1-1.html" target="_blank" rel="noopener">关于内核页表初始化的问题</a>。<br><strong>目的</strong>主要是为了让cpu从real mode平稳过渡到protected mode，read mode下分页尚未开启。</p><p>更多细节请参考<a href="http://students.mimuw.edu.pl/ZSO/Wyklady/04_pamiec/4_pamiec_en.html#tablice_stron_jadra" target="_blank" rel="noopener">Kernel page tables</a></p><hr><p>参考资料：</p><ol><li><a href="http://bbs.chinaunix.net/thread-4190879-1-1.html" target="_blank" rel="noopener">关于内核页表初始化的问题</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自：&lt;a href=&quot;http://blog.chinaunix.net/uid-14528823-id-4334619.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;chinaunix&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;初学内核时，经常被“内核页表”和“进程页表”搞晕，不知道这到底是个啥东东，跟我们平时理解的页表有和关系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;内核页表：即书上说的&lt;strong&gt;主内核页表&lt;/strong&gt;，在内核中其实就是一段内存，存放在主内核页全局目录init_mm.pgd(swapper_pg_dir)中，硬件并不直接使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;进程页表：每个进程自己的页表，放在进程自身的页目录task_struct.pgd中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Memory Addressing</title>
    <link href="http://liujunming.github.io/2018/11/27/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Memory-Addressing/"/>
    <id>http://liujunming.github.io/2018/11/27/Understanding-the-Linux-Kernel-读书笔记-Memory-Addressing/</id>
    <published>2018-11-27T11:24:42.000Z</published>
    <updated>2018-11-27T12:54:18.242Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Memory-Addresses"><a href="#Memory-Addresses" class="headerlink" title="Memory Addresses"></a>Memory Addresses</h2><p><strong>Logical address</strong></p><ul><li>Included in the machine language instructions to specify the address of an operand or of an instruction. Each logical address consists of a segment and an offset that denotes the distance from the start of the segment to the actual address.<a id="more"></a></li></ul><p><strong>Linear address (also known as virtual address)</strong></p><ul><li>A single 32-bit unsigned integer that can be used to address up to 4 GB. Linear addresses are usually represented in hexadecimal notation; their values range from 0x00000000 to 0xffffffff.</li></ul><p><strong>Physical address</strong></p><ul><li>Used to address memory cells in memory chips. Physical addresses are represented as 32-bit or 36-bit unsigned integers.</li></ul><p>The Memory Management Unit (MMU) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address.<br><img src="/images/2018/11/15.png" alt=""></p><h2 id="Segmentation-in-Hardware"><a href="#Segmentation-in-Hardware" class="headerlink" title="Segmentation in Hardware"></a>Segmentation in Hardware</h2><p>Starting with the 80286 model, Intel microprocessors perform address translation in two different ways called <em>real mode</em> and <em>protected mode</em>. Real mode exists mostly to maintain processor compatibility with older models and to allow the operating system to bootstrap.<br>详情参见<a href="http://liujunming.top/2018/11/16/%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/" target="_blank" rel="noopener">分段机制解析</a></p><h2 id="Paging-in-Hardware"><a href="#Paging-in-Hardware" class="headerlink" title="Paging in Hardware"></a>Paging in Hardware</h2><p>The paging unit thinks of all RAM as partitioned into fixed-length <em>page frames</em>(sometimes referred to as <em>physical pages</em>). Each page frame contains a page — that is, the length of a page frame coincides with that of a page. A page frame is a constituent of main memory, and hence it is a storage area. It is important to distinguish a page from a page frame; the former is just a block of data, which may be stored in any page frame or on disk.</p><p>The data structures that map linear to physical addresses are called <em>page tables</em>; they are stored in main memory and must be properly initialized by the kernel before enabling the paging unit.</p><h3 id="3-1-Regular-Paging"><a href="#3-1-Regular-Paging" class="headerlink" title="3.1 Regular Paging"></a>3.1 Regular Paging</h3><p><img src="/images/2018/11/16.png" alt=""></p><h3 id="3-2-Hardware-Protection-Scheme"><a href="#3-2-Hardware-Protection-Scheme" class="headerlink" title="3.2 Hardware Protection Scheme"></a>3.2 Hardware Protection Scheme</h3><p>While 80×86 processors allow four possible privilege levels to a segment, only two privilege levels are associated with pages and Page Tables.</p><p><strong>User/Supervisor flag</strong></p><ul><li>0, can be addressed only when the CPL is less than 3 (Kernel Mode)</li><li>1, can always be addressed</li></ul><p><strong>Read/Write flag</strong></p><ul><li>0, read-only</li><li>1, can be read and written</li></ul><h3 id="3-3-The-Physical-Address-Extension-PAE-Paging-Mechanism"><a href="#3-3-The-Physical-Address-Extension-PAE-Paging-Mechanism" class="headerlink" title="3.3 The Physical Address Extension (PAE) Paging Mechanism"></a>3.3 The Physical Address Extension (PAE) Paging Mechanism</h3><h3 id="3-4-Hardware-Cache"><a href="#3-4-Hardware-Cache" class="headerlink" title="3.4 Hardware Cache"></a>3.4 Hardware Cache</h3><p>Hardware cache memories were introduced to reduce the speed mismatch between CPU and RAM. They are based on the well-known <em>locality</em> principle, which holds both for programs and data structures.</p><h3 id="3-5-Translation-Lookaside-Buffers-TLB"><a href="#3-5-Translation-Lookaside-Buffers-TLB" class="headerlink" title="3.5 Translation Lookaside Buffers (TLB)"></a>3.5 Translation Lookaside Buffers (TLB)</h3><p>Besides general-purpose hardware caches, 80x86 processors include another cache called <em>Translation Lookaside Buffers</em> (TLB) to speed up linear address translation. When a linear address is used for the first time, the corresponding physical address is computed through slow accesses to the Page Tables in RAM. The physical address is then stored in a TLB entry so that further references to the same linear address can be quickly translated.</p><p>In a multiprocessor system, each CPU has its own TLB, called the <em>local TLB</em> of the CPU. When the <em>cr3</em> control register of a CPU is modified, the hardware automatically invalidates all entries of the local TLB, because a new set of page tables is in use and the TLBs are pointing to old data.</p><h2 id="Paging-in-Linux"><a href="#Paging-in-Linux" class="headerlink" title="Paging in Linux"></a>Paging in Linux</h2><p>Linux adopts a common paging model that fits both 32-bit and 64-bit architectures. Starting with version 2.6.11, a four-level paging model has been adopted:</p><ul><li>Page Global Directory</li><li>Page Upper Directory</li><li>Page Middle Directory</li><li>Page Table</li></ul><p><img src="/images/2018/11/17.png" alt=""></p><p>Each process has its own Page Global Directory and its own set of Page Tables. When a process switch occurs, Linux saves the cr3 control register in the descriptor of the process previously in execution and then loads cr3 with the value stored in the descriptor of the process to be executed next. Thus, when the new process resumes its execution on the CPU, the paging unit refers to the correct set of Page Tables.</p><h3 id="4-1-Page-Table-Handling"><a href="#4-1-Page-Table-Handling" class="headerlink" title="4.1 Page Table Handling"></a>4.1 Page Table Handling</h3><h3 id="4-2-Physical-Memory-Layout"><a href="#4-2-Physical-Memory-Layout" class="headerlink" title="4.2 Physical Memory Layout"></a>4.2 Physical Memory Layout</h3><p>As a general rule, the Linux kernel is installed in RAM starting from the physical address 0x00100000 — i.e., from the second megabyte. The reason that kernel isn’t loaded starting with the first availalbe megabyte of RAM includes:</p><ul><li>Page frame 0 is used by BIOS to store the system hardware configuration detected during the Power-On Self-Test (POST); the BIOS of many laptops, moreover, writes data on this page frame even after the system is initialized.</li><li>Physical addresses ranging from 0x000a0000 to 0x000fffff are usually reserved to BIOS routines and to map the internal memory of ISA graphics cards.</li><li>Additional page frames within the first megabyte may be reserved by specific computer models.</li></ul><h3 id="4-3-Process-Page-Tables"><a href="#4-3-Process-Page-Tables" class="headerlink" title="4.3 Process Page Tables"></a>4.3 Process Page Tables</h3><p>The linear address space of a process is divided into two parts:</p><ul><li>Linear addresses from <code>0x00000000</code> to <code>0xbfffffff</code> can be addressed when the process runs in either User or Kernel Mode.</li><li>Linear addresses from <code>0xc0000000</code> to <code>0xffffffff</code> can be addressed only when the process runs in Kernel Mode.</li></ul><p>The <code>PAGE_OFFSET</code> macro yields the value 0xc0000000; this is the offset in the linear address space of a process where the kernel lives.</p><h3 id="4-4-Handling-the-Hardware-Cache-and-the-TLB"><a href="#4-4-Handling-the-Hardware-Cache-and-the-TLB" class="headerlink" title="4.4 Handling the Hardware Cache and the TLB"></a>4.4 Handling the Hardware Cache and the TLB</h3><hr><p>参考资料：</p><ol><li><a href="https://medium.com/hungys-blog/linux-kernel-memory-addressing-a0d304283af3" target="_blank" rel="noopener">Linux Kernel: Memory Addressing</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Memory-Addresses&quot;&gt;&lt;a href=&quot;#Memory-Addresses&quot; class=&quot;headerlink&quot; title=&quot;Memory Addresses&quot;&gt;&lt;/a&gt;Memory Addresses&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Logical address&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Included in the machine language instructions to specify the address of an operand or of an instruction. Each logical address consists of a segment and an offset that denotes the distance from the start of the segment to the actual address.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Process</title>
    <link href="http://liujunming.github.io/2018/11/26/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process/"/>
    <id>http://liujunming.github.io/2018/11/26/Understanding-the-Linux-Kernel-读书笔记-Process/</id>
    <published>2018-11-26T09:03:51.000Z</published>
    <updated>2018-11-27T09:01:16.481Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Processes-Lightweight-Processes-and-Threads"><a href="#Processes-Lightweight-Processes-and-Threads" class="headerlink" title="Processes, Lightweight Processes, and Threads"></a>Processes, Lightweight Processes, and Threads</h2><p>A process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. The purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated.<br><a id="more"></a><br>Linux uses <em>lightweight processes</em> to offer better support for multithreaded applications. A straightforward way to implement multithreaded applications is to associate a lightweight process with each thread. Each thread can be scheduled independently by the kernel so that one may sleep while another remains runnable.</p><h2 id="Process-Descriptor"><a href="#Process-Descriptor" class="headerlink" title="Process Descriptor"></a>Process Descriptor</h2><p>This is the role of the process descriptor—a <code>task_ struct</code> type structure whose fields contain all the information related to a single process.</p><h3 id="2-1-Process-State"><a href="#2-1-Process-State" class="headerlink" title="2.1 Process State"></a>2.1 Process State</h3><h3 id="2-2-Identifying-a-Process"><a href="#2-2-Identifying-a-Process" class="headerlink" title="2.2 Identifying a Process"></a>2.2 Identifying a Process</h3><p>Linux associates a different PID with each process or lightweight process in the system. However, POSIX standard states that all threads of a multithreaded application must have the same PID.<br>To comply with this standard, the identifier shared by the threads is the PID of the <em>thread group leader</em>, that is, the PID of the first lightweight process in the group; it is stored in the <code>tgid</code> field of the process descriptors.<br>The <code>getpid()</code> system call returns the value of <code>tgid</code> relative to the current process instead of the value of <code>pid</code>, so all the threads of a multithreaded application share the same identifier.</p><h4 id="2-2-1-Process-descriptors-handling"><a href="#2-2-1-Process-descriptors-handling" class="headerlink" title="2.2.1 Process descriptors handling"></a>2.2.1 Process descriptors handling</h4><p>For each process, Linux packs two different data structures in a single per-process memory area: a small data structure linked to the process descriptor, namely the <code>thread_info</code> structure, and the <strong>Kernel Mode process stack</strong>.<br><img src="/images/2018/11/12.png" alt=""></p><h4 id="2-2-2-Identifying-the-current-process"><a href="#2-2-2-Identifying-the-current-process" class="headerlink" title="2.2.2 Identifying the current process"></a>2.2.2 Identifying the current process</h4><p>The kernel can easily obtain the address of the <code>thread_info</code> structure of the process currently running on a CPU from the value of the esp register.</p><h4 id="2-2-3-The-process-list"><a href="#2-2-3-The-process-list" class="headerlink" title="2.2.3 The process list"></a>2.2.3 The process list</h4><h4 id="2-2-4-The-lists-of-TASK-RUNNING-processes"><a href="#2-2-4-The-lists-of-TASK-RUNNING-processes" class="headerlink" title="2.2.4 The lists of TASK_RUNNING processes"></a>2.2.4 The lists of TASK_RUNNING processes</h4><p>When looking for a new process to run on a CPU, the kernel has to consider only the <strong>runnable</strong> processes (that is, the processes in the <code>TASK_RUNNING</code>state).</p><p>Linux 2.6 implements the runqueue without putting all runnable processes in the same list. The aim is to allow the scheduler to select the best runnable process in constant time.</p><p>The trick is splitting the runqueue in many lists of runnable processes, one list per process priority. Each <code>task_struct</code> descriptor includes a <code>run_list</code>field which points to the corresponding runqueue.</p><p>All these are implemented by a single data structure <code>prio_array_t</code>. Noted that each CPU has its own runqueue.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">prio_array</span> &#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> nr_active; <span class="comment">/* The number of process descriptors linked into the lists */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> bitmap[BITMAP_SIZE];  <span class="comment">/* A priority bitmap: each flag is set if and only if the corre- sponding priority list is not empty */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">queue</span>[<span class="title">MAX_PRIO</span>];</span> <span class="comment">/* The 140 heads of the priority lists */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="2-3-Relationships-Among-Processes"><a href="#2-3-Relationships-Among-Processes" class="headerlink" title="2.3 Relationships Among Processes"></a>2.3 Relationships Among Processes</h3><p>The pidhash table and chained lists<br><img src="/images/2018/11/13.png" alt=""></p><p><img src="/images/2018/11/14.png" alt=""></p><h3 id="2-4-How-Processes-Are-Organized"><a href="#2-4-How-Processes-Are-Organized" class="headerlink" title="2.4 How Processes Are Organized"></a>2.4 How Processes Are Organized</h3><h4 id="2-4-1-Wait-queues"><a href="#2-4-1-Wait-queues" class="headerlink" title="2.4.1 Wait queues"></a>2.4.1 Wait queues</h4><p>A wait queue represents a set of sleeping processes, which are woken up by the kernel when some condition becomes true.</p><h4 id="2-4-2-Handling-wait-queues"><a href="#2-4-2-Handling-wait-queues" class="headerlink" title="2.4.2 Handling wait queues"></a>2.4.2 Handling wait queues</h4><h3 id="2-5-Process-Resource-Limits"><a href="#2-5-Process-Resource-Limits" class="headerlink" title="2.5 Process Resource Limits"></a>2.5 Process Resource Limits</h3><p>Each process has an associated set of resource limits, which specify the amount of system resources it can use.<br>The resource limits are stored in an array (<code>task_struct-&gt;signal</code>) of elements of type struct rlimit.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">rlimit</span> &#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span>   rlim_cur;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span>   rlim_max;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h2 id="Process-Switch"><a href="#Process-Switch" class="headerlink" title="Process Switch"></a>Process Switch</h2><h3 id="3-1-Hardware-Context"><a href="#3-1-Hardware-Context" class="headerlink" title="3.1 Hardware Context"></a>3.1 Hardware Context</h3><p>The set of data that must be loaded into the registers before the process resumes its execution on the CPU is called the <em>hardware context</em>. The hardware context is a subset of the process execution context, which includes all information needed for the process execution. In Linux, a part of the hardware context of a process is stored in the process descriptor, while the remaining part is saved in the Kernel Mode stack.</p><p>Process switching occurs only in Kernel Mode. The contents of all registers used by a process in User Mode have already been saved on the <strong>Kernel Mode</strong>stack before performing process switching.</p><h3 id="3-2-Task-State-Segment"><a href="#3-2-Task-State-Segment" class="headerlink" title="3.2 Task State Segment"></a>3.2 Task State Segment</h3><p>The 80x86 architecture includes a specific segment type called the <em>Task State Segment</em>(TSS), to store hardware contexts. While in Linux, there is only single TSS for each processor, each process descriptor includes additional field <code>struct thread_struct thread</code>, in which the kernel saves the hardware context whenever the process is being switched out. The data structutr includes fields for most of the CPU registers, except the general-purpose registers such as eax, ebx, etc., which are stored in the Kernel Mode stack.</p><h3 id="3-3-Performing-the-Process-Switch"><a href="#3-3-Performing-the-Process-Switch" class="headerlink" title="3.3 Performing the Process Switch"></a>3.3 Performing the Process Switch</h3><p>Essentially, every process switch consists of two steps:</p><ol><li>Switching the Page Global Directory to install a new address space;</li><li>Switching the Kernel Mode stack and the hardware context</li></ol><p>The <code>switch_to</code> macro is used to switch the Kernel Mode stack and the hardware context. The macro is a hardware-dependent routine.</p><h2 id="Creating-Processes"><a href="#Creating-Processes" class="headerlink" title="Creating Processes"></a>Creating Processes</h2><ul><li><em>Copy On Write</em>: Allow both the parent and the child to read the same physical pages. Whenever either one tries to write on a physical page, the kernel copies its contents into a new physical page that is assigned to the writing process.</li><li>Lightweight process: Allow both the parent and the child to share many per-process kernel data structures, such as the paging tables (and therefore the entire User Mode address space), the open file tables, and the signal dispositions.</li><li><code>vfork()</code> system call: Create a process that shares the memory address space of its parent. To prevent the parent from overwriting data needed by the child, the parent’s execution is blocked until the child exits or executes a new program.</li></ul><h3 id="4-1-The-clone-fork-and-vfork-System-Calls"><a href="#4-1-The-clone-fork-and-vfork-System-Calls" class="headerlink" title="4.1 The clone(), fork(), and vfork() System Calls"></a>4.1 The clone(), fork(), and vfork() System Calls</h3><p>Lightweight processes are created in Linux by using a function named <code>clone()</code>.clone() is actually a wrapper function defined in the C library, which sets up the stack of the new lightweight process and invokes a clone() system call hidden to the programmer.</p><p><code>fork</code> will make an exact copy of the parent’s address space and give it to the child. Therefore, the parent and child processes have separate address spaces.</p><h3 id="4-2-Kernel-Threads"><a href="#4-2-Kernel-Threads" class="headerlink" title="4.2 Kernel Threads"></a>4.2 Kernel Threads</h3><p>In Linux, kernel threads differ from regular processes in the following ways:</p><ul><li>Kernel threads run only in Kernel Mode, while regular processes run alterna- tively in Kernel Mode and in User Mode.</li><li>Because kernel threads run only in Kernel Mode, they use only linear addresses greater than PAGE_OFFSET.</li></ul><p>0号线程是scheduler，1号线程是init/systemd（所有user thread的祖先），2号线程是[kthreadd]（所有kernel thread的父进程）。</p><hr><p>参考资料：</p><ol><li><a href="https://medium.com/hungys-blog/linux-kernel-process-99629d91423c" target="_blank" rel="noopener">Linux Kernel: Process</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Processes-Lightweight-Processes-and-Threads&quot;&gt;&lt;a href=&quot;#Processes-Lightweight-Processes-and-Threads&quot; class=&quot;headerlink&quot; title=&quot;Processes, Lightweight Processes, and Threads&quot;&gt;&lt;/a&gt;Processes, Lightweight Processes, and Threads&lt;/h2&gt;&lt;p&gt;A process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. The purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated.&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>进程切换switch_to宏第三个参数分析</title>
    <link href="http://liujunming.github.io/2018/11/20/%E8%BF%9B%E7%A8%8B%E5%88%87%E6%8D%A2switch-to%E5%AE%8F%E7%AC%AC%E4%B8%89%E4%B8%AA%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90/"/>
    <id>http://liujunming.github.io/2018/11/20/进程切换switch-to宏第三个参数分析/</id>
    <published>2018-11-20T08:55:16.000Z</published>
    <updated>2018-11-26T09:00:09.614Z</updated>
    
    <content type="html"><![CDATA[<p>switch_to宏的<a href="https://elixir.bootlin.com/linux/v4.19.4/source/arch/x86/include/asm/switch_to.h#L70" target="_blank" rel="noopener">代码</a>如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> switch_to(prev, next, last)                 \</span></span><br><span class="line"><span class="keyword">do</span> &#123;                                    \</span><br><span class="line">    prepare_switch_to(next);                    \</span><br><span class="line">                                    \</span><br><span class="line">    ((last) = __switch_to_asm((prev), (next)));         \</span><br><span class="line">&#125; <span class="keyword">while</span> (<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>上下文切换不仅涉及两个进程，而是三个进程。该情形如下图所示：<br><img src="/images/2018/11/11.png" alt=""></p><p>假定3个进程A、B和C在系统上运行。在某个时间点，内核决定从进程A切换到进程B，然后从进程B切换到进程C，接下来再从进程C切换回进程A。在每个switch_to调用之前，next和prev指针位于各进程的内核栈上，prev指向当前运行的进程，而next指向将要运行的下一个进程。为执行从prev到next的切换，switch_to的前两个参数足够了。对进程A来说，prev指向进程A而next指向进程B。</p><p>在进程A被选中再次执行时，会出现一个问题，控制权返回至switch_to之后的点，如果栈准确地恢复到切换之前的状态，那么prev和next仍然指向切换之前的值，即next=B，而prev=A。在这种情况下，内核无法知道实际上在进程A之前运行的进程是C。</p><p>在新进程被选中时，底层的进程切换例程必须将此前执行的进程提供给context_switch。该宏是效果是，仿佛switch_to是带有两个参数的函数，而且返回了一个指向此前运行进程的指针。switch_to宏实际上执行的代码如下：<br><code>prev = switch_to(prev, next)</code><br>其中返回的prev值并不是用作参数的prev值，而是上一个执行的进程。在上述例子中，进程A提供给switch_to是参数是A和B，但恢复执行后得到的返回值是prev = C。内核实现该行为特性的方式依赖于底层的体系结构。</p><hr><p>参考资料：</p><ol><li>《Professional Linux Kernel Architecture》 p104</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;switch_to宏的&lt;a href=&quot;https://elixir.bootlin.com/linux/v4.19.4/source/arch/x86/include/asm/switch_to.h#L70&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;代码&lt;/a&gt;如下：&lt;br&gt;&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;define&lt;/span&gt; switch_to(prev, next, last)                 \&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;do&lt;/span&gt; &amp;#123;                                    \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    prepare_switch_to(next);                    \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                                    \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ((last) = __switch_to_asm((prev), (next)));         \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125; &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt; (&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>分段机制解析</title>
    <link href="http://liujunming.github.io/2018/11/16/%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/"/>
    <id>http://liujunming.github.io/2018/11/16/分段机制解析/</id>
    <published>2018-11-16T11:59:23.000Z</published>
    <updated>2018-11-17T07:30:23.959Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬件中的分段"><a href="#硬件中的分段" class="headerlink" title="硬件中的分段"></a>硬件中的分段</h2><h3 id="1-1-段选择符和段寄存器"><a href="#1-1-段选择符和段寄存器" class="headerlink" title="1.1 段选择符和段寄存器"></a>1.1 段选择符和段寄存器</h3><p>分段是一种朴素的内存管理机制，它将内存划分成以起始地址(Base)和长度(Limit)描述的块，这些内存块就被称为“段”。<a id="more"></a>一个逻辑地址由两部分组成：一个段选择符和段内偏移量。段选择符是一个16位长的字段，格式如下图所示：<br><img src="/images/2018/11/6.png" alt=""><br>为了快速方便地找到段选择符，处理器提供段寄存器，段寄存器的唯一目的就是存放段选择符。这些寄存器称为cs，ss，ds，es，fs和gs。</p><h3 id="1-2-段描述符"><a href="#1-2-段描述符" class="headerlink" title="1.2 段描述符"></a>1.2 段描述符</h3><p>段描述符描述某个段的基地址、长度以及各种属性。段描述符放在全局描述符表(Global Descriptor Table,GDT)或局部描述符表(Local Descriptor Table,LDT)中。</p><p>系统中至少有一个GDT可以被所有进程共享。相对的，系统中可以有一个或多个LDT,可以被某个进程私有，也可以被多个进程共享。GDT仅仅是内存中的一个数据结构，可以把它看成一个数组，每个元素由基地址(Base)和长度(Limit)描述。LDT则是一个段，它需要一个段描述符来描述它。LDT的的段描述符存放在GDT中，当系统中有多个LDT时，GDT中就必须有对应数量的段描述符。</p><p>GDT在主存中的地址和大小存放在gdtr寄存器中，当前正在被使用的LDT的地址和大小放在ldtr寄存器中。</p><p>通过段选择符索引GDT/LDT的过程如下图：<br><img src="/images/2018/11/7.png" alt=""></p><h3 id="1-3-快速访问段描述符"><a href="#1-3-快速访问段描述符" class="headerlink" title="1.3 快速访问段描述符"></a>1.3 快速访问段描述符</h3><p>段寄存器仅仅存放段选择符。为了加速逻辑地址到线性地址的转换，80x86处理器提供一种非编程的寄存器，供6个可编程的段寄存器使用。每一个非编程的寄存器含有8个字节的段描述符，由相应的段寄存器中的段选择符来指定。每当一个段选择符被装入段寄存器时，相应的段描述符就由内存装入到对应的非编程CPU寄存器。从那时起，针对那个段的逻辑地址转换就可以不访问主存中的GDT或LDT，处理器只需要直接引用存放段描述符的CPU寄存器即可。仅当段寄存器的内容改变时，才有必要访问GDT或LDT，过程如下图：<br><img src="/images/2018/11/5.png" alt=""></p><h3 id="1-4-分段单元"><a href="#1-4-分段单元" class="headerlink" title="1.4 分段单元"></a>1.4 分段单元</h3><p><img src="/images/2018/11/8.png" alt=""></p><h2 id="Linux中的分段"><a href="#Linux中的分段" class="headerlink" title="Linux中的分段"></a>Linux中的分段</h2><p>四个主要的Linux段的段描述符字段的值如下：<br><img src="/images/2018/11/9.png" alt=""></p><h3 id="2-1-Linux-GDT"><a href="#2-1-Linux-GDT" class="headerlink" title="2.1 Linux GDT"></a>2.1 Linux GDT</h3><p>在单处理器系统中只有一个GDT，而在多处理器系统中每个CPU对应一个GDT。</p><p>下图是GDT的布局示意图：<br><img src="/images/2018/11/10.png" alt=""></p><h3 id="2-2-The-Linux-LDTs"><a href="#2-2-The-Linux-LDTs" class="headerlink" title="2.2 The Linux LDTs"></a>2.2 The Linux LDTs</h3><p>大多数用户态下的Linux程序不使用局部描述符表，这样内核就定义了一个缺省的LDT供大多数进程共享。</p><hr><p>参考资料：</p><ol><li>《深入理解linux内核》</li><li>《系统虚拟化：原理与实现》</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;硬件中的分段&quot;&gt;&lt;a href=&quot;#硬件中的分段&quot; class=&quot;headerlink&quot; title=&quot;硬件中的分段&quot;&gt;&lt;/a&gt;硬件中的分段&lt;/h2&gt;&lt;h3 id=&quot;1-1-段选择符和段寄存器&quot;&gt;&lt;a href=&quot;#1-1-段选择符和段寄存器&quot; class=&quot;headerlink&quot; title=&quot;1.1 段选择符和段寄存器&quot;&gt;&lt;/a&gt;1.1 段选择符和段寄存器&lt;/h3&gt;&lt;p&gt;分段是一种朴素的内存管理机制，它将内存划分成以起始地址(Base)和长度(Limit)描述的块，这些内存块就被称为“段”。
    
    </summary>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>内核中统计mysql进程匿名页的访问热度</title>
    <link href="http://liujunming.github.io/2018/11/14/%E5%86%85%E6%A0%B8%E4%B8%AD%E7%BB%9F%E8%AE%A1mysql%E8%BF%9B%E7%A8%8B%E5%8C%BF%E5%90%8D%E9%A1%B5%E7%9A%84%E8%AE%BF%E9%97%AE%E7%83%AD%E5%BA%A6/"/>
    <id>http://liujunming.github.io/2018/11/14/内核中统计mysql进程匿名页的访问热度/</id>
    <published>2018-11-14T02:06:05.000Z</published>
    <updated>2018-12-10T06:11:00.247Z</updated>
    
    <content type="html"><![CDATA[<p>遇到一个需求，需要在内核中统计mysql进程匿名页的访问热度。<br><a id="more"></a></p><h3 id="配置系统环境"><a href="#配置系统环境" class="headerlink" title="配置系统环境"></a>配置系统环境</h3><p>将透明大页关闭:<br><code>echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</code></p><h3 id="寻找mysql的task-struct"><a href="#寻找mysql的task-struct" class="headerlink" title="寻找mysql的task_struct"></a>寻找mysql的task_struct</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> *<span class="title">task</span> = <span class="title">NULL</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//0:not found 1:found</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findMysql</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> *<span class="title">proc</span>;</span></span><br><span class="line">    for_each_process(proc)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">strcmp</span>(proc-&gt;comm, <span class="string">"mysqld"</span>))&#123;</span><br><span class="line">            task = proc;</span><br><span class="line">            ret = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="统计匿名页热度"><a href="#统计匿名页热度" class="headerlink" title="统计匿名页热度"></a>统计匿名页热度</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> tem;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">page</span> *<span class="title">page</span> = <span class="title">NULL</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">page_cgroup</span> *<span class="title">pc</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vm_area_struct</span> *<span class="title">vma</span>;</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> addr = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(vma = task-&gt;mm-&gt;mmap; vma != <span class="literal">NULL</span>; vma = vma-&gt;vm_next)&#123;</span><br><span class="line">    <span class="keyword">for</span>(addr = vma-&gt;vm_start; addr &lt; vma-&gt;vm_end; addr += PAGE_SIZE)&#123;</span><br><span class="line">        page = follow_page(vma, addr, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (page &amp;&amp; PageAnon(page))&#123;</span><br><span class="line">            pc = lookup_page_cgroup(page);</span><br><span class="line">            tem = page_referenced(page, <span class="number">0</span>, pc, &amp;vm_flags) ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">            <span class="built_in">sprintf</span>(str_buf, <span class="string">"22222222:%ul:%d\n"</span>, page_to_pfn(page), tem);</span><br><span class="line">            write_to_file((<span class="keyword">void</span> *)str_buf, <span class="built_in">strlen</span>(str_buf));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遇到一个需求，需要在内核中统计mysql进程匿名页的访问热度。&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>内核中遍历所有的page</title>
    <link href="http://liujunming.github.io/2018/11/09/%E5%86%85%E6%A0%B8%E4%B8%AD%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E7%9A%84page/"/>
    <id>http://liujunming.github.io/2018/11/09/内核中遍历所有的page/</id>
    <published>2018-11-09T01:25:44.000Z</published>
    <updated>2018-11-09T01:35:12.330Z</updated>
    
    <content type="html"><![CDATA[<p>遇到一个需求，需要在内核模块中统计page cache页面的数量。<br><a id="more"></a><br>此刻，需要遍历所有的page，下面是相关代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> scan_pfn=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(;scan_pfn &lt; max_pfn; scan_pfn++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(pfn_valid(scan_pfn))&#123;</span><br><span class="line">        page = pfn_to_page(scan_pfn);</span><br><span class="line">        <span class="keyword">if</span>(page)&#123;</span><br><span class="line">            ......</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>从上述代码可以看出，通过<code>max_pfn</code>即可获取最大的page frame号。</p><p>判断page是否为page cache可使用下面的函数:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">is_file_page</span><span class="params">(struct page* pg)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> ret = <span class="literal">false</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">address_space</span> *<span class="title">mapping</span> = <span class="title">NULL</span>;</span></span><br><span class="line">    <span class="keyword">if</span>(pg &amp;&amp; !PageAnon(pg) &amp;&amp; !PageSwapCache(pg)) &#123;</span><br><span class="line">        mapping = page_file_mapping(pg);</span><br><span class="line">        <span class="keyword">if</span> (mapping) &#123;</span><br><span class="line">            BUG_ON(!mapping-&gt;host);</span><br><span class="line">        &#125;</span><br><span class="line">        ret = mapping;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遇到一个需求，需要在内核模块中统计page cache页面的数量。&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Linux内核中使用文件</title>
    <link href="http://liujunming.github.io/2018/11/08/Linux%E5%86%85%E6%A0%B8%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6/"/>
    <id>http://liujunming.github.io/2018/11/08/Linux内核中使用文件/</id>
    <published>2018-11-08T12:28:18.000Z</published>
    <updated>2018-11-08T13:00:12.749Z</updated>
    
    <content type="html"><![CDATA[<p>在Linux内核中，printk函数可以打印相关信息，并在/var/log/syslog中查看到对应的输出。但是，当需要在内核中统计一些信息的时候，由于数据较多，此刻，在syslog中会发生大量的信息丢失，因而，我们需要在内核中使用文件来存储统计信息。<br><a id="more"></a><br>由于需要在内存模块中使用文件，这就引发了一个问题：文件系统可能还未加载到内核中，在内存模块中无法使用文件。解决方法：在内存管理模块中，应该不断循环判断，直到文件系统加载到内核中，此刻，就可以使用文件来存储统计信息。</p><p><a href="https://github.com/liujunming/Tools/tree/master/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%9E%E9%AA%8C" target="_blank" rel="noopener">虚拟机实验</a>中ksm(页面访问频率统计)是具体使用的例子，在这里只解析使用文件的代码片段。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MY_FILE <span class="meta-string">"/home/test/stat.txt"</span> <span class="comment">//定义存放统计信息的文件</span></span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file</span> *<span class="title">log_file</span> = <span class="title">NULL</span>;</span> <span class="comment">//定义对应的file结构体</span></span><br><span class="line"><span class="keyword">char</span> str_buf[<span class="number">64</span>];  <span class="comment">//暂存数据缓冲区</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">write_to_file</span><span class="params">(<span class="keyword">void</span> *buffer, <span class="keyword">size_t</span> length)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (IS_ERR(log_file))&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">mm_segment_t</span> old_fs;</span><br><span class="line">    old_fs = get_fs();</span><br><span class="line">    set_fs(KERNEL_DS);</span><br><span class="line">    vfs_write(log_file, (<span class="keyword">char</span> *)buffer, length, &amp;log_file-&gt;f_pos);</span><br><span class="line">    set_fs(old_fs);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>处理逻辑:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log_file == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        log_file = filp_open(MY_FILE, O_RDWR | O_APPEND | O_CREAT, <span class="number">0777</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (IS_ERR(log_file))&#123;</span><br><span class="line">        log_file = filp_open(MY_FILE, O_RDWR | O_APPEND | O_CREAT, <span class="number">0777</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(log_file != <span class="literal">NULL</span> &amp;&amp; !IS_ERR(log_file))&#123;</span><br><span class="line">        <span class="built_in">sprintf</span>(str_buf, <span class="string">"22222222:%ul:%d\n"</span>, page_to_pfn(page), tem); <span class="comment">//sprintf与printf用法相同</span></span><br><span class="line">        write_to_file((<span class="keyword">void</span> *)str_buf, <span class="built_in">strlen</span>(str_buf));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这样，就可以在内存模块使用文件了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Linux内核中，printk函数可以打印相关信息，并在/var/log/syslog中查看到对应的输出。但是，当需要在内核中统计一些信息的时候，由于数据较多，此刻，在syslog中会发生大量的信息丢失，因而，我们需要在内核中使用文件来存储统计信息。&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -Consistency and Consensus</title>
    <link href="http://liujunming.github.io/2018/10/07/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Consistency-and-Consensus/"/>
    <id>http://liujunming.github.io/2018/10/07/Designing-Data-Intensive-Applications-读书笔记-Consistency-and-Consensus/</id>
    <published>2018-10-07T09:08:17.000Z</published>
    <updated>2018-11-06T04:58:45.491Z</updated>
    
    <content type="html"><![CDATA[<p><em>consensus</em>: that is, getting all of the nodes to agree on something. </p><p>If two nodes both believe that they are the leader, that situation is called split brain, and it often leads to data loss. Correct implementations of consensus help avoid such problems.<br><a id="more"></a></p><h2 id="Consistency-Guarantees"><a href="#Consistency-Guarantees" class="headerlink" title="Consistency Guarantees"></a>Consistency Guarantees</h2><p>Systems with stronger guarantees may have worse performance or be less fault-tolerant than systems with weaker guarantees.</p><p>Transaction isolation is primarily about avoiding race conditions due to concurrently executing transactions, whereas distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults.</p><h2 id="Linearizability"><a href="#Linearizability" class="headerlink" title="Linearizability"></a>Linearizability</h2><p>中文翻译：线性一致性(强一致性)</p><p>Linearizability，the basic idea is to make a system appear as if there were only one copy of the data, and all operations on it are atomic.</p><p><img src="/images/2018/10/1.png" alt=""></p><h3 id="What-Makes-a-System-Linearizable"><a href="#What-Makes-a-System-Linearizable" class="headerlink" title="What Makes a System Linearizable?"></a>What Makes a System Linearizable?</h3><p><img src="/images/2018/10/2.png" alt=""></p><p>To make the system linearizable, we need to add another constraint.<br><img src="/images/2018/10/3.png" alt=""></p><p><img src="/images/2018/10/4.png" alt=""></p><p><strong>Linearizability Versus Serializability</strong></p><ul><li><em>Serializability</em></li></ul><p>Serializability is an isolation property of transactions, where every transaction may read and write multiple objects (rows, documents, records).It guarantees that transactions behave the same as if they had executed in some serial order (each transaction running to completion before the next transaction starts).</p><ul><li><em>Linearizability</em></li></ul><p>Linearizability is a recency guarantee on reads and writes of a register (an individual object).  It doesn’t group operations together into transactions, so it does not prevent problems such as write skew, unless you take additional measures.</p><h3 id="Relying-on-Linearizability"><a href="#Relying-on-Linearizability" class="headerlink" title="Relying on Linearizability"></a>Relying on Linearizability</h3><h4 id="Locking-and-leader-election"><a href="#Locking-and-leader-election" class="headerlink" title="Locking and leader election"></a>Locking and leader election</h4><h4 id="Constraints-and-uniqueness-guarantees"><a href="#Constraints-and-uniqueness-guarantees" class="headerlink" title="Constraints and uniqueness guarantees"></a>Constraints and uniqueness guarantees</h4><p>Uniqueness constraints are common in databases.</p><p>Similar issues arise if you want to ensure that a bank account balance never goes neg‐ ative, or that you don’t sell more items than you have in stock in the warehouse, or that two people don’t concurrently book the same seat on a flight or in a theater. These constraints all require there to be a single up-to-date value (the account balance, the stock level, the seat occupancy) that all nodes agree on.</p><h4 id="Cross-channel-timing-dependencies"><a href="#Cross-channel-timing-dependencies" class="headerlink" title="Cross-channel timing dependencies"></a>Cross-channel timing dependencies</h4><p><img src="/images/2018/10/5.png" alt=""></p><p> The message queue (steps 3 and 4 in Figure 9-5) might be faster than the internal replication inside the storage service.</p><p> This problem arises because there are two different communication channels between the web server and the resizer: the file storage and the message queue.</p><h3 id="Implementing-Linearizable-Systems"><a href="#Implementing-Linearizable-Systems" class="headerlink" title="Implementing Linearizable Systems"></a>Implementing Linearizable Systems</h3><p>Linearizability essentially means “behave as though there is only a single copy of the data, and all operations on it are atomic”.</p><ul><li>Single-leader replication (potentially linearizable)</li><li>Consensus algorithms (linearizable)</li><li>Multi-leader replication (not linearizable)</li><li>Leaderless replication (probably not linearizable)</li></ul><h4 id="Linearizability-and-quorums"><a href="#Linearizability-and-quorums" class="headerlink" title="Linearizability and quorums"></a>Linearizability and quorums</h4><p>Intuitively, it seems as though strict quorum reads and writes should be linearizable in a Dynamo-style model. However, when we have variable network delays, it is possible to have race conditions, as demonstrated in Figure 9-6.</p><p><img src="/images/2018/10/6.png" alt=""></p><p>It is safest to assume that a leaderless system with Dynamo-style replication does not provide linearizability.</p><h3 id="The-Cost-of-Linearizability"><a href="#The-Cost-of-Linearizability" class="headerlink" title="The Cost of Linearizability"></a>The Cost of Linearizability</h3><p><img src="/images/2018/10/7.png" alt=""></p><h4 id="The-CAP-theorem"><a href="#The-CAP-theorem" class="headerlink" title="The CAP theorem"></a>The CAP theorem</h4><p>The CAP theorem as formally defined is of very narrow scope: it only considers one consistency model (namely linearizability) and one kind of fault(network partitions). It doesn’t say anything about network delays, dead nodes, or other trade-offs. Thus, although CAP has been historically influential, it has little practical value for designing systems.</p><h4 id="Linearizability-and-network-delays"><a href="#Linearizability-and-network-delays" class="headerlink" title="Linearizability and network delays"></a>Linearizability and network delays</h4><h2 id="Ordering-Guarantees"><a href="#Ordering-Guarantees" class="headerlink" title="Ordering Guarantees"></a>Ordering Guarantees</h2><h3 id="Ordering-and-Causality"><a href="#Ordering-and-Causality" class="headerlink" title="Ordering and Causality"></a>Ordering and Causality</h3><p>Causality imposes an ordering on events: cause comes before effect.If a system obeys the ordering imposed by causality, we say that it is <em>causally consistent</em>. </p><h4 id="The-causal-order-is-not-a-total-order"><a href="#The-causal-order-is-not-a-total-order" class="headerlink" title="The causal order is not a total order"></a>The causal order is not a total order</h4><h4 id="Linearizability-is-stronger-than-causal-consistency"><a href="#Linearizability-is-stronger-than-causal-consistency" class="headerlink" title="Linearizability is stronger than causal consistency"></a>Linearizability is stronger than causal consistency</h4><p>Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.</p><h4 id="Capturing-causal-dependencies"><a href="#Capturing-causal-dependencies" class="headerlink" title="Capturing causal dependencies"></a>Capturing causal dependencies</h4><p>In order to maintain causality, you need to know which operation happened before which other operation. </p><h3 id="Sequence-Number-Ordering"><a href="#Sequence-Number-Ordering" class="headerlink" title="Sequence Number Ordering"></a>Sequence Number Ordering</h3><p>We can use sequence numbers or timestamps to order events.</p><h4 id="Lamport-timestamps"><a href="#Lamport-timestamps" class="headerlink" title="Lamport timestamps"></a>Lamport timestamps</h4><p><img src="/images/2018/10/8.png" alt=""></p><h4 id="Timestamp-ordering-is-not-sufficient"><a href="#Timestamp-ordering-is-not-sufficient" class="headerlink" title="Timestamp ordering is not sufficient"></a>Timestamp ordering is not sufficient</h4><p>It is not sufficient when a node has just received a request from a user to create a username, and needs to decide <em>right now</em> whether the request should succeed or fail. </p><p>In order to be sure that no other node is in the process of concurrently creating an account with the same username and a lower timestamp, you would have to check with every other node to see what it is doing.</p><p>The problem here is that the total order of operations only emerges after you have collected all of the operations.</p><p>In order to implement something like a uniqueness constraint for usernames, it’s not sufficient to have a total ordering of operations—you also need to know when that order is finalized. </p><p>This idea of knowing when your total order is finalized is captured in the topic of total order broadcast.</p><h3 id="Total-Order-Broadcast"><a href="#Total-Order-Broadcast" class="headerlink" title="Total Order Broadcast"></a>Total Order Broadcast</h3><p>Total order broadcast is usually described as a protocol for exchanging messages between nodes. Informally, it requires that two safety properties always be satisfied:</p><ul><li><em>Reliable delivery</em></li><li><em>Totally ordered delivery</em></li></ul><h4 id="Using-total-order-broadcast"><a href="#Using-total-order-broadcast" class="headerlink" title="Using total order broadcast"></a>Using total order broadcast</h4><p>Consensus services such as ZooKeeper and etcd actually implement total order broadcast. </p><p>An important aspect of total order broadcast is that the order is fixed at the time the messages are delivered.</p><h4 id="Implementing-linearizable-storage-using-total-order-broadcast"><a href="#Implementing-linearizable-storage-using-total-order-broadcast" class="headerlink" title="Implementing linearizable storage using total order broadcast"></a>Implementing linearizable storage using total order broadcast</h4><p>Total order broadcast is asynchronous: messages are guaranteed to be delivered reliably in a fixed order, but there is no guarantee about when a message will be delivered. By contrast, linearizability is a recency guarantee: a read is guaranteed to see the latest value written.</p><h2 id="Distributed-Transactions-and-Consensus"><a href="#Distributed-Transactions-and-Consensus" class="headerlink" title="Distributed Transactions and Consensus"></a>Distributed Transactions and Consensus</h2><ul><li><em>Leader election</em></li><li><em>Atomic commit</em></li></ul><h3 id="Atomic-Commit-and-Two-Phase-Commit-2PC"><a href="#Atomic-Commit-and-Two-Phase-Commit-2PC" class="headerlink" title="Atomic Commit and Two-Phase Commit (2PC)"></a>Atomic Commit and Two-Phase Commit (2PC)</h3><h4 id="From-single-node-to-distributed-atomic-commit"><a href="#From-single-node-to-distributed-atomic-commit" class="headerlink" title="From single-node to distributed atomic commit"></a>From single-node to distributed atomic commit</h4><h4 id="Introduction-to-two-phase-commit"><a href="#Introduction-to-two-phase-commit" class="headerlink" title="Introduction to two-phase commit"></a>Introduction to two-phase commit</h4><p>Two-phase commit is an algorithm for achieving atomic transaction commit across multiple nodes—i.e., to ensure that either all nodes commit or all nodes abort.</p><p>The commit/abort process in 2PC is split into two phases (hence the name).</p><p><img src="/images/2018/10/9.png" alt=""></p><h4 id="Coordinator-failure"><a href="#Coordinator-failure" class="headerlink" title="Coordinator failure"></a>Coordinator failure</h4><p>If any of the prepare requests fail or time out, the coordinator aborts the transaction; if any of the commit or abort requests fail, the coordinator retries them indefinitely.</p><p><img src="/images/2018/10/10.png" alt=""></p><p>The only way 2PC can complete is by waiting for the coordinator to recover.</p><h4 id="Three-phase-commit"><a href="#Three-phase-commit" class="headerlink" title="Three-phase commit"></a>Three-phase commit</h4><p>Two-phase commit is called a <em>blocking</em> atomic commit protocol due to the fact that 2PC can become stuck waiting for the coordinator to recover. </p><p>three-phase commit (3PC)  assumes a network with bounded delay and nodes with bounded response times; in most practical systems with unbounded network delay and process pauses , it cannot guarantee atomicity.</p><h3 id="Distributed-Transactions-in-Practice"><a href="#Distributed-Transactions-in-Practice" class="headerlink" title="Distributed Transactions in Practice"></a>Distributed Transactions in Practice</h3><p><em>Database-internal distributed transactions</em><br><em>Heterogeneous distributed transactions</em></p><h4 id="Exactly-once-message-processing"><a href="#Exactly-once-message-processing" class="headerlink" title="Exactly-once message processing"></a>Exactly-once message processing</h4><h4 id="XA-transactions"><a href="#XA-transactions" class="headerlink" title="XA transactions"></a>XA transactions</h4><p>XA is not a network protocol—it is merely a C API for interfacing with a transaction coordinator.</p><h4 id="Holding-locks-while-in-doubt"><a href="#Holding-locks-while-in-doubt" class="headerlink" title="Holding locks while in doubt"></a>Holding locks while in doubt</h4><h4 id="Recovering-from-coordinator-failure"><a href="#Recovering-from-coordinator-failure" class="headerlink" title="Recovering from coordinator failure"></a>Recovering from coordinator failure</h4><h4 id="Limitations-of-distributed-transactions"><a href="#Limitations-of-distributed-transactions" class="headerlink" title="Limitations of distributed transactions"></a>Limitations of distributed transactions</h4><h3 id="Fault-Tolerant-Consensus"><a href="#Fault-Tolerant-Consensus" class="headerlink" title="Fault-Tolerant Consensus"></a>Fault-Tolerant Consensus</h3><p>Informally, consensus means getting several nodes to agree on something.</p><p>The consensus problem is normally formalized as follows: one or more nodes may <em>propose</em> values, and the consensus algorithm <em>decides</em> on one of those values.</p><p>In this formalism, a consensus algorithm must satisfy the following properties:</p><ul><li>Uniform agreement</li><li>Integrity</li><li>Validity</li><li>Termination</li></ul><h4 id="Consensus-algorithms-and-total-order-broadcast"><a href="#Consensus-algorithms-and-total-order-broadcast" class="headerlink" title="Consensus algorithms and total order broadcast"></a>Consensus algorithms and total order broadcast</h4><p>The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft, and Zab.</p><p>They decide on a <em>sequence</em> of values, which makes them <em>total order broadcast</em> algorithms</p><h4 id="Single-leader-replication-and-consensus"><a href="#Single-leader-replication-and-consensus" class="headerlink" title="Single-leader replication and consensus"></a>Single-leader replication and consensus</h4><h4 id="Epoch-numbering-and-quorums"><a href="#Epoch-numbering-and-quorums" class="headerlink" title="Epoch numbering and quorums"></a>Epoch numbering and quorums</h4><p>All of the consensus protocols discussed so far internally use a leader in some form or another, but they don’t guarantee that the leader is unique. Instead, they can make a weaker guarantee: the protocols define an <em>epoch number</em> and guarantee that within each epoch, the leader is unique.</p><p>We have two rounds of voting: once to choose a leader, and a second time to vote on a leader’s proposal. </p><p>This voting process looks superficially similar to two-phase commit. The biggest differences are that in 2PC the coordinator is not elected, and that fault-tolerant consensus algorithms only require votes from a majority of nodes, whereas 2PC requires a “yes” vote from every participant. Moreover, consensus algorithms define a recovery process by which nodes can get into a consistent state after a new leader is elected, ensuring that the safety properties are always met. These differences are key to the correctness and fault tolerance of a consensus algorithm.</p><h4 id="Limitations-of-consensus"><a href="#Limitations-of-consensus" class="headerlink" title="Limitations of consensus"></a>Limitations of consensus</h4><p>The process by which nodes vote on proposals before they are decided is a kind of synchronous replication.<br>Consensus systems always require a strict majority to operate.<br>Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can’t just add or remove nodes in the cluster.<br>Consensus systems generally rely on timeouts to detect failed nodes.<br>Sometimes, consensus algorithms are particularly sensitive to network problems.</p><h3 id="Membership-and-Coordination-Services"><a href="#Membership-and-Coordination-Services" class="headerlink" title="Membership and Coordination Services"></a>Membership and Coordination Services</h3><p>ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory.That small amount of data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm. </p><p>ZooKeeper is modeled after Google’s Chubby lock service , implementing not only total order broadcast (and hence consensus), but also an interesting set of other features that turn out to be particularly useful when building distributed systems:</p><ul><li>Linearizable atomic operations</li><li>Total ordering of operations</li><li>Failure detection</li><li>Change notifications</li></ul><h4 id="Allocating-work-to-nodes"><a href="#Allocating-work-to-nodes" class="headerlink" title="Allocating work to nodes"></a>Allocating work to nodes</h4><p>Normally, the kind of data managed by ZooKeeper is quite slow-changing.</p><h4 id="Service-discovery"><a href="#Service-discovery" class="headerlink" title="Service discovery"></a>Service discovery</h4><h4 id="Membership-services"><a href="#Membership-services" class="headerlink" title="Membership services"></a>Membership services</h4><p>A membership service determines which nodes are currently active and live members of a cluster. </p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>It turns out that a wide range of problems are actually reducible to consensus and are equivalent to each other.</p><ul><li>Linearizable compare-and-set registers</li><li>Atomic transaction commit</li><li>Total order broadcast</li><li>Locks and leases</li><li>Membership/coordination service</li><li>Uniqueness constraint</li></ul><p>Not every system necessarily requires consensus: for example, leaderless and multi-leader replication systems typically do not use global consensus.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;consensus&lt;/em&gt;: that is, getting all of the nodes to agree on something. &lt;/p&gt;
&lt;p&gt;If two nodes both believe that they are the leader, that situation is called split brain, and it often leads to data loss. Correct implementations of consensus help avoid such problems.&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -The Trouble with Distributed Systems</title>
    <link href="http://liujunming.github.io/2018/09/25/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-The-Trouble-with-Distributed-Systems/"/>
    <id>http://liujunming.github.io/2018/09/25/Designing-Data-Intensive-Applications-读书笔记-The-Trouble-with-Distributed-Systems/</id>
    <published>2018-09-25T06:02:17.000Z</published>
    <updated>2018-11-06T08:18:30.576Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Unreliable-Networks"><a href="#Unreliable-Networks" class="headerlink" title="Unreliable Networks"></a>Unreliable Networks</h2><p><img src="/images/2018/9/41.png" alt=""><br>The usual way of handling this issue is a <em>timeout</em>: after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don’t know whether the remote node got your request or not.<br><a id="more"></a></p><h3 id="Network-Faults-in-Practice"><a href="#Network-Faults-in-Practice" class="headerlink" title="Network Faults in Practice"></a>Network Faults in Practice</h3><h3 id="Detecting-Faults"><a href="#Detecting-Faults" class="headerlink" title="Detecting Faults"></a>Detecting Faults</h3><h3 id="Timeouts-and-Unbounded-Delays"><a href="#Timeouts-and-Unbounded-Delays" class="headerlink" title="Timeouts and Unbounded Delays"></a>Timeouts and Unbounded Delays</h3><p>如果超时是检测故障的唯一可靠方法，那么超时应该等待多久？不幸的是没有简单的答案。</p><h3 id="Synchronous-Versus-Asynchronous-Networks"><a href="#Synchronous-Versus-Asynchronous-Networks" class="headerlink" title="Synchronous Versus Asynchronous Networks"></a>Synchronous Versus Asynchronous Networks</h3><p>It is possible to give hard real-time response guarantees and bounded delays in networks, but doing so is very expensive and results in lower utilization of hardware resources. Most non-safety-critical systems choose cheap and unreliable over expensive and reliable.</p><h2 id="Unreliable-Clocks"><a href="#Unreliable-Clocks" class="headerlink" title="Unreliable Clocks"></a>Unreliable Clocks</h2><p>可以在一定程度上同步时钟：最常用的机制是网络时间协议（NTP），它允许根据一组服务器报告的时间来调整计算机时钟。服务器则从更精确的时间源（如GPS接收机）获取时间。</p><h3 id="Monotonic-Versus-Time-of-Day-Clocks"><a href="#Monotonic-Versus-Time-of-Day-Clocks" class="headerlink" title="Monotonic Versus Time-of-Day Clocks"></a>Monotonic Versus Time-of-Day Clocks</h3><p>时钟可以及时跳回。<br>单调钟适用于测量持续时间（时间间隔），例如超时或服务的响应时间。</p><p>​在分布式系统中，使用单调钟测量经过时间（比如超时）通常很好，因为它不假定不同节点的时钟之间存在任何同步，并且对测量的轻微不准确性不敏感。</p><h3 id="Clock-Synchronization-and-Accuracy"><a href="#Clock-Synchronization-and-Accuracy" class="headerlink" title="Clock Synchronization and Accuracy"></a>Clock Synchronization and Accuracy</h3><h3 id="Relying-on-Synchronized-Clocks"><a href="#Relying-on-Synchronized-Clocks" class="headerlink" title="Relying on Synchronized Clocks"></a>Relying on Synchronized Clocks</h3><h3 id="Process-Pauses"><a href="#Process-Pauses" class="headerlink" title="Process Pauses"></a>Process Pauses</h3><p> Say you have a database with a single leader per partition. Only the leader is allowed to accept writes. How does a node know that it is still leader (that it hasn’t been declared dead by the others), and that it may safely accept writes?</p><p> One option is for the leader to obtain a <em>lease</em> from the other nodes. Only one node can hold the lease at any one time—thus, when a node obtains a lease, it knows that it is the leader for some amount of time, until the lease expires. In order to remain leader, the node must periodically renew the lease before it expires. If the node fails, it stops renewing the lease, so another node can take over when it expires.</p><p> You can imagine the request-handling loop looking something like this:</p><p><img src="/images/2018/9/42.png" alt=""></p><p>Firstly, it’s relying on synchronized clocks.However, what if there is an unexpected pause in the execution of the program? For example, imagine the thread stops for 15 seconds around the line <code>lease.isValid()</code> before finally continuing. In that case, it’s likely that the lease will have expired by the time the request is processed, and another node has already taken over as leader. However, there is nothing to tell this thread that it was paused for so long, so this code won’t notice that the lease has expired until the next iteration of the loop—by which time it may have already done something unsafe by processing the request.</p><h2 id="Knowledge-Truth-and-Lies"><a href="#Knowledge-Truth-and-Lies" class="headerlink" title="Knowledge, Truth, and Lies"></a>Knowledge, Truth, and Lies</h2><p>So far in this chapter we have explored the ways in which distributed systems are different from programs running on a single computer: there is no shared memory, only message passing via an unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks, and processing pauses.</p><h3 id="The-Truth-Is-Defined-by-the-Majority"><a href="#The-Truth-Is-Defined-by-the-Majority" class="headerlink" title="The Truth Is Defined by the Majority"></a>The Truth Is Defined by the Majority</h3><p>A node cannot necessarily trust its own judgment of a situation.A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes.</p><p>That includes decisions about declaring nodes dead. If a quorum of nodes declares another node dead, then it must be considered dead, even if that node still very much feels alive. The individual node must abide by the quorum decision and step down.</p><h4 id="The-leader-and-the-lock"><a href="#The-leader-and-the-lock" class="headerlink" title="The leader and the lock"></a>The leader and the lock</h4><p><img src="/images/2018/9/43.png" alt=""><br>If the client holding the lease is paused for too long, its lease expires. Another client can obtain a lease for the same file, and start writing to the file. When the paused client comes back, it believes (incorrectly) that it still has a valid lease and proceeds to also write to the file. As a result, the clients’ writes clash and corrupt the file.</p><h4 id="Fencing-tokens"><a href="#Fencing-tokens" class="headerlink" title="Fencing tokens"></a>Fencing tokens</h4><p>Fencing tokens(防护令牌)</p><p>We need to ensure that a node that is under a false belief of being “the chosen one” cannot disrupt the rest of the system.</p><p><img src="/images/2018/9/44.png" alt=""></p><h3 id="Byzantine-Faults"><a href="#Byzantine-Faults" class="headerlink" title="Byzantine Faults"></a>Byzantine Faults</h3><p>Fencing tokens can detect and block a node that is <em>inadvertently</em> acting in error.If the node deliberately wanted to subvert the system’s guarantees, it could easily do so by sending messages with a fake fencing token.</p><p>Distributed systems problems become much harder if there is a risk that nodes may “lie” (send arbitrary faulty or corrupted responses)—for example, if a node may claim to have received a particular message when in fact it didn’t. Such behavior is known as a <em>Byzantine fault</em>, and the problem of reaching consensus in this untrusting environment is known as the <em>Byzantine Generals Problem</em>.</p><p>A system is <em>Byzantine fault-tolerant</em> if it continues to operate correctly even if some of the nodes are malfunctioning and not obeying the protocol, or if malicious attackers are interfering with the network.</p><p>Byzantine是错综复杂的意思。</p><h3 id="System-Model-and-Reality"><a href="#System-Model-and-Reality" class="headerlink" title="System Model and Reality"></a>System Model and Reality</h3><p>With regard to timing assumptions, three system models are in common use:</p><ul><li><em>Synchronous model</em></li><li><em>Partially synchronous model</em></li><li><em>Asynchronous model</em></li></ul><p>Moreover, besides timing issues, we have to consider node failures. The three most common system models for nodes are:</p><ul><li><em>Crash-stop faults</em></li><li><em>Crash-recovery faults</em></li><li><em>Byzantine (arbitrary) faults</em></li></ul><p>For modeling real systems, the partially synchronous model with crash-recovery faults is generally the most useful model.</p><h4 id="Correctness-of-an-algorithm"><a href="#Correctness-of-an-algorithm" class="headerlink" title="Correctness of an algorithm"></a>Correctness of an algorithm</h4><h4 id="Safety-and-liveness"><a href="#Safety-and-liveness" class="headerlink" title="Safety and liveness"></a>Safety and liveness</h4><p>Liveness properties often include the word “eventually” in their definition.</p><p>Safety is often informally defined as <em>nothing bad happens</em>, and liveness as <em>something good eventually happens</em>.</p><p>An advantage of distinguishing between safety and liveness properties is that it helps us deal with difficult system models. </p><h4 id="Mapping-system-models-to-the-real-world"><a href="#Mapping-system-models-to-the-real-world" class="headerlink" title="Mapping system models to the real world"></a>Mapping system models to the real world</h4><p>Safety and liveness properties and system models are very useful for reasoning about the correctness of a distributed algorithm.</p><p>Proving an algorithm correct does not mean its <em>implementation</em> on a real system will necessarily always behave correctly.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Unreliable-Networks&quot;&gt;&lt;a href=&quot;#Unreliable-Networks&quot; class=&quot;headerlink&quot; title=&quot;Unreliable Networks&quot;&gt;&lt;/a&gt;Unreliable Networks&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/2018/9/41.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;The usual way of handling this issue is a &lt;em&gt;timeout&lt;/em&gt;: after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don’t know whether the remote node got your request or not.&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -Transactions</title>
    <link href="http://liujunming.github.io/2018/09/23/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Transactions/"/>
    <id>http://liujunming.github.io/2018/09/23/Designing-Data-Intensive-Applications-读书笔记-Transactions/</id>
    <published>2018-09-23T05:03:31.000Z</published>
    <updated>2018-11-06T08:19:49.434Z</updated>
    
    <content type="html"><![CDATA[<p>本章主要是在单机数据库的上下文中，探讨了各种概念与想法。<br><a id="more"></a></p><h2 id="The-Slippery-Concept-of-a-Transaction"><a href="#The-Slippery-Concept-of-a-Transaction" class="headerlink" title="The Slippery Concept of a Transaction"></a>The Slippery Concept of a Transaction</h2><h3 id="The-Meaning-of-ACID"><a href="#The-Meaning-of-ACID" class="headerlink" title="The Meaning of ACID"></a>The Meaning of ACID</h3><p>Systems that do not meet the ACID criteria are sometimes called BASE, which stands for Basically Available, Soft state, and Eventual consistency. </p><h4 id="Atomicity"><a href="#Atomicity" class="headerlink" title="Atomicity"></a>Atomicity</h4><p>ACID atomicity describes what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed.</p><h4 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h4><p>The word consistency is terribly overloaded:</p><ul><li>replica consistency and the issue of eventual consistency that arises in asynchronously replicated systems.</li><li>Consistent hashing is an approach to partitioning that some systems use for rebalancing.</li><li>In the CAP theorem , the word consistency is used to mean linearizability.</li></ul><p>The idea of ACID consistency is that you have certain statements about your data (invariants) that must always be true.</p><h4 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h4><p>Most databases are accessed by several clients at the same time. That is no problem if they are reading and writing different parts of the database, but if they are accessing the same database records, you can run into concurrency problems (race conditions).</p><p>ACID意义上的隔离性意味着，同时执行的事务是相互隔离的：它们不能相互冒犯。</p><h4 id="Durability"><a href="#Durability" class="headerlink" title="Durability"></a>Durability</h4><p>持久性 是一个承诺，即一旦事务成功完成，即使发生硬件故障或数据库崩溃，写入的任何数据也不会丢失。</p><h3 id="Single-Object-and-Multi-Object-Operations"><a href="#Single-Object-and-Multi-Object-Operations" class="headerlink" title="Single-Object and Multi-Object Operations"></a>Single-Object and Multi-Object Operations</h3><p>图7-2展示了一个来自邮件应用的例子。执行以下查询来显示用户未读邮件数量：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> emails <span class="keyword">WHERE</span> recipient_id = <span class="number">2</span> <span class="keyword">AND</span> unread_flag = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>但如果邮件太多，你可能会觉得这个查询太慢，并决定用单独的字段存储未读邮件的数量。现在每当一个新消息写入时，也必须增长未读计数器，每当一个消息被标记为已读时，也必须减少未读计数器。</p><p>在图7-2中，用户2 遇到异常情况：邮件列表里显示有未读消息，但计数器显示为零未读消息，因为计数器增长还没有发生。隔离性可以避免这个问题：通过确保用户2要么同时看到新邮件和增长后的计数器，要么都看不到，反正不会看到执行到一半的中间结果。</p><p><img src="/images/2018/9/28.png" alt=""></p><p>图7-3说明了对原子性的需求：如果在事务过程中发生错误，邮箱和未读计数器的内容可能会失去同步。在原子事务中，如果对计数器的更新失败，事务将被中止，并且插入的电子邮件将被回滚。</p><p><img src="/images/2018/9/29.png" alt=""></p><p>A transaction is usually understood as a mechanism for grouping multiple operations on multiple objects into one unit of execution.</p><h2 id="Weak-Isolation-Levels"><a href="#Weak-Isolation-Levels" class="headerlink" title="Weak Isolation Levels"></a>Weak Isolation Levels</h2><p>如果两个事务不触及相同的数据，它们可以安全地并行（parallel） 运行，因为两者都不依赖于另一个。当一个事务读取由另一个事务同时修改的数据时，或者当两个事务试图同时修改相同的数据时，并发问题（竞争条件）才会出现。</p><h3 id="Read-Committed"><a href="#Read-Committed" class="headerlink" title="Read Committed"></a>Read Committed</h3><p>The most basic level of transaction isolation is read committed.It makes two guarantees:</p><ol><li>When reading from the database, you will only see data that has been committed (no dirty reads).</li><li>When writing to the database, you will only overwrite data that has been committed (no dirty writes).</li></ol><h4 id="No-dirty-reads"><a href="#No-dirty-reads" class="headerlink" title="No dirty reads"></a>No dirty reads</h4><p>Imagine a transaction has written some data to the database, but the transaction has not yet committed or aborted. Can another transaction see that uncommitted data? If yes, that is called a <em>dirty read</em>.</p><p><img src="/images/2018/9/31.png" alt=""></p><h4 id="No-dirty-writes"><a href="#No-dirty-writes" class="headerlink" title="No dirty writes"></a>No dirty writes</h4><p>What happens if two transactions concurrently try to update the same object in a database? We don’t know in which order the writes will happen, but we normally assume that the later write overwrites the earlier write.</p><p>However, what happens if the earlier write is part of a transaction that has not yet committed, so the later write overwrites an uncommitted value? This is called a <em>dirty write</em>. Transactions running at the read committed isolation level must prevent dirty writes, usually by delaying the second write until the first write’s transaction has committed or aborted.</p><p><img src="/images/2018/9/32.png" alt=""></p><h4 id="Implementing-read-committed"><a href="#Implementing-read-committed" class="headerlink" title="Implementing read committed"></a>Implementing read committed</h4><p>Most commonly, databases prevent dirty writes by using row-level locks.</p><p>Most databases prevent dirty reads using the approach illustrated in Figure 7-4: for every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. </p><h3 id="Snapshot-Isolation"><a href="#Snapshot-Isolation" class="headerlink" title="Snapshot Isolation"></a>Snapshot Isolation</h3><p>在PostgreSQL and MySQL中，Snapshot Isolation即为Repeatable Read。</p><p>图7-6说明了read committed可能发生的问题。<br><img src="/images/2018/9/33.png" alt=""></p><p>这种异常被称为不可重复读（nonrepeatable read）或读取偏差（read skew）。</p><p><em>Snapshot isolation</em>能解决read skew问题。The idea is that each transaction reads from a <em>consistent snapshot</em> of the database—that is, the transaction sees all the data that was committed in the database at the start of the transaction. Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.</p><h4 id="Implementing-snapshot-isolation"><a href="#Implementing-snapshot-isolation" class="headerlink" title="Implementing snapshot isolation"></a>Implementing snapshot isolation</h4><p>A key principle of snapshot isolation is <em>readers never block writers, and writers never block readers</em>.</p><p>The database must potentially keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as <em>multi-version concurrency control</em>(MVCC).</p><p>图7-7说明了如何在PostgreSQL中实现基于MVCC的快照隔离。当一个事务开始时，它被赋予一个唯一的事务ID。每当事务向数据库写入任何内容时，它所写入的数据都会被标记上写入者的事务ID。</p><p><img src="/images/2018/9/34.png" alt=""></p><h4 id="Visibility-rules-for-observing-a-consistent-snapshot"><a href="#Visibility-rules-for-observing-a-consistent-snapshot" class="headerlink" title="Visibility rules for observing a consistent snapshot"></a>Visibility rules for observing a consistent snapshot</h4><p>When a transaction reads from the database, transaction IDs are used to decide which objects it can see and which are invisible. By carefully defining visibility rules,the database can present a consistent snapshot of the database to the application. </p><h4 id="Indexes-and-snapshot-isolation"><a href="#Indexes-and-snapshot-isolation" class="headerlink" title="Indexes and snapshot isolation"></a>Indexes and snapshot isolation</h4><p><img src="/images/2018/9/30.png" alt=""></p><h3 id="Preventing-Lost-Updates"><a href="#Preventing-Lost-Updates" class="headerlink" title="Preventing Lost Updates"></a>Preventing Lost Updates</h3><p>到目前为止已经讨论的read committed和snapshot isolation级别，主要保证了只读事务在并发写入时可以看到什么。却忽略了两个事务并发写入的问题——我们只讨论了脏写。</p><p>并发的写入事务之间还有其他几种有趣的冲突。其中最着名的是丢失更新（lost update） 问题，如下图所示，以两个并发计数器增量为例。</p><p><img src="/images/2018/9/35.png" alt=""></p><p>The lost update problem can occur if an application reads some value from the database, modifies it, and writes back the modified value (a <em>read-modify-write cycle</em>). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification.</p><p>Because this is such a common problem, a variety of solutions have been developed.</p><h4 id="Atomic-write-operations"><a href="#Atomic-write-operations" class="headerlink" title="Atomic write operations"></a>Atomic write operations</h4><p>Many databases provide atomic update operations, which remove the need to implement read-modify-write cycles in application code.</p><h4 id="Explicit-locking"><a href="#Explicit-locking" class="headerlink" title="Explicit locking"></a>Explicit locking</h4><p>Another option for preventing lost updates, if the database’s built-in atomic operations don’t provide the necessary functionality, is for the application to explicitly lock objects that are going to be updated.</p><p><img src="/images/2018/9/36.png" alt=""></p><h4 id="Automatically-detecting-lost-updates"><a href="#Automatically-detecting-lost-updates" class="headerlink" title="Automatically detecting lost updates"></a>Automatically detecting lost updates</h4><p>Atomic operations and locks are ways of preventing lost updates by forcing the read-modify-write cycles to happen sequentially. An alternative is to allow them to execute in parallel and, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.</p><h4 id="Compare-and-set-CAS"><a href="#Compare-and-set-CAS" class="headerlink" title="Compare-and-set(CAS)"></a>Compare-and-set(CAS)</h4><p>In databases that don’t provide transactions, you sometimes find an <strong>atomic</strong> compare-and-set operation. The purpose of this operation is to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it. If the current value does not match what you previously read, the update has no effect, and the read-modify-write cycle must be retried.</p><p>For example, to prevent two users concurrently updating the same wiki page, you might try something like this, expecting the update to occur only if the content of the page hasn’t changed since the user started editing it:</p><p><img src="/images/2018/9/37.png" alt=""></p><h4 id="Conflict-resolution-and-replication"><a href="#Conflict-resolution-and-replication" class="headerlink" title="Conflict resolution and replication"></a>Conflict resolution and replication</h4><p>Locks and compare-and-set operations assume that there is a single up-to-date copy of the data. However, databases with multi-leader or leaderless replication usually allow several writes to happen concurrently and replicate them asynchronously, so they cannot guarantee that there is a single up-to-date copy of the data. Thus, techniques based on locks or CAS do not apply in this context.</p><h3 id="Write-Skew-and-Phantoms"><a href="#Write-Skew-and-Phantoms" class="headerlink" title="Write Skew and Phantoms"></a>Write Skew and Phantoms</h3><p>phantoms在本文中的含义是幻读。</p><p>想象一下这个例子：你正在为医院写一个医生轮班管理程序。医院通常会同时要求几位医生值班，但底线是至少有一位医生在值班。医生可以放弃他们的班次（例如，如果他们自己生病了），只要至少有一个同事在这一班中继续工作。<br>现在想象一下，Alice和Bob是两位值班医生。两人都感到不适，所以他们都决定请假。不幸的是，他们恰好在同一时间点击按钮下班。图7-8说明了接下来的事情。</p><p><img src="/images/2018/9/38.png" alt=""></p><p>在两个事务中，应用首先检查是否有两个或以上的医生正在值班；如果是的话，它就假定一名医生可以安全地休班。由于数据库使用Snapshot Isolation，两次检查都返回 2 ，所以两个事务都进入下一个阶段。Alice更新自己的记录休班了，而Bob也做了一样的事情。两个事务都成功提交了，现在没有医生值班了。违反了至少有一名医生在值班的要求。</p><h4 id="Characterizing-write-skew"><a href="#Characterizing-write-skew" class="headerlink" title="Characterizing write skew"></a>Characterizing write skew</h4><p>这种异常称为 <em>write skew</em>.</p><p> Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</p><h4 id="More-examples-of-write-skew"><a href="#More-examples-of-write-skew" class="headerlink" title="More examples of write skew"></a>More examples of write skew</h4><h4 id="Phantoms-causing-write-skew"><a href="#Phantoms-causing-write-skew" class="headerlink" title="Phantoms causing write skew"></a>Phantoms causing write skew</h4><p>This effect, where a write in one transaction changes the result of a search query in another transaction, is called a <em>phantom</em>.</p><h2 id="Serializability"><a href="#Serializability" class="headerlink" title="Serializability"></a>Serializability</h2><p>目前大多数提供可序列化的数据库都使用了三种技术。</p><h3 id="Actual-Serial-Execution"><a href="#Actual-Serial-Execution" class="headerlink" title="Actual Serial Execution"></a>Actual Serial Execution</h3><p>If you can make each transaction very fast to execute, and the transaction throughput is low enough to process on a single CPU core, this is a simple and effective option.</p><h3 id="Two-Phase-Locking-2PL"><a href="#Two-Phase-Locking-2PL" class="headerlink" title="Two-Phase Locking (2PL)"></a>Two-Phase Locking (2PL)</h3><h4 id="Implementation-of-two-phase-locking"><a href="#Implementation-of-two-phase-locking" class="headerlink" title="Implementation of two-phase locking"></a>Implementation of two-phase locking</h4><p>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two-phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.</p><h3 id="Serializable-Snapshot-Isolation-SSI"><a href="#Serializable-Snapshot-Isolation-SSI" class="headerlink" title="Serializable Snapshot Isolation (SSI)"></a>Serializable Snapshot Isolation (SSI)</h3><h4 id="Pessimistic-versus-optimistic-concurrency-control"><a href="#Pessimistic-versus-optimistic-concurrency-control" class="headerlink" title="Pessimistic versus optimistic concurrency control"></a>Pessimistic versus optimistic concurrency control</h4><p>Two-phase locking is a so-called pessimistic concurrency control mechanism.<br>Serializable snapshot isolation is an optimistic concurrency control technique. </p><p>SSI is based on snapshot isolation—that is, all reads within a transaction are made from a consistent snapshot of the database. This is the main difference compared to earlier optimistic concurrency control techniques. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort.</p><p>In order to provide serializable isolation, the database must detect situations in which a transaction may have acted on an outdated premise and abort the transaction in that case.</p><p>How does the database know if a query result might have changed? There are two cases to consider:</p><ul><li>Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)</li><li>Detecting writes that affect prior reads (the write occurs after the read)</li></ul><h4 id="Detecting-stale-MVCC-reads"><a href="#Detecting-stale-MVCC-reads" class="headerlink" title="Detecting stale MVCC reads"></a>Detecting stale MVCC reads</h4><p><img src="/images/2018/9/39.png" alt=""></p><h4 id="Detecting-writes-that-affect-prior-reads"><a href="#Detecting-writes-that-affect-prior-reads" class="headerlink" title="Detecting writes that affect prior reads"></a>Detecting writes that affect prior reads</h4><p><img src="/images/2018/9/40.png" alt=""></p><hr><p>参考资料：</p><ol><li><a href="https://github.com/Vonng/ddia/blob/master/ch7.md" target="_blank" rel="noopener">Vonng ddia</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章主要是在单机数据库的上下文中，探讨了各种概念与想法。&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
