<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2024-07-20T00:36:35.147Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>IRQ bypass for posted interrupt</title>
    <link href="http://liujunming.github.io/2024/07/20/IRQ-bypass-for-posted-interrupt/"/>
    <id>http://liujunming.github.io/2024/07/20/IRQ-bypass-for-posted-interrupt/</id>
    <published>2024-07-20T00:00:30.000Z</published>
    <updated>2024-07-20T00:36:35.147Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>IRQ bypass仅仅是一套软件框架而已!<a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">When a physical I/O device is assigned to a virtual machine through</span><br><span class="line">facilities like VFIO and KVM, the interrupt for the device generally</span><br><span class="line">bounces through the host system before being injected into the VM.</span><br><span class="line">However, hardware technologies exist that often allow the host to be</span><br><span class="line">bypassed for some of these scenarios.  Intel Posted Interrupts allow</span><br><span class="line">the specified physical edge interrupts to be directly injected into a</span><br><span class="line">guest when delivered to a physical processor while the vCPU is</span><br><span class="line">running.  ARM IRQ Forwarding allows forwarded physical interrupts to</span><br><span class="line">be directly deactivated by the guest.</span><br><span class="line"></span><br><span class="line">The IRQ bypass manager here is meant to provide the shim to connect</span><br><span class="line">interrupt producers, generally the host physical device driver, with</span><br><span class="line">interrupt consumers, generally the hypervisor, in order to configure</span><br><span class="line">these bypass mechanism.  To do this, we base the connection on a</span><br><span class="line">shared, opaque token.  For KVM-VFIO this is expected to be an</span><br><span class="line">eventfd_ctx since this is the connection we already use to connect an</span><br><span class="line">eventfd to an irqfd on the in-kernel path.  When a producer and</span><br><span class="line">consumer with matching tokens is found, callbacks via both registered</span><br><span class="line">participants allow the bypass facilities to be automatically enabled.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Theory of operation</span><br><span class="line"> *</span><br><span class="line"> * The IRQ bypass manager is a simple set of lists and callbacks that allows</span><br><span class="line"> * IRQ producers (ex. physical interrupt sources) to be matched to IRQ</span><br><span class="line"> * consumers (ex. virtualization hardware that allows IRQ bypass or offload)</span><br><span class="line"> * via a shared token (ex. eventfd_ctx).  Producers and consumers register</span><br><span class="line"> * independently.  When a token match is found, the optional @stop callback</span><br><span class="line"> * will be called for each participant.  The pair will then be connected via</span><br><span class="line"> * the @add_* callbacks, and finally the optional @start callback will allow</span><br><span class="line"> * any final coordination.  When either participant is unregistered, the</span><br><span class="line"> * process is repeated using the @del_* callbacks in place of the @add_*</span><br><span class="line"> * callbacks.  Match tokens must be unique per producer/consumer, 1:N pairings</span><br><span class="line"> * are not supported.</span><br><span class="line"> */</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_producer</span><span class="params">(struct irq_bypass_producer *)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">irq_bypass_unregister_producer</span><span class="params">(struct irq_bypass_producer *)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_consumer</span><span class="params">(struct irq_bypass_consumer *)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">irq_bypass_unregister_consumer</span><span class="params">(struct irq_bypass_consumer *)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="kvm-arch-irq-bypass-add-producer"><a href="#kvm-arch-irq-bypass-add-producer" class="headerlink" title="kvm_arch_irq_bypass_add_producer"></a>kvm_arch_irq_bypass_add_producer</h2><h3 id="irq-bypass-register-producer"><a href="#irq-bypass-register-producer" class="headerlink" title="irq_bypass_register_producer"></a>irq_bypass_register_producer</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vfio_pci_set_irqs_ioctl</span><br><span class="line">└── vfio_pci_set_msi_trigger</span><br><span class="line">    └── vfio_msi_set_block</span><br><span class="line">        └── vfio_msi_set_vector_signal</span><br><span class="line">            └── irq_bypass_register_producer</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">vfio_msi_set_vector_signal</span><span class="params">(struct vfio_pci_device *vdev,</span></span></span><br><span class="line"><span class="function"><span class="params">                      <span class="keyword">int</span> <span class="built_in">vector</span>, <span class="keyword">int</span> fd, <span class="keyword">bool</span> msix)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">eventfd_ctx</span> *<span class="title">trigger</span>;</span></span><br><span class="line">    ...</span><br><span class="line">    trigger = eventfd_ctx_fdget(fd); <span class="comment">// 正常情况下，这里的fd就是irqfd</span></span><br><span class="line">    ...</span><br><span class="line">    vdev-&gt;ctx[<span class="built_in">vector</span>].producer.token = trigger;</span><br><span class="line">    vdev-&gt;ctx[<span class="built_in">vector</span>].producer.irq = irq;</span><br><span class="line">    ret = irq_bypass_register_producer(&amp;vdev-&gt;ctx[<span class="built_in">vector</span>].producer);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * irq_bypass_register_producer - register IRQ bypass producer</span></span><br><span class="line"><span class="comment"> * @producer: pointer to producer structure</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Add the provided IRQ producer to the list of producers and connect</span></span><br><span class="line"><span class="comment"> * with any matching token found on the IRQ consumers list.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_producer</span><span class="params">(struct irq_bypass_producer *producer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_producer</span> *<span class="title">tmp</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_consumer</span> *<span class="title">consumer</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!producer-&gt;token)</span><br><span class="line">        <span class="keyword">return</span> -EINVAL;</span><br><span class="line"></span><br><span class="line">    might_sleep();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!try_module_get(THIS_MODULE))</span><br><span class="line">        <span class="keyword">return</span> -ENODEV;</span><br><span class="line"></span><br><span class="line">    mutex_lock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(tmp, &amp;producers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tmp-&gt;token == producer-&gt;token) &#123;</span><br><span class="line">            mutex_unlock(&amp;lock);</span><br><span class="line">            module_put(THIS_MODULE);</span><br><span class="line">            <span class="keyword">return</span> -EBUSY;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(consumer, &amp;consumers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (consumer-&gt;token == producer-&gt;token) &#123;</span><br><span class="line">            <span class="keyword">int</span> ret = __connect(producer, consumer);</span><br><span class="line">            <span class="keyword">if</span> (ret) &#123;</span><br><span class="line">                mutex_unlock(&amp;lock);</span><br><span class="line">                module_put(THIS_MODULE);</span><br><span class="line">                <span class="keyword">return</span> ret;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_add(&amp;producer-&gt;node, &amp;producers);</span><br><span class="line"></span><br><span class="line">    mutex_unlock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(irq_bypass_register_producer);</span><br></pre></td></tr></table></figure><h3 id="irq-bypass-register-consumer"><a href="#irq-bypass-register-consumer" class="headerlink" title="irq_bypass_register_consumer"></a>irq_bypass_register_consumer</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_vm_ioctl</span><br><span class="line">└── kvm_irqfd</span><br><span class="line">    └── kvm_irqfd_assign</span><br><span class="line">        └── irq_bypass_register_consumer</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span></span><br><span class="line">kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_HAVE_KVM_IRQ_BYPASS</span></span><br><span class="line">    <span class="keyword">if</span> (kvm_arch_has_irq_bypass()) &#123;</span><br><span class="line">        irqfd-&gt;consumer.token = (<span class="keyword">void</span> *)irqfd-&gt;eventfd;</span><br><span class="line">        irqfd-&gt;consumer.add_producer = kvm_arch_irq_bypass_add_producer;</span><br><span class="line">        irqfd-&gt;consumer.del_producer = kvm_arch_irq_bypass_del_producer;</span><br><span class="line">        irqfd-&gt;consumer.stop = kvm_arch_irq_bypass_stop;</span><br><span class="line">        irqfd-&gt;consumer.start = kvm_arch_irq_bypass_start;</span><br><span class="line">        ret = irq_bypass_register_consumer(&amp;irqfd-&gt;consumer);</span><br><span class="line">        <span class="keyword">if</span> (ret)</span><br><span class="line">            pr_info(<span class="string">"irq bypass consumer (token %p) registration fails: %d\n"</span>,</span><br><span class="line">                irqfd-&gt;consumer.token, ret);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * irq_bypass_register_consumer - register IRQ bypass consumer</span></span><br><span class="line"><span class="comment"> * @consumer: pointer to consumer structure</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Add the provided IRQ consumer to the list of consumers and connect</span></span><br><span class="line"><span class="comment"> * with any matching token found on the IRQ producer list.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_consumer</span><span class="params">(struct irq_bypass_consumer *consumer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_consumer</span> *<span class="title">tmp</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_producer</span> *<span class="title">producer</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!consumer-&gt;token ||</span><br><span class="line">        !consumer-&gt;add_producer || !consumer-&gt;del_producer)</span><br><span class="line">        <span class="keyword">return</span> -EINVAL;</span><br><span class="line"></span><br><span class="line">    might_sleep();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!try_module_get(THIS_MODULE))</span><br><span class="line">        <span class="keyword">return</span> -ENODEV;</span><br><span class="line"></span><br><span class="line">    mutex_lock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(tmp, &amp;consumers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tmp-&gt;token == consumer-&gt;token || tmp == consumer) &#123;</span><br><span class="line">            mutex_unlock(&amp;lock);</span><br><span class="line">            module_put(THIS_MODULE);</span><br><span class="line">            <span class="keyword">return</span> -EBUSY;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(producer, &amp;producers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (producer-&gt;token == consumer-&gt;token) &#123;</span><br><span class="line">            <span class="keyword">int</span> ret = __connect(producer, consumer);</span><br><span class="line">            <span class="keyword">if</span> (ret) &#123;</span><br><span class="line">                mutex_unlock(&amp;lock);</span><br><span class="line">                module_put(THIS_MODULE);</span><br><span class="line">                <span class="keyword">return</span> ret;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_add(&amp;consumer-&gt;node, &amp;consumers);</span><br><span class="line"></span><br><span class="line">    mutex_unlock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(irq_bypass_register_consumer);</span><br></pre></td></tr></table></figure><h3 id="vmx-update-pi-irte"><a href="#vmx-update-pi-irte" class="headerlink" title="vmx_update_pi_irte"></a>vmx_update_pi_irte</h3><p>当irq bypass的producer和consumer token(eventfd_ctx)匹配成功时，才会调用<code>kvm_arch_irq_bypass_add_producer</code>。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* @lock must be held when calling connect */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __connect(struct irq_bypass_producer *prod,</span><br><span class="line">             struct irq_bypass_consumer *cons)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (prod-&gt;stop)</span><br><span class="line">        prod-&gt;stop(prod);</span><br><span class="line">    <span class="keyword">if</span> (cons-&gt;stop)</span><br><span class="line">        cons-&gt;stop(cons);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (prod-&gt;add_consumer)</span><br><span class="line">        ret = prod-&gt;add_consumer(prod, cons);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!ret) &#123;</span><br><span class="line">        ret = cons-&gt;add_producer(cons, prod);</span><br><span class="line">        <span class="keyword">if</span> (ret &amp;&amp; prod-&gt;del_consumer)</span><br><span class="line">            prod-&gt;del_consumer(prod, cons);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cons-&gt;start)</span><br><span class="line">        cons-&gt;start(cons);</span><br><span class="line">    <span class="keyword">if</span> (prod-&gt;start)</span><br><span class="line">        prod-&gt;start(prod);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_arch_irq_bypass_add_producer</span><span class="params">(struct irq_bypass_consumer *cons,</span></span></span><br><span class="line"><span class="function"><span class="params">                      struct irq_bypass_producer *prod)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">kvm_kernel_irqfd</span> *<span class="title">irqfd</span> =</span></span><br><span class="line"><span class="class">        <span class="title">container_of</span>(<span class="title">cons</span>, <span class="title">struct</span> <span class="title">kvm_kernel_irqfd</span>, <span class="title">consumer</span>);</span></span><br><span class="line"></span><br><span class="line">    irqfd-&gt;producer = prod;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> kvm_x86_ops-&gt;update_pi_irte(irqfd-&gt;kvm,</span><br><span class="line">                       prod-&gt;irq, irqfd-&gt;gsi, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>add_producer其实就是设置irte为Posted-Interrupts而已！<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmx_update_pi_irte</span><br><span class="line">└── irq_set_vcpu_affinity</span><br><span class="line">    └── intel_ir_set_vcpu_affinity</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * vmx_update_pi_irte - set IRTE for Posted-Interrupts</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @kvm: kvm</span></span><br><span class="line"><span class="comment"> * @host_irq: host irq of the interrupt</span></span><br><span class="line"><span class="comment"> * @guest_irq: gsi of the interrupt</span></span><br><span class="line"><span class="comment"> * @set: set or unset PI</span></span><br><span class="line"><span class="comment"> * returns 0 on success, &lt; 0 on failure</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">vmx_update_pi_irte</span><span class="params">(struct kvm *kvm, <span class="keyword">unsigned</span> <span class="keyword">int</span> host_irq,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">uint32_t</span> guest_irq, <span class="keyword">bool</span> <span class="built_in">set</span>)</span></span></span><br></pre></td></tr></table></figure><h2 id="kvm-arch-update-irqfd-routing"><a href="#kvm-arch-update-irqfd-routing" class="headerlink" title="kvm_arch_update_irqfd_routing"></a>kvm_arch_update_irqfd_routing</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_vm_ioctl</span><br><span class="line">└── kvm_set_irq_routing</span><br><span class="line">    └── kvm_irq_routing_update</span><br><span class="line">        └── kvm_arch_update_irqfd_routing</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_arch_update_irqfd_routing</span><span class="params">(struct kvm *kvm, <span class="keyword">unsigned</span> <span class="keyword">int</span> host_irq,</span></span></span><br><span class="line"><span class="function"><span class="params">                   <span class="keyword">uint32_t</span> guest_irq, <span class="keyword">bool</span> <span class="built_in">set</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!kvm_x86_ops-&gt;update_pi_irte)</span><br><span class="line">        <span class="keyword">return</span> -EINVAL;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> kvm_x86_ops-&gt;update_pi_irte(kvm, host_irq, guest_irq, <span class="built_in">set</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>既然irq bypass的producer和consumer token匹配成功时就调用了<code>update_pi_irte</code>，为什么还要在<code>kvm_irq_routing_update</code>中也调用<code>update_pi_irte</code>呢？其实是在guest内部做irq balance时才会触发。</p><p>以guest内部对msi-x table中断做irq balance为例, qemu的函数调用链如下:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">msix_table_mmio_write</span><br><span class="line">└── msix_handle_mask_update</span><br><span class="line">    └── msix_fire_vector_notifier</span><br><span class="line">        └── vfio_msix_vector_use</span><br><span class="line">            └── vfio_msix_vector_do_use</span><br><span class="line">                └── vfio_update_kvm_msi_virq</span><br><span class="line">                    ├── kvm_irqchip_update_msi_route</span><br><span class="line">                    └── kvm_irqchip_commit_routes</span><br><span class="line">                        └── kvm_vm_ioctl(s, KVM_SET_GSI_ROUTING, s-&gt;irq_routes)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2024/07/011.jpg" alt></p><p><img src="/images/2024/07/012.jpg" alt></p><p>guest内部做irq balance时，可能会更改vCPU与vector号，需要将guest更改后的vector更新到IRTE中的vv字段。所以在guest内部做irq balance时，需要调用<code>update_pi_irte</code>来更新IRTE。</p><hr><p>参考资料:</p><ol><li><a href="https://lwn.net/Articles/653706/" target="_blank" rel="noopener">virt: IRQ bypass manager</a></li><li><a href="https://github.com/torvalds/linux/blob/master/include/linux/irqbypass.h" target="_blank" rel="noopener">include/linux/irqbypass.h</a></li><li><a href="https://lore.kernel.org/kvm/1442586596-5920-2-git-send-email-feng.wu@intel.com/" target="_blank" rel="noopener">virt: IRQ bypass manager</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;IRQ bypass仅仅是一套软件框架而已!
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="QEMU" scheme="http://liujunming.github.io/tags/QEMU/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
      <category term="VFIO" scheme="http://liujunming.github.io/tags/VFIO/"/>
    
  </entry>
  
  <entry>
    <title>File Descriptor Transfer over Unix Domain Sockets</title>
    <link href="http://liujunming.github.io/2024/07/14/File-Descriptor-Transfer-over-Unix-Domain-Sockets/"/>
    <id>http://liujunming.github.io/2024/07/14/File-Descriptor-Transfer-over-Unix-Domain-Sockets/</id>
    <published>2024-07-14T10:33:15.000Z</published>
    <updated>2024-07-14T10:52:39.859Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Transfer-over-UDS"><a href="#Data-Transfer-over-UDS" class="headerlink" title="Data Transfer over UDS"></a>Data Transfer over UDS</h2><p>Now that we’ve established that a Unix domain socket allows communication between two processes on the same host, it’s time to explore <em>what</em> kind of data can be transferred over a Unix domain socket.<a id="more"></a></p><p>Since a Unix domain socket is similar to network sockets in many respects, <em>any</em> data that one might usually send over a network socket can be sent over a Unix domain socket.</p><p>Furthermore, the special system calls <code>sendmsg</code> and <code>recvmsg</code> allow sending a <em>special</em> message across the Unix domain socket. This message is handled specially by the kernel, which allows passing open <strong>file descriptions</strong> from the sender to the receiver.</p><h3 id="File-Descriptors-vs-File-Description"><a href="#File-Descriptors-vs-File-Description" class="headerlink" title="File Descriptors vs File Description"></a>File Descriptors vs File Description</h3><p>Note that I mentioned <strong>file descripTION</strong> and not <strong>file descripTOR</strong>. The difference between the two is subtle and isn’t often well understood.</p><p>A <strong>file descriptor</strong> really is just a <em>per process</em> pointer to an underlying kernel data structure called the <strong>file description</strong>. The kernel maintains a table of all open <strong>file descriptions</strong> called the <strong>open file table</strong>. If two processes (A and B) try to open the same file, the two processes might have their own separate <strong>file descriptors</strong>, which point to the same <strong>file description</strong> in the open file table.</p><p><img src="/images/2024/07/010.webp" alt></p><p>So “sending a file descriptor” from one Unix domain socket to another with <code>sendmsg()</code> really just means sending a <em>reference to the file description</em>. If process A were to send file descriptor 0 (fd0) to process B, the file descriptor might very well be referenced by the number 3 (fd3) in process B. They will, however, refer to the same <em>file description</em>.</p><p>The sending process calls <code>sendmsg</code> to send the descriptor across the Unix domain socket. The receiving process calls <code>recvmsg</code> to receive the descriptor on the Unix domain socket.</p><p>Even if the sending process closes its file descriptor referencing the file description being passed via <code>sendmsg</code> before the receiving process calls <code>recvmsg</code>, the file description remains open for the receiving process. Sending a descriptor increments the description’s reference count by one. The kernel only removes file descriptions from its open file table if the reference count drops to 0.</p><h3 id="sendmsg-and-recvmsg"><a href="#sendmsg-and-recvmsg" class="headerlink" title="sendmsg and recvmsg"></a>sendmsg and recvmsg</h3><p>The signature for the <code>sendmsg</code> function call on Linux is the following:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> sendmsg(</span><br><span class="line">    <span class="keyword">int</span> socket,</span><br><span class="line">    <span class="keyword">const</span> struct msghdr *message,</span><br><span class="line">    <span class="keyword">int</span> flags</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><p>The counterpart of <code>sendmsg</code> is <code>recvmsg</code>:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> recvmsg(</span><br><span class="line">     <span class="keyword">int</span> sockfd,</span><br><span class="line">     <span class="keyword">const</span> struct msghdr *msg,</span><br><span class="line">     <span class="keyword">int</span> flags</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><p>The special “message” that one can transfer with <code>sendmsg</code> over a Unix domain socket is specified by the <code>msghdr</code>. The process which wishes to send the file description over to another process creates a <code>msghdr</code> structure containing the description to be passed.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">msghdr</span> &#123;</span></span><br><span class="line">    <span class="keyword">void</span>            *msg_name;      <span class="comment">/* optional address */</span></span><br><span class="line">    <span class="keyword">socklen_t</span>       msg_namelen;    <span class="comment">/* size of address */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span>          <span class="title">iovec</span> *<span class="title">msg_iov</span>;</span> <span class="comment">/* scatter/gather array */</span></span><br><span class="line">    <span class="keyword">int</span>             msg_iovlen;     <span class="comment">/* # elements in msg_iov */</span></span><br><span class="line">    <span class="keyword">void</span>            *msg_control;   <span class="comment">/* ancillary data, see below */</span></span><br><span class="line">    <span class="keyword">socklen_t</span>       msg_controllen; <span class="comment">/* ancillary data buffer len */</span></span><br><span class="line">    <span class="keyword">int</span>             msg_flags;      <span class="comment">/* flags on received message */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The <code>msg_control</code> member of the <code>msghdr</code> structure, which has length <code>msg_controllen</code>, points to a buffer of messages of the form:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cmsghdr</span> &#123;</span></span><br><span class="line">    <span class="keyword">socklen_t</span> cmsg_len;    <span class="comment">/* data byte count, including header */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_level;  <span class="comment">/* originating protocol */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_type;   <span class="comment">/* protocol-specific type */</span></span><br><span class="line">    <span class="comment">/* followed by */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> cmsg_data[];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>In POSIX, a buffer of <strong>struct cmsghdr</strong> structures <strong>with appended data</strong> is called <strong>ancillary data</strong>. On Linux, the maximum buffer size allowed per socket can be set by modifying <code>/proc/sys/net/core/optmem_max</code>.</p><h2 id="Ancillary-Data-Transfer"><a href="#Ancillary-Data-Transfer" class="headerlink" title="Ancillary Data Transfer"></a>Ancillary Data Transfer</h2><p>While there are a plethora of gotchas with such data transfer, when used correctly, it can be a pretty powerful mechanism to achieve a number of goals.</p><p>On Linux, there are three such types of “ancillary data” that can be shared between two Unix domain sockets:</p><ul><li><code>SCM_RIGHTS</code></li><li><code>SCM_CREDENTIALS</code></li><li><code>SCM_SECURITY</code></li></ul><p>All three forms of ancillary data should <strong>only</strong> be accessed using the macros described below and never directly.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct cmsghdr *<span class="title">CMSG_FIRSTHDR</span><span class="params">(struct msghdr *msgh)</span></span>;</span><br><span class="line"><span class="function">struct cmsghdr *<span class="title">CMSG_NXTHDR</span><span class="params">(struct msghdr *msgh, struct cmsghdr *cmsg)</span></span>;</span><br><span class="line"><span class="keyword">size_t</span> CMSG_ALIGN(<span class="keyword">size_t</span> length);</span><br><span class="line"><span class="keyword">size_t</span> CMSG_SPACE(<span class="keyword">size_t</span> length);</span><br><span class="line"><span class="keyword">size_t</span> CMSG_LEN(<span class="keyword">size_t</span> length);</span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="keyword">char</span> *<span class="title">CMSG_DATA</span><span class="params">(struct cmsghdr *cmsg)</span></span>;</span><br></pre></td></tr></table></figure><p>While I’ve never had a need to use the latter two, <code>SCM_RIGHTS</code> is what I hope to explore more in this post.</p><h3 id="SCM-RIGHTS"><a href="#SCM-RIGHTS" class="headerlink" title="SCM_RIGHTS"></a>SCM_RIGHTS</h3><p><code>SCM_RIGHTS</code> allows a process to send or receive a set of open file descriptors from another process using <code>sendmsg</code>.</p><p>The <code>cmsg_data</code> component of the <code>cmsghdr</code> structure can contain an array of the file descriptors that a process wants to send to another.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cmsghdr</span> &#123;</span></span><br><span class="line">    <span class="keyword">socklen_t</span> cmsg_len;    <span class="comment">/* data byte count, including header */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_level;  <span class="comment">/* originating protocol */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_type;   <span class="comment">/* protocol-specific type */</span></span><br><span class="line">    <span class="comment">/* followed by */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> cmsg_data[];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The receiving process uses <code>recvmsg</code> to receive the data.</p><p>The book The Linux Programming Interface has a <a href="https://man7.org/tlpi/code/online/dist/sockets/scm_rights_send.c.html" target="_blank" rel="noopener">good programmatic guide</a> on how to use the <code>sendmsg</code> and <code>recvmsg</code>.</p><hr><p>参考资料:</p><ol><li><a href="https://copyconstruct.medium.com/file-descriptor-transfer-over-unix-domain-sockets-dcbbf5b3b6ec" target="_blank" rel="noopener">File Descriptor Transfer over Unix Domain Sockets</a></li><li><a href="https://broman.dev/download/The%20Linux%20Programming%20Interface.pdf" target="_blank" rel="noopener">The Linux Programming Interface</a></li><li><a href="https://man7.org/linux/man-pages/man7/unix.7.html" target="_blank" rel="noopener">man unix</a></li><li><a href="https://dengking.github.io/Linux-OS/Programming/IO/IO-data-structure/File-descriptor/Pass-file-descriptor/" target="_blank" rel="noopener">Share file descriptor between process</a></li><li><a href="https://www.cnblogs.com/nufangrensheng/p/3571370.html" target="_blank" rel="noopener">高级进程间通信之传送文件描述符</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Data-Transfer-over-UDS&quot;&gt;&lt;a href=&quot;#Data-Transfer-over-UDS&quot; class=&quot;headerlink&quot; title=&quot;Data Transfer over UDS&quot;&gt;&lt;/a&gt;Data Transfer over UDS&lt;/h2&gt;&lt;p&gt;Now that we’ve established that a Unix domain socket allows communication between two processes on the same host, it’s time to explore &lt;em&gt;what&lt;/em&gt; kind of data can be transferred over a Unix domain socket.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>源码解析:vhost ioeventfd与irqfd</title>
    <link href="http://liujunming.github.io/2024/07/13/vhost-eventfd-pov/"/>
    <id>http://liujunming.github.io/2024/07/13/vhost-eventfd-pov/</id>
    <published>2024-07-13T08:21:01.000Z</published>
    <updated>2024-07-14T10:29:47.554Z</updated>
    
    <content type="html"><![CDATA[<p>本文将结合qemu与linux源码，解析vhost中ioeventfd与irqfd相关内容。<a id="more"></a></p><h2 id="prerequisite"><a href="#prerequisite" class="headerlink" title="prerequisite"></a>prerequisite</h2><ul><li><a href="/2024/03/24/QEMU-Internals-vhost-architecture/">QEMU Internals: vhost architecture</a></li><li><a href="http://liujunming.top/2021/10/26/Dive-into-ioeventfd%28kvm%20side%29-mechanism/" target="_blank" rel="noopener">Dive into ioeventfd(KVM side) mechanism</a></li><li><a href="/2021/10/27/Dive-into-irqfd-KVM-side-mechanism/">Dive into irqfd(KVM side) mechanism</a></li></ul><h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><p><img src="/images/2024/03/010.png" alt></p><p>ioeventfd与kick绑定，irqfd与中断绑定</p><p>ioeventfd:</p><ul><li>qemu利用<code>KVM_IOEVENTFD</code> ioctl，将ioeventfd与guest kick寄存器的地址(pio/mmio地址)和vq index的值绑定，传给kvm<ul><li>当kvm检测到guest往kick寄存器写入vq index后，写eventfd通知vhost</li></ul></li><li>qemu利用<code>VHOST_SET_VRING_KICK</code> ioctl，将ioeventfd传给vhost，vhost就会poll ioeventfd的写<ul><li>当vhost poll到ioeventfd的写后，就会开始从avai ring中拉取请求，处理完io请求后，更新used ring，最后给guest注入中断(需要借助于irqfd)</li></ul></li></ul><p>irqfd:</p><ul><li>qemu利用<code>VHOST_SET_VRING_CALL</code> ioctl，将irqfd传给vhost<ul><li>vhost在更新完used ring后，写eventfd通知kvm注入中断</li></ul></li><li>qemu利用<code>KVM_IRQFD</code> ioctl，将irqfd与vq的中断绑定，传给kvm，kvm就会poll irqfd的写<ul><li>当kvm poll到irqfd的写后，就会根据中断路由信息，给guest注入中断</li></ul></li></ul><h2 id="ioeventfd"><a href="#ioeventfd" class="headerlink" title="ioeventfd"></a>ioeventfd</h2><h3 id="qemu侧ioeventfd的关联"><a href="#qemu侧ioeventfd的关联" class="headerlink" title="qemu侧ioeventfd的关联"></a>qemu侧ioeventfd的关联</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">virtio_bus_start_ioeventfd</span><br><span class="line">└── virtio_device_start_ioeventfd_impl[start_ioeventfd]</span><br><span class="line">    ├── event_notifier_set(&amp;vq-&gt;host_notifier)</span><br><span class="line">    └── memory_region_transaction_commit</span><br><span class="line">        └── address_space_update_ioeventfds</span><br><span class="line">            └── address_space_add_del_ioeventfds</span><br><span class="line">                ├── kvm_io_ioeventfd_add[eventfd_add] <span class="comment">//pio</span></span><br><span class="line">                │   └── kvm_set_ioeventfd_pio</span><br><span class="line">                │       └── kvm_vm_ioctl(kvm_state, KVM_IOEVENTFD, &amp;kick)</span><br><span class="line">                └── kvm_mem_ioeventfd_add[eventfd_add] <span class="comment">//mmio</span></span><br><span class="line">                    └── kvm_set_ioeventfd_mmio</span><br><span class="line">                        └── kvm_vm_ioctl(kvm_state, KVM_IOEVENTFD, &amp;iofd)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vhost_virtqueue_start</span><br><span class="line">├── event_notifier_get_fd(virtio_queue_get_host_notifier(vvq))</span><br><span class="line">└── vhost_kernel_set_vring_kick[vhost_set_vring_kick]</span><br><span class="line">    └── vhost_kernel_call(dev, VHOST_SET_VRING_KICK, file)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">VirtQueue</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        ...</span><br><span class="line">        EventNotifier host_notifier;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由上述代码可知，qemu侧通过host_notifier的ioeventfd，将vhost与kvm关联了起来；</p><ul><li>vhost负责poll ioeventfd</li><li>kvm负责写ioeventfd来通知vhost guest的kick操作</li></ul><h3 id="kvm侧ioeventfd处理"><a href="#kvm侧ioeventfd处理" class="headerlink" title="kvm侧ioeventfd处理"></a>kvm侧ioeventfd处理</h3><p>参考<a href="http://liujunming.top/2021/10/26/Dive-into-ioeventfd%28kvm%20side%29-mechanism/" target="_blank" rel="noopener">Dive into ioeventfd(KVM side) mechanism</a>即可。</p><h3 id="vhost侧ioeventfd处理"><a href="#vhost侧ioeventfd处理" class="headerlink" title="vhost侧ioeventfd处理"></a>vhost侧ioeventfd处理</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">vhost_vring_ioctl</span><span class="params">(struct vhost_dev *d, <span class="keyword">unsigned</span> <span class="keyword">int</span> ioctl, <span class="keyword">void</span> __user *argp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">switch</span> (ioctl) &#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">case</span> VHOST_SET_VRING_KICK:</span><br><span class="line"><span class="keyword">if</span> (copy_from_user(&amp;f, argp, <span class="keyword">sizeof</span> f)) &#123;</span><br><span class="line">r = -EFAULT;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">eventfp = f.fd == VHOST_FILE_UNBIND ? <span class="literal">NULL</span> : eventfd_fget(f.fd);</span><br><span class="line"><span class="keyword">if</span> (IS_ERR(eventfp)) &#123;</span><br><span class="line">r = PTR_ERR(eventfp);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (eventfp != vq-&gt;kick) &#123;</span><br><span class="line">pollstop = (filep = vq-&gt;kick) != <span class="literal">NULL</span>;</span><br><span class="line">pollstart = (vq-&gt;kick = eventfp) != <span class="literal">NULL</span>;</span><br><span class="line">&#125; <span class="keyword">else</span></span><br><span class="line">filep = eventfp;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">        ...</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> (pollstart &amp;&amp; vq-&gt;handle_kick)</span><br><span class="line">        r = vhost_poll_start(&amp;vq-&gt;poll, vq-&gt;kick);</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>qemu利用<code>VHOST_SET_VRING_KICK</code> ioctl，将ioeventfd传给vhost，然后vhost就开始poll ioeventfd(<code>vhost_poll_start</code>)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Start polling a file. We add ourselves to file's wait queue. The caller must</span></span><br><span class="line"><span class="comment"> * keep a reference to a file until after vhost_poll_stop is called. */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">vhost_poll_start</span><span class="params">(struct vhost_poll *poll, struct file *file)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">__poll_t</span> mask;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (poll-&gt;wqh)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">mask = vfs_poll(file, &amp;poll-&gt;table); <span class="comment">//调用callback vhost_poll_func</span></span><br><span class="line"><span class="keyword">if</span> (mask)</span><br><span class="line">vhost_poll_wakeup(&amp;poll-&gt;wait, <span class="number">0</span>, <span class="number">0</span>, poll_to_key(mask));</span><br><span class="line"><span class="keyword">if</span> (mask &amp; EPOLLERR) &#123;</span><br><span class="line">vhost_poll_stop(poll);</span><br><span class="line"><span class="keyword">return</span> -EINVAL;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(vhost_poll_start);</span><br></pre></td></tr></table></figure><p>当vhost poll到ioeventfd写后，就会触发<code>vhost_poll_wakeup</code>回调。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vhost_poll_wakeup</span><br><span class="line">└── vhost_poll_queue</span><br><span class="line">    └── vhost_vq_work_queue</span><br><span class="line">        └── vhost_worker_queue</span><br><span class="line">            ├── llist_add(&amp;work-&gt;node, &amp;worker-&gt;work_list)</span><br><span class="line">            └── vhost_task_wake(worker-&gt;vtsk)</span><br></pre></td></tr></table></figure></p><p><code>worker-&gt;vtsk</code>又会如何操作呢？且看<code>worker-&gt;vtsk</code>的初始化情况以及<code>worker-&gt;vtsk</code>的执行函数吧。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> struct vhost_worker *<span class="title">vhost_worker_create</span><span class="params">(struct vhost_dev *dev)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_worker</span> *<span class="title">worker</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_task</span> *<span class="title">vtsk</span>;</span></span><br><span class="line"><span class="keyword">char</span> name[TASK_COMM_LEN];</span><br><span class="line"><span class="keyword">int</span> ret;</span><br><span class="line">u32 id;</span><br><span class="line"></span><br><span class="line">worker = kzalloc(<span class="keyword">sizeof</span>(*worker), GFP_KERNEL_ACCOUNT);</span><br><span class="line"><span class="keyword">if</span> (!worker)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">snprintf</span>(name, <span class="keyword">sizeof</span>(name), <span class="string">"vhost-%d"</span>, current-&gt;pid);</span><br><span class="line"></span><br><span class="line">vtsk = vhost_task_create(vhost_worker, worker, name);</span><br><span class="line"><span class="keyword">if</span> (!vtsk)</span><br><span class="line"><span class="keyword">goto</span> free_worker;</span><br><span class="line"></span><br><span class="line">mutex_init(&amp;worker-&gt;mutex);</span><br><span class="line">init_llist_head(&amp;worker-&gt;work_list);</span><br><span class="line">worker-&gt;kcov_handle = kcov_common_handle();</span><br><span class="line">worker-&gt;vtsk = vtsk;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">bool</span> <span class="title">vhost_worker</span><span class="params">(<span class="keyword">void</span> *data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_worker</span> *<span class="title">worker</span> = <span class="title">data</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_work</span> *<span class="title">work</span>, *<span class="title">work_next</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">llist_node</span> *<span class="title">node</span>;</span></span><br><span class="line"></span><br><span class="line">node = llist_del_all(&amp;worker-&gt;work_list);</span><br><span class="line"><span class="keyword">if</span> (node) &#123;</span><br><span class="line">__set_current_state(TASK_RUNNING);</span><br><span class="line"></span><br><span class="line">node = llist_reverse_order(node);</span><br><span class="line"><span class="comment">/* make sure flag is seen after deletion */</span></span><br><span class="line">smp_wmb();</span><br><span class="line">llist_for_each_entry_safe(work, work_next, node, node) &#123;</span><br><span class="line">clear_bit(VHOST_WORK_QUEUED, &amp;work-&gt;flags);</span><br><span class="line">kcov_remote_start_common(worker-&gt;kcov_handle);</span><br><span class="line">work-&gt;fn(work); <span class="comment">//vq-&gt;handle_kick</span></span><br><span class="line">kcov_remote_stop();</span><br><span class="line">cond_resched();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> !!node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在初始化过程中，vhost会创建一个名为<code>vhost-$pid</code>的内核线程，其中$pid是QEMU进程的pid。该线程被称为“vhost工作线程”。<br>vhost工作线程的运行函数为<code>vhost_worker</code>，而<code>vhost_worker</code>就会触发<code>vq-&gt;handle_kick</code>的回调。</p><p>那么<code>vhost_poll</code>与<code>vq-&gt;handle_kick</code>等又是如何初始化的呢？</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vhost_dev_init</span><span class="params">(struct vhost_dev *dev,</span></span></span><br><span class="line"><span class="function"><span class="params">    struct vhost_virtqueue **vqs, <span class="keyword">int</span> nvqs,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> iov_limit, <span class="keyword">int</span> weight, <span class="keyword">int</span> byte_weight,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">bool</span> use_worker,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> (*msg_handler)</span><span class="params">(struct vhost_dev *dev, u32 asid,</span></span></span><br><span class="line">       struct vhost_iotlb_msg *msg))</span><br><span class="line">&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_virtqueue</span> *<span class="title">vq</span>;</span></span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">dev-&gt;vqs = vqs;</span><br><span class="line">dev-&gt;nvqs = nvqs;</span><br><span class="line">mutex_init(&amp;dev-&gt;mutex);</span><br><span class="line">dev-&gt;log_ctx = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;umem = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;iotlb = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;mm = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;iov_limit = iov_limit;</span><br><span class="line">dev-&gt;weight = weight;</span><br><span class="line">dev-&gt;byte_weight = byte_weight;</span><br><span class="line">dev-&gt;use_worker = use_worker;</span><br><span class="line">dev-&gt;msg_handler = msg_handler;</span><br><span class="line">init_waitqueue_head(&amp;dev-&gt;wait);</span><br><span class="line">INIT_LIST_HEAD(&amp;dev-&gt;read_list);</span><br><span class="line">INIT_LIST_HEAD(&amp;dev-&gt;pending_list);</span><br><span class="line">spin_lock_init(&amp;dev-&gt;iotlb_lock);</span><br><span class="line">xa_init_flags(&amp;dev-&gt;worker_xa, XA_FLAGS_ALLOC);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; dev-&gt;nvqs; ++i) &#123;</span><br><span class="line">vq = dev-&gt;vqs[i];</span><br><span class="line">vq-&gt;<span class="built_in">log</span> = <span class="literal">NULL</span>;</span><br><span class="line">vq-&gt;indirect = <span class="literal">NULL</span>;</span><br><span class="line">vq-&gt;heads = <span class="literal">NULL</span>;</span><br><span class="line">vq-&gt;dev = dev;</span><br><span class="line">mutex_init(&amp;vq-&gt;mutex);</span><br><span class="line">vhost_vq_reset(dev, vq);</span><br><span class="line"><span class="keyword">if</span> (vq-&gt;handle_kick)</span><br><span class="line">vhost_poll_init(&amp;vq-&gt;poll, vq-&gt;handle_kick,</span><br><span class="line">EPOLLIN, dev, vq);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>vhost设备初始化时，会为每个vq调用<code>vhost_poll_init</code>来初始化<code>vhost_poll</code>与<code>vq-&gt;handle_kick</code>等内容。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Init poll structure */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vhost_poll_init</span><span class="params">(struct vhost_poll *poll, <span class="keyword">vhost_work_fn_t</span> fn,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">__poll_t</span> mask, struct vhost_dev *dev,</span></span></span><br><span class="line"><span class="function"><span class="params">     struct vhost_virtqueue *vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">init_waitqueue_func_entry(&amp;poll-&gt;wait, vhost_poll_wakeup); <span class="comment">//设置wait的回调，在vhost poll到ioeventfd的写后，会触发回调</span></span><br><span class="line">init_poll_funcptr(&amp;poll-&gt;table, vhost_poll_func); <span class="comment">//vhost_poll_func在vfs_poll时会触发回调，加入到file's wait queue中</span></span><br><span class="line">poll-&gt;mask = mask;</span><br><span class="line">poll-&gt;dev = dev;</span><br><span class="line">poll-&gt;wqh = <span class="literal">NULL</span>;</span><br><span class="line">poll-&gt;vq = vq;</span><br><span class="line"></span><br><span class="line">vhost_work_init(&amp;poll-&gt;work, fn); <span class="comment">//fn为vq-&gt;handle_kick，初始化为work-&gt;fn，在vhost_worker中会回调</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">vhost_poll_func</span><span class="params">(struct file *file, <span class="keyword">wait_queue_head_t</span> *wqh, <span class="comment">//在vfs_poll时会触发回调，加入到file's wait queue中</span></span></span></span><br><span class="line"><span class="function"><span class="params">    poll_table *pt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_poll</span> *<span class="title">poll</span>;</span></span><br><span class="line"></span><br><span class="line">poll = container_of(pt, struct vhost_poll, table);</span><br><span class="line">poll-&gt;wqh = wqh;</span><br><span class="line">add_wait_queue(wqh, &amp;poll-&gt;wait);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* The virtqueue structure describes a queue attached to a device. */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_virtqueue</span> &#123;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_dev</span> *<span class="title">dev</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_worker</span> __<span class="title">rcu</span> *<span class="title">worker</span>;</span></span><br><span class="line">...</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_poll</span> <span class="title">poll</span>;</span></span><br><span class="line"><span class="comment">/* The routine to call when the Guest pings us, or timeout. */</span></span><br><span class="line"><span class="keyword">vhost_work_fn_t</span> handle_kick;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结下vhost侧poll ioeventfd的流程:</p><ol><li><code>vhost_poll_func</code>会让vhost poll ioeventfd，加入到file’s wait queue中</li><li>kvm写ioeventfd通知vhost</li><li>vhost回调<code>vhost_poll_wakeup</code>，将work加入到workqueue中，唤醒vhost工作线程</li><li>vhost工作线程回调<code>vq-&gt;handle_kick</code>，处理vq中的io请求</li></ol><h2 id="irqfd"><a href="#irqfd" class="headerlink" title="irqfd"></a>irqfd</h2><h3 id="qemu侧irqfd的关联"><a href="#qemu侧irqfd的关联" class="headerlink" title="qemu侧irqfd的关联"></a>qemu侧irqfd的关联</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">virtio_pci_set_guest_notifiers</span><br><span class="line">└── kvm_virtio_pci_vector_use</span><br><span class="line">    └── kvm_virtio_pci_irqfd_use</span><br><span class="line">        ├── virtio_queue_get_guest_notifier</span><br><span class="line">        └── kvm_irqchip_add_irqfd_notifier_gsi</span><br><span class="line">            └── kvm_irqchip_assign_irqfd</span><br><span class="line">                └── kvm_vm_ioctl(s, KVM_IRQFD, &amp;irqfd)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vhost_virtqueue_start</span><br><span class="line">└── vhost_virtqueue_mask</span><br><span class="line">    ├── event_notifier_get_wfd(virtio_queue_get_guest_notifier(vvq))</span><br><span class="line">    └── vhost_kernel_set_vring_call[vhost_set_vring_call]</span><br><span class="line">        └── vhost_kernel_call(dev, VHOST_SET_VRING_CALL, file)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">VirtQueue</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        ...</span><br><span class="line">        EventNotifier guest_notifier</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由上述代码可知，qemu侧通过guest_notifier的irqfd，将vhost与kvm关联了起来；</p><ul><li>kvm负责poll irqfd，然后给vm注入中断</li><li>vhost在更新完used ring后，写irqfd来通知kvm注入中断</li></ul><h3 id="kvm侧irqfd处理"><a href="#kvm侧irqfd处理" class="headerlink" title="kvm侧irqfd处理"></a>kvm侧irqfd处理</h3><p>参考<a href="/2021/10/27/Dive-into-irqfd-KVM-side-mechanism/">Dive into irqfd(KVM side) mechanism</a>即可。</p><h3 id="vhost侧irqfd处理"><a href="#vhost侧irqfd处理" class="headerlink" title="vhost侧irqfd处理"></a>vhost侧irqfd处理</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">vhost_vring_ioctl</span><span class="params">(struct vhost_dev *d, <span class="keyword">unsigned</span> <span class="keyword">int</span> ioctl, <span class="keyword">void</span> __user *argp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">switch</span> (ioctl) &#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">case</span> VHOST_SET_VRING_CALL:</span><br><span class="line"><span class="keyword">if</span> (copy_from_user(&amp;f, argp, <span class="keyword">sizeof</span> f)) &#123;</span><br><span class="line">r = -EFAULT;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">ctx = f.fd == VHOST_FILE_UNBIND ? <span class="literal">NULL</span> : eventfd_ctx_fdget(f.fd);</span><br><span class="line"><span class="keyword">if</span> (IS_ERR(ctx)) &#123;</span><br><span class="line">r = PTR_ERR(ctx);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">swap(ctx, vq-&gt;call_ctx.ctx);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">        ...</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>qemu调用<code>VHOST_SET_VRING_CALL</code>，将irqfd传递给vhost</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* This actually signals the guest, using eventfd. */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vhost_signal</span><span class="params">(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/* Signal the Guest tell them we used something up. */</span></span><br><span class="line"><span class="keyword">if</span> (vq-&gt;call_ctx.ctx &amp;&amp; vhost_notify(dev, vq))</span><br><span class="line">eventfd_signal(vq-&gt;call_ctx.ctx, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(vhost_signal);</span><br></pre></td></tr></table></figure><p>vhost在处理完io请求，并更新used ring后，调用<code>vhost_signal</code>，触发irqfd的写；kvm poll到后，就会给vm注入中断。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将结合qemu与linux源码，解析vhost中ioeventfd与irqfd相关内容。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/tags/QEMU-KVM/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe command retry机制</title>
    <link href="http://liujunming.github.io/2024/07/07/Notes-about-NVMe-command-retry%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2024/07/07/Notes-about-NVMe-command-retry机制/</id>
    <published>2024-07-07T13:04:08.000Z</published>
    <updated>2024-07-07T13:57:13.439Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下NVMe协议中的command retry机制。<a id="more"></a><br><img src="/images/2024/07/007.jpg" alt></p><p><img src="/images/2024/07/008.jpg" alt></p><p><img src="/images/2024/07/009.jpg" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NVME_SC_DNR         = <span class="number">0x4000</span>,</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">nvme_complete_rq</span><span class="params">(struct request *req)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">switch</span> (nvme_decide_disposition(req)) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">case</span> RETRY:</span><br><span class="line">        nvme_retry_req(req);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> nvme_disposition &#123;</span><br><span class="line">    COMPLETE,</span><br><span class="line">    RETRY,</span><br><span class="line">    FAILOVER,</span><br><span class="line">    AUTHENTICATE,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">enum</span> nvme_disposition <span class="title">nvme_decide_disposition</span><span class="params">(struct request *req)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (likely(nvme_req(req)-&gt;status == <span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> COMPLETE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((nvme_req(req)-&gt;status &amp; <span class="number">0x7ff</span>) == NVME_SC_AUTH_REQUIRED)</span><br><span class="line">        <span class="keyword">return</span> AUTHENTICATE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (blk_noretry_request(req) ||</span><br><span class="line">        (nvme_req(req)-&gt;status &amp; NVME_SC_DNR) ||</span><br><span class="line">        nvme_req(req)-&gt;retries &gt;= nvme_max_retries)</span><br><span class="line">        <span class="keyword">return</span> COMPLETE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (req-&gt;cmd_flags &amp; REQ_NVME_MPATH) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nvme_is_path_error(nvme_req(req)-&gt;status) ||</span><br><span class="line">            blk_queue_dying(req-&gt;q))</span><br><span class="line">            <span class="keyword">return</span> FAILOVER;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (blk_queue_dying(req-&gt;q))</span><br><span class="line">            <span class="keyword">return</span> COMPLETE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> RETRY;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> u8 nvme_max_retries = <span class="number">5</span>;</span><br><span class="line">module_param_named(max_retries, nvme_max_retries, byte, <span class="number">0644</span>);</span><br><span class="line">MODULE_PARM_DESC(max_retries, <span class="string">"max number of retries a command may have"</span>);</span><br></pre></td></tr></table></figure><p>默认情况下，每个command最多retry5次。</p><hr><p>参考资料:</p><ol><li>NVMe 1.3 spec</li><li>Linux kernel source code</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下NVMe协议中的command retry机制。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about VIRTIO_F_EVENT_IDX feature</title>
    <link href="http://liujunming.github.io/2024/07/06/Notes-about-VIRTIO-F-EVENT-IDX-feature/"/>
    <id>http://liujunming.github.io/2024/07/06/Notes-about-VIRTIO-F-EVENT-IDX-feature/</id>
    <published>2024-07-06T05:03:38.000Z</published>
    <updated>2024-07-06T05:33:07.340Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio VIRTIO_F_EVENT_IDX feature相关notes。<a id="more"></a></p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>In many systems used and available buffer notifications involve significant overhead. To mitigate it, each VQ maintains a flag to indicate when it wants to be notified. Remember that the driver’s one is read-only by the device, and the device’s one is read-only by the driver.<br>在许多系统中，已使用和可用的缓冲区通知涉及大量开销。为了减少开销，每个 VQ 都会保留一个标志，用于指示何时需要通知。请记住，驱动程序的标志是设备只读的，而设备的标志是驱动程序只读的。</p><p>We already know all of this, and its use is pretty straightforward. The only thing you need to take care of is the asynchronous nature of this method: The side of the communication that disables or enables it can’t be sure that the other end is going to know the change, so you can miss notifications or to have more than expected.</p><p>这些我们都已经知道了，使用起来也非常简单。唯一需要注意的是该方法的异步性质： 通信的一端禁用或启用它时，无法确定另一端是否会知道这一变化，因此可能会错过通知或出现比预期更多的通知。</p><p>A more effective way of notifications toggle is enabled if the <code>VIRTIO_F_EVENT_IDX</code> feature bit is negotiated by device and driver: Instead of disable them in a binary fashion, driver and device can specify how far the other can progress before a notification is required using an specific descriptor id.<br>如果通过设备和驱动程序协商<code>VIRTIO_F_EVENT_IDX</code>功能位，就能启用更有效的通知切换方法： 与二进制禁用方式不同，驱动程序和设备可以使用特定的描述符 id 来指定对方在需要通知之前可以进行到什么程度。</p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p><img src="/images/2024/07/001.jpg" alt></p><h2 id="Used-Buffer-Notification-Suppression"><a href="#Used-Buffer-Notification-Suppression" class="headerlink" title="Used Buffer Notification Suppression"></a>Used Buffer Notification Suppression</h2><p><img src="/images/2024/07/002.jpg" alt></p><p><img src="/images/2024/07/003.jpg" alt></p><p><img src="/images/2024/07/004.jpg" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>driver往avail ring中写flags与used_event</li><li>当设备写的used_idx &gt;= used_event时，设备MUST发送中断</li><li>当设备写的used_idx &lt; used_event时，设备SHOULD NOT发送中断</li></ul><h2 id="Available-Buffer-Notification-Suppression"><a href="#Available-Buffer-Notification-Suppression" class="headerlink" title="Available Buffer Notification Suppression"></a>Available Buffer Notification Suppression</h2><p><img src="/images/2024/07/005.jpg" alt></p><p><img src="/images/2024/07/006.jpg" alt></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>设备往used ring中写flags与avail_event</li><li>当driver写的avail_idx &gt;= avail_event时，driver MUST kick</li><li>当driver写的avail_idx &lt; avail_event时，driver SHOULD NOT kick</li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.redhat.com/en/blog/virtqueues-and-virtio-ring-how-data-travels" target="_blank" rel="noopener">Virtqueues and virtio ring: How the data travels</a></li><li>virtio 1.3 spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio VIRTIO_F_EVENT_IDX feature相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>深入理解NVMe CMB机制</title>
    <link href="http://liujunming.github.io/2024/06/30/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3NVMe-CMB%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2024/06/30/深入理解NVMe-CMB机制/</id>
    <published>2024-06-30T02:00:52.000Z</published>
    <updated>2024-06-30T10:52:45.680Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下NVMe CMB机制相关notes。<a id="more"></a></p><h2 id="1-What"><a href="#1-What" class="headerlink" title="1. What"></a>1. What</h2><p>CMB（Controller Memory Buffer）是指SSD控制器内部的读写缓冲区，与HMB（Host Memory Buffer）的不同处在于所使用的内存地址位于控制器自己的内存中，而不是位于主机内存中，但它们使用队列的方式都是一样的。</p><p>NVMe CMB是NVMe SSD上的一块内存空间，可以通过PCIe MMIO BAR的方式暴露到主机内存空间中，并可由主机直接读写。这一块内存空间可以用来放置一些NVMe上特定的数据结构（如 SQ，CQ等），也可以将需要传输的数据直接放在CMB中。</p><h2 id="2-CMB-的功能"><a href="#2-CMB-的功能" class="headerlink" title="2. CMB 的功能"></a>2. CMB 的功能</h2><p>CMB 至少需要NVMe设备增加下面两个额外的寄存器，其中CMBLOC表示CMB的位置，CMBSZ表示CMB的大小，另外其中还附带了一些feature的标志位。关于这两个寄存器的细节可以阅读NVMe spec中的相关内容：</p><ul><li>3.1.3.11 Offset 38h: CMBLOC – Controller Memory Buffer Location</li><li>3.1.3.12 Offset 3Ch: CMBSZ – Controller Memory Buffer Size</li></ul><p>下图展示了CMBSZ中所包含的一些feature的介绍:</p><p><img src="/images/2024/06/016.png" alt></p><ol><li>Write Data Support (WDS): 对于将数据从主机写入到设备的命令（如Write），可以直接写入到CMB中</li><li>Read Data Support (RDS): 对于将数据从设备读取到主机的命令（如Read），可以直接从CMB中读取</li><li>PRP SGL List Support (LISTS): 可以将PRP List和SGL List放在CMB中</li><li>Completion Queue Support (CQS): 可以将CQ放在CMB中</li><li>Submission Queue Support (SQS): 可以将SQ放在CMB中</li></ol><p>PRP List的定义如下:<br><img src="/images/2024/06/017.jpg" alt></p><h2 id="3-使用CMB的例子"><a href="#3-使用CMB的例子" class="headerlink" title="3. 使用CMB的例子"></a>3. 使用CMB的例子</h2><h3 id="3-1-将CQ和SQ都放在CMB中"><a href="#3-1-将CQ和SQ都放在CMB中" class="headerlink" title="3.1 将CQ和SQ都放在CMB中"></a>3.1 将CQ和SQ都放在CMB中</h3><p>以写SQ为例，原来是主机先把请求写到内存的SQ，然后写Doorbell通知SSD，然后SSD再从内存中的SQ将命令拷贝过来。</p><p>现在是主机直接写到CMB中的SQ，然后写Doorbell通知SSD。</p><p>两个相比较，后者少了一次one read from the controller to the host，将SQ放在CMB上降低了执行命令的延迟。</p><blockquote><p>Submission Queues in host memory require the controller to perform a PCI Express read from host memory in order to fetch the queue entries. Submission Queues in controller memory enable host software to directly write the entire Submission Queue Entry to the controller’s internal memory space, avoiding one read from the controller to the host. This approach reduces latency in command execution and improves efficiency in a PCI Express fabric topology that may include multiple switches.</p></blockquote><h3 id="3-2-让CMB支持数据传输，优化NIC和NVMe-SSD之间的数据传输"><a href="#3-2-让CMB支持数据传输，优化NIC和NVMe-SSD之间的数据传输" class="headerlink" title="3.2 让CMB支持数据传输，优化NIC和NVMe SSD之间的数据传输"></a>3.2 让CMB支持数据传输，优化NIC和NVMe SSD之间的数据传输</h3><p>原本将数据从NIC发送到SSD需要从内存中转一次，现在不需要，直接发送到CMB里就好。</p><p><img src="/images/2024/06/020.jpg" alt></p><h3 id="3-2-让CMB支持数据传输，优化NVMe-SSD之间的数据传输"><a href="#3-2-让CMB支持数据传输，优化NVMe-SSD之间的数据传输" class="headerlink" title="3.2 让CMB支持数据传输，优化NVMe SSD之间的数据传输"></a>3.2 让CMB支持数据传输，优化NVMe SSD之间的数据传输</h3><p>利用了p2p功能，在NVMe设备之间直接传数据到CMB上即可，完全无需CPU和内存的介入，也不需要Root Complex参与其中。</p><p><img src="/images/2024/06/021.jpg" alt></p><h2 id="4-Spec关键notes"><a href="#4-Spec关键notes" class="headerlink" title="4. Spec关键notes"></a>4. Spec关键notes</h2><p><img src="/images/2024/06/018.jpg" alt></p><h2 id="5-p2p操作序列"><a href="#5-p2p操作序列" class="headerlink" title="5. p2p操作序列"></a>5. p2p操作序列</h2><p><img src="/images/2024/06/019.jpg" alt><br><strong>RDMA网卡利用p2p将数据写入到NVMe盘</strong></p><ol><li>driver计算出要写入到NVMe盘的数据量，从CMB中分配一段连续的buffer</li><li>RDMA利用p2p(在RDMA的MTT中，将VA映射到CMB的MMIO地址即可)将数据写入到CMB中的buffer</li><li>driver往submission queue中下发写盘的命令</li><li>driver更新SQ tail doorbell寄存器</li><li>NVMe controller根据command中的metadata与CMB buffer中的内容，将数据刷到NVMe盘中</li></ol><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>在CMB之前，像SQ、CQ、PRP List、Write Data、Read Data都是放置在HMB(DRAM)中；有了CMB之后，SQ、CQ、PRP List、Write Data、Read Data可以放置在CMB(MMIO)中。</p><p>driver为了效率，将与NVMe controller交互的信息，由DRAM移动到了MMIO(CMB)中。CMB的RDS和WDS可以支持p2p，同时SQS这些feature可以提升NVMe controller的执行效率(比如避免了NVMe controller DMA读取SQE这些信息，直接从CMB中读取即可)。</p><hr><p>参考资料:</p><ol><li>NVMe 1.3 spec</li><li><a href="https://zhuanlan.zhihu.com/p/457874205" target="_blank" rel="noopener">NVME CMB详解</a></li><li><a href="https://nvmexpress.org/wp-content/uploads/Session-2-Enabling-the-NVMe-CMB-and-PMR-Ecosystem-Eideticom-and-Mell....pdf" target="_blank" rel="noopener">Enabling the NVMe™ CMB and PMR Ecosystem</a></li><li><a href="https://0x10.sh/controller-memory-buffers" target="_blank" rel="noopener">Controller Memory Buffers</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下NVMe CMB机制相关notes。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Flow Bifurcation vs RSS</title>
    <link href="http://liujunming.github.io/2024/06/23/Flow-Bifurcation-vs-RSS/"/>
    <id>http://liujunming.github.io/2024/06/23/Flow-Bifurcation-vs-RSS/</id>
    <published>2024-06-23T12:30:14.000Z</published>
    <updated>2024-06-23T13:00:05.922Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/2024/06/23/Notes-about-Flow-Bifurcation-mechanism/">Flow Bifurcation</a>与<a href="/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/">RSS(Receive Side Scaling)</a>都会控制网络流量到指定RX Queue，那么两者的区别又是什么呢？<a id="more"></a></p><blockquote><p>RSS is trying to spread incoming packets across cores while directing packets from common flows to the same core. Unlike RSS, Intel Ethernet Flow Director is trying to establish a unique association between flows and the core with the consuming application. Indexing solely by a hash won’t do that. Distinct flows must be uniquely characterized with a high probability which cannot be accomplished by a hash alone.</p></blockquote><p>RSS只是为了load balance，无法做到queue隔离；比如dpdk的某些流量会导向RX Q1，但是如果只基于RSS的话，根据hash值的计算，内核协议栈的某些流量也会导向RX Q1；此时dpdk的网络流量就会与内核协议栈的流量导向到相同RX Queue，就无法做到queue隔离了。</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/intel-ethernet-flow-director.pdf" target="_blank" rel="noopener">Introduction to Intel® Ethernet Flow Director and Memcached Performance</a></li><li><a href="https://zhuanlan.zhihu.com/p/544596830" target="_blank" rel="noopener">浅谈RSS 和 Flow Director</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/training/setting-up-intel-ethernet-flow-director.html" target="_blank" rel="noopener">How to Set Up Intel® Ethernet Flow Director</a></li><li><a href="https://blog.csdn.net/Rong_Toa/article/details/108987658" target="_blank" rel="noopener">网卡多队列：RPS、RFS、RSS、Flow Director（DPDK支持）</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/2024/06/23/Notes-about-Flow-Bifurcation-mechanism/&quot;&gt;Flow Bifurcation&lt;/a&gt;与&lt;a href=&quot;/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/&quot;&gt;RSS(Receive Side Scaling)&lt;/a&gt;都会控制网络流量到指定RX Queue，那么两者的区别又是什么呢？
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Flow Bifurcation</title>
    <link href="http://liujunming.github.io/2024/06/23/Notes-about-Flow-Bifurcation-mechanism/"/>
    <id>http://liujunming.github.io/2024/06/23/Notes-about-Flow-Bifurcation-mechanism/</id>
    <published>2024-06-23T02:04:52.000Z</published>
    <updated>2024-06-23T10:00:17.313Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要转载自<a href="https://blog.csdn.net/legend050709/article/details/124872468" target="_blank" rel="noopener">DPDK中的流量分叉(flow bifurcation)</a>。<a id="more"></a></p><h2 id="flow-bifurcation介绍"><a href="#flow-bifurcation介绍" class="headerlink" title="flow bifurcation介绍"></a>flow bifurcation介绍</h2><p><img src="/images/2024/06/008.png" alt></p><p><img src="/images/2024/06/009.png" alt></p><p><img src="/images/2024/06/010.png" alt></p><p>使用分岔驱动程序的PMDs与设备内核驱动程序共存。在这样的模型中，NIC由内核控制，而数据路径则由设备上的PMD直接执行。</p><h2 id="流量分叉的优缺点"><a href="#流量分叉的优缺点" class="headerlink" title="流量分叉的优缺点"></a>流量分叉的优缺点</h2><p>这种模式有以下好处:</p><ul><li>它是安全且健壮的.</li></ul><p>因为内存管理和隔离是由内核完成的。</p><ul><li><p>它允许用户在相同的网络端口上运行DPDK应用程序时使用传统的linux工具，如ethtool或ifconfig。</p></li><li><p>它允许DPDK应用程序只过滤部分流量，而其余的流量将由内核驱动程序定向和处理；流分岔由NIC硬件完成，例如，使用流隔离模式<a href="https://doc.dpdk.org/guides/prog_guide/rte_flow.html#flow-isolated-mode" target="_blank" rel="noopener">dpdk rte_flow isolated mode</a>可以严格选择DPDK中接收到的内容。</p></li></ul><p><img src="/images/2024/06/011.png" alt></p><p>优点总结：</p><ul><li>更好的性能</li></ul><p>流量分叉是硬件特性，不需要CPU的参与。可以提供更好的性能。</p><ul><li>和 kni 对比</li></ul><p>kni 的话，需要在DPDK中实现具体的代码来进行流量从DPDK应用到内核协议栈。流量分叉只需要通过软件给硬件配置对应的规则即可。</p><ul><li>实现流量分叉的方式<ul><li>SR-IOV</li><li>Flow filter(rte_flow/ fdir)</li></ul></li></ul><p>SR-IOV 实现流量分叉的优缺点：</p><p><img src="/images/2024/06/012.png" alt></p><h2 id="不同网卡的流量分叉实现"><a href="#不同网卡的流量分叉实现" class="headerlink" title="不同网卡的流量分叉实现"></a>不同网卡的流量分叉实现</h2><h3 id="mellanox"><a href="#mellanox" class="headerlink" title="mellanox"></a>mellanox</h3><p><img src="/images/2024/06/013.png" alt></p><p>Mellanox Cx系列网卡天然支持流量分叉，不需要配置SR-IOV PF/VF 进行流量分叉。Mellanox Cx系列流量分叉的好处有：</p><ul><li>更好的性能，DPDK应用直接处理数据面的流量。</li><li>网卡依然可以被内核控制。</li><li>Linux kernel 的控制工具/命令依然可以使用。比如，ethtool</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>其他系列的网卡，比如Intel，博通的网卡不是天然支持流量分叉。需要配置SR-IOV或者 FDIR(Flow Director)规则来实现流量分叉。</p><h2 id="流量分叉的应用场景"><a href="#流量分叉的应用场景" class="headerlink" title="流量分叉的应用场景"></a>流量分叉的应用场景</h2><h3 id="减少网卡的数量"><a href="#减少网卡的数量" class="headerlink" title="减少网卡的数量"></a>减少网卡的数量</h3><p>正常情况下，在一个机器上部署DPDK程序，需要2块网卡：</p><ul><li>管理口</li></ul><p>登陆机器，管理机器。</p><ul><li>业务口</li></ul><p>一个单卡双口的网卡作为DPDK程序转发流量的业务口。</p><p>如果使用基于SR-IOV的 flow bifurcation，只需要一块卡即可。利用网卡的SR-IOV，存在一个PF以及多个VF。 在网卡上配置管理IP，以及一些flow filter。 将DPDK的流量，交给VF对应的队列，进而给VF处理。 将其他流量交给PF处理，对设备进行管理。这样即使DPDK程序退出，不影响设备的管理。</p><h3 id="DPDK程序中控制流量和业务流量分离"><a href="#DPDK程序中控制流量和业务流量分离" class="headerlink" title="DPDK程序中控制流量和业务流量分离"></a>DPDK程序中控制流量和业务流量分离</h3><p>目前的DPDK程序，比如DPVS，控制流量（健康检查、bgp流量）也可能和业务流量混在一起（比如交给了相同的接收队列）。在业务流量很大的情况下，有可能导致控制流量丢包，进而导致BGP保活失败，VIP路由发送失败，流量偶发断连的情况。 如果可以将控制流量和业务流量分发到不同的队列中，可以做到互不影响。</p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>ATC’23 <a href="https://www.usenix.org/conference/atc23/presentation/zhu-lingjun" target="_blank" rel="noopener">Deploying User-space TCP at Cloud Scale with LUNA</a></p><p><img src="/images/2024/06/015.jpg" alt><br>细节可参考论文中的”7.1 Traffic Split”一节:</p><blockquote><p>LUNA routes the incoming TCP packets with certain destination ports to the specific virtual functions, which are then processed in user space. The TCP packets that do not align with the port filters and the not-TCP packets would still be accepted and processed by the kernel network stack.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://www.redhat.com/en/blog/getting-best-both-worlds-queue-splitting-bifurcated-driver" target="_blank" rel="noopener">Getting the Best of Both Worlds with Queue Splitting (Bifurcated Driver)</a></li><li><a href="https://www.dpdk.org/wp-content/uploads/sites/35/2016/10/Day02-Session05-JingjingWu-Userspace2016.pdf" target="_blank" rel="noopener">Flow Bifurcation on Intel® Ethernet Controller X710/XL710</a></li><li><a href="https://doc.dpdk.org/guides/howto/flow_bifurcation.html" target="_blank" rel="noopener">Flow Bifurcation How-to Guide</a></li><li><a href="https://blog.csdn.net/legend050709/article/details/124872468" target="_blank" rel="noopener">DPDK中的流量分叉(flow bifurcation)</a></li><li><a href="https://www.usenix.org/conference/atc23/presentation/zhu-lingjun" target="_blank" rel="noopener">Deploying User-space TCP at Cloud Scale with LUNA</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要转载自&lt;a href=&quot;https://blog.csdn.net/legend050709/article/details/124872468&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DPDK中的流量分叉(flow bifurcation)&lt;/a&gt;。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DPDK" scheme="http://liujunming.github.io/tags/DPDK/"/>
    
  </entry>
  
  <entry>
    <title>Notes about MBEC(Mode Based Execution Control)</title>
    <link href="http://liujunming.github.io/2024/06/22/Notes-about-MBEC-Mode-Based-Execution-Control/"/>
    <id>http://liujunming.github.io/2024/06/22/Notes-about-MBEC-Mode-Based-Execution-Control/</id>
    <published>2024-06-22T08:45:21.000Z</published>
    <updated>2024-06-22T12:31:37.174Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下MBEC(Mode Based Execution Control)相关notes。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>MBEC provides finer grain control on execute permissions to help protect the integrity of the system code from malicious changes. It provides additional refinement within the Extended Page Tables (EPT) by turning the Execute Enable (X) permission bit into two options:</p><ul><li>XU for user pages</li><li>XS for supervisor pages</li></ul><p>The CPU selects one or the other based on permission of the guest page and <strong>maintains an invariant for every page that does not allow it to be writable and supervisor-executable at the same time</strong>. A benefit of this feature is that a hypervisor can more reliably verify and enforce the integrity of kernel-level code(hypervisor可以更可靠地验证和执行内核级代码的完整性). The value of the XU/XS bits is delivered through the hypervisor, so hypervisor support is necessary.</p><p>这个特性主要是安全方面的考虑，如果想了解安全相关的更多背景，可以参考<a href="https://www.blackhat.com/docs/us-16/materials/us-16-Wojtczuk-Analysis-Of-The-Attack-Surface-Of-Windows-10-Virtualization-Based-Security-wp.pdf" target="_blank" rel="noopener">Analysis of the Attack Surface of Windows 10 Virtualization-based Security</a>。本文主要侧重于虚拟化侧的内容，安全相关的细节，不在本文的范围之内。</p><h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><p><img src="/images/2024/06/006.jpg" alt></p><p><img src="/images/2024/06/007.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html#-12" target="_blank" rel="noopener">Intel Xeon Processor Scalable Family Technical Overview</a></li><li><a href="https://www.wikiwand.com/en/Second_Level_Address_Translation" target="_blank" rel="noopener">wikiwand: Second Level Address Translation</a></li><li><a href="https://www.blackhat.com/docs/us-16/materials/us-16-Wojtczuk-Analysis-Of-The-Attack-Surface-Of-Windows-10-Virtualization-Based-Security-wp.pdf" target="_blank" rel="noopener">Analysis of the Attack Surface of Windows 10 Virtualization-based Security</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下MBEC(Mode Based Execution Control)相关notes。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe Identify command</title>
    <link href="http://liujunming.github.io/2024/06/02/Notes-about-NVMe-Identify-command/"/>
    <id>http://liujunming.github.io/2024/06/02/Notes-about-NVMe-Identify-command/</id>
    <published>2024-06-02T11:33:21.000Z</published>
    <updated>2024-06-02T13:21:51.159Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Opcode is 0x06.</li><li>The base address of the output (a single page) should be put in the DWORDs 6 and 7 of the command.</li><li>The low byte of command DWORD 10 indicates what is to be identified: 0 - a namespace, 1 - the controller, 2 - the namespace list.</li><li>If identifying a namespace, set DWORD 1 to the namespace ID.</li></ul><a id="more"></a><p><img src="/images/2024/06/003.jpg" alt></p><p><img src="/images/2024/06/005.jpg" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> &#123;</span><br><span class="line">    NVME_ID_CNS_NS          = <span class="number">0x00</span>,</span><br><span class="line">    NVME_ID_CNS_CTRL        = <span class="number">0x01</span>,</span><br><span class="line">    NVME_ID_CNS_NS_ACTIVE_LIST  = <span class="number">0x02</span>,</span><br><span class="line">    NVME_ID_CNS_NS_DESC_LIST    = <span class="number">0x03</span>,</span><br><span class="line">    NVME_ID_CNS_CS_NS       = <span class="number">0x05</span>,</span><br><span class="line">    NVME_ID_CNS_CS_CTRL     = <span class="number">0x06</span>,</span><br><span class="line">    NVME_ID_CNS_NS_CS_INDEP     = <span class="number">0x08</span>,</span><br><span class="line">    NVME_ID_CNS_NS_PRESENT_LIST = <span class="number">0x10</span>,</span><br><span class="line">    NVME_ID_CNS_NS_PRESENT      = <span class="number">0x11</span>,</span><br><span class="line">    NVME_ID_CNS_CTRL_NS_LIST    = <span class="number">0x12</span>,</span><br><span class="line">    NVME_ID_CNS_CTRL_LIST       = <span class="number">0x13</span>,</span><br><span class="line">    NVME_ID_CNS_SCNDRY_CTRL_LIST    = <span class="number">0x15</span>,</span><br><span class="line">    NVME_ID_CNS_NS_GRANULARITY  = <span class="number">0x16</span>,</span><br><span class="line">    NVME_ID_CNS_UUID_LIST       = <span class="number">0x17</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>CDW1.NSID的含义:<br><img src="/images/2024/06/004.jpg" alt></p><p>总结:</p><ol><li>想要某个controller或者namespace的identity，需要在command中指定controller id或者namespace id</li><li>controller将结果以DMA的形式放到DWORDs 6 and 7指定的地址中</li></ol><hr><p>参考资料:</p><ol><li>NVMe 1.3 spec</li><li><a href="https://wiki.osdev.org/NVMe" target="_blank" rel="noopener">https://wiki.osdev.org/NVMe</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Opcode is 0x06.&lt;/li&gt;
&lt;li&gt;The base address of the output (a single page) should be put in the DWORDs 6 and 7 of the command.&lt;/li&gt;
&lt;li&gt;The low byte of command DWORD 10 indicates what is to be identified: 0 - a namespace, 1 - the controller, 2 - the namespace list.&lt;/li&gt;
&lt;li&gt;If identifying a namespace, set DWORD 1 to the namespace ID.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>virtio-net offloads reconfiguration</title>
    <link href="http://liujunming.github.io/2024/06/02/virtio-net-offloads-reconfiguration/"/>
    <id>http://liujunming.github.io/2024/06/02/virtio-net-offloads-reconfiguration/</id>
    <published>2024-06-02T00:46:16.000Z</published>
    <updated>2024-06-02T12:49:07.166Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Virtio-net driver currently negotiates network offloads on startup via features mechanism and have no ability to disable and re-enable offloads later.<br>This patch introduced a new control command that allows to configure device network offloads state dynamically. The patch also introduces a new feature flag VIRTIO_NET_F_CTRL_GUEST_OFFLOADS.</p></blockquote><blockquote><p>VIRTIO_NET_F_CTRL_GUEST_OFFLOADS (2) Control channel offloads reconfiguration support.</p></blockquote><p>control virtqueue “Offloads State Configuration”.<br><a id="more"></a></p><p><img src="/images/2024/06/001.jpg" alt></p><p>“offloads”是名词，按照名词去阅读spec。</p><p>“negotiate”指的是在driver_feature中正确配置好了feature bits。<br><img src="/images/2024/06/002.jpg" alt></p><p>VIRTIO_NET_F_CTRL_GUEST_OFFLOADS支持动态地更改offloads的状态(enable/disbale)配置。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// feature bits</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_F_GUEST_CSUM       1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_F_GUEST_TSO4       7</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_F_GUEST_TSO6       8</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_F_GUEST_ECN        9</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_F_GUEST_UFO        10</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_F_GUEST_USO4       54</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_F_GUEST_USO6       55</span></span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Control network offloads</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Reconfigures the network offloads that Guest can handle.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Available with the VIRTIO_NET_F_CTRL_GUEST_OFFLOADS feature bit.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Command data format matches the feature bit mask exactly.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * See VIRTIO_NET_F_GUEST_* for the list of offloads</span></span><br><span class="line"><span class="comment"> * that can be enabled/disabled.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_CTRL_GUEST_OFFLOADS   5</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VIRTIO_NET_CTRL_GUEST_OFFLOADS_SET        0</span></span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.pdf" target="_blank" rel="noopener">virtio v1.3</a></li><li><a href="https://lists.nongnu.org/archive/html/qemu-devel/2013-05/msg02505.html" target="_blank" rel="noopener">virtio-net: dynamic network offloads configuration</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Virtio-net driver currently negotiates network offloads on startup via features mechanism and have no ability to disable and re-enable offloads later.&lt;br&gt;This patch introduced a new control command that allows to configure device network offloads state dynamically. The patch also introduces a new feature flag VIRTIO_NET_F_CTRL_GUEST_OFFLOADS.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;VIRTIO_NET_F_CTRL_GUEST_OFFLOADS (2) Control channel offloads reconfiguration support.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;control virtqueue “Offloads State Configuration”.&lt;br&gt;
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Interrupt Coalescing under virtio-net</title>
    <link href="http://liujunming.github.io/2024/05/26/Interrupt-Coalescing-under-virtio-net/"/>
    <id>http://liujunming.github.io/2024/05/26/Interrupt-Coalescing-under-virtio-net/</id>
    <published>2024-05-26T07:13:44.000Z</published>
    <updated>2024-05-26T09:46:55.384Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio-net的Interrupt Coalescing技术相关notes。<a id="more"></a></p><h2 id="SPEC"><a href="#SPEC" class="headerlink" title="SPEC"></a>SPEC</h2><h3 id="feature-bit"><a href="#feature-bit" class="headerlink" title="feature bit"></a>feature bit</h3><p>spec中<code>VIRTIO_NET_F_VQ_NOTF_COAL</code>和<code>VIRTIO_NET_F_NOTF_COAL</code>这两个feature bit是Interrupt Coalescing相关特性。</p><blockquote><p>VIRTIO_NET_F_VQ_NOTF_COAL(52) Device supports virtqueue notification coalescing.<br>VIRTIO_NET_F_NOTF_COAL(53) Device supports notifications coalescing.</p></blockquote><h3 id="Coalescing-parameters"><a href="#Coalescing-parameters" class="headerlink" title="Coalescing parameters"></a>Coalescing parameters</h3><p>virtio-net通过control queue来配置Interrupt Coalescing的参数，详情可参见”Notifications Coalescing”一节。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">virtio_net_ctrl_coal</span> &#123;</span></span><br><span class="line">    le32 max_packets;</span><br><span class="line">    le32 max_usecs;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Coalescing parameters:</p><ul><li>max_usecs for RX: Maximum number of microseconds to delay a RX notification.</li><li>max_usecs for TX: Maximum number of microseconds to delay a TX notification.</li><li>max_packets for RX: Maximum number of packets to receive before a RX notification.</li><li>max_packets for TX: Maximum number of packets to send before a TX notification.</li></ul><h3 id="Operation"><a href="#Operation" class="headerlink" title="Operation"></a>Operation</h3><p>The device sends a used buffer notification once the notification conditions are met and if the notifications are not suppressed.</p><p>When the device has non-zero max_usecs and non-zero max_packets, it starts counting microseconds and packets upon receiving/sending a packet. The device counts packets and microseconds for each receive virtqueue and transmit virtqueue separately. In this case, the notification conditions are met when max_usecs microseconds elapse, or upon sending/receiving max_packets packets, whichever happens first. Afterwards, the device waits for the next packet and starts counting packets and microseconds again.</p><p>When the device has max_usecs = 0 or max_packets = 0, the notification conditions are met after every packet received/sent.</p><h2 id="virtqueue-coalescing-moderation"><a href="#virtqueue-coalescing-moderation" class="headerlink" title="virtqueue coalescing moderation"></a>virtqueue coalescing moderation</h2><p><a href="https://lists.oasis-open.org/archives/virtio-dev/202303/msg00415.html" target="_blank" rel="noopener">virtio-net: support the virtqueue coalescing moderation</a></p><p><a href="https://lore.kernel.org/all/20230731070656.96411-1-gavinl@nvidia.com/" target="_blank" rel="noopener">virtio_net: add per queue interrupt coalescing support</a></p><p>Currently, coalescing parameters are grouped for all transmit and receive virtqueues. virtqueue coalescing moderation supports setting or getting the parameters for a specified virtqueue, and a typical application of this function is <a href="https://github.com/torvalds/linux/blob/master/lib/dim/net_dim.c" target="_blank" rel="noopener">netdim</a>.</p><p>When the traffic between virtqueues is unbalanced, for example, one virtqueue is busy and another virtqueue is idle, then it will be very useful to control coalescing parameters at the virtqueue granularity.</p><h2 id="DIM"><a href="#DIM" class="headerlink" title="DIM"></a>DIM</h2><p>virtio-net already supports per-queue moderation parameter setting. Based on this, we use the <a href="https://github.com/torvalds/linux/blob/master/lib/dim/net_dim.c" target="_blank" rel="noopener">netdim</a> library of linux to support dynamic coalescing moderation for virtio-net.</p><p><a href="https://docs.kernel.org/networking/net_dim.html" target="_blank" rel="noopener">Net DIM算法</a>通过统计当前网络中单个队列的流量信息和中断次数，自适应计算中断调整方向和步长，并将结果配置下发到设备，以达到提升网络吞吐量的目的。virtqueue coalescing moderation 可以让 virtio-net支持动态中断调节，并对逐个队列下发中断调节参数。</p><p><strong>Dynamic Interrupt Moderation (DIM) (in networking) refers to changing the interrupt moderation configuration of a channel in order to optimize packet processing</strong>. <em>The mechanism includes an algorithm which decides if and how to change moderation parameters for a channel, usually by performing an analysis on runtime data sampled from the system</em>. Net DIM is such a mechanism. In each iteration of the algorithm, it analyses a given sample of the data, compares it to the previous sample and if required, it can decide to change some of the interrupt moderation configuration fields. The data sample is composed of data bandwidth, the number of packets and the number of events. The time between samples is also measured. <strong>Net DIM compares the current and the previous data and returns an adjusted interrupt moderation configuration object</strong>. In some cases, the algorithm might decide not to change anything. <strong>The configuration fields are the minimum duration (microseconds) allowed between events and the maximum number of wanted packets per event</strong>(笔者注：在virtio-net中，configuration fields就是max_usecs和max_packets，感觉是maximum duration allowed between events才对). The Net DIM algorithm ascribes importance to increase bandwidth over reducing interrupt rate.</p><hr><p>参考资料:</p><ol><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.pdf" target="_blank" rel="noopener">virtio v1.3</a></li><li><a href="https://mp.weixin.qq.com/s/hQN7Y0XCJwMPfo--IGA_kg" target="_blank" rel="noopener">高性能网络SIG月度动态：virtio-net 支持动态中断调节，SMC v2 协议增加新扩展</a></li><li><a href="https://mp.weixin.qq.com/s/bT3ANrOjzJdTu1oP9qO89w" target="_blank" rel="noopener">高性能网络SIG月度动态：virtio 支持动态中断聚合，SMCv2.1协议正式发布</a></li><li><a href="https://mp.weixin.qq.com/s/syR8nt2poO96bA9-oDSR2g" target="_blank" rel="noopener">高性能网络SIG月度动态：virtio 动态中断调节优化、多项内核网络缺陷修复</a></li><li><a href="https://lists.oasis-open.org/archives/virtio-dev/202303/msg00415.html" target="_blank" rel="noopener">virtio-net: support the virtqueue coalescing moderation</a></li><li><a href="https://docs.kernel.org/networking/net_dim.html" target="_blank" rel="noopener">Net DIM - Generic Network Dynamic Interrupt Moderation</a></li><li><a href="https://enterprise-support.nvidia.com/s/article/dynamically-tuned-interrupt-moderation--dim-x" target="_blank" rel="noopener">DYNAMICALLY-TUNED INTERRUPT MODERATION (DIM)</a></li><li><a href="https://techdocs.broadcom.com/us/en/storage-and-ethernet-connectivity/ethernet-nic-controllers/bcm957xxx/adapters/Tuning/tcp-performance-tuning/nic-tuning_22/interrupt-moderation.html" target="_blank" rel="noopener">Broadcom Ethernet Network Adapter User Guide:Interrupt Moderation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio-net的Interrupt Coalescing技术相关notes。
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Interrupt Coalescing under NVMe</title>
    <link href="http://liujunming.github.io/2024/05/19/Interrupt-Coalescing-under-NVMe/"/>
    <id>http://liujunming.github.io/2024/05/19/Interrupt-Coalescing-under-NVMe/</id>
    <published>2024-05-19T11:23:08.000Z</published>
    <updated>2024-05-25T14:29:15.944Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Interrupt-Coalescing"><a href="#Interrupt-Coalescing" class="headerlink" title="Interrupt Coalescing"></a>Interrupt Coalescing</h2><p>Interrupt coalescing, also known as interrupt moderation, is a technique in which events which would normally trigger a hardware interrupt are held back, either until a certain amount of work is pending, or a timeout timer triggers. Used correctly, this technique can reduce interrupt load by up to an order of magnitude, while only incurring relatively small latency penalties.<a id="more"></a></p><h2 id="NVMe-feature-Interrupt-Coalescing"><a href="#NVMe-feature-Interrupt-Coalescing" class="headerlink" title="NVMe feature: Interrupt Coalescing"></a>NVMe feature: Interrupt Coalescing</h2><p><img src="/images/2024/05/016.jpg" alt></p><p><img src="/images/2024/05/017.jpg" alt></p><p><img src="/images/2024/05/018.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://nvmexpress.org/wp-content/uploads/NVM_Express_Revision_1.3.pdf" target="_blank" rel="noopener">NVMe 1.3 spec</a></li><li><a href="https://www.usenix.org/system/files/osdi21-tai.pdf" target="_blank" rel="noopener">Optimizing Storage Performance with Calibrated Interrupts</a></li><li><a href="https://www.wikiwand.com/en/Interrupt_coalescing" target="_blank" rel="noopener">wikiwand:Interrupt coalescing</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Interrupt-Coalescing&quot;&gt;&lt;a href=&quot;#Interrupt-Coalescing&quot; class=&quot;headerlink&quot; title=&quot;Interrupt Coalescing&quot;&gt;&lt;/a&gt;Interrupt Coalescing&lt;/h2&gt;&lt;p&gt;Interrupt coalescing, also known as interrupt moderation, is a technique in which events which would normally trigger a hardware interrupt are held back, either until a certain amount of work is pending, or a timeout timer triggers. Used correctly, this technique can reduce interrupt load by up to an order of magnitude, while only incurring relatively small latency penalties.
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about RSS(Receive Side Scaling)</title>
    <link href="http://liujunming.github.io/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/"/>
    <id>http://liujunming.github.io/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/</id>
    <published>2024-05-05T05:16:56.000Z</published>
    <updated>2024-05-05T10:10:54.472Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下RSS(Receive Side Scaling)相关notes，内容主要转载自<a href="https://medium.com/@anubhavchoudhary/introduction-to-receive-side-scaling-rss-7cd97307d220" target="_blank" rel="noopener">Introduction to Receive Side Scaling (RSS)</a>。<a id="more"></a></p><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><p>A network interface card (NIC) can be configured with multiple receive queue (Rx). This is done to improve the throughput (i.e, the number of packets that can be processed per second) of the device and to achieve higher throughput, more than one CPU is used to poll(笔者注:这里指的是MSI-x中断收包) packets from the device independently. Each CPU polls(笔者注:这里指的是MSI-x中断收包) for its assigned Rx queue, but still we need a strategy to distribute the packets over all the queues, otherwise all the packets will go to one queue and only one CPU will be utilised.</p><h2 id="2-Hash计算"><a href="#2-Hash计算" class="headerlink" title="2. Hash计算"></a>2. Hash计算</h2><h3 id="2-1-RSS-Input-Fields"><a href="#2-1-RSS-Input-Fields" class="headerlink" title="2.1 RSS Input Fields"></a>2.1 RSS Input Fields</h3><p>The RSS input fields plays a vital role in deciding the criteria for distributing the packets over multiple receive queue. There are lot of hash input fields available like source mac address, destination mac address, source IP, destination IP etc.</p><h3 id="2-2-RSS-Hash-Key"><a href="#2-2-RSS-Hash-Key" class="headerlink" title="2.2 RSS Hash Key"></a>2.2 RSS Hash Key</h3><p>The RSS hash is a 40/52 byte key used to randomize the distribution of packets over the receive queues. The size of the hash key depends on the hardware of the NIC.</p><h3 id="2-3-Hash-Algorithm"><a href="#2-3-Hash-Algorithm" class="headerlink" title="2.3 Hash Algorithm"></a>2.3 Hash Algorithm</h3><p>Once the hash function and hash key is configured, the hash algorithm is used to determine the receive queue index using the input fields and hash Key.</p><p>The hash functions which are generally used are:</p><ul><li>Simple XOR Hash Function</li><li>Toeplitz Hash Function</li></ul><h3 id="2-4-hash计算总结"><a href="#2-4-hash计算总结" class="headerlink" title="2.4 hash计算总结"></a>2.4 hash计算总结</h3><p><img src="/images/2024/05/014.png" alt></p><p>Incoming packets进入到parser模块，五元组src ip、dst ip、src port、dst port和protocol中的子集作为input set，结合一个任意的hash key以及hash function(默认hash function是Toeplitz function)计算得到hash value。</p><h2 id="3-Working-of-RSS"><a href="#3-Working-of-RSS" class="headerlink" title="3. Working of RSS"></a>3. Working of RSS</h2><p>When a NIC is started with multiple Rx queues then based on the configured RSS configurations like Input Fields, hash key and number of Rx queues, an <strong>indirection table</strong> is generated. An indirection table represents mapping between the hash value and the queue index.<br><img src="/images/2024/05/012.webp" alt></p><p>When a packet is received by the NIC, all the values from the configured fields are extracted and those values are fed into the hash function. The generated hash value is used to do a lookup in the indirection table in order to get the destination queue index.</p><p><img src="/images/2024/05/013.svg" alt><br>A number of least significant bits (LSBs) of the hash value are used to index an indirection table.</p><h2 id="4-Advantage"><a href="#4-Advantage" class="headerlink" title="4. Advantage"></a>4. Advantage</h2><p>There are two main advantages of using RSS:</p><ol><li>Higher throughput as more number of CPUs can be used independently.</li><li>The computational cost of load balancing is offloaded to the NIC’s hardware instead of doing software based load-balancing.</li></ol><h2 id="5-Usage"><a href="#5-Usage" class="headerlink" title="5. Usage"></a>5. Usage</h2><p>RSS is the mechanism to process packets with multiple RX queues. When the network card with RSS receives packets, it will apply a filter to packets and distribute the packets to RX queues. The filter is usually a hash function and can be configured from <code>ethtool -X</code>. If you want to spread flows evenly among the first 3 queues:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -X eth0 equal 3</span><br></pre></td></tr></table></figure></p><p>Or, if you find a magic hash key that is particularly useful:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -X eth0 hkey &lt;magic hash key&gt;</span><br></pre></td></tr></table></figure></p><p><img src="/images/2024/05/011.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/network/introduction-to-receive-side-scaling" target="_blank" rel="noopener">Introduction to Receive Side Scaling</a></li><li><a href="https://medium.com/@anubhavchoudhary/introduction-to-receive-side-scaling-rss-7cd97307d220" target="_blank" rel="noopener">Introduction to Receive Side Scaling (RSS)</a></li><li><a href="https://garycplin.blogspot.com/2017/06/linux-network-scaling-receives-packets.html" target="_blank" rel="noopener">Linux Network Scaling: Receiving Packets</a></li><li><a href="https://zhuanlan.zhihu.com/p/467319331" target="_blank" rel="noopener">关于RSS的一些知识</a></li><li><a href="https://zhuanlan.zhihu.com/p/148756667" target="_blank" rel="noopener">Linux网络栈的性能缩放</a></li><li><a href="https://blog.csdn.net/weixin_35804181/article/details/123519222" target="_blank" rel="noopener">网卡调优RSS、RPS、RFS和XPS</a></li><li><a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt" target="_blank" rel="noopener">Scaling in the Linux Networking Stack</a></li><li><a href="https://doc.dpdk.org/guides/prog_guide/toeplitz_hash_lib.html" target="_blank" rel="noopener">Toeplitz Hash Library</a></li><li><a href="https://www.sdnlab.com/24494.html" target="_blank" rel="noopener">Intel E810 Advanced RSS介绍</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下RSS(Receive Side Scaling)相关notes，内容主要转载自&lt;a href=&quot;https://medium.com/@anubhavchoudhary/introduction-to-receive-side-scaling-rss-7cd97307d220&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introduction to Receive Side Scaling (RSS)&lt;/a&gt;。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Vdmabuf</title>
    <link href="http://liujunming.github.io/2024/05/04/Notes-about-Vdmabuf/"/>
    <id>http://liujunming.github.io/2024/05/04/Notes-about-Vdmabuf/</id>
    <published>2024-05-04T10:36:36.000Z</published>
    <updated>2024-05-04T12:00:19.282Z</updated>
    
    <content type="html"><![CDATA[<p>The Virtio based Dmabuf (Vdmabuf) driver can be used to “transfer” a page-backed dmabuf created in the Guest to the Host without making any copies. This is mostly accomplished by recreating the dmabuf on the Host using the PFNs and other meta-data shared by the guest.<a id="more"></a></p><p><img src="/images/2024/05/006.jpg" alt></p><p><img src="/images/2024/05/007.jpg" alt></p><p><img src="/images/2024/05/008.jpg" alt></p><p><a href="https://lists.nongnu.org/archive/html/qemu-devel/2021-02/msg02976.html" target="_blank" rel="noopener">vhost-vdmabuf: Add virtio based Dmabuf device</a><br>在此处，virtio based Dmabuf device与virtio-gpu的功能类似。<br>但是virtio-gpu的生态更好，所以最终选择了virtio-gpu方案！</p><p><img src="/images/2024/05/010.jpg" alt></p><p><img src="/images/2024/05/009.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://lwn.net/Articles/846810/" target="_blank" rel="noopener">Introduce Virtio based Dmabuf driver(s)</a></li><li><a href="https://kvmforum2021.sched.com/event/ke3p/passthroughheadless-gpu-gets-ahead-tina-zhang-vivek-kasireddy-intel" target="_blank" rel="noopener">slides: Passthrough/Headless GPU Gets A Head</a></li><li><a href="https://www.youtube.com/watch?v=iUIwvXQU5H4" target="_blank" rel="noopener">video: Passthrough/Headless GPU Gets Ahead</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The Virtio based Dmabuf (Vdmabuf) driver can be used to “transfer” a page-backed dmabuf created in the Guest to the Host without making any copies. This is mostly accomplished by recreating the dmabuf on the Host using the PFNs and other meta-data shared by the guest.
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about MPTCP</title>
    <link href="http://liujunming.github.io/2024/05/04/notes-about-MPTCP/"/>
    <id>http://liujunming.github.io/2024/05/04/notes-about-MPTCP/</id>
    <published>2024-05-04T08:52:30.000Z</published>
    <updated>2024-05-04T10:36:45.652Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Multipath TCP (MPTCP)相关notes。<a id="more"></a></p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>Aims at allowing a TCP connection to use multiple paths to maximize throughput and increase redundancy.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Hosts connected to the Internet or within a data center environment are often connected by multiple paths. However, when TCP is used for data transport, communication is restricted to a single network path. It is possible that some paths between the two hosts are congested, whereas alternate paths are underutilized. A more efficient use of network resources is possible if these multiple paths are used concurrently. In addition, the use of multiple connections enhances the user experience, because it provides higher throughput and improved resilience against network failures.<br>连接到互联网或数据中心环境中的主机通常由多条路径连接。然而，当使用 TCP 进行数据传输时，通信仅限于一条网络路径。两台主机之间的某些路径可能会出现拥塞，而备用路径的利用率却很低。如果同时使用这些多路径，就能更有效地利用网络资源。此外，多连接的使用还能增强用户体验，因为它提供了更高的吞吐量，并提高了对网络故障的恢复能力。</p><p>MPTCP is a set of extensions to regular TCP that enables a single data flow to be separated and carried across multiple connections.<br>MPTCP 是对普通 TCP 的一系列扩展，可将单个数据流分离并在多个连接中传输。</p><p>As shown in this diagram, MPTCP is able to separate the 9mbps flow into three different sub-flows on the sender node, which is subsequently aggregated back into the original data flow on the receiving node.<br>如图所示，MPTCP 能够在发送节点上将 9mbps 的数据流分离成三个不同的子数据流，然后在接收节点上汇总成原始数据流。</p><p><img src="/images/2024/05/004.avif" alt></p><p>The data that enters the MPTCP connection acts exactly as it does through a regular TCP connection; the transmitted data has guaranteed an in-order delivery. Since MPTCP adjusts the network stack and operates within the transport layer, it is used transparently by the application.<br>进入 MPTCP 连接的数据与通过普通 TCP 连接的数据完全相同；传输的数据保证按顺序传送。由于 MPTCP 调整了网络堆栈并在传输层内运行，因此应用程序可以透明地使用它。</p><p><img src="/images/2024/05/005.avif" alt></p><h2 id="Simplified-description"><a href="#Simplified-description" class="headerlink" title="Simplified description"></a>Simplified description</h2><p>Difference between TCP and MPTCP:<br><img src="/images/2024/05/003.png" alt></p><p>The core idea of multipath TCP is to define a way to build a connection between two hosts and not between two interfaces (as standard TCP does).</p><p>For instance, Alice has a smartphone with 3G and WiFi interfaces (with IP addresses 10.11.12.13 and 10.11.12.14) and Bob has a computer with an Ethernet interface (with IP address 20.21.22.23).</p><p>In standard TCP, the connection should be established between two IP addresses. Each TCP connection is identified by a four-tuple (source and destination addresses and ports). Given this restriction, an application can only create one TCP connection through a single link. Multipath TCP allows the connection to use several paths simultaneously. For this, Multipath TCP creates one TCP connection, called subflow, over each path that needs to be used.</p><hr><p>参考资料:</p><ol><li><a href="https://www.wikiwand.com/en/Multipath_TCP" target="_blank" rel="noopener">https://www.wikiwand.com/en/Multipath_TCP</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc8684" target="_blank" rel="noopener">https://datatracker.ietf.org/doc/html/rfc8684</a></li><li><a href="https://www.cisco.com/c/en/us/support/docs/ip/transmission-control-protocol-tcp/116519-technote-mptcp-00.html" target="_blank" rel="noopener">MPTCP and Product Support Overview</a></li><li><a href="https://www.usenix.org/system/files/login/articles/login1210_bonaventure.pdf" target="_blank" rel="noopener">An Overview of Multipath TCP</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Multipath TCP (MPTCP)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about virtio per-virtqueue reset</title>
    <link href="http://liujunming.github.io/2024/05/03/Notes-about-virtio-per-virtqueue-reset/"/>
    <id>http://liujunming.github.io/2024/05/03/Notes-about-virtio-per-virtqueue-reset/</id>
    <published>2024-05-03T10:47:16.000Z</published>
    <updated>2024-05-03T12:10:53.338Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio per-virtqueue reset特性的相关notes，主要内容转载自<a href="https://developer.aliyun.com/article/996430" target="_blank" rel="noopener">virtio 1.2 来了！</a>。<a id="more"></a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>引入这个特性的目的是解决virtio-net不支持队列级别的reset操作的问题。这在很多现代化的网卡中是一个比较常见的功能, 是实现很多功能的基础能力, 为了让 virtio-net 支持更多能力, 这个特性的引入是必须的。但是 Per-virtqueue reset 并不只限于 virtio-net 这一种设备，它是一个 virtio 的基础能力，相信其它的 virtio 设备也会慢慢支持这个 feature。</p><h2 id="Implementation-Process"><a href="#Implementation-Process" class="headerlink" title="Implementation Process"></a>Implementation Process</h2><p>Per-virtqueue reset 由 driver 针对某一个队列发起，基于某一种 transport(比如 PCIe) 通知 device。device 停止使用队列，driver 在 reset 之后可以重新 re-enable 队列。virtio spec 定义了这个过程中详细的交互流程和信息。<br>以下是 virtio spec 中定义的详细流程：</p><ul><li>driver 基于 transport 通知 device 某个指定的队列要 reset。</li><li>device 收到请求之后设置 reset 状态为 1，停止此队列的所有操作，包括中断等，并设置队列的所有的状态到初始值。在 device 完成 reset 操作之前，返回给 driver 的 reset 状态都是 1，直到 reset 操作完成。reset 完成之后 reset 及 enable 的值都要设置成 0。</li><li>driver 在检查到队列的 reset 状态变成 0 之后，就表示device reset 操作已经完成了。这个时候开始，driver 就可以安全地回收队列占用的相关资源了。</li></ul><p>到此 driver 对于队列的 reset 操作就已经完成了。</p><ul><li>之后 virtio driver 可选地进行 re-enable 操作，在操作的过程中，driver 可以给 device 新的参数来 re-enable 这个队列。比如新的队列大小。</li></ul><p>以上是一个完整的 reset &amp; re-enable 的过程，理论上 re-enable 是可选的。</p><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>对于现代的很多硬件设备来讲，对于队列进行 reset 是一个比较常见的功能，所以这个功能的引入让 virtio 设备更加现代化。早期 virtio 的出现是伴随着高性能的需求而来的，我们原来更加关注它在性能上的基本功能，一些高级功能并不重视。per-virtqueue reset 让 virtio 对于队列的使用更加灵活，譬如我们可以基于 per-vertqueue reset 实现下面两个功能：</p><ol><li>调整virtio-net 网卡队列的ring size。在 virtio-net 的场景下，基于 per-virtqueue reset 我们可以实现网卡队列 ring size 的调整。目前一般的网卡都支持使用<code>ethtool -G eth0 rx &lt;new size&gt; tx &lt;new size&gt;</code>来调整队列的大小，但是原来的 virtio-net 一直是不支持这样一个简单的功能的，现在基于 per-virtqueue reset，我们很快就可以在 Linux 下使用这个命令来调整队列的大小。</li><li>支持AF_XDP，扩展云上应用的边界。除了应用于上述简单的场景之外，我们还可以在更高级的场景应用到这个功能。<strong>per-virtqueue reset 也可以视作一种资源的快速回收机制</strong>。比如在 virtio-net 的情况下，我们必须要等待收到新的数据包或者硬件完成数据包的发送才能完成对于 buffer 资源的回收。而现在基于 per-virtqueue reset，driver 可以不用被动地等待而是可以主动调用 reset 快速地让 device 释放对于某个队列上的 buffer 资源的占用，实现资源的快速回收。这可以让 virtio-net 支持 AF_XDP 这样的高级功能，实现在 linux 内核框架下的高性能收发包。</li></ol><h2 id="Spec-Details"><a href="#Spec-Details" class="headerlink" title="Spec Details"></a>Spec Details</h2><p>per-virtqueue reset的细节请参考<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">virtio 1.2 spec</a>。</p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html#x1-280001" target="_blank" rel="noopener">2.6.1 Virtqueue Reset</a></p><blockquote><p><strong>VIRTIO_F_RING_RESET(40)</strong> This feature indicates that the driver can reset a queue individually.</p></blockquote><blockquote><p>2.6.1.1.2 Driver Requirements: Virtqueue Reset<br>After the driver tells the device to reset a queue, the driver MUST verify that the queue has actually been reset.<br>After the queue has been successfully reset, the driver MAY release any resource associated with that virtqueue.</p></blockquote><blockquote><p>2.6.1.2.1 Device Requirements: Virtqueue Re-enable<br>The device MUST observe any queue configuration that may have been changed by the driver, like the maximum queue size.</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">virtio_pci_common_cfg</span> &#123;</span> </span><br><span class="line">        ...</span><br><span class="line"> </span><br><span class="line">        <span class="comment">/* About a specific virtqueue. */</span> </span><br><span class="line">        le16 queue_select;              <span class="comment">/* read-write */</span> </span><br><span class="line">        le16 queue_size;                <span class="comment">/* read-write */</span> </span><br><span class="line">        le16 queue_msix_vector;         <span class="comment">/* read-write */</span> </span><br><span class="line">        le16 queue_enable;              <span class="comment">/* read-write */</span> </span><br><span class="line">        ...</span><br><span class="line">        le16 queue_reset;               <span class="comment">/* read-write */</span> </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><blockquote><p><strong>queue_size</strong><br>Queue Size. On reset, specifies the maximum queue size supported by the device. This can be modified by the driver to reduce memory requirements.</p><p><strong>queue_reset</strong><br>The driver uses this to selectively reset the queue.</p></blockquote><blockquote><p><strong>4.1.4.3.1 Device Requirements: Common configuration structure layout</strong><br>If VIRTIO_F_RING_RESET has been negotiated, the device MUST present a 0 in queue_reset on reset.<br>If VIRTIO_F_RING_RESET has been negotiated, the device MUST present a 0 in queue_reset after the virtqueue is enabled with queue_enable.<br>The device MUST reset the queue when 1 is written to queue_reset. The device MUST continue to present 1 in queue_reset as long as the queue reset is ongoing. The device MUST present 0 in both queue_reset and queue_enable when queue reset has completed.</p></blockquote><blockquote><p><strong>4.1.4.3.2 Driver Requirements: Common configuration structure layout</strong><br>If VIRTIO_F_RING_RESET has been negotiated, after the driver writes 1 to queue_reset to reset the queue, the driver MUST NOT consider queue reset to be complete until it reads back 0 in queue_reset. The driver MAY re-enable the queue by writing 1 to queue_enable after ensuring that other virtqueue fields have been set up correctly. The driver MAY set driver-writeable queue configuration values to different values than those that were used before the queue reset.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://www.alibabacloud.com/blog/virtio-1-2-is-coming_599615" target="_blank" rel="noopener">Virtio 1.2 is Coming!</a></li><li><a href="https://developer.aliyun.com/article/996430" target="_blank" rel="noopener">virtio 1.2 来了！</a></li><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">virtio 1.2 spec</a></li><li><a href="https://lore.kernel.org/kvm/20220801063902.129329-1-xuanzhuo@linux.alibaba.com/" target="_blank" rel="noopener">virtio pci support VIRTIO_F_RING_RESET</a></li><li><a href="https://lists.oasis-open.org/archives/virtio-dev/202111/msg00013.html" target="_blank" rel="noopener">virtio: pci support virtqueue reset</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio per-virtqueue reset特性的相关notes，主要内容转载自&lt;a href=&quot;https://developer.aliyun.com/article/996430&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;virtio 1.2 来了！&lt;/a&gt;。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about AF_XDP</title>
    <link href="http://liujunming.github.io/2024/05/03/Notes-about-AF-XDP/"/>
    <id>http://liujunming.github.io/2024/05/03/Notes-about-AF-XDP/</id>
    <published>2024-05-03T01:05:40.000Z</published>
    <updated>2024-05-03T09:34:14.955Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要转载自<a href="https://hpnpl.net/posts/recapituatling-af-xdp/" target="_blank" rel="noopener">Recapitulating AF_XDP</a>。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>this time I will talk about a pretty awesome feature in the Linux kernel: AF_XDP. Please keep in mind, that this is a summary and explanation in my own words, and it’s not intended to fully cover all technical depths. The focus lies on understanding AF_XDP’s core concepts, learn how to use it, and what to consider while using it.<br>这次，我将谈谈 Linux 内核中一个非常棒的功能： AF_XDP。请记住，这只是用我自己的话进行的总结和解释，并不打算完全涵盖所有的技术深度。重点在于理解AF_XDP的核心概念，学习如何使用它，以及使用时需要注意的事项。</p><p>The official <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html?highlight=af_xdp" target="_blank" rel="noopener">kernel documentation</a> describes AF_XDP as “an address family that is optimized for high performance packet processing”. Why do we need an additional address family (or in other words an additional type of network socket)? Reading this sentence from the doc implies, that the existing address families are not suitable for high performance networking. And that’s exactly the case. While the Linux networking stack does a really good job abstracting layers from applications, it suffers performance due to exactly these abstractions. That’s why other libraries like DPDK completely bypass the kernel networking stack with their so called <code>Poll Mode Drivers</code> (PMD). This is very, very fast, reaching line rate for 100Gbit/s NICs. But this performance comes with some drawbacks: DPDK code is difficult to maintain, there is no chance to benefit from any kernel functionality (e.g. existing networking drivers), the number of supported NICs is limited and smaller than by the kernel, and PMD drivers completely block each used core to 100%.</p><p>内核官方文档将 AF_XDP 描述为 “为高性能数据包处理而优化的address family”。为什么我们需要一个额外的address family（或者换句话说，一个额外的网络套接字类型）？从文档中的这句话可以看出，现有的address families并不适合高性能网络。事实正是如此。虽然Linux网络协议栈在应用程序抽象层方面做得非常好，但正是由于这些抽象，它的性能受到了影响。这就是为什么其他库（如 DPDK）会通过所谓的轮询模式驱动程序（PMD）完全绕过内核网络协议栈。这样做的速度非常非常快，可以达到 100Gbit/s 网卡的速度。但这种性能也有一些缺点： DPDK 代码难以维护，无法从任何内核功能（如现有的网络驱动程序）中获益，支持的网卡数量有限，比内核还少，而且PMD 驱动程序将每个使用的core完全阻塞到100%(笔者注：PMD比较占用CPU资源)。</p><p>Consequently, getting some functionality in the Linux kernel that allows high-performance packet processing sounds pretty awesome. At first, there is one important thing to name which sometimes confuses people: AF_XDP is not a kernel bypass, like DPDK, it’s a fastpath inside the kernel. This means, e.g. normal kernel networking drivers are used. After clarifying this important difference, let’s dig into AF_XDP to see how it works and what we need to consider.<br>因此，在 Linux内核中加入一些允许高性能数据包处理的功能听起来非常棒。首先，有一点很重要，有时会让人感到困惑： AF_XDP 并不是像 DPDK 那样的绕过内核，而是内核中的fastpath。这意味着，例如，会使用正常的内核网络驱动程序。在澄清了这一重要区别之后，让我们深入了解 AF_XDP，看看它是如何工作的，以及我们需要考虑的事项。</p><p>Note: For deep explanations of all used concepts, please visit the <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html" target="_blank" rel="noopener">kernel documentation</a>, it’s really great! A complete working program and tutorial can be found <a href="https://github.com/xdp-project/xdp-tutorial/tree/master/advanced03-AF_XDP" target="_blank" rel="noopener">here</a>.<br>注：要深入了解所有使用的概念，请访问<a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html" target="_blank" rel="noopener">内核文档</a>，它真的很棒！在<a href="https://github.com/xdp-project/xdp-tutorial/tree/master/advanced03-AF_XDP" target="_blank" rel="noopener">这里</a>可以找到完整运行的程序和教程。</p><h2 id="Data-Flow-eBPF-and-XDP"><a href="#Data-Flow-eBPF-and-XDP" class="headerlink" title="Data Flow: eBPF and XDP"></a>Data Flow: eBPF and XDP</h2><p>In the mentioned kernel documentation, the authors assume that the reader is familiar with bpf and xdp, otherwise pointing to <a href="https://docs.cilium.io/en/latest/bpf/" target="_blank" rel="noopener">cilium docs</a> as reference. However, I think it’s important to mention how these two things work together with AF_XDP, to understand how AF_XDP differs from e.g. DPDK. XDP itself is a way to bypass the normal networking stack (not the whole kernel) to achieve high performance packet processing speeds. eBPF is used to run verified code in the kernel on a set of different events, called hooks. One of these hooks is the XDP hook. An eBPF program using the XDP hook gets called for every incoming packet arriving at the driver (if the driver supports running eBPF), getting a reference to the raw packet representation. The eBPF program can now perform different tasks with the packet, like modifying it, dropping it, passing it to the network stack, sending it back to the NIC or redirecting it. In our AF_XDP case, the redirecting (XDP_REDIRECT) is the most important action, because it allows to send packets directly to userspace. The following figure shows the flow of packets using a normal socket and AF_XDP.</p><p>在提到的内核文档中，作者假定读者熟悉bpf和xdp，否则会将<a href="https://docs.cilium.io/en/latest/bpf/" target="_blank" rel="noopener">cilium文档</a>作为参考。不过，我认为有必要提及这两样东西如何与 AF_XDP 协同工作，以了解 AF_XDP 与 DPDK 等的不同之处。XDP本身是一种绕过普通网络堆栈（而非整个内核）以实现高性能数据包处理速度的方法。eBPF用于在内核中不同的事件（称为钩子）运行验证代码。其中一个钩子就是 XDP 钩子。使用 XDP 钩子的 eBPF 程序会调用到达驱动程序的每个传入数据包（如果驱动程序支持运行 eBPF），并获得原始数据包的引用。现在，eBPF程序可以对数据包执行不同的任务，如修改、丢弃、传递给网络协议栈、发送回 NIC 或重定向。在我们的 AF_XDP 案例中，重定向（XDP_REDIRECT）是最重要的操作，因为它允许将数据包直接发送到用户空间。下图显示了使用普通套接字和 AF_XDP 的数据包流程。</p><p><img src="/images/2024/05/001.jpeg" alt></p><p>After being received by the NIC, the first layer the packets pass is the networking driver. In the driver, applications may load eBPF programs using the XDP hook to perform the actions explained above. In AF_XDP, the eBPF program redirects the packet to a particular XDP socket that was created in userspace. Bypassing the Linux networking (Traffic control, IP, TCP/UDP, etc), the userspace application can now handle the packets without further actions performed in the kernel. If the driver supports ZEROCOPY, the packets are written directly into address space of the application, otherwise, one copy operation needs to be performed. In contrast to AF_XDP, packets targeted to normal sockets (UDP/TCP) traverse the networking stack. They can either be passed to the stack using XDP_PASS or there is no eBPF program using the XDP hook, and packets are forwarded directly to the networking stack.<br>数据包被网卡接收后，首先经过的一层是网络驱动程序。在驱动程序中，应用程序可使用 XDP 钩子加载 eBPF 程序，以执行上述操作。在 AF_XDP 中，eBPF 程序会将数据包重定向到在用户空间创建的特定 XDP socket。绕过 Linux 网络（Traffic control、IP、TCP/UDP等），用户空间应用程序现在可以处理数据包，而无需在内核中执行进一步操作。如果驱动程序支持 ZEROCOPY，数据包就会直接写入应用程序的地址空间，否则就需要执行一次复制操作。与 AF_XDP 不同，针对普通套接字（UDP/TCP）的数据包会走网络协议栈。这些数据包可以使用XDP_PASS传递到协议栈，或者不使用 XDP 钩子的 eBPF 程序，直接转发到网络协议栈。</p><p>Now let’s consider the backwards direction. In AF_XDP, packets can be passed directly to the NIC driver by passing a block of memory containing them to the driver, which then processes them and send them to the NIC. On the other hand, normal sockets send packets using syscalls like <code>sendto</code>, where the packets traverse the whole networking stack backwards. On the outgoing side, there is no XDP hook that can be attached using eBPF, so no further packet processing here.</p><p>现在，让我们考虑一下相反方向(笔者注：发包)。在 AF_XDP 中，数据包可以直接传递给网卡驱动程序，方法是将包含数据包的内存块传递给驱动程序，然后由驱动程序处理数据包并将其发送给网卡。另一方面，普通套接字使用<code>sendto</code>等系统调用发送数据包，数据包会经过整个网络协议栈。发包时，没有可以使用 eBPF attached的 XDP 钩子，因此这里没有进一步的数据包处理。</p><p>Note: Please consider that there are some SmartNICs that also support running XDP programs directly on the NIC. However, this is not the common case, therefor the driver mode is focused here.<br>注：请注意，有些 SmartNIC 也支持直接在 NIC 上运行 XDP程序。不过，这种情况并不常见，因此这里主要介绍驱动程序模式。</p><h2 id="Structure-and-Concepts"><a href="#Structure-and-Concepts" class="headerlink" title="Structure and Concepts"></a>Structure and Concepts</h2><p>In the previous section, we saw how packets flow until they arrive at our application. So now let’s look at how AF_XDP sockets read and write packets from/to the NIC driver. AF_XDP works in a completely different way from what we already know about socket programming. The setup of the socket is quite similar, but reading and writing from/to the NIC differs a lot. In AF_XDP, you create a UMEM region, and you have four rings assigned to the UMEM: RX, TX, completion and fill ring. Wow, sounds really complication. But trust me, it’s not. UMEM is basically just an area of continuos virtual memory, divided into equal-sized frames. The mentioned 4 rings contain pointers to particular offsets in the UMEM. To understand the rings, let’s consider an example, shown in the next figure.<br>在上一节中，我们了解了数据包在到达应用程序之前是如何流动的。现在我们来看看 AF_XDP 套接字是如何从网卡驱动程序读写数据包的。AF_XDP的工作方式与我们已经了解的套接字编程完全不同。套接字的设置非常相似，但从 NIC 读取和向 NIC 写入数据却有很大不同。在 AF_XDP 中，您需要创建一个 UMEM 区域，并为UMEM分配四个环：RX、TX、completion和fill环。听起来真复杂。但相信我，其实并不复杂。UMEM 基本上只是一个连续虚拟内存区域，被划分为大小相等的帧。上述 4 个环包含指向 UMEM中特定偏移量的指针。为了理解这些环，让我们看一个例子，如下图所示。</p><p><img src="/images/2024/05/002.jpeg" alt></p><p>This figure covers the reading of packets from the driver. So we produce UMEM addresses to the fill ring, meaning we put some slots of our UMEM into the fill ring (1). Afterwards, we notify the kernel: Hey, there are entries in our fill ring, please write arriving packets there. After passing the fill ring (2) and the rx ring (3) to the kernel, the kernel writes packets at the slots we produced beforehand (4) to the rx ring. We can now fetch new packets using the rx ring, after the kernel gives us back both rings (5) (6). The rx ring contains packet descriptors in the slots we passed via the fill ring to the kernel, in case there were packets that arrived. Great, we can now handle all of our packets, and then start again putting some references in the fill ring, and continue the same reading packets from the NIC.</p><p>To send packets via the NIC, the remaining two rings are used, in a similar way seen before on the receive side. We produce packet descriptors to the tx ring, meaning we put some references to our UMEM into the tx ring. Once we filled the ring, we pass it to the kernel. After the kernel transferred the packets, the respective references are filled into the completion ring and our application can reuse the slots in the UMEM.</p><p>In summary, using AF_XDP, we get a pretty awesome tradeoff between using existing code of the kernel (NIC drivers) and gain high performance for packet processing. I hope this article gives you at least an idea of how AF_XDP works.</p><h2 id="4个ring的总结"><a href="#4个ring的总结" class="headerlink" title="4个ring的总结"></a>4个ring的总结</h2><p>对于fill ring、completion ring、rx ring、tx ring的总结:</p><ul><li>fill ring与rx ring配合，用于收包<ul><li>fill ring(生产者是用户态程序，消费者是内核态中的XDP程序)类比于virtio-net收包时的avail ring</li><li>rx ring(生产者是XDP程序，消费者是用户态程序)类比于virtio-net收包时的used ring</li></ul></li><li>completion ring与tx ring配合，用于发包<ul><li>tx ring(生产者是用户态程序，消费者是XDP程序)类比于virtio-net发包时的avail ring</li><li>completion ring(生产者是XDP程序，消费者是用户态程序)类比于virtio-net发包时的used ring</li></ul></li><li>The UMEM uses two rings: FILL and COMPLETION. Each socket associated with the UMEM must have an RX queue, TX queue or both. Say, that there is a setup with four sockets (all doing TX and RX). Then there will be one FILL ring, one COMPLETION ring, four TX rings and four RX rings.</li><li>The rings are head(producer)/tail(consumer) based rings. A producer writes the data ring at the index pointed out by struct xdp_ring producer member, and increasing the producer index. A consumer reads the data ring at the index pointed out by struct xdp_ring consumer member, and increasing the consumer index.</li></ul><hr><p>参考资料:</p><ol><li><a href="https://rexrock.github.io/post/af_xdp1/" target="_blank" rel="noopener">AF_XDP技术详解</a></li><li><a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html?highlight=af_xdp" target="_blank" rel="noopener">https://www.kernel.org/doc/html/latest/networking/af_xdp.html?highlight=af_xdp</a></li><li><a href="https://docs.cilium.io/en/latest/bpf/" target="_blank" rel="noopener">https://docs.cilium.io/en/latest/bpf/</a></li><li><a href="https://www.youtube.com/watch?v=9bbdhnbVbDk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=9bbdhnbVbDk</a></li><li><a href="https://www.youtube.com/watch?v=Gv-nG6F_09I&amp;t=1417s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=Gv-nG6F_09I&amp;t=1417s</a></li><li><a href="http://vger.kernel.org/lpc_net2018_talks/lpc18_paper_af_xdp_perf-v2.pdf" target="_blank" rel="noopener">http://vger.kernel.org/lpc_net2018_talks/lpc18_paper_af_xdp_perf-v2.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要转载自&lt;a href=&quot;https://hpnpl.net/posts/recapituatling-af-xdp/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Recapitulating AF_XDP&lt;/a&gt;。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>notes about AMD spec</title>
    <link href="http://liujunming.github.io/2024/04/21/notes-about-AMD-spec/"/>
    <id>http://liujunming.github.io/2024/04/21/notes-about-AMD-spec/</id>
    <published>2024-04-21T12:44:30.000Z</published>
    <updated>2024-04-22T14:19:53.653Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下AMD spec相关信息。<a id="more"></a>本文将随着笔者的研发经验的积累持续更新。</p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><h4 id="搜索网站"><a href="#搜索网站" class="headerlink" title="搜索网站"></a>搜索网站</h4><p><a href="https://www.amd.com/en/search/documentation/hub.html#sortCriteria=%40amd_release_date%20descending" target="_blank" rel="noopener">https://www.amd.com/en/search/documentation/hub.html#sortCriteria=%40amd_release_date%20descending</a></p><h4 id="搜索技巧"><a href="#搜索技巧" class="headerlink" title="搜索技巧"></a>搜索技巧</h4><p>作为虚拟化开发人员，Programmer References和Specifications这两个Document Type用的比较多。<br><img src="/images/2024/04/015.jpg" alt></p><h3 id="APM"><a href="#APM" class="headerlink" title="APM"></a>APM</h3><p>APM(Architecture Programmer’s Manual)与Intel的SDM对应。</p><ul><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/24592.pdf" target="_blank" rel="noopener">APM Vol1</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/24593.pdf" target="_blank" rel="noopener">APM Vol2</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/24594.pdf" target="_blank" rel="noopener">APM Vol3</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/26568.pdf" target="_blank" rel="noopener">APM Vol4</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/26569.pdf" target="_blank" rel="noopener">APM Vol5</a></li></ul><h3 id="IOMMU"><a href="#IOMMU" class="headerlink" title="IOMMU"></a>IOMMU</h3><ul><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/specifications/48882_IOMMU.pdf" target="_blank" rel="noopener">AMD I/O Virtualization Technology (IOMMU) Specification, 48882</a></li></ul><h3 id="MISC"><a href="#MISC" class="headerlink" title="MISC"></a>MISC</h3><p><img src="/images/2024/04/016.jpg" alt></p><ul><li><a href="https://kib.kiev.ua/x86docs/AMD/" target="_blank" rel="noopener">https://kib.kiev.ua/x86docs/AMD/</a></li><li><a href="https://lore.kernel.org/kvm/?q=24593" target="_blank" rel="noopener">https://lore.kernel.org/kvm/?q=24593</a></li><li><a href="https://lore.kernel.org/kvm/?q=APM" target="_blank" rel="noopener">https://lore.kernel.org/kvm/?q=APM</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下AMD spec相关信息。
    
    </summary>
    
      <category term="AMD" scheme="http://liujunming.github.io/categories/AMD/"/>
    
    
      <category term="AMD" scheme="http://liujunming.github.io/tags/AMD/"/>
    
  </entry>
  
  <entry>
    <title>深入理解intel vmfunc指令</title>
    <link href="http://liujunming.github.io/2024/04/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3intel-vmfunc%E6%8C%87%E4%BB%A4/"/>
    <id>http://liujunming.github.io/2024/04/21/深入理解intel-vmfunc指令/</id>
    <published>2024-04-21T11:31:37.000Z</published>
    <updated>2024-04-21T12:28:28.358Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>VMFUNC is an Intel hardware instruction that allows software in non-root mode (in both kernel and user modes) to invoke a VM function. VM functions are processor features managed by the hypervisor. EPTP (the pointer to an EPT) switching is one of these VM functions<a id="more"></a>, which allows the guest to load a new value for the EPTP from an EPTP list stored in the Virtual Machine Control Structure (VMCS) configured by the hypervisor. The new EPT then translates subsequent guest physical addresses (GPA) to host physical addresses (HPA). The EPTP list can hold at most 512 EPTP entries. The typical usage of EPTP switching is to create multiple domains for one physical address space and these domains usually have different memory mappings and privileges. With the Virtual Processor ID (VPID) feature enabled, the VMFUNC instruction does not flush TLB. </p><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><p><img src="/images/2024/04/010.jpg" alt></p><p><img src="/images/2024/04/011.jpg" alt></p><p><img src="/images/2024/04/012.jpg" alt></p><p><img src="/images/2024/04/013.jpg" alt></p><p><img src="/images/2024/04/014.jpg" alt></p><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>Q: TLB是gva-&gt;hpa的映射，EPTP切换之后，此时HPA可能会发生变化，那么vmfunc需要flush TLB吗？</p><p>A: The logical processor starts creating and using guest-physical and combined mappings associated with the new value of bits 51:12 of EPTP; the combined mappings created and used are associated with the current VPID and PCID (these are not changed by VMFUNC).<br>当enable VPID时，硬件会重新creating mappings！所以是由硬件保证了TLB的正确性，无需flush TLB!<br>笔者猜测硬件的行为：执行vmfunc命令时，处理器会检查TLB中当前VPID和PCID的相关entry，然后更新TLB中的相关entry(比如会更新HPA的值或者权限)。</p><hr><p>参考资料:</p><ol><li><a href="https://www.cse.unsw.edu.au/~cs9242/19/exam/paper1.pdf" target="_blank" rel="noopener">SkyBridge: Fast and Secure Inter-Process Communication for Microkernels</a></li><li>Intel SDM Vol3</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h3&gt;&lt;p&gt;VMFUNC is an Intel hardware instruction that allows software in non-root mode (in both kernel and user modes) to invoke a VM function. VM functions are processor features managed by the hypervisor. EPTP (the pointer to an EPT) switching is one of these VM functions
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
</feed>
