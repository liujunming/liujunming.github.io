<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2024-08-18T12:56:39.938Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>network: USO vs UFO</title>
    <link href="http://liujunming.github.io/2024/08/18/network-USO-vs-UFO/"/>
    <id>http://liujunming.github.io/2024/08/18/network-USO-vs-UFO/</id>
    <published>2024-08-18T12:24:36.000Z</published>
    <updated>2024-08-18T12:56:39.938Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下USO(UDP Segmentation offload) vs UFO(UDP Fragmentation Offload)相关notes。<a id="more"></a><br>需阅读<a href="/2024/08/11/Network-Segmentation-vs-Fragmentation/">Network Segmentation vs Fragmentation</a>，值得注意的是，UDP也是存在Segmentation的。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>UDP Segmentation Offload (USO)is a feature that enables network interface cards (NICs) to offload the segmentation of UDP datagrams that are larger than the maximum transmission unit (MTU) of the network medium.</p><p>UDP fragmentation offload allows a device to fragment an oversized UDP datagram into multiple IPv4 fragments.</p><h2 id="USO-vs-UFO"><a href="#USO-vs-UFO" class="headerlink" title="USO vs UFO"></a>USO vs UFO</h2><p>There is a USO feature that is different from existing UFO:</p><ul><li>UFO fragments the UDP packet and only first fragment carries the UDP header (SKB_GSO_UDP in the Linux network stack)</li><li>USO segments the UDP packet, each segment has a UDP header and IP identification field is incremented for each segment. It is designated as SKB_GSO_UDP_L4 in the Linux network stack</li></ul><blockquote><p>VIRTIO_NET_F_HOST_USO (56)<br>Device can receive USO packets. Unlike UFO (fragmenting the packet) the USO splits large UDP packet to several segments when each of these smaller packets has UDP header.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">Virtual I/O Device (VIRTIO) Version 1.2</a></li><li><a href="https://github.com/oasis-tcs/virtio-spec/issues/104" target="_blank" rel="noopener">virtio-net: define USO feature</a></li><li><a href="https://lore.kernel.org/netdev/20221207113558.19003-4-andrew@daynix.com/T/" target="_blank" rel="noopener">udp: allow header check for dodgy GSO_UDP_L4 packets</a></li><li><a href="https://www.slideshare.net/slideshow/20140928-gso-eurobsdcon2014/43398725" target="_blank" rel="noopener">Software segmentation offloading for FreeBSD by Stefano Garzarella</a></li><li><a href="https://www.cnblogs.com/sammyliu/p/5227121.html" target="_blank" rel="noopener">理解 Linux 网络栈（2）：非虚拟化Linux 环境中的 Segmentation Offloading 技术</a></li><li><a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/network/udp-segmentation-offload-uso-" target="_blank" rel="noopener">UDP Segmentation Offload (USO)</a></li><li><a href="https://www.kernel.org/doc/html/next/networking/segmentation-offloads.html" target="_blank" rel="noopener">Segmentation Offloads</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下USO(UDP Segmentation offload) vs UFO(UDP Fragmentation Offload)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about network checksum offload</title>
    <link href="http://liujunming.github.io/2024/08/18/Notes-about-network-checksum-offload/"/>
    <id>http://liujunming.github.io/2024/08/18/Notes-about-network-checksum-offload/</id>
    <published>2024-08-18T11:21:20.000Z</published>
    <updated>2024-08-18T12:11:53.952Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下network checksum offload技术的相关notes。<a id="more"></a></p><h2 id="软件协议checksum"><a href="#软件协议checksum" class="headerlink" title="软件协议checksum"></a>软件协议checksum</h2><p>很多网络协议，例如IP、TCP、UDP都有自己的校验和(checksum)。</p><h3 id="TCP-checksum"><a href="#TCP-checksum" class="headerlink" title="TCP checksum"></a>TCP checksum</h3><p>TCP校验和计算三部分：TCP头部、TCP数据和TCP伪头部。TCP校验和是必须的。<br><img src="/images/2024/08/016.jpg" alt></p><p><img src="/images/2024/08/017.jpg" alt></p><h3 id="UDP-checksum"><a href="#UDP-checksum" class="headerlink" title="UDP checksum"></a>UDP checksum</h3><p>UDP校验和计算三部分：UDP头部、UDP数据和UDP伪头部。UDP校验和是可选的。</p><p><img src="/images/2024/08/018.jpg" alt></p><p><img src="/images/2024/08/019.jpg" alt></p><h3 id="IP-checksum"><a href="#IP-checksum" class="headerlink" title="IP checksum"></a>IP checksum</h3><p>IP校验和只计算检验IP数据报的首部，但不包括IP数据报中的数据部分。</p><p><img src="/images/2024/08/020.jpg" alt></p><p><img src="/images/2024/08/021.jpg" alt></p><h2 id="checksum-offload"><a href="#checksum-offload" class="headerlink" title="checksum offload"></a>checksum offload</h2><p>传统上，校验和的计算（发送数据包）和验证（接收数据包）是通过CPU完成的。这对CPU的影响很大，因为校验和需要每个字节的数据都参与计算。对于一个100G带宽的网络，需要CPU最多每秒计算大约12G的数据。</p><p>为了减轻这部分的影响，现在的网卡，都支持校验和的计算和验证。系统内核在封装网络数据包的时候，可以跳过校验和。网卡收到网络数据包之后，根据网络协议的规则，进行计算，再将校验和填入相应的位置。</p><p>因为Checksum offload的存在，在用tcpdump之类的抓包分析工具时，有时会发现抓到的包提示校验和错误（checksum incorrect）。tcpdump抓到的网络包就是系统内核发给网卡的网络包，如果校验和放到网卡去计算，那么tcpdump抓到包的时刻，校验和还没有被计算出来，自然看到的是错误的值。</p><h2 id="virtio-net"><a href="#virtio-net" class="headerlink" title="virtio-net"></a>virtio-net</h2><blockquote><p>VIRTIO_NET_F_CSUM (0)<br>Device handles packets with partial checksum. This “checksum offload” is a common feature on modern network cards.</p></blockquote><blockquote><p>VIRTIO_NET_F_HOST_TSO4<br>Requires VIRTIO_NET_F_CSUM.</p></blockquote><p>由上述描述可知<a href="/2023/04/23/Notes-about-TSO、GSO、LRO、GRO/#TSO">TSO</a>需要Checksum offload的支持。因为在enable TSO时，TCP/IP协议栈并不知道最终的网络数据包是什么样，自然也没办法完成校验和计算。</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/44635205" target="_blank" rel="noopener">常见网络加速技术浅谈（一）</a></li><li><a href="https://zhuanlan.zhihu.com/p/106400339" target="_blank" rel="noopener">Wireshark 提示和技巧 | Checksum Offload</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc9293" target="_blank" rel="noopener">RFC 9293: Transmission Control Protocol</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc768" target="_blank" rel="noopener">RFC 768: User Datagram Protocol</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc791" target="_blank" rel="noopener">RFC 791: INTERNET PROTOCOL</a></li><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">Virtual I/O Device (VIRTIO) Version 1.2</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下network checksum offload技术的相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Network Segmentation vs Fragmentation</title>
    <link href="http://liujunming.github.io/2024/08/11/Network-Segmentation-vs-Fragmentation/"/>
    <id>http://liujunming.github.io/2024/08/11/Network-Segmentation-vs-Fragmentation/</id>
    <published>2024-08-11T09:28:22.000Z</published>
    <updated>2024-08-11T12:03:43.222Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下计算机网络中的分段(Segmentation)与分片(Fragmentation)操作。<a id="more"></a>本文主要内容转载自<a href="https://cloud.tencent.com/developer/article/1828823" target="_blank" rel="noopener">动图图解！既然IP层会分片，为什么TCP层也还要分段？</a>。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>分段特指发生在使用TCP协议的传输层中的数据切分行为<br><img src="/images/2024/08/011.gif" alt></p><p>分片特指发生在使用IP协议的网络IP层中的数据切分行为<br><img src="/images/2024/08/012.gif" alt></p><p>TCP协议在将用户数据传给IP层之前，会先将大段的数据根据MSS（Maximum Segment Size）分成多个小段，这个过程是Segmentation，分出来的数据是Segments。IP协议因为MTU（Maximum Transmission Unit）的限制，会将上层传过来的并且超过MTU的数据，分成多个分片，这个过程是Fragmentation，分出来的数据是Fragments。这两个过程都是大块的数据分成多个小块数据，区别就是一个在TCP（L4），一个在IP（L3）完成。</p><h2 id="MSS与MTU的区别"><a href="#MSS与MTU的区别" class="headerlink" title="MSS与MTU的区别"></a>MSS与MTU的区别</h2><p>TCP 提交给 IP 层最大分段大小，不包含 TCP Header 和  TCP Option，只包含 TCP Payload ，MSS 是 TCP 用来限制应用层最大的发送字节数。<br>假设 MTU= 1500 byte，那么 MSS = 1500- 20(IP Header) -20 (TCP Header) = 1460 byte，如果应用层有 2000 byte 发送，那么需要两个切片才可以完成发送，第一个 TCP 切片 = 1460，第二个 TCP 切片 = 540。</p><p>MTU是由<strong>数据链路层</strong>提供，为了告诉上层IP层，自己的传输能力是多大。IP层就会根据它进行数据包切分。一般 MTU=1500 Byte。<br>假设IP层有 &lt;= 1500 byte 需要发送，只需要一个 IP 包就可以完成发送任务；假设 IP 层有 &gt; 1500 byte 数据需要发送，需要分片才能完成发送，分片后的 IP Header ID 相同，同时为了分片后能在接收端把切片组装起来，还需要在分片后的IP包里加上各种信息。比如这个分片在原来的IP包里的偏移offset。</p><p><img src="/images/2024/08/015.jpg" alt></p><p>在一台机器的应用层到这台机器的网卡，<strong>这条链路上</strong>，基本上可以保证，MSS &lt; MTU。</p><p><img src="/images/2024/08/013.png" alt></p><h2 id="为什么MTU一般是1500"><a href="#为什么MTU一般是1500" class="headerlink" title="为什么MTU一般是1500"></a>为什么MTU一般是1500</h2><p>这其实是由传输效率决定的。虽然我们平时用的网络感觉挺稳定的，但其实这是因为TCP在背地里做了各种重传等保证了传输的可靠，其实背地里线路是动不动就丢包的，而越大的包，发生丢包的概率就越大。</p><p>那是不是包越小就越好？也不是</p><p>如果选择一个比较小的长度，假设选择MTU为300Byte，TCP payload = 300 - IP Header - TCP Header = 300 - 20 - 20 = 260 byte。那有效传输效率= 260 / 300 = 86%</p><p>而如果以太网MTU长度为1500，那有效传输效率= 1460 / 1500 = 96% ，显然比 86% 高多了。</p><p>所以，包越小越不容易丢包，包越大，传输效率又越高，因此权衡之下，选了1500。</p><h2 id="为什么IP层会分片，TCP还要分段"><a href="#为什么IP层会分片，TCP还要分段" class="headerlink" title="为什么IP层会分片，TCP还要分段"></a>为什么IP层会分片，TCP还要分段</h2><p>由于本身IP层就会做分片这件事情。就算TCP不分段，到了IP层，数据包也会被分片，数据也能正常传输。</p><p>既然网络层就会分片了，那么TCP为什么还要分段？是不是有些多此一举？</p><p>假设有一份数据，较大，且在TCP层不分段，如果这份数据在发送的过程中出现丢包现象，TCP会发生重传，那么重传的就是这一大份数据（虽然IP层会把数据切分为MTU长度的N多个小包，但是TCP重传的单位却是那一大份数据）。</p><p>如果TCP把这份数据，分段为N个小于等于MSS长度的数据包，到了IP层后加上IP头和TCP头，还是小于MTU，那么IP层也不会再进行分片。此时在传输路上发生了丢包，那么TCP重传的时候也只是重传那一小部分的MSS段。效率会比TCP不分段时更高。</p><p>类似的，传输层除了TCP外，还有UDP协议，但UDP本身不会分段，所以当数据量较大时，只能交给IP层去分片，然后传到底层进行发送。</p><p>正常情况下，在一台机器的传输层到网络层<strong>这条链路</strong>上，如果传输层对数据做了分段，那么IP层就不会再分片。如果传输层没分段，那么IP层就可能会进行分片。</p><p><strong>数据在TCP分段，就是为了在IP层不需要分片，同时发生重传的时候只重传分段后的小份数据</strong>。</p><h2 id="TCP分段了，IP层就一定不会分片了吗"><a href="#TCP分段了，IP层就一定不会分片了吗" class="headerlink" title="TCP分段了，IP层就一定不会分片了吗"></a>TCP分段了，IP层就一定不会分片了吗</h2><p>在发送端，TCP分段后，IP层就不会再分片了。</p><p>但是整个传输链路中，可能还会有其他网络层设备，而这些设备的MTU可能小于发送端的MTU。此时虽然数据包在发送端已经分段过了，但是在IP层还会再分片一次。</p><p>如果链路上还有设备<strong>有更小的MTU</strong>，那么还会再分片，最后所有的分片都会在<strong>接收端</strong>进行组装。</p><p><img src="/images/2024/08/014.gif" alt></p><p>因此，就算TCP分段过后，在链路上的其他节点的IP层也是有可能再分片的，而且哪怕数据被第一次IP分片过了，也是有可能被其他机器的IP层进行二次、三次、四次….分片的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>(TCP)分段和(IP)分片各自发生在不同的协议层(分段-TCP传输层，分片-IP层)</li><li>TCP分段的原因是TCP报文段大小受MSS限制，IP分片则是因为IP数据报大小受MTU限制</li><li>在发送方，数据在TCP分段，在IP层就不需要分片，同时发生重传的时候只重传分段后的小份数据</li><li>虽然分段和分片不会在发送方同时发生，但却可能在同一次通信过程中分别在发送主机(分段)和转发设备(分片)中发生</li><li>IP分片是<strong>不得已</strong>的行为，尽量不在IP层分片，尤其是链路上中间设备的IP分片</li><li>UDP不会分段，就由IP来分片</li></ul><hr><p>参考资料:</p><ol><li><a href="https://cloud.tencent.com/developer/article/1828823" target="_blank" rel="noopener">动图图解！既然IP层会分片，为什么TCP层也还要分段？</a></li><li><a href="https://cloud.tencent.com/developer/article/1173790" target="_blank" rel="noopener">TCP分段与IP分片的区别与联系</a></li><li><a href="https://zhuanlan.zhihu.com/p/44635205" target="_blank" rel="noopener">常见网络加速技术浅谈（一）</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下计算机网络中的分段(Segmentation)与分片(Fragmentation)操作。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Network RFC合集</title>
    <link href="http://liujunming.github.io/2024/08/11/Network-RFC%E5%90%88%E9%9B%86/"/>
    <id>http://liujunming.github.io/2024/08/11/Network-RFC合集/</id>
    <published>2024-08-11T08:40:33.000Z</published>
    <updated>2024-08-18T11:18:22.585Z</updated>
    
    <content type="html"><![CDATA[<p>本文将持续记录计算机网络中协议的RFC号。<a id="more"></a></p><ul><li><a href="https://datatracker.ietf.org/doc/html/rfc791" target="_blank" rel="noopener">RFC 791: INTERNET PROTOCOL</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc9293" target="_blank" rel="noopener">RFC 9293: Transmission Control Protocol</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc1180" target="_blank" rel="noopener">RFC 9293: A TCP/IP Tutorial</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc768" target="_blank" rel="noopener">RFC 768: User Datagram Protocol</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将持续记录计算机网络中协议的RFC号。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about pci-pci bridge</title>
    <link href="http://liujunming.github.io/2024/08/10/Notes-about-pci-pci-bridge/"/>
    <id>http://liujunming.github.io/2024/08/10/Notes-about-pci-pci-bridge/</id>
    <published>2024-08-10T03:52:21.000Z</published>
    <updated>2024-08-10T06:37:21.231Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下pci-pci bridge相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>For PCI-PCI bridges to pass PCI I/O, PCI Memory or PCI Configuration address space reads and writes across them, they need to know the following:</p><ul><li><p><strong>Primary Bus Number</strong><br>The bus number immediately upstream of the PCI-PCI Bridge,</p></li><li><p><strong>Secondary Bus Number</strong><br>The bus number immediately downstream of the PCI-PCI Bridge,</p></li><li><p><strong>Subordinate Bus Number</strong><br>The highest bus number of all of the busses that can be reached downstream of the bridge.</p></li><li><p><strong>PCI I/O and PCI Memory Windows</strong><br>The window base and size for PCI I/O address space and PCI Memory address space for all addresses downstream of the PCI-PCI Bridge.</p></li></ul><h2 id="Bus-Number"><a href="#Bus-Number" class="headerlink" title="Bus Number"></a>Bus Number</h2><p><img src="/images/2024/08/007.jpg" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Header type 1 (PCI-to-PCI bridges) */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PCI_PRIMARY_BUS0x18<span class="comment">/* Primary bus number */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PCI_SECONDARY_BUS0x19<span class="comment">/* Secondary bus number */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PCI_SUBORDINATE_BUS0x1a<span class="comment">/* Highest bus number behind the bridge */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PCI_SEC_LATENCY_TIMER0x1b<span class="comment">/* Latency timer for secondary interface */</span></span></span><br></pre></td></tr></table></figure><blockquote><p>The problem is that at the time when you wish to configure any given PCI-PCI bridge you do not know the subordinate bus number for that bridge. You do not know if there are further PCI-PCI bridges downstream and if you did, you do not know what numbers will be assigned to them. The answer is to use a depthwise recursive algorithm and scan each bus for any PCI-PCI bridges assigning them numbers as they are found. As each PCI-PCI bridge is found and its secondary bus numbered, assign it a temporary subordinate number of 0xFF and scan and assign numbers to all PCI-PCI bridges downstream of it.</p></blockquote><p>其实subordinate的计算是基于DFS算法的。</p><p>算法详细请参考<a href="https://www.science.unitn.it/~fiorella/guidelinux/tlk/node76.html" target="_blank" rel="noopener">Configuring PCI-PCI Bridges - Assigning PCI Bus Numbers</a></p><p><img src="/images/2024/08/008.gif" alt></p><h2 id="PCI-I-O-and-PCI-Memory-Windows"><a href="#PCI-I-O-and-PCI-Memory-Windows" class="headerlink" title="PCI I/O and PCI Memory Windows"></a>PCI I/O and PCI Memory Windows</h2><p>PCI-PCI bridges only pass a subset of PCI I/O and PCI memory read and write requests downstream. For example, in the following Figure, the PCI-PCI bridge will only pass read and write addresses from PCI bus 0 to PCI bus 1 if they are for PCI I/O or PCI memory addresses owned by either the SCSI or ethernet device; all other PCI I/O and memory addresses are ignored. This filtering stops addresses propogating needlessly throughout the system. To do this, the PCI-PCI bridges must be programmed with a base and limit for PCI I/O and PCI Memory space access that they have to pass from their primary bus onto their secondary bus.</p><p><img src="/images/2024/08/009.gif" alt></p><ul><li><p><strong>The PCI-PCI Bridge</strong><br>We now cross the PCI-PCI Bridge and allocate PCI memory there:</p><ul><li><strong>The Ethernet Device</strong><br>This is asking for 0xB0 bytes of both PCI I/O and PCI Memory space. It gets allocated PCI I/O at 0x4000 and PCI Memory at 0x400000. The PCI Memory base is moved to 0x4000B0 and the PCI I/O base to 0x40B0.</li><li><strong>The SCSI Device</strong><br>This is asking for 0x1000 PCI Memory and so it is allocated it at 0x401000 after it has been naturally aligned. The PCI I/O base is still 0x40B0 and the PCI Memory base has been moved to 0x402000.</li></ul></li><li><p><strong>The PCI-PCI Bridge’s PCI I/O and Memory Windows</strong><br>We now return to the bridge and set its PCI I/O window at between 0x4000 and 0x40B0 and it’s PCI Memory window at between 0x400000 and 0x402000. This means that the PCI-PCI Bridge will ignore the PCI Memory accesses for the video device and pass them on if they are for the ethernet or scsi devices.</p></li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.science.unitn.it/~fiorella/guidelinux/tlk/node76.html" target="_blank" rel="noopener">Configuring PCI-PCI Bridges - Assigning PCI Bus Numbers</a></li><li><a href="https://www.science.unitn.it/~fiorella/guidelinux/tlk/node80.html" target="_blank" rel="noopener">Allocating PCI I/O and PCI Memory to PCI-PCI Bridges and Devices</a></li><li><a href="https://wiki.osdev.org/PCI" target="_blank" rel="noopener">https://wiki.osdev.org/PCI</a></li><li><a href="https://tldp.org/LDP/tlk/dd/pci.html" target="_blank" rel="noopener">https://tldp.org/LDP/tlk/dd/pci.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下pci-pci bridge相关notes。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about CUDA Unified Memory</title>
    <link href="http://liujunming.github.io/2024/08/04/Notes-about-NVDIA-GPU-Unified-Memory/"/>
    <id>http://liujunming.github.io/2024/08/04/Notes-about-NVDIA-GPU-Unified-Memory/</id>
    <published>2024-08-04T09:50:41.000Z</published>
    <updated>2024-08-04T12:12:44.095Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下CUDA Unified Memory相关notes。<a id="more"></a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img src="/images/2024/08/002.jpg" alt></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2024/08/001.jpg" alt></p><p>Traditionally, GPUs and CPUs have their own memory spaces, and applications running on one particular GPU cannot access the data directly from the memory of other GPUs or CPUs. To improve memory utilization, the latest NVIDIA PASCAL GPU released in 2016 supports unified memory , i.e., each GPU can access the whole memory space of both GPUs and CPUs via uniform memory addresses. In particular, the unified memory provides to all GPUs and CPUs a single memory address space, with an automatic page migration for data locality. The page migration engine also allows GPU threads to trigger page fault when the accessed data does not reside in GPU memory, and this makes the system eficiently migrate pages from anywhere in the system to the memory of GPUs in an on-demand manner.</p><p>The benefits of unified memory are twofold. First, <strong>it enables a GPU to handle dataset which is larger than its own memory size</strong>, because the unified memory can migrate data from CPU memory to GPU memory in an on-demand fashion. Second, <strong>using the unified memory can simplify the programming model</strong>. In particular, programmers can simply use a pointer to access data pages no matter where they reside, instead of explicitly calling data migration.</p><h2 id="CUDA-6-UNIFIED-MEMORY"><a href="#CUDA-6-UNIFIED-MEMORY" class="headerlink" title="CUDA 6+:UNIFIED MEMORY"></a>CUDA 6+:UNIFIED MEMORY</h2><p><img src="/images/2024/08/003.jpg" alt></p><h3 id="simplify-the-programming-model"><a href="#simplify-the-programming-model" class="headerlink" title="simplify the programming model"></a>simplify the programming model</h3><p><img src="/images/2024/08/005.jpg" alt></p><p><img src="/images/2024/08/006.jpg" alt></p><h2 id="CUDA-8-UNIFIED-MEMORY"><a href="#CUDA-8-UNIFIED-MEMORY" class="headerlink" title="CUDA 8+: UNIFIED MEMORY"></a>CUDA 8+: UNIFIED MEMORY</h2><p><img src="/images/2024/08/004.jpg" alt></p><h2 id="SVA"><a href="#SVA" class="headerlink" title="SVA"></a>SVA</h2><p><img src="/images/2024/02/003.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/06/06_Managed_Memory.pdf" target="_blank" rel="noopener">CUDA UNIFIED MEMORY</a></li><li><a href="https://www.cse.cuhk.edu.hk/~cslui/PUBLICATION/SOCC_2019_B.pdf" target="_blank" rel="noopener">DCUDA: Dynamic GPU Scheduling with Live Migration Support</a></li><li><a href="https://zhuanlan.zhihu.com/p/640161668" target="_blank" rel="noopener">GPU 是如何使用内存的</a></li><li><a href="https://zhuanlan.zhihu.com/p/430101220" target="_blank" rel="noopener">浅谈GPU通信和PCIe P2P DMA</a></li><li><a href="https://zhuanlan.zhihu.com/p/82651065" target="_blank" rel="noopener">CUDA中的Unified Memory</a></li><li><a href="https://soft.cs.tsinghua.edu.cn/os2atc2018/ppt/osd5.pdf" target="_blank" rel="noopener">SVA：基于异构系统的内存管理技术</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下CUDA Unified Memory相关notes。
    
    </summary>
    
      <category term="GPU" scheme="http://liujunming.github.io/categories/GPU/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about flock 文件锁</title>
    <link href="http://liujunming.github.io/2024/08/04/Notes-about-flock-%E6%96%87%E4%BB%B6%E9%94%81/"/>
    <id>http://liujunming.github.io/2024/08/04/Notes-about-flock-文件锁/</id>
    <published>2024-08-04T07:16:39.000Z</published>
    <updated>2024-08-04T09:08:58.375Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下flock 文件锁相关notes。<a id="more"></a>本文内容主要转载自<a href="https://zhuanlan.zhihu.com/p/25134841" target="_blank" rel="noopener">被遗忘的桃源——flock 文件锁</a>。</p><h2 id="文件锁-flock"><a href="#文件锁-flock" class="headerlink" title="文件锁 flock"></a>文件锁 flock</h2><p>为解决多进程对同一文件的读写冲突，在linux 系统中，提供了 flock 这一系统调用，用来实现对文件的读写保护，即文件锁的功能。文件锁保护文件的功能，与pthread 库中多线程使用读写锁来保护内存资源的方式是类似的。 flock 的 man page 中有如下介绍：</p><blockquote><p>flock - apply or remove an advisory lock on an open file</p></blockquote><p>从中可以解读出两点内容：</p><ul><li>flock 提供的文件锁是<strong>建议性质</strong>的。所谓 “建议性锁”，通常也叫作非强制性锁，即一个进程可以忽略其他进程加的锁，直接对目标文件进行读写操作。因而，<strong>只有当前进程主动调用 flock去检测是否已有其他进程对目标文件加了锁，文件锁才会在多进程的同步中起到作用</strong>。表述的更明确一点，就是如果其他进程已经用 flock 对某个文件加了锁，当前进程在读写这一文件时，未使用 flock 加锁（即未检测是否已有其他进程锁定文件），那么当前进程可以直接操作这一文件，其他进程加的文件锁对当前进程的操作不会有任何影响。<strong>这种可以被忽略、需要双方互相检测确认的加锁机制，就被称为 ”建议性“ 锁</strong>。</li><li>文件锁必须作用在一个打开的文件上，即从应用的角度看，文件锁应当作用于一个打开的文件句柄上。</li></ul><h2 id="共享锁与互斥锁"><a href="#共享锁与互斥锁" class="headerlink" title="共享锁与互斥锁"></a>共享锁与互斥锁</h2><p>linux 中 flock 系统调用的原型如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/file.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">flock</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">int</span> operation)</span></span>;</span><br></pre></td></tr></table></figure></p><p>当 flock 执行成功时，会返回0；当出现错误时，会返回 -1，并设置相应的 errno 值。</p><p>在flock 原型中，参数 operation 可以使用 LOCK_SH 或 LOCK_EX 常量，分别对应共享锁和排他锁。这两个常量的定义在 file.h 中。与 flock 相关的常量定义如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Operations for the `flock' call.  */</span>                                          </span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LOCK_SH 1 <span class="comment">/* Shared lock.  */</span>                                            </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LOCK_EX 2   <span class="comment">/* Exclusive lock.  */</span>                                       </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LOCK_UN 8 <span class="comment">/* Unlock.  */</span>                                                 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Can be OR'd in to one of the above.  */</span>                                       </span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LOCK_NB 4 <span class="comment">/* Don't block when locking.  */</span></span></span><br></pre></td></tr></table></figure><p>当使用 LOCK_SH 共享锁时，多个进程都可以使用共享锁锁定同一个文件，从而实现多个进程对文件的并行读取。由此可见，LOCK_SH 共享锁类似于多线程读写锁 pthread_rwlock_t 类型中的读锁。当使用LOCK_EX 排他锁时，同一时刻只能有一个进程锁定成功，其余进行只能阻塞，这种行为与多线程读写锁中的写锁类似。</p><h2 id="阻塞与非阻塞"><a href="#阻塞与非阻塞" class="headerlink" title="阻塞与非阻塞"></a>阻塞与非阻塞</h2><p>flock 文件锁提供了阻塞和非阻塞两种使用方式。当处于阻塞模式时，如果当前进程无法成功获取到文件锁，那么进程就会一直阻塞等待，直到其他进程在对应文件上释放了锁，本进程能成功持有锁为止。在默认情况下，flock 提供是阻塞模式的文件锁。</p><p>在日常使用中，文件锁还会使用在另外一种场景下，即进程首先尝试对文件加锁，当加锁失败时，不希望进程阻塞，而是希望 flock 返回错误信息，进程进行错误处理后，继续进行下面的处理。在这种情形下就需要使用 flock 的非阻塞模式。把flock 的工作模式设置为非阻塞模式非常简单，只要将原有的 operation 参数改为锁的类型与 LOCK_NB 常量进行按位或操作即可，例如：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> ret = flock(open_fd, LOCK_SH | LOCK_NB);</span><br><span class="line"><span class="keyword">int</span> ret = flock(open_fd, LOCK_EX | LOCK_NB);</span><br></pre></td></tr></table></figure><p>在非阻塞模式下，加文件锁失败并不影响进程流程的执行，但要注意加入错误处理逻辑，在加锁失败时，不能对目标文件进行操作。</p><h2 id="flock-命令"><a href="#flock-命令" class="headerlink" title="flock 命令"></a>flock 命令</h2><p>除了多种语言提供 flock 系统调用或函数，linux shell 中也提供了 flock 命令。<br><a href="https://man7.org/linux/man-pages/man1/flock.1.html" target="_blank" rel="noopener">flock(1)</a></p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/25134841" target="_blank" rel="noopener">被遗忘的桃源——flock 文件锁</a></li><li><a href="https://man7.org/linux/man-pages/man1/flock.1.html" target="_blank" rel="noopener">flock(1)</a></li><li><a href="https://man7.org/linux/man-pages/man2/flock.2.html" target="_blank" rel="noopener">flock(2)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下flock 文件锁相关notes。
    
    </summary>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/categories/Concurrency/"/>
    
    
      <category term="Concurrency" scheme="http://liujunming.github.io/tags/Concurrency/"/>
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>深入理解eventfd_signal</title>
    <link href="http://liujunming.github.io/2024/08/03/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3eventfd-signal/"/>
    <id>http://liujunming.github.io/2024/08/03/深入理解eventfd-signal/</id>
    <published>2024-08-03T06:24:14.000Z</published>
    <updated>2024-08-03T07:07:21.257Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下eventfd_signal的实现。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">eventfd_signal</span><br><span class="line">└── eventfd_signal_mask</span><br><span class="line">    └── wake_up_locked_poll[__wake_up_locked_key]</span><br><span class="line">        └── __wake_up_common</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __wake_up_common(struct wait_queue_head *wq_head, <span class="keyword">unsigned</span> <span class="keyword">int</span> mode,</span><br><span class="line"><span class="keyword">int</span> nr_exclusive, <span class="keyword">int</span> wake_flags, <span class="keyword">void</span> *key,</span><br><span class="line"><span class="keyword">wait_queue_entry_t</span> *bookmark)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">wait_queue_entry_t</span> *curr, *next;</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">list_for_each_entry_safe_from(curr, next, &amp;wq_head-&gt;head, entry) &#123;</span><br><span class="line"><span class="keyword">unsigned</span> flags = curr-&gt;flags;</span><br><span class="line"><span class="keyword">int</span> ret;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (flags &amp; WQ_FLAG_BOOKMARK)</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">ret = curr-&gt;func(curr, mode, wake_flags, key);</span><br><span class="line">                ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由<code>__wake_up_common</code>的实现可知，最终<code>eventfd_signal</code>调用了<code>wait_queue_entry</code>的<code>func</code>回调。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * A single wait-queue entry structure:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">wait_queue_entry</span> &#123;</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span>flags;</span><br><span class="line"><span class="keyword">void</span>*<span class="keyword">private</span>;</span><br><span class="line"><span class="keyword">wait_queue_func_t</span>func;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">list_head</span><span class="title">entry</span>;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h2 id="vhost-poll-wakeup"><a href="#vhost-poll-wakeup" class="headerlink" title="vhost_poll_wakeup"></a>vhost_poll_wakeup</h2><p><a href="/2024/07/13/vhost-eventfd-pov/">源码解析:vhost ioeventfd与irqfd</a>中提到过<code>vhost_poll_wakeup</code>，那么这个函数又是如何与<code>eventfd_signal</code>关联起来的呢？</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vhost_poll_init</span><span class="params">(struct vhost_poll *poll, <span class="keyword">vhost_work_fn_t</span> fn,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">__poll_t</span> mask, struct vhost_dev *dev,</span></span></span><br><span class="line"><span class="function"><span class="params">     struct vhost_virtqueue *vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        ...</span><br><span class="line">        init_waitqueue_func_entry(&amp;poll-&gt;wait, vhost_poll_wakeup);</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span></span><br><span class="line">init_waitqueue_func_entry(struct wait_queue_entry *wq_entry, <span class="keyword">wait_queue_func_t</span> func)</span><br><span class="line">&#123;</span><br><span class="line">wq_entry-&gt;flags= <span class="number">0</span>;</span><br><span class="line">wq_entry-&gt;<span class="keyword">private</span>= <span class="literal">NULL</span>;</span><br><span class="line">wq_entry-&gt;func= func;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由上述代码片段可知，<code>vhost_poll_wakeup</code>被设置为了<code>wait_queue_entry</code>的<code>func</code>回调。</p><p>由此可知，<code>eventfd_signal</code>最终调用了<code>vhost_poll_wakeup</code>函数；因此，<code>vhost_poll_wakeup</code>函数运行上下文是vCPU线程(kvm调用了<code>eventfd_signal</code>，而kvm的运行上下文是vCPU线程)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ioeventfd_write</span><br><span class="line">└── eventfd_signal</span><br><span class="line">    └── eventfd_signal_mask</span><br><span class="line">        └── wake_up_locked_poll[__wake_up_locked_key]</span><br><span class="line">            └── __wake_up_common</span><br><span class="line">                └── vhost_poll_wakeup</span><br></pre></td></tr></table></figure><h2 id="select-poll-epoll-wait-queue-entry的func回调"><a href="#select-poll-epoll-wait-queue-entry的func回调" class="headerlink" title="select/poll/epoll wait_queue_entry的func回调"></a>select/poll/epoll <code>wait_queue_entry</code>的<code>func</code>回调</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// for select and poll</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __pollwait(struct file *filp, <span class="keyword">wait_queue_head_t</span> *wait_address,</span><br><span class="line">poll_table *p)</span><br><span class="line">&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">poll_wqueues</span> *<span class="title">pwq</span> = <span class="title">container_of</span>(<span class="title">p</span>, <span class="title">struct</span> <span class="title">poll_wqueues</span>, <span class="title">pt</span>);</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">poll_table_entry</span> *<span class="title">entry</span> = <span class="title">poll_get_entry</span>(<span class="title">pwq</span>);</span></span><br><span class="line"><span class="keyword">if</span> (!entry)</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">entry-&gt;filp = get_file(filp);</span><br><span class="line">entry-&gt;wait_address = wait_address;</span><br><span class="line">entry-&gt;key = p-&gt;_key;</span><br><span class="line">init_waitqueue_func_entry(&amp;entry-&gt;wait, pollwake);</span><br><span class="line">entry-&gt;wait.<span class="keyword">private</span> = pwq;</span><br><span class="line">add_wait_queue(wait_address, &amp;entry-&gt;wait);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于<code>select</code>和<code>poll</code>，<code>wait_queue_entry</code>的<code>func</code>回调是<code>pollwake</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// for epoll</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ep_ptable_queue_proc</span><span class="params">(struct file *file, <span class="keyword">wait_queue_head_t</span> *whead,</span></span></span><br><span class="line"><span class="function"><span class="params"> poll_table *pt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ep_pqueue</span> *<span class="title">epq</span> = <span class="title">container_of</span>(<span class="title">pt</span>, <span class="title">struct</span> <span class="title">ep_pqueue</span>, <span class="title">pt</span>);</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epitem</span> *<span class="title">epi</span> = <span class="title">epq</span>-&gt;<span class="title">epi</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eppoll_entry</span> *<span class="title">pwq</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (unlikely(!epi))<span class="comment">// an earlier allocation has failed</span></span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL);</span><br><span class="line"><span class="keyword">if</span> (unlikely(!pwq)) &#123;</span><br><span class="line">epq-&gt;epi = <span class="literal">NULL</span>;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">init_waitqueue_func_entry(&amp;pwq-&gt;wait, ep_poll_callback);</span><br><span class="line">pwq-&gt;whead = whead;</span><br><span class="line">pwq-&gt;base = epi;</span><br><span class="line"><span class="keyword">if</span> (epi-&gt;event.events &amp; EPOLLEXCLUSIVE)</span><br><span class="line">add_wait_queue_exclusive(whead, &amp;pwq-&gt;wait);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">add_wait_queue(whead, &amp;pwq-&gt;wait);</span><br><span class="line">pwq-&gt;next = epi-&gt;pwqlist;</span><br><span class="line">epi-&gt;pwqlist = pwq;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于<code>epoll</code>，<code>wait_queue_entry</code>的<code>func</code>回调是<code>ep_poll_callback</code>。</p><p>为了方便起见，本文只详细介绍下<code>pollwake</code>。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在等待队列(wait_queue_t)上回调函数(func)  </span></span><br><span class="line"><span class="comment">// 文件就绪后被调用，唤醒调用进程，其中key是文件提供的当前状态掩码  </span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">pollwake</span><span class="params">(<span class="keyword">wait_queue_t</span> *wait, <span class="keyword">unsigned</span> mode, <span class="keyword">int</span> sync, <span class="keyword">void</span> *key)</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">poll_table_entry</span> *<span class="title">entry</span>;</span>  </span><br><span class="line">    <span class="comment">// 取得文件对应的poll_table_entry  </span></span><br><span class="line">    entry = container_of(wait, struct poll_table_entry, wait);  </span><br><span class="line">    <span class="comment">// 过滤不关注的事件  </span></span><br><span class="line">    <span class="keyword">if</span> (key &amp;&amp; !((<span class="keyword">unsigned</span> <span class="keyword">long</span>)key &amp; entry-&gt;key)) &#123;  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">// 唤醒  </span></span><br><span class="line">    <span class="keyword">return</span> __pollwake(wait, mode, sync, key);  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __pollwake(<span class="keyword">wait_queue_t</span> *wait, <span class="keyword">unsigned</span> mode, <span class="keyword">int</span> sync, <span class="keyword">void</span> *key)  </span><br><span class="line">&#123;  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">poll_wqueues</span> *<span class="title">pwq</span> = <span class="title">wait</span>-&gt;<span class="title">private</span>;</span>  </span><br><span class="line">    <span class="comment">// 将调用进程 pwq-&gt;polling_task 关联到 dummy_wait  </span></span><br><span class="line">    DECLARE_WAITQUEUE(dummy_wait, pwq-&gt;polling_task);  </span><br><span class="line">    smp_wmb();  </span><br><span class="line">    pwq-&gt;triggered = <span class="number">1</span>;<span class="comment">// 标记为已触发  </span></span><br><span class="line">    <span class="comment">// 唤醒调用进程  </span></span><br><span class="line">    <span class="keyword">return</span> default_wake_function(&amp;dummy_wait, mode, sync, key);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// 默认的唤醒函数,poll/select 设置的回调函数会调用此函数唤醒  </span></span><br><span class="line"><span class="comment">// 直接唤醒等待队列上的线程,即将线程移到运行队列(rq)  </span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">default_wake_function</span><span class="params">(<span class="keyword">wait_queue_t</span> *curr, <span class="keyword">unsigned</span> mode, <span class="keyword">int</span> wake_flags,  </span></span></span><br><span class="line"><span class="function"><span class="params">                          <span class="keyword">void</span> *key)</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="comment">// 这个函数比较复杂, 这里就不具体分析了  </span></span><br><span class="line">    <span class="keyword">return</span> try_to_wake_up(curr-&gt;<span class="keyword">private</span>, mode, wake_flags);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/398699496" target="_blank" rel="noopener">linux 内核poll/select/epoll实现剖析（经典）-上</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下eventfd_signal的实现。
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>深入理解virtio kick操作</title>
    <link href="http://liujunming.github.io/2024/07/28/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3virtio-kick%E6%93%8D%E4%BD%9C/"/>
    <id>http://liujunming.github.io/2024/07/28/深入理解virtio-kick操作/</id>
    <published>2024-07-28T02:09:42.000Z</published>
    <updated>2024-07-28T04:17:40.507Z</updated>
    
    <content type="html"><![CDATA[<p>本文将结合virtio spec与linux 源码，深入解析下virtio中的kick操作。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>本文将详细的阐述virtio中的kick操作。根据kick操作的发展历史，按照如下顺序去介绍:</p><ol><li>legacy device kick</li><li>modern device kick</li><li>VIRTIO_F_NOTIFICATION_DATA feature的kick</li></ol><h2 id="legacy-device"><a href="#legacy-device" class="headerlink" title="legacy device"></a>legacy device</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* the notify function used when creating a virt queue */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">vp_notify</span><span class="params">(struct virtqueue *vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/* we write the queue's selector into the notification register to</span></span><br><span class="line"><span class="comment"> * signal the other end */</span></span><br><span class="line">iowrite16(vq-&gt;index, (<span class="keyword">void</span> __iomem *)vq-&gt;priv);</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// legacy device</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> struct virtqueue *<span class="title">setup_vq</span><span class="params">(struct virtio_pci_device *vp_dev,</span></span></span><br><span class="line"><span class="function"><span class="params">  struct virtio_pci_vq_info *info,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">unsigned</span> <span class="keyword">int</span> index,</span></span></span><br><span class="line">  void (*callback)(struct virtqueue *vq),</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span> *name,</span><br><span class="line">  <span class="keyword">bool</span> ctx,</span><br><span class="line">  u16 msix_vec)</span><br><span class="line">&#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="comment">/* create the vring */</span></span><br><span class="line">vq = vring_create_virtqueue(index, num,</span><br><span class="line">    VIRTIO_PCI_VRING_ALIGN, &amp;vp_dev-&gt;vdev,</span><br><span class="line">    <span class="literal">true</span>, <span class="literal">false</span>, ctx,</span><br><span class="line">    vp_notify, callback, name);</span><br><span class="line">        ...</span><br><span class="line">        vq-&gt;priv = (<span class="keyword">void</span> __force *)vp_dev-&gt;ldev.ioaddr + VIRTIO_PCI_QUEUE_NOTIFY;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/2024/07/033.jpg" alt></p><p>kick寄存器位于bar0中的<code>VIRTIO_PCI_QUEUE_NOTIFY</code>位置；不同vq使用同一个kick寄存器地址，往kick寄存器写入vq的index，告诉virtio后端要处理哪个vq。</p><h2 id="modern-device"><a href="#modern-device" class="headerlink" title="modern device"></a>modern device</h2><p><img src="/images/2024/07/037.jpg" alt></p><p>在设备实现中，一般会将<code>queue_notify_off</code>设置为vq index；也就是说，<code>vp_modern_get_queue_notify_off</code>的返回值，一般会与输入变量index相同。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * vp_modern_get_queue_notify_off - get notification offset for a virtqueue</span></span><br><span class="line"><span class="comment"> * @mdev: the modern virtio-pci device</span></span><br><span class="line"><span class="comment"> * @index: the queue index</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Returns the notification offset for a virtqueue</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> u16 <span class="title">vp_modern_get_queue_notify_off</span><span class="params">(struct virtio_pci_modern_device *mdev,</span></span></span><br><span class="line"><span class="function"><span class="params">  u16 index)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">vp_iowrite16(index, &amp;mdev-&gt;common-&gt;queue_select);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> vp_ioread16(&amp;mdev-&gt;common-&gt;queue_notify_off);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><img src="/images/2024/07/038.jpg" alt></p><p><img src="/images/2024/07/036.jpg" alt></p><ul><li>当notify_off_multiplier不为0时，不同vq使用不同的kick寄存器地址，往kick寄存器写入vq的index，告诉virtio后端要处理哪个vq</li><li>当notify_off_multiplier为0时，不同vq使用相同的kick寄存器地址，往kick寄存器写入vq的index，告诉virtio后端要处理哪个vq</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// modern device</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> struct virtqueue *<span class="title">setup_vq</span><span class="params">(struct virtio_pci_device *vp_dev,</span></span></span><br><span class="line"><span class="function"><span class="params">  struct virtio_pci_vq_info *info,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">unsigned</span> <span class="keyword">int</span> index,</span></span></span><br><span class="line">  void (*callback)(struct virtqueue *vq),</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span> *name,</span><br><span class="line">  <span class="keyword">bool</span> ctx,</span><br><span class="line">  u16 msix_vec)</span><br><span class="line">&#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="comment">/* create the vring */</span></span><br><span class="line">vq = vring_create_virtqueue(index, num,</span><br><span class="line">    SMP_CACHE_BYTES, &amp;vp_dev-&gt;vdev,</span><br><span class="line">    <span class="literal">true</span>, <span class="literal">true</span>, ctx,</span><br><span class="line">    notify, callback, name);</span><br><span class="line">        ...</span><br><span class="line">        vq-&gt;priv = (<span class="keyword">void</span> __force *)vp_modern_map_vq_notify(mdev, index, <span class="literal">NULL</span>);</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * vp_modern_map_vq_notify - map notification area for a</span></span><br><span class="line"><span class="comment"> * specific virtqueue</span></span><br><span class="line"><span class="comment"> * @mdev: the modern virtio-pci device</span></span><br><span class="line"><span class="comment"> * @index: the queue index</span></span><br><span class="line"><span class="comment"> * @pa: the pointer to the physical address of the nofity area</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Returns the address of the notification area</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> __<span class="function">iomem *<span class="title">vp_modern_map_vq_notify</span><span class="params">(struct virtio_pci_modern_device *mdev,</span></span></span><br><span class="line"><span class="function"><span class="params">      u16 index, <span class="keyword">resource_size_t</span> *pa)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">u16 off = vp_modern_get_queue_notify_off(mdev, index);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (mdev-&gt;notify_base) &#123;</span><br><span class="line"><span class="comment">/* offset should not wrap */</span></span><br><span class="line"><span class="keyword">if</span> ((u64)off * mdev-&gt;notify_offset_multiplier + <span class="number">2</span></span><br><span class="line">&gt; mdev-&gt;notify_len) &#123;</span><br><span class="line">dev_warn(&amp;mdev-&gt;pci_dev-&gt;dev,</span><br><span class="line"> <span class="string">"bad notification offset %u (x %u) "</span></span><br><span class="line"> <span class="string">"for queue %u &gt; %zd"</span>,</span><br><span class="line"> off, mdev-&gt;notify_offset_multiplier,</span><br><span class="line"> index, mdev-&gt;notify_len);</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (pa)</span><br><span class="line">*pa = mdev-&gt;notify_pa +</span><br><span class="line">      off * mdev-&gt;notify_offset_multiplier;</span><br><span class="line"><span class="keyword">return</span> mdev-&gt;notify_base + off * mdev-&gt;notify_offset_multiplier;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="VIRTIO-F-NOTIFICATION-DATA-feature"><a href="#VIRTIO-F-NOTIFICATION-DATA-feature" class="headerlink" title="VIRTIO_F_NOTIFICATION_DATA feature"></a>VIRTIO_F_NOTIFICATION_DATA feature</h2><p><img src="/images/2024/07/034.jpg" alt></p><p>值得注意的是，对于split vq，desc table的最大size为2^16；对于packed vq，desc table的最大size为2^15；</p><p><img src="/images/2024/07/031.jpg" alt></p><p><img src="/images/2024/07/032.jpg" alt></p><p>在kick寄存器中，不止存放了vq index:</p><ul><li>对于split vq，kick寄存器中还存放了avail_idx</li><li>对于packed vq，kick寄存器中还存放了avail_idx(为了表述的方便，严格来说，packed vq已经没有了avail ring，也就不存在avail_idx了)与wrap counter</li></ul><p><img src="/images/2024/07/035.jpg" alt></p><p><code>vq_notif_config_data</code>在一般情况下，就是vq index。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// modern device</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> struct virtqueue *<span class="title">setup_vq</span><span class="params">(struct virtio_pci_device *vp_dev,</span></span></span><br><span class="line"><span class="function"><span class="params">  struct virtio_pci_vq_info *info,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">unsigned</span> <span class="keyword">int</span> index,</span></span></span><br><span class="line">  void (*callback)(struct virtqueue *vq),</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span> *name,</span><br><span class="line">  <span class="keyword">bool</span> ctx,</span><br><span class="line">  u16 msix_vec)</span><br><span class="line">&#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">if</span> (__virtio_test_bit(&amp;vp_dev-&gt;vdev, VIRTIO_F_NOTIFICATION_DATA))</span><br><span class="line">notify = vp_notify_with_data;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">notify = vp_notify;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">bool</span> <span class="title">vp_notify_with_data</span><span class="params">(struct virtqueue *vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">u32 data = vring_notification_data(vq);</span><br><span class="line"></span><br><span class="line">iowrite32(data, (<span class="keyword">void</span> __iomem *)vq-&gt;priv);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">u32 <span class="title">vring_notification_data</span><span class="params">(struct virtqueue *_vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_virtqueue</span> *<span class="title">vq</span> = <span class="title">to_vvq</span>(_<span class="title">vq</span>);</span></span><br><span class="line">u16 next;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vq-&gt;packed_ring)</span><br><span class="line">next = (vq-&gt;packed.next_avail_idx &amp;</span><br><span class="line">~(-(<span class="number">1</span> &lt;&lt; VRING_PACKED_EVENT_F_WRAP_CTR))) |</span><br><span class="line">vq-&gt;packed.avail_wrap_counter &lt;&lt;</span><br><span class="line">VRING_PACKED_EVENT_F_WRAP_CTR;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">next = vq-&gt;split.avail_idx_shadow;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> next &lt;&lt; <span class="number">16</span> | _vq-&gt;index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li>virtio 0.9.5 spec</li><li>virtio 1.3 spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将结合virtio spec与linux 源码，深入解析下virtio中的kick操作。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>(转)用户态GPU池化技术</title>
    <link href="http://liujunming.github.io/2024/07/27/%E7%94%A8%E6%88%B7%E6%80%81GPU%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF/"/>
    <id>http://liujunming.github.io/2024/07/27/用户态GPU池化技术/</id>
    <published>2024-07-27T03:29:50.000Z</published>
    <updated>2024-07-27T11:34:15.923Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自: <a href="https://mp.weixin.qq.com/s/oD99Qf9H0nLKABN1y5rJ-A" target="_blank" rel="noopener">用户态GPU池化技术</a>。主要mark下内核态虚拟化和用户态虚拟化两类方案。<a id="more"></a></p><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>目前有若干种软件技术方案能实现GPU虚拟化，这些方案可以分为<strong>内核态虚拟化和用户态虚拟化两类</strong>。本文主要论述这两种方案的差异。</p><h2 id="2-内核态与用户态解析"><a href="#2-内核态与用户态解析" class="headerlink" title="2. 内核态与用户态解析"></a>2. 内核态与用户态解析</h2><p>以英伟达的GPU为例，应用到硬件从上至下分为用户态、内核态、GPU硬件三个层次(见下图:CUDA软件栈)。</p><p><img src="/images/2024/07/026.webp" alt></p><h3 id="2-1-用户态层"><a href="#2-1-用户态层" class="headerlink" title="2.1 用户态层"></a>2.1 用户态层</h3><p>用户态是应用程序运行的环境。各种使用英伟达GPU的应用程序，比如人工智能计算类的应用，2D/3D图形渲染类的应用，都运行在用户态。为了便利编程以及安全因素，英伟达提供了用户态的运行库CUDA(Compute Unified Device Architecture)作为GPU并行计算的编程接口（类似的接口也包括由社区共同制订的OpenGL、Vulkan接口等），应用程序可以使用CUDA API来编写并行计算任务，并通过调用CUDA API与GPU用户态驱动进行通信。GPU用户态驱动再通过ioctl、mmap、read、write接口直接和GPU的内核态驱动进行交互。</p><h3 id="2-2-内核态层"><a href="#2-2-内核态层" class="headerlink" title="2.2 内核态层"></a>2.2 内核态层</h3><p>该层主要运行的是GPU的内核态驱动程序，它与操作系统内核紧密集成，受到操作系统以及CPU硬件的特殊保护。内核态驱动可以执行特权指令，提供对硬件的访问和操作接口，并对GPU硬件进行底层控制。出于系统安全考虑，用户态的代码只能通过操作系统预先定义好的标准接口(Linux下有例如ioctl，mmap，read，write 等少量接口)，调用内核态的代码。通过这些接口被调用的内核态代码一般是预先安装好的设备的内核态驱动。这样保证内核态和用户态的安全隔离，防止不安全的用户态代码破坏整个计算机系统。GPU的内核态驱动通过PCIe接口（也可能是其他硬件接口）以TLP报文的形式跟硬件进行通信。</p><p>特别的，包括英伟达在内的各类AI芯片产品的内核态和用户态之间的接口定义并不包含在例如CUDA、OpenGL、Vulkan等协议标准里面，他们也未曾向行业公开这一层的接口定义。因此各类行业应用也不会基于这一层的接口进行编程。</p><h2 id="3-两种虚拟化技术难度解析"><a href="#3-两种虚拟化技术难度解析" class="headerlink" title="3. 两种虚拟化技术难度解析"></a>3. 两种虚拟化技术难度解析</h2><p>从技术可能性的角度来看，用户态与内核态各有相应的接口可以实现GPU虚拟化或者GPU池化：<strong>用户态</strong>的CUDA、OpenGL、Vulkan等应用运行时接口；<strong>内核态</strong>暴露的 ioctl、read、write等设备驱动接口。</p><h3 id="3-1-用户态虚拟化"><a href="#3-1-用户态虚拟化" class="headerlink" title="3.1 用户态虚拟化"></a>3.1 用户态虚拟化</h3><p><img src="/images/2024/07/027.webp" alt></p><p>利用CUDA、OpenGL、Vulkan等标准接口，对API进行拦截和转发，对被拦截的函数进行解析，然后调用硬件厂商提供的用户态库中的相应函数（见上图）。拦截CUDA等用户态接口不需要在OS内核层进行设备文件的插入，因为这些接口的使用方式是操作系统在运行可执行文件的时候（例如Linux下的elf二进制），由操作系统的加载器自动在系统中按照固定的规则来寻找其依赖的外部接口，学术名称做符号（symbol）。那么根据操作系统寻找依赖的规则，很容易可以通过替换symbol的来源，使得当可执行文件发生例如CUDA接口调用的时候，调用的不是英伟达的闭源用户态软件提供的接口，而是一个经过修改后的同名接口，从而拦截到例如CUDA接口的调用。</p><p>经过API拦截之后，用户态虚拟化方案还可以利用RPC的方式进行远程API Remoting（见上图），即CPU主机可以通过网络调用GPU主机的GPU，实现GPU的远程调用。如此一来，多个GPU服务器可以组成资源池，供多个AI业务任意调用，达到实现GPU池化的目的。用户态虚拟化是一种软件的实现方案。目前业内已经成型的产品有：趋动科技的OrionX GPU池化产品，VMware的Bitfusion产品。这类技术方案拥有几个优点：</p><ol><li>CUDA、OpenGL、Vulkan等接口都是公开的标准化接口，具有开放性和接口稳定性。所以基于这些接口的实现方案具有很好的兼容性和可持续性。</li><li>因为该方案运行在用户态，因此可以规避内核态代码过于复杂容易引入安全问题的工程实践，可以在用户态通过复杂的网络协议栈和操作系统支持来实现及优化远程GPU的能力，从而高效率地支持GPU池化。</li><li>由于该方案工作在用户态，从部署形态上对用户环境的侵入性最小，也最安全，即使发生故障也可以迅速被操作系统隔离，而通过一些软件工程的设计可以有很强的自恢复能力。</li></ol><p>当然，这类方案也有缺点：相比于内核态接口，用户态API接口支持更复杂的参数和功能，因此用户态API接口的数量比内核态接口的数量要高几个数量级。这导致在用户态层实现GPU虚拟化和GPU池化的研发工作量要比在内核态实现要大得多。</p><h3 id="3-2-内核态虚拟化"><a href="#3-2-内核态虚拟化" class="headerlink" title="3.2 内核态虚拟化"></a>3.2 内核态虚拟化</h3><p><img src="/images/2024/07/028.webp" alt></p><p>跟上述用户态拦截API类似的，第三方厂商所做的内核态虚拟化方案通过拦截ioctl、mmap、read、write等这类内核态与用户态之间的接口来实现GPU虚拟化。<strong>这类方案的关键点在于需要在操作系统内核里面增加一个内核拦截模块，并且在操作系统上创建一些设备文件来模拟正常的GPU设备文件</strong>。例如，英伟达GPU在Linux上的设备文件有/dev/nvidiactl、 /dev/nvidia0等多个文件。因此，在使用虚拟化的GPU时，把虚拟化出来的设备文件mount到业务容器内部，同时通过挂载重命名的机制伪装成英伟达的同名设备文件名，让应用程序访问。这样在容器内部的应用程序通过CUDA去访问设备文件的时候，仍然会去打开例如/dev/nvidiactl 和 /dev/nvidia0这样的设备文件，该访问就会被转发到模拟的设备文件，并向内核态发送例如ioctl这样的接口调用，进而被内核拦截模块截获并进行解析。目前国内的qGPU和cGPU方案都是工作在这一层。这类技术方案的优点是：</p><ol><li>有较好的灵活性，而且不依赖GPU硬件，可以在数据中心级和消费级的GPU上使用。</li><li>在GPU共享的同时，具备不错的隔离能力。</li><li>由于只支持运行在容器环境中，研发工作量相比用户态方案要小得多。</li></ol><p>这类方案由于工作在内核态，缺点也是显而易见的：</p><ol><li>需要在内核态层插入文件，对系统的侵入性大，容易引入安全隐患。</li><li>由于英伟达GPU内核态驱动的ioctl等接口以及用户态模块都是闭源的，接口也不开放，因此只有英伟达自己可以在这层支持所有的GPU虚拟化能力，其他第三方厂商只能通过一定程度的逆向工程来实现对这些接口的解析。这种行为存在着极大的法律风险和不确定性，可持续性远低于用户态方案。</li><li>第三方厂商由于缺少完整的接口细节，目前只能通过接口“规避”的方式来支持。所谓“规避”，简单来说就是只解析必要的少数几个接口，其他的不劫持直接放过。为了方便实现“规避”效果，这类方案目前都只能支持基于容器虚拟化的环境（因为很容易实现），无法支持非容器化环境以及KVM虚拟化环境，更加无法跨越操作系统支持GPU池化最核心的远程GPU调用，因此这类方案不是完整的GPU池化方案。</li></ol><h3 id="3-3-接口解析"><a href="#3-3-接口解析" class="headerlink" title="3.3 接口解析"></a>3.3 接口解析</h3><p>上述两种虚拟化方案在经过接口拦截之后，就可以在当前的接口调用中被激活，接下来就是对该接口进行解析。不管是 ioctl 接口还是 CUDA接口，从计算机设计上，都可以表达为interface_name(paramerA, parameterB, …)这样的形式。也就是接口名称，接口参数（返回值也是一种参数形式）。而不管基于哪一层接口的拦截，这里的解析又分为两种：</p><p><img src="/images/2024/07/029.webp" alt></p><ol><li><p><strong>同一个进程空间的接口解析</strong>（见上图）：在现代操作系统中，不管在用户态还是内核态，代码都执行在由CPU硬件 + 操作系统维护的一个进程空间里面，在一个进程空间里面有统一的进程上下文（context），并且所有的资源在进程空间内都是共享的，视图是统一的，包括访存地址空间(address space)，也包括GPU设备上的资源。这个现代操作系统的设计可以为同一个进程空间的接口解析带来极大的便利。因为对于一个接口interface_name(paramerA, parameterB, …)，即使存在不公开含义的参数，例如parameterB是不公开的，但是利用一个进程空间内所有的资源都是共享且视图统一的这个特点，只要确定该部分内容不需要被GPU虚拟化模拟执行所需要，那么虚拟化软件可以不需要对其进行解析，在截获之后，直接透传给英伟达自己的闭源模块就可以。实际上，只有少量接口，少量参数会被需要在一个进程内被解析并且模拟执行，因此选择这个技术路线可以“规避”掉绝大多数接口、参数的解析工作。具体以针对英伟达的GPU为例，只有非常少的接口、参数需要被真正解析并模拟执行。一些产品之所以能在非公开的内核接口层实现GPU虚拟化，是利用了同一个操作系统的特点，基于少量接口信息，来达到GPU虚拟化的目的。但是这样的技术路线也有一个非常明显的限制，就是只能在同一个进程空间内进行接口的拦截、解析和执行。因此这种技术路线从原理上就无法支持跨OS内核的KVM虚拟化，更无法跨越物理节点做到远程调用GPU。</p></li><li><p><strong>不同进程空间的接口解析</strong>（见上图）：当GPU应用所在的操作系统和管理物理GPU所在的操作系统是两个不同的操作系统的时候，要达到GPU虚拟化、GPU池化的目的，就需要跨进程对选定的GPU接口层进行跨进程的接口解析。典型的场景如 KVM虚拟机，还有跨物理节点调用GPU。由于应用程序和GPU管理软件栈（例如GPU驱动）已经不在一个操作系统的管理下，因此资源就不再是共享的了，视图也不再是统一的了。例如，同样的一个虚拟地址(virtual address)在不同的进程空间代表的很可能是不一样的内容。所以对于所有接口interface_name(paramerA, parameterB, …)，都要进行完善的解析、处理，并通过例如网络的方式跨越操作系统进行传送。以英伟达的 CUDA 为例有数万个接口，需要对每一个接口都进行跨进程空间的接口解析，然后进行行为模拟。因此，在不公开的接口层进行跨进程空间的接口解析，原理上是行不通的。</p></li></ol><p>经过接口解析之后，则需要向GPU应用提供一个模拟的GPU执行环境，这个模拟的动作是由GPU虚拟化和GPU池化的软件来完成的。不同软件提供的模拟的能力是有差异的，但是其基础的能力，都是要保持对上层应用的透明性，使得应用不需要改动实现，不需要重新编译。</p><h3 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 总结</h3><p>对于GPU虚拟化和资源池化，由于在接口层的的选择上有两个分支，在接口解析上也有两个分支，所以排列组合起来有4种可能，下面对4种方式做一个对比。</p><p><img src="/images/2024/07/030.webp" alt></p><p>通过对比这4种可能的方式，我们做个总结：</p><ol><li>内核态方案仅能在同一个进程空间工作，无法跨机，因此无法实现GPU池化。</li><li>内核态方案要在同一个进程空间实现GPU虚拟化是相对简单的。</li><li><strong>只有用户态方案可以实现跨不同进程空间工作，可以跨机，因此可以实现GPU池化</strong>。</li><li><strong>用户态方案要想跨不同进程空间实现GPU池化，有大量接口需要解析，难度与门槛很高</strong>。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自: &lt;a href=&quot;https://mp.weixin.qq.com/s/oD99Qf9H0nLKABN1y5rJ-A&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;用户态GPU池化技术&lt;/a&gt;。主要mark下内核态虚拟化和用户态虚拟化两类方案。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>(转)GPU虚拟化调度方案</title>
    <link href="http://liujunming.github.io/2024/07/27/%E8%BD%AC-GPU%E8%99%9A%E6%8B%9F%E5%8C%96%E8%B0%83%E5%BA%A6%E6%96%B9%E6%A1%88/"/>
    <id>http://liujunming.github.io/2024/07/27/转-GPU虚拟化调度方案/</id>
    <published>2024-07-27T03:16:38.000Z</published>
    <updated>2024-07-27T06:21:48.677Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自: <a href="https://mp.weixin.qq.com/s/wIvCnlfndPp05fMqPfX8gA" target="_blank" rel="noopener">大模型时代的基础架构长什么样？</a><a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>无论是基于虚拟机的PCI-E设备直通，还是基于Kubernetes的Device Plugin，对GPU调度的颗粒度都是整颗GPU芯片，这样，是不能将一颗GPU芯片共享给多个应用使用的。然而，在实践中，将GPU共享给多个应用使用是很常见的需求，特别是对于推理场景，往往不需要一直使用整颗GPU芯片的算力资源。所以，将昂贵的GPU分享给其他应用的能力就变得非常有价值。</p><p>因此，无论是GPU厂商、云计算厂商，还是开源社区，都推出了一系列GPU虚拟化方案。</p><p>Nvidia（英伟达）作为GPU领域的Top供应商，从2010年起就推出了GPU虚拟化的方案，其大致的发展路线图如图8-1所示。</p><p><img src="/images/2024/07/021.webp" alt></p><p>在图8-1中，GPU虚拟化的发展路线分为三个阶段：以vCUDA为代表的API Remoting阶段、以GRID vGPU为代表的Driver Virtualization（驱动虚拟化）阶段，以及以MIG为代表的Hardware Virtualization（硬件虚拟化）阶段。</p><h2 id="API-Remoting与vCUDA"><a href="#API-Remoting与vCUDA" class="headerlink" title="API Remoting与vCUDA"></a>API Remoting与vCUDA</h2><p>vCUDA技术出现于2010年前后，其实现思路是：在虚拟机中提供一个物理GPU的逻辑映像——虚拟GPU，在用户态拦截CUDA API，在虚拟GPU中重定向到真正的物理GPU上执行计算。同时，在宿主机上基于原生的CUDA运行时库和GPU驱动，运行vCUDA服务端，接管虚拟GPU拦截的CUDA API，同时进行计算任务的调度。</p><p>vCUDA的工作原理如图8-2所示。</p><p><img src="/images/2024/07/022.webp" alt></p><p>从图8-2可以看出，虚拟机的CUDA运行时库被替换为vCUDA，其作用就是拦截来自CUDA App的所有CUDA API调用。vCUDA运行时库会在内核中调用vGPU驱动（或称之为“客户端驱动”），vGPU驱动实际的作用就是通过虚拟机到宿主机的VMRPC（Virtual Machine Remote Procedure Call）通道，将CUDA调用发送到宿主机。宿主机的vCUDA Stub（管理端）接收到CUDA调用后，调用宿主机上真正的CUDA运行时库和物理GPU驱动，完成GPU运算。</p><p>在客户端驱动处理API之前，还需要向管理端申请GPU资源。每一个独立的调用过程都必须向宿主机的管理端申请GPU资源，从而实现GPU资源和任务的实时调度。</p><p>显然，vCUDA是一种时间片调度的虚拟化技术，也就是“时分复用”。此种实现对于用户的应用而言是透明的，无须针对虚拟GPU做任何修改，而且也可以实现非常灵活的调度，单GPU能服务的虚拟机数量不受限制。但缺点也是显而易见的：CUDA API只是GPU运算使用的API中的一种，业界还有DirectX/OpenGL等其他API标准，而且同一套API又有多个不同的版本（如DirectX 9和DirectX 11等），兼容性非常复杂。</p><p>Nvidia如何在下一代GPU虚拟化技术中解决这一问题呢？</p><h2 id="GRID-vGPU"><a href="#GRID-vGPU" class="headerlink" title="GRID vGPU"></a>GRID vGPU</h2><p>Nvidia在2014年前后推出了vCUDA的替代品——GRID vGPU。</p><p>GRID vGPU是一种GPU分片虚拟化方案，也可以被认为是一种半虚拟化方案。“分片”实际上还是采用“时分复用”。</p><p>GRID vGPU的实现原理如图8-3所示。</p><p><img src="/images/2024/07/023.webp" alt></p><p>在图8-3中，VM中的CUDA应用调用的是原生的CUDA运行时库，但GuestOS（虚拟机操作系统）中的GPU驱动并不是访问GPU物理的BAR（Base Address Register），而是访问虚拟的BAR。</p><p>在进行计算工作时，GuestOS的GPU驱动会将保存待计算Workload的GPA通过MMIO CSR（Configuration and Status Register）传递给HostOS中的GPU驱动，从而让HostOS的GPU驱动拿到GPA并将其转换为HPA，写入物理GPU的MMIO CSR，也就是启动物理GPU的计算任务。</p><p>物理GPU在计算完成后，会发送一个MSI中断到HostOS的驱动，HostOS的驱动根据Workload反查提交这个Workload的vGPU实例，发送中断到对应的VM中。VM的GuestOS处理该中断，直到完成计算Workload，上报CUDA和应用，vGPU计算过程处理完毕。</p><p>vGPU方案也被称为MPT（Mediated Pass Through，受控直通）方案。该方案的思路是：将一些敏感资源和关键资源（如PCI-E配置空间和MMIO CSR）虚拟化，而GPU显存的MMIO则进行直通，并在HostOS上增加一个能够感知虚拟化的驱动程序，以进行硬件资源的调度。这样，在VM中就可以看出一个PCI-E设备，并安装原生的GPU驱动。</p><p>该方案的优势在于，继承了vCUDA的调度灵活性，并且不需要替换原有的CUDA API库，解决了上一代vCUDA的兼容性问题。该方案的缺陷在于，宿主机上的驱动为硬件厂商所控制，而该物理GPU驱动是实现整个调度能力的核心。也就是说，该方案存在着对厂商软件的依赖，厂商软件可以基于这个收取高额的软件授权费用。</p><h2 id="Nvidia-MIG"><a href="#Nvidia-MIG" class="headerlink" title="Nvidia MIG"></a>Nvidia MIG</h2><p>在业界的推动下，Nvidia又在2020年前后更新了一代GPU虚拟化方案MIG（Multi-Instance GPU）。MIG的实现原理如图8-4所示。</p><p><img src="/images/2024/07/024.webp" alt></p><p>我们对比图8-3和图8-4会发现，MIG与vGPU的相同点在于，VM上的CUDA运行时库和GPU驱动均为原生版本。但其差异在于，MIG上看到的GPU设备实际上是真实物理硬件的一部分，其BAR和MMIO CSR的背后都是真实的物理硬件。</p><p>这是Nvidia在Nvidia A100和Nvidia H100等高端GPU上引入的硬件能力，它不仅能将一个GPU芯片虚拟出7个实例，提供给不同的VM使用，还可以为虚拟化的实例分配指定的GPU算力和GPU内存，这实际上是一种空分复用，也就是硬件资源隔离（Hardware Partition）。</p><p>硬件资源隔离所带来的一个重要价值就是硬件故障隔离。在前两种方案中，从本质上说，GPU侧并没有实现真正的故障隔离，一旦某个提交给Nvidia的CUDA作业程序越界访问了GPU显存，其他VM的CUDA应用就都有可能在抛出的异常中被中止。而MIG提供了硬件安全机制，不同MIG实例中的程序不会相互影响，从而从根源上解决了这一问题。</p><p>SR-IOV is a pre-requisite for running vGPUs on MIG，所以MMIO BAR是直通的，无需trap(而GRID vGPU需要trap MMIO BAR)。</p><p><img src="/images/2024/07/025.jpg" alt></p><p>MIG看起来是一个完美的方案，但实际上并非如此。</p><p>首先，MIG只在高端的训练GPU上才得到了支持，但实际上推理场景需要使用GPU虚拟化技术来实现多应用共享GPU的可能性更大；其次，MIG支持的实例数受硬件设计限制，目前只能支持7个GPU实例；最后，MIG只支持CUDA计算，对于渲染等其他场景不支持。</p><p>因此，工程师们也构思了更多的方案，特别是云计算厂商也推出了一系列基于容器的GPU调度方案。</p><hr><p>参考资料:</p><ol><li><a href="https://cloud-atlas.readthedocs.io/zh-cn/latest/kvm/iommu/mig/intro_mig.html" target="_blank" rel="noopener">NVIDIA Multi-Instance GPU(MIG) 技术简介¶</a></li><li><a href="https://blogs.vmware.com/apps/2020/09/vsphere-7-0-u1-with-multi-instance-gpus-mig-on-the-nvidia-a100-for-machine-learning-applications-part-2-profiles-and-setup.html" target="_blank" rel="noopener">vSphere 7 with Multi-Instance GPUs (MIG) on the NVIDIA A100 for Machine Learning Applications – Part 2 : Profiles and Setup</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自: &lt;a href=&quot;https://mp.weixin.qq.com/s/wIvCnlfndPp05fMqPfX8gA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;大模型时代的基础架构长什么样？&lt;/a&gt;
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Smart Data Accelerator Interface (SDXI)</title>
    <link href="http://liujunming.github.io/2024/07/21/Notes-about-Smart-Data-Accelerator-Interface-SDXI/"/>
    <id>http://liujunming.github.io/2024/07/21/Notes-about-Smart-Data-Accelerator-Interface-SDXI/</id>
    <published>2024-07-21T13:09:23.000Z</published>
    <updated>2024-07-21T13:55:07.863Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Smart Data Accelerator Interface (SDXI)相关notes。本文只是简单介绍下SDXI技术，不涉及技术细节。<a id="more"></a></p><h2 id="Motivaion"><a href="#Motivaion" class="headerlink" title="Motivaion"></a>Motivaion</h2><p><img src="/images/2024/07/016.jpg" alt></p><p><img src="/images/2024/07/017.jpg" alt></p><p><img src="/images/2024/07/018.jpg" alt></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2024/07/019.jpg" alt></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p><img src="/images/2024/07/020.jpg" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>(个人理解)<br><a href="/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/">DSA</a>是当前Intel最新的data copy accelerator；SDXI是各大厂商(不包括Intel)一起定义的Memory to Memory Data Movement and Acceleration Interface标准，个人感觉SDXI的定位可能就是与DSA相竞争的。</p><p><img src="/images/2024/07/015.jpg" alt></p><p>SDXI是否能成为业界标准呢？ecosystem是成功的关键，且待时间给出答案吧。</p><hr><p>参考资料:</p><ol><li><a href="https://www.snia.org/sdxi" target="_blank" rel="noopener">Smart Data Accelerator Interface (SDXI) Specification</a></li><li><a href="https://www.snia.org/educational-library/introducing-sdxi-smart-data-acceleration-interface-new-twg-standardize-memory" target="_blank" rel="noopener">Introducing SDXI (Smart Data Acceleration Interface)</a></li><li><a href="https://www.snia.org/educational-library/smart-data-accelerator-interface-use-cases-futures-and-proof-points-2024-0" target="_blank" rel="noopener">Smart Data Accelerator Interface: Use Cases, Futures, and Proof Points</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Smart Data Accelerator Interface (SDXI)相关notes。本文只是简单介绍下SDXI技术，不涉及技术细节。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Mellanox SF technology</title>
    <link href="http://liujunming.github.io/2024/07/20/Notes-about-Mellanox-sub-functions-technology/"/>
    <id>http://liujunming.github.io/2024/07/20/Notes-about-Mellanox-sub-functions-technology/</id>
    <published>2024-07-20T10:02:45.000Z</published>
    <updated>2024-07-20T13:07:28.722Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Mellanox的SF(Scalable Function/Sub Function)技术。<br><a id="more"></a><img src="/images/2024/07/013.png" alt></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Scalable functions (SFs), or sub-functions, are very similar to virtual functions (VFs) which are part of a Single Root I/O Virtualization (SR-IOV) solution. I/O virtualization is one of the key features used in data centers today. It improves the performance of enterprise servers by giving virtual machines direct access to hardware I/O devices. The SR-IOV specification allows one PCI Express (PCIe) device to present itself to the host as multiple distinct “virtual” devices. This is done with a new PCIe capability structure added to a traditional PCIe function (i.e., a physical function or PF).</p><p>The PF provides control over the creation and allocation of new VFs. VFs share the device’s underlying hardware and PCIe. A key feature of the SR-IOV specification is that VFs are very lightweight so that many of them can be implemented in a single device.</p><p><strong>To utilize the capabilities of VF in the BlueField, SFs are used. SFs allow support for a larger number of functions than VFs</strong>, and more importantly, they allow running multiple services concurrently on the DPU.</p><p>An SF is a lightweight function which has a parent PCIe function on which it is deployed. The SF, therefore, has access to the capabilities and resources of its parent PCIe function and <strong>has its own function capabilities and its own resources. This means that an SF would also have its own dedicated queues (i.e., txq, rxq).</strong></p><p>SFs co-exist with PCIe SR-IOV virtual functions (on the host) but also do not require enabling PCIe SR-IOV.</p><p>SFs support E-Switch representation offload like existing PF and VF representors. An SF shares PCIe-level resources with other SFs and/or with its parent PCIe function.</p><p><img src="/images/2024/07/014.webp" alt></p><h2 id="Internals"><a href="#Internals" class="headerlink" title="Internals"></a>Internals</h2><ul><li><p>When scalable function is RDMA capable, it has its own <a href="https://zhuanlan.zhihu.com/p/195757767" target="_blank" rel="noopener">QP1</a>, <a href="https://zhuanlan.zhihu.com/p/163552044" target="_blank" rel="noopener">GID table</a> and rdma resources neither shared nor stolen from the parent PCI function.</p></li><li><p><strong>A scalable function has dedicated window in PCI BAR space</strong> that is not shared with the other scalable functions or parent PCI function. <em>This ensures that all class devices of the scalable function accesses only assigned PCI BAR space</em>.</p></li><li><p>A scalable function supports eswitch representation through which it supports <a href="https://netdevconf.info/2.2/papers/horman-tcflower-talk.pdf" target="_blank" rel="noopener">tc offloads</a>. User must configure eswitch to send/receive packets from/to scalable function port.</p></li><li><p>Scalable functions share PCI level resources such as PCI MSI-X IRQs with their other scalable function and/or with its parent PCI function.</p></li></ul><h2 id="SFs-vs-VFs"><a href="#SFs-vs-VFs" class="headerlink" title="SFs vs VFs"></a>SFs vs VFs</h2><ol><li>SFs are deployed in unit of one unlike SR-IOV VFs which are enabled all together. When a new container is spawned, at that point needed SF can be created and deployed.</li><li><strong>SFs do not have to implement full PCI config space, reset, registers</strong>. This makes the device light weight.</li><li>SFs share MSI-X vectors with owner PCI PF and other peer SFs. This reduces the demand on total number of vectors in hardware and platform interrupt controller.</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以下内容只是个人的理解与猜测:</p><ul><li>SF与SR-IOV是正交的<ul><li>不支持SR-IOV，也是可以支持SF的</li><li>支持SR-IOV的话，可以在VF中支持SF</li></ul></li><li>SF类比于Intel的SIOV技术，但是当前SF只支持容器，还不支持vm</li><li>当前SF应该不支持PASID，SF的DMA视角如下:<ul><li>对于netdevice的queue，不同容器(进程)的页表是不一样的，但是只有一个parent PCI function，因此只有一个BDF，如果使用IOMMU页表的话，无法保证不同进程使用不同的IOMMU页表，所以在容器场景下，不会使用IOMMU；因此对于DMA内存地址，queue中entry使用的是HPA，而非HVA</li><li>对于RDMA的queue，由于MTT的支持，所以queue中entry使用的是HVA，MTT会完成HVA到HPA的翻译</li></ul></li><li>如果支持PASID的话，SF也是可以支持VM的</li></ul><hr><p>参考资料:</p><ol><li><a href="https://github.com/Mellanox/scalablefunctions/wiki" target="_blank" rel="noopener">https://github.com/Mellanox/scalablefunctions/wiki</a></li><li><a href="https://github.com/Mellanox/scalablefunctions" target="_blank" rel="noopener">https://github.com/Mellanox/scalablefunctions</a></li><li><a href="https://docs.nvidia.com/doca/sdk/scalable-functions/index.html" target="_blank" rel="noopener">https://docs.nvidia.com/doca/sdk/scalable-functions/index.html</a></li><li><a href="https://lore.kernel.org/kvm/20191107.155709.1716879557397915384.davem@davemloft.net/T/#mcad775350561ccb706f671920a874967dd7376d3" target="_blank" rel="noopener">net/mlx5: E-switch, Move devlink port close to eswitch port</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Mellanox的SF(Scalable Function/Sub Function)技术。&lt;br&gt;
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>IRQ bypass for posted interrupt</title>
    <link href="http://liujunming.github.io/2024/07/20/IRQ-bypass-for-posted-interrupt/"/>
    <id>http://liujunming.github.io/2024/07/20/IRQ-bypass-for-posted-interrupt/</id>
    <published>2024-07-20T00:00:30.000Z</published>
    <updated>2024-07-20T00:36:35.147Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>IRQ bypass仅仅是一套软件框架而已!<a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">When a physical I/O device is assigned to a virtual machine through</span><br><span class="line">facilities like VFIO and KVM, the interrupt for the device generally</span><br><span class="line">bounces through the host system before being injected into the VM.</span><br><span class="line">However, hardware technologies exist that often allow the host to be</span><br><span class="line">bypassed for some of these scenarios.  Intel Posted Interrupts allow</span><br><span class="line">the specified physical edge interrupts to be directly injected into a</span><br><span class="line">guest when delivered to a physical processor while the vCPU is</span><br><span class="line">running.  ARM IRQ Forwarding allows forwarded physical interrupts to</span><br><span class="line">be directly deactivated by the guest.</span><br><span class="line"></span><br><span class="line">The IRQ bypass manager here is meant to provide the shim to connect</span><br><span class="line">interrupt producers, generally the host physical device driver, with</span><br><span class="line">interrupt consumers, generally the hypervisor, in order to configure</span><br><span class="line">these bypass mechanism.  To do this, we base the connection on a</span><br><span class="line">shared, opaque token.  For KVM-VFIO this is expected to be an</span><br><span class="line">eventfd_ctx since this is the connection we already use to connect an</span><br><span class="line">eventfd to an irqfd on the in-kernel path.  When a producer and</span><br><span class="line">consumer with matching tokens is found, callbacks via both registered</span><br><span class="line">participants allow the bypass facilities to be automatically enabled.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Theory of operation</span><br><span class="line"> *</span><br><span class="line"> * The IRQ bypass manager is a simple set of lists and callbacks that allows</span><br><span class="line"> * IRQ producers (ex. physical interrupt sources) to be matched to IRQ</span><br><span class="line"> * consumers (ex. virtualization hardware that allows IRQ bypass or offload)</span><br><span class="line"> * via a shared token (ex. eventfd_ctx).  Producers and consumers register</span><br><span class="line"> * independently.  When a token match is found, the optional @stop callback</span><br><span class="line"> * will be called for each participant.  The pair will then be connected via</span><br><span class="line"> * the @add_* callbacks, and finally the optional @start callback will allow</span><br><span class="line"> * any final coordination.  When either participant is unregistered, the</span><br><span class="line"> * process is repeated using the @del_* callbacks in place of the @add_*</span><br><span class="line"> * callbacks.  Match tokens must be unique per producer/consumer, 1:N pairings</span><br><span class="line"> * are not supported.</span><br><span class="line"> */</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_producer</span><span class="params">(struct irq_bypass_producer *)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">irq_bypass_unregister_producer</span><span class="params">(struct irq_bypass_producer *)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_consumer</span><span class="params">(struct irq_bypass_consumer *)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">irq_bypass_unregister_consumer</span><span class="params">(struct irq_bypass_consumer *)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="kvm-arch-irq-bypass-add-producer"><a href="#kvm-arch-irq-bypass-add-producer" class="headerlink" title="kvm_arch_irq_bypass_add_producer"></a>kvm_arch_irq_bypass_add_producer</h2><h3 id="irq-bypass-register-producer"><a href="#irq-bypass-register-producer" class="headerlink" title="irq_bypass_register_producer"></a>irq_bypass_register_producer</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vfio_pci_set_irqs_ioctl</span><br><span class="line">└── vfio_pci_set_msi_trigger</span><br><span class="line">    └── vfio_msi_set_block</span><br><span class="line">        └── vfio_msi_set_vector_signal</span><br><span class="line">            └── irq_bypass_register_producer</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">vfio_msi_set_vector_signal</span><span class="params">(struct vfio_pci_device *vdev,</span></span></span><br><span class="line"><span class="function"><span class="params">                      <span class="keyword">int</span> <span class="built_in">vector</span>, <span class="keyword">int</span> fd, <span class="keyword">bool</span> msix)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">eventfd_ctx</span> *<span class="title">trigger</span>;</span></span><br><span class="line">    ...</span><br><span class="line">    trigger = eventfd_ctx_fdget(fd); <span class="comment">// 正常情况下，这里的fd就是irqfd</span></span><br><span class="line">    ...</span><br><span class="line">    vdev-&gt;ctx[<span class="built_in">vector</span>].producer.token = trigger;</span><br><span class="line">    vdev-&gt;ctx[<span class="built_in">vector</span>].producer.irq = irq;</span><br><span class="line">    ret = irq_bypass_register_producer(&amp;vdev-&gt;ctx[<span class="built_in">vector</span>].producer);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * irq_bypass_register_producer - register IRQ bypass producer</span></span><br><span class="line"><span class="comment"> * @producer: pointer to producer structure</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Add the provided IRQ producer to the list of producers and connect</span></span><br><span class="line"><span class="comment"> * with any matching token found on the IRQ consumers list.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_producer</span><span class="params">(struct irq_bypass_producer *producer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_producer</span> *<span class="title">tmp</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_consumer</span> *<span class="title">consumer</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!producer-&gt;token)</span><br><span class="line">        <span class="keyword">return</span> -EINVAL;</span><br><span class="line"></span><br><span class="line">    might_sleep();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!try_module_get(THIS_MODULE))</span><br><span class="line">        <span class="keyword">return</span> -ENODEV;</span><br><span class="line"></span><br><span class="line">    mutex_lock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(tmp, &amp;producers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tmp-&gt;token == producer-&gt;token) &#123;</span><br><span class="line">            mutex_unlock(&amp;lock);</span><br><span class="line">            module_put(THIS_MODULE);</span><br><span class="line">            <span class="keyword">return</span> -EBUSY;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(consumer, &amp;consumers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (consumer-&gt;token == producer-&gt;token) &#123;</span><br><span class="line">            <span class="keyword">int</span> ret = __connect(producer, consumer);</span><br><span class="line">            <span class="keyword">if</span> (ret) &#123;</span><br><span class="line">                mutex_unlock(&amp;lock);</span><br><span class="line">                module_put(THIS_MODULE);</span><br><span class="line">                <span class="keyword">return</span> ret;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_add(&amp;producer-&gt;node, &amp;producers);</span><br><span class="line"></span><br><span class="line">    mutex_unlock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(irq_bypass_register_producer);</span><br></pre></td></tr></table></figure><h3 id="irq-bypass-register-consumer"><a href="#irq-bypass-register-consumer" class="headerlink" title="irq_bypass_register_consumer"></a>irq_bypass_register_consumer</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_vm_ioctl</span><br><span class="line">└── kvm_irqfd</span><br><span class="line">    └── kvm_irqfd_assign</span><br><span class="line">        └── irq_bypass_register_consumer</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span></span><br><span class="line">kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_HAVE_KVM_IRQ_BYPASS</span></span><br><span class="line">    <span class="keyword">if</span> (kvm_arch_has_irq_bypass()) &#123;</span><br><span class="line">        irqfd-&gt;consumer.token = (<span class="keyword">void</span> *)irqfd-&gt;eventfd;</span><br><span class="line">        irqfd-&gt;consumer.add_producer = kvm_arch_irq_bypass_add_producer;</span><br><span class="line">        irqfd-&gt;consumer.del_producer = kvm_arch_irq_bypass_del_producer;</span><br><span class="line">        irqfd-&gt;consumer.stop = kvm_arch_irq_bypass_stop;</span><br><span class="line">        irqfd-&gt;consumer.start = kvm_arch_irq_bypass_start;</span><br><span class="line">        ret = irq_bypass_register_consumer(&amp;irqfd-&gt;consumer);</span><br><span class="line">        <span class="keyword">if</span> (ret)</span><br><span class="line">            pr_info(<span class="string">"irq bypass consumer (token %p) registration fails: %d\n"</span>,</span><br><span class="line">                irqfd-&gt;consumer.token, ret);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * irq_bypass_register_consumer - register IRQ bypass consumer</span></span><br><span class="line"><span class="comment"> * @consumer: pointer to consumer structure</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Add the provided IRQ consumer to the list of consumers and connect</span></span><br><span class="line"><span class="comment"> * with any matching token found on the IRQ producer list.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">irq_bypass_register_consumer</span><span class="params">(struct irq_bypass_consumer *consumer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_consumer</span> *<span class="title">tmp</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">irq_bypass_producer</span> *<span class="title">producer</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!consumer-&gt;token ||</span><br><span class="line">        !consumer-&gt;add_producer || !consumer-&gt;del_producer)</span><br><span class="line">        <span class="keyword">return</span> -EINVAL;</span><br><span class="line"></span><br><span class="line">    might_sleep();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!try_module_get(THIS_MODULE))</span><br><span class="line">        <span class="keyword">return</span> -ENODEV;</span><br><span class="line"></span><br><span class="line">    mutex_lock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(tmp, &amp;consumers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tmp-&gt;token == consumer-&gt;token || tmp == consumer) &#123;</span><br><span class="line">            mutex_unlock(&amp;lock);</span><br><span class="line">            module_put(THIS_MODULE);</span><br><span class="line">            <span class="keyword">return</span> -EBUSY;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_for_each_entry(producer, &amp;producers, node) &#123;</span><br><span class="line">        <span class="keyword">if</span> (producer-&gt;token == consumer-&gt;token) &#123;</span><br><span class="line">            <span class="keyword">int</span> ret = __connect(producer, consumer);</span><br><span class="line">            <span class="keyword">if</span> (ret) &#123;</span><br><span class="line">                mutex_unlock(&amp;lock);</span><br><span class="line">                module_put(THIS_MODULE);</span><br><span class="line">                <span class="keyword">return</span> ret;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    list_add(&amp;consumer-&gt;node, &amp;consumers);</span><br><span class="line"></span><br><span class="line">    mutex_unlock(&amp;lock);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(irq_bypass_register_consumer);</span><br></pre></td></tr></table></figure><h3 id="vmx-update-pi-irte"><a href="#vmx-update-pi-irte" class="headerlink" title="vmx_update_pi_irte"></a>vmx_update_pi_irte</h3><p>当irq bypass的producer和consumer token(eventfd_ctx)匹配成功时，才会调用<code>kvm_arch_irq_bypass_add_producer</code>。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* @lock must be held when calling connect */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __connect(struct irq_bypass_producer *prod,</span><br><span class="line">             struct irq_bypass_consumer *cons)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (prod-&gt;stop)</span><br><span class="line">        prod-&gt;stop(prod);</span><br><span class="line">    <span class="keyword">if</span> (cons-&gt;stop)</span><br><span class="line">        cons-&gt;stop(cons);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (prod-&gt;add_consumer)</span><br><span class="line">        ret = prod-&gt;add_consumer(prod, cons);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!ret) &#123;</span><br><span class="line">        ret = cons-&gt;add_producer(cons, prod);</span><br><span class="line">        <span class="keyword">if</span> (ret &amp;&amp; prod-&gt;del_consumer)</span><br><span class="line">            prod-&gt;del_consumer(prod, cons);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cons-&gt;start)</span><br><span class="line">        cons-&gt;start(cons);</span><br><span class="line">    <span class="keyword">if</span> (prod-&gt;start)</span><br><span class="line">        prod-&gt;start(prod);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_arch_irq_bypass_add_producer</span><span class="params">(struct irq_bypass_consumer *cons,</span></span></span><br><span class="line"><span class="function"><span class="params">                      struct irq_bypass_producer *prod)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">kvm_kernel_irqfd</span> *<span class="title">irqfd</span> =</span></span><br><span class="line"><span class="class">        <span class="title">container_of</span>(<span class="title">cons</span>, <span class="title">struct</span> <span class="title">kvm_kernel_irqfd</span>, <span class="title">consumer</span>);</span></span><br><span class="line"></span><br><span class="line">    irqfd-&gt;producer = prod;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> kvm_x86_ops-&gt;update_pi_irte(irqfd-&gt;kvm,</span><br><span class="line">                       prod-&gt;irq, irqfd-&gt;gsi, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>add_producer其实就是设置irte为Posted-Interrupts而已！<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmx_update_pi_irte</span><br><span class="line">└── irq_set_vcpu_affinity</span><br><span class="line">    └── intel_ir_set_vcpu_affinity</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * vmx_update_pi_irte - set IRTE for Posted-Interrupts</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @kvm: kvm</span></span><br><span class="line"><span class="comment"> * @host_irq: host irq of the interrupt</span></span><br><span class="line"><span class="comment"> * @guest_irq: gsi of the interrupt</span></span><br><span class="line"><span class="comment"> * @set: set or unset PI</span></span><br><span class="line"><span class="comment"> * returns 0 on success, &lt; 0 on failure</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">vmx_update_pi_irte</span><span class="params">(struct kvm *kvm, <span class="keyword">unsigned</span> <span class="keyword">int</span> host_irq,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">uint32_t</span> guest_irq, <span class="keyword">bool</span> <span class="built_in">set</span>)</span></span></span><br></pre></td></tr></table></figure><h2 id="kvm-arch-update-irqfd-routing"><a href="#kvm-arch-update-irqfd-routing" class="headerlink" title="kvm_arch_update_irqfd_routing"></a>kvm_arch_update_irqfd_routing</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_vm_ioctl</span><br><span class="line">└── kvm_set_irq_routing</span><br><span class="line">    └── kvm_irq_routing_update</span><br><span class="line">        └── kvm_arch_update_irqfd_routing</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_arch_update_irqfd_routing</span><span class="params">(struct kvm *kvm, <span class="keyword">unsigned</span> <span class="keyword">int</span> host_irq,</span></span></span><br><span class="line"><span class="function"><span class="params">                   <span class="keyword">uint32_t</span> guest_irq, <span class="keyword">bool</span> <span class="built_in">set</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!kvm_x86_ops-&gt;update_pi_irte)</span><br><span class="line">        <span class="keyword">return</span> -EINVAL;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> kvm_x86_ops-&gt;update_pi_irte(kvm, host_irq, guest_irq, <span class="built_in">set</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>既然irq bypass的producer和consumer token匹配成功时就调用了<code>update_pi_irte</code>，为什么还要在<code>kvm_irq_routing_update</code>中也调用<code>update_pi_irte</code>呢？其实是在guest内部做irq balance时才会触发。</p><p>以guest内部对msi-x table中断做irq balance为例, qemu的函数调用链如下:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">msix_table_mmio_write</span><br><span class="line">└── msix_handle_mask_update</span><br><span class="line">    └── msix_fire_vector_notifier</span><br><span class="line">        └── vfio_msix_vector_use</span><br><span class="line">            └── vfio_msix_vector_do_use</span><br><span class="line">                └── vfio_update_kvm_msi_virq</span><br><span class="line">                    ├── kvm_irqchip_update_msi_route</span><br><span class="line">                    └── kvm_irqchip_commit_routes</span><br><span class="line">                        └── kvm_vm_ioctl(s, KVM_SET_GSI_ROUTING, s-&gt;irq_routes)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2024/07/011.jpg" alt></p><p><img src="/images/2024/07/012.jpg" alt></p><p>guest内部做irq balance时，可能会更改vCPU与vector号，需要将guest更改后的vector更新到IRTE中的vv字段。所以在guest内部做irq balance时，需要调用<code>update_pi_irte</code>来更新IRTE。</p><hr><p>参考资料:</p><ol><li><a href="https://lwn.net/Articles/653706/" target="_blank" rel="noopener">virt: IRQ bypass manager</a></li><li><a href="https://github.com/torvalds/linux/blob/master/include/linux/irqbypass.h" target="_blank" rel="noopener">include/linux/irqbypass.h</a></li><li><a href="https://lore.kernel.org/kvm/1442586596-5920-2-git-send-email-feng.wu@intel.com/" target="_blank" rel="noopener">virt: IRQ bypass manager</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;IRQ bypass仅仅是一套软件框架而已!
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="QEMU" scheme="http://liujunming.github.io/tags/QEMU/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
      <category term="VFIO" scheme="http://liujunming.github.io/tags/VFIO/"/>
    
  </entry>
  
  <entry>
    <title>File Descriptor Transfer over Unix Domain Sockets</title>
    <link href="http://liujunming.github.io/2024/07/14/File-Descriptor-Transfer-over-Unix-Domain-Sockets/"/>
    <id>http://liujunming.github.io/2024/07/14/File-Descriptor-Transfer-over-Unix-Domain-Sockets/</id>
    <published>2024-07-14T10:33:15.000Z</published>
    <updated>2024-07-14T10:52:39.859Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Transfer-over-UDS"><a href="#Data-Transfer-over-UDS" class="headerlink" title="Data Transfer over UDS"></a>Data Transfer over UDS</h2><p>Now that we’ve established that a Unix domain socket allows communication between two processes on the same host, it’s time to explore <em>what</em> kind of data can be transferred over a Unix domain socket.<a id="more"></a></p><p>Since a Unix domain socket is similar to network sockets in many respects, <em>any</em> data that one might usually send over a network socket can be sent over a Unix domain socket.</p><p>Furthermore, the special system calls <code>sendmsg</code> and <code>recvmsg</code> allow sending a <em>special</em> message across the Unix domain socket. This message is handled specially by the kernel, which allows passing open <strong>file descriptions</strong> from the sender to the receiver.</p><h3 id="File-Descriptors-vs-File-Description"><a href="#File-Descriptors-vs-File-Description" class="headerlink" title="File Descriptors vs File Description"></a>File Descriptors vs File Description</h3><p>Note that I mentioned <strong>file descripTION</strong> and not <strong>file descripTOR</strong>. The difference between the two is subtle and isn’t often well understood.</p><p>A <strong>file descriptor</strong> really is just a <em>per process</em> pointer to an underlying kernel data structure called the <strong>file description</strong>. The kernel maintains a table of all open <strong>file descriptions</strong> called the <strong>open file table</strong>. If two processes (A and B) try to open the same file, the two processes might have their own separate <strong>file descriptors</strong>, which point to the same <strong>file description</strong> in the open file table.</p><p><img src="/images/2024/07/010.webp" alt></p><p>So “sending a file descriptor” from one Unix domain socket to another with <code>sendmsg()</code> really just means sending a <em>reference to the file description</em>. If process A were to send file descriptor 0 (fd0) to process B, the file descriptor might very well be referenced by the number 3 (fd3) in process B. They will, however, refer to the same <em>file description</em>.</p><p>The sending process calls <code>sendmsg</code> to send the descriptor across the Unix domain socket. The receiving process calls <code>recvmsg</code> to receive the descriptor on the Unix domain socket.</p><p>Even if the sending process closes its file descriptor referencing the file description being passed via <code>sendmsg</code> before the receiving process calls <code>recvmsg</code>, the file description remains open for the receiving process. Sending a descriptor increments the description’s reference count by one. The kernel only removes file descriptions from its open file table if the reference count drops to 0.</p><h3 id="sendmsg-and-recvmsg"><a href="#sendmsg-and-recvmsg" class="headerlink" title="sendmsg and recvmsg"></a>sendmsg and recvmsg</h3><p>The signature for the <code>sendmsg</code> function call on Linux is the following:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> sendmsg(</span><br><span class="line">    <span class="keyword">int</span> socket,</span><br><span class="line">    <span class="keyword">const</span> struct msghdr *message,</span><br><span class="line">    <span class="keyword">int</span> flags</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><p>The counterpart of <code>sendmsg</code> is <code>recvmsg</code>:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> recvmsg(</span><br><span class="line">     <span class="keyword">int</span> sockfd,</span><br><span class="line">     <span class="keyword">const</span> struct msghdr *msg,</span><br><span class="line">     <span class="keyword">int</span> flags</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><p>The special “message” that one can transfer with <code>sendmsg</code> over a Unix domain socket is specified by the <code>msghdr</code>. The process which wishes to send the file description over to another process creates a <code>msghdr</code> structure containing the description to be passed.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">msghdr</span> &#123;</span></span><br><span class="line">    <span class="keyword">void</span>            *msg_name;      <span class="comment">/* optional address */</span></span><br><span class="line">    <span class="keyword">socklen_t</span>       msg_namelen;    <span class="comment">/* size of address */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span>          <span class="title">iovec</span> *<span class="title">msg_iov</span>;</span> <span class="comment">/* scatter/gather array */</span></span><br><span class="line">    <span class="keyword">int</span>             msg_iovlen;     <span class="comment">/* # elements in msg_iov */</span></span><br><span class="line">    <span class="keyword">void</span>            *msg_control;   <span class="comment">/* ancillary data, see below */</span></span><br><span class="line">    <span class="keyword">socklen_t</span>       msg_controllen; <span class="comment">/* ancillary data buffer len */</span></span><br><span class="line">    <span class="keyword">int</span>             msg_flags;      <span class="comment">/* flags on received message */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The <code>msg_control</code> member of the <code>msghdr</code> structure, which has length <code>msg_controllen</code>, points to a buffer of messages of the form:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cmsghdr</span> &#123;</span></span><br><span class="line">    <span class="keyword">socklen_t</span> cmsg_len;    <span class="comment">/* data byte count, including header */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_level;  <span class="comment">/* originating protocol */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_type;   <span class="comment">/* protocol-specific type */</span></span><br><span class="line">    <span class="comment">/* followed by */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> cmsg_data[];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>In POSIX, a buffer of <strong>struct cmsghdr</strong> structures <strong>with appended data</strong> is called <strong>ancillary data</strong>. On Linux, the maximum buffer size allowed per socket can be set by modifying <code>/proc/sys/net/core/optmem_max</code>.</p><h2 id="Ancillary-Data-Transfer"><a href="#Ancillary-Data-Transfer" class="headerlink" title="Ancillary Data Transfer"></a>Ancillary Data Transfer</h2><p>While there are a plethora of gotchas with such data transfer, when used correctly, it can be a pretty powerful mechanism to achieve a number of goals.</p><p>On Linux, there are three such types of “ancillary data” that can be shared between two Unix domain sockets:</p><ul><li><code>SCM_RIGHTS</code></li><li><code>SCM_CREDENTIALS</code></li><li><code>SCM_SECURITY</code></li></ul><p>All three forms of ancillary data should <strong>only</strong> be accessed using the macros described below and never directly.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct cmsghdr *<span class="title">CMSG_FIRSTHDR</span><span class="params">(struct msghdr *msgh)</span></span>;</span><br><span class="line"><span class="function">struct cmsghdr *<span class="title">CMSG_NXTHDR</span><span class="params">(struct msghdr *msgh, struct cmsghdr *cmsg)</span></span>;</span><br><span class="line"><span class="keyword">size_t</span> CMSG_ALIGN(<span class="keyword">size_t</span> length);</span><br><span class="line"><span class="keyword">size_t</span> CMSG_SPACE(<span class="keyword">size_t</span> length);</span><br><span class="line"><span class="keyword">size_t</span> CMSG_LEN(<span class="keyword">size_t</span> length);</span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="keyword">char</span> *<span class="title">CMSG_DATA</span><span class="params">(struct cmsghdr *cmsg)</span></span>;</span><br></pre></td></tr></table></figure><p>While I’ve never had a need to use the latter two, <code>SCM_RIGHTS</code> is what I hope to explore more in this post.</p><h3 id="SCM-RIGHTS"><a href="#SCM-RIGHTS" class="headerlink" title="SCM_RIGHTS"></a>SCM_RIGHTS</h3><p><code>SCM_RIGHTS</code> allows a process to send or receive a set of open file descriptors from another process using <code>sendmsg</code>.</p><p>The <code>cmsg_data</code> component of the <code>cmsghdr</code> structure can contain an array of the file descriptors that a process wants to send to another.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cmsghdr</span> &#123;</span></span><br><span class="line">    <span class="keyword">socklen_t</span> cmsg_len;    <span class="comment">/* data byte count, including header */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_level;  <span class="comment">/* originating protocol */</span></span><br><span class="line">    <span class="keyword">int</span>       cmsg_type;   <span class="comment">/* protocol-specific type */</span></span><br><span class="line">    <span class="comment">/* followed by */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> cmsg_data[];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The receiving process uses <code>recvmsg</code> to receive the data.</p><p>The book The Linux Programming Interface has a <a href="https://man7.org/tlpi/code/online/dist/sockets/scm_rights_send.c.html" target="_blank" rel="noopener">good programmatic guide</a> on how to use the <code>sendmsg</code> and <code>recvmsg</code>.</p><hr><p>参考资料:</p><ol><li><a href="https://copyconstruct.medium.com/file-descriptor-transfer-over-unix-domain-sockets-dcbbf5b3b6ec" target="_blank" rel="noopener">File Descriptor Transfer over Unix Domain Sockets</a></li><li><a href="https://broman.dev/download/The%20Linux%20Programming%20Interface.pdf" target="_blank" rel="noopener">The Linux Programming Interface</a></li><li><a href="https://man7.org/linux/man-pages/man7/unix.7.html" target="_blank" rel="noopener">man unix</a></li><li><a href="https://dengking.github.io/Linux-OS/Programming/IO/IO-data-structure/File-descriptor/Pass-file-descriptor/" target="_blank" rel="noopener">Share file descriptor between process</a></li><li><a href="https://www.cnblogs.com/nufangrensheng/p/3571370.html" target="_blank" rel="noopener">高级进程间通信之传送文件描述符</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Data-Transfer-over-UDS&quot;&gt;&lt;a href=&quot;#Data-Transfer-over-UDS&quot; class=&quot;headerlink&quot; title=&quot;Data Transfer over UDS&quot;&gt;&lt;/a&gt;Data Transfer over UDS&lt;/h2&gt;&lt;p&gt;Now that we’ve established that a Unix domain socket allows communication between two processes on the same host, it’s time to explore &lt;em&gt;what&lt;/em&gt; kind of data can be transferred over a Unix domain socket.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>源码解析:vhost ioeventfd与irqfd</title>
    <link href="http://liujunming.github.io/2024/07/13/vhost-eventfd-pov/"/>
    <id>http://liujunming.github.io/2024/07/13/vhost-eventfd-pov/</id>
    <published>2024-07-13T08:21:01.000Z</published>
    <updated>2024-07-14T10:29:47.554Z</updated>
    
    <content type="html"><![CDATA[<p>本文将结合qemu与linux源码，解析vhost中ioeventfd与irqfd相关内容。<a id="more"></a></p><h2 id="prerequisite"><a href="#prerequisite" class="headerlink" title="prerequisite"></a>prerequisite</h2><ul><li><a href="/2024/03/24/QEMU-Internals-vhost-architecture/">QEMU Internals: vhost architecture</a></li><li><a href="http://liujunming.top/2021/10/26/Dive-into-ioeventfd%28kvm%20side%29-mechanism/" target="_blank" rel="noopener">Dive into ioeventfd(KVM side) mechanism</a></li><li><a href="/2021/10/27/Dive-into-irqfd-KVM-side-mechanism/">Dive into irqfd(KVM side) mechanism</a></li></ul><h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><p><img src="/images/2024/03/010.png" alt></p><p>ioeventfd与kick绑定，irqfd与中断绑定</p><p>ioeventfd:</p><ul><li>qemu利用<code>KVM_IOEVENTFD</code> ioctl，将ioeventfd与guest kick寄存器的地址(pio/mmio地址)和vq index的值绑定，传给kvm<ul><li>当kvm检测到guest往kick寄存器写入vq index后，写eventfd通知vhost</li></ul></li><li>qemu利用<code>VHOST_SET_VRING_KICK</code> ioctl，将ioeventfd传给vhost，vhost就会poll ioeventfd的写<ul><li>当vhost poll到ioeventfd的写后，就会开始从avai ring中拉取请求，处理完io请求后，更新used ring，最后给guest注入中断(需要借助于irqfd)</li></ul></li></ul><p>irqfd:</p><ul><li>qemu利用<code>VHOST_SET_VRING_CALL</code> ioctl，将irqfd传给vhost<ul><li>vhost在更新完used ring后，写eventfd通知kvm注入中断</li></ul></li><li>qemu利用<code>KVM_IRQFD</code> ioctl，将irqfd与vq的中断绑定，传给kvm，kvm就会poll irqfd的写<ul><li>当kvm poll到irqfd的写后，就会根据中断路由信息，给guest注入中断</li></ul></li></ul><h2 id="ioeventfd"><a href="#ioeventfd" class="headerlink" title="ioeventfd"></a>ioeventfd</h2><h3 id="qemu侧ioeventfd的关联"><a href="#qemu侧ioeventfd的关联" class="headerlink" title="qemu侧ioeventfd的关联"></a>qemu侧ioeventfd的关联</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">virtio_bus_start_ioeventfd</span><br><span class="line">└── virtio_device_start_ioeventfd_impl[start_ioeventfd]</span><br><span class="line">    ├── event_notifier_set(&amp;vq-&gt;host_notifier)</span><br><span class="line">    └── memory_region_transaction_commit</span><br><span class="line">        └── address_space_update_ioeventfds</span><br><span class="line">            └── address_space_add_del_ioeventfds</span><br><span class="line">                ├── kvm_io_ioeventfd_add[eventfd_add] <span class="comment">//pio</span></span><br><span class="line">                │   └── kvm_set_ioeventfd_pio</span><br><span class="line">                │       └── kvm_vm_ioctl(kvm_state, KVM_IOEVENTFD, &amp;kick)</span><br><span class="line">                └── kvm_mem_ioeventfd_add[eventfd_add] <span class="comment">//mmio</span></span><br><span class="line">                    └── kvm_set_ioeventfd_mmio</span><br><span class="line">                        └── kvm_vm_ioctl(kvm_state, KVM_IOEVENTFD, &amp;iofd)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vhost_virtqueue_start</span><br><span class="line">├── event_notifier_get_fd(virtio_queue_get_host_notifier(vvq))</span><br><span class="line">└── vhost_kernel_set_vring_kick[vhost_set_vring_kick]</span><br><span class="line">    └── vhost_kernel_call(dev, VHOST_SET_VRING_KICK, file)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">VirtQueue</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        ...</span><br><span class="line">        EventNotifier host_notifier;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由上述代码可知，qemu侧通过host_notifier的ioeventfd，将vhost与kvm关联了起来；</p><ul><li>vhost负责poll ioeventfd</li><li>kvm负责写ioeventfd来通知vhost guest的kick操作</li></ul><h3 id="kvm侧ioeventfd处理"><a href="#kvm侧ioeventfd处理" class="headerlink" title="kvm侧ioeventfd处理"></a>kvm侧ioeventfd处理</h3><p>参考<a href="http://liujunming.top/2021/10/26/Dive-into-ioeventfd%28kvm%20side%29-mechanism/" target="_blank" rel="noopener">Dive into ioeventfd(KVM side) mechanism</a>即可。</p><h3 id="vhost侧ioeventfd处理"><a href="#vhost侧ioeventfd处理" class="headerlink" title="vhost侧ioeventfd处理"></a>vhost侧ioeventfd处理</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">vhost_vring_ioctl</span><span class="params">(struct vhost_dev *d, <span class="keyword">unsigned</span> <span class="keyword">int</span> ioctl, <span class="keyword">void</span> __user *argp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">switch</span> (ioctl) &#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">case</span> VHOST_SET_VRING_KICK:</span><br><span class="line"><span class="keyword">if</span> (copy_from_user(&amp;f, argp, <span class="keyword">sizeof</span> f)) &#123;</span><br><span class="line">r = -EFAULT;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">eventfp = f.fd == VHOST_FILE_UNBIND ? <span class="literal">NULL</span> : eventfd_fget(f.fd);</span><br><span class="line"><span class="keyword">if</span> (IS_ERR(eventfp)) &#123;</span><br><span class="line">r = PTR_ERR(eventfp);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (eventfp != vq-&gt;kick) &#123;</span><br><span class="line">pollstop = (filep = vq-&gt;kick) != <span class="literal">NULL</span>;</span><br><span class="line">pollstart = (vq-&gt;kick = eventfp) != <span class="literal">NULL</span>;</span><br><span class="line">&#125; <span class="keyword">else</span></span><br><span class="line">filep = eventfp;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">        ...</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> (pollstart &amp;&amp; vq-&gt;handle_kick)</span><br><span class="line">        r = vhost_poll_start(&amp;vq-&gt;poll, vq-&gt;kick);</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>qemu利用<code>VHOST_SET_VRING_KICK</code> ioctl，将ioeventfd传给vhost，然后vhost就开始poll ioeventfd(<code>vhost_poll_start</code>)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Start polling a file. We add ourselves to file's wait queue. The caller must</span></span><br><span class="line"><span class="comment"> * keep a reference to a file until after vhost_poll_stop is called. */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">vhost_poll_start</span><span class="params">(struct vhost_poll *poll, struct file *file)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">__poll_t</span> mask;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (poll-&gt;wqh)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">mask = vfs_poll(file, &amp;poll-&gt;table); <span class="comment">//调用callback vhost_poll_func</span></span><br><span class="line"><span class="keyword">if</span> (mask)</span><br><span class="line">vhost_poll_wakeup(&amp;poll-&gt;wait, <span class="number">0</span>, <span class="number">0</span>, poll_to_key(mask));</span><br><span class="line"><span class="keyword">if</span> (mask &amp; EPOLLERR) &#123;</span><br><span class="line">vhost_poll_stop(poll);</span><br><span class="line"><span class="keyword">return</span> -EINVAL;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(vhost_poll_start);</span><br></pre></td></tr></table></figure><p>当vhost poll到ioeventfd写后，就会触发<code>vhost_poll_wakeup</code>回调。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vhost_poll_wakeup</span><br><span class="line">└── vhost_poll_queue</span><br><span class="line">    └── vhost_vq_work_queue</span><br><span class="line">        └── vhost_worker_queue</span><br><span class="line">            ├── llist_add(&amp;work-&gt;node, &amp;worker-&gt;work_list)</span><br><span class="line">            └── vhost_task_wake(worker-&gt;vtsk)</span><br></pre></td></tr></table></figure></p><p><code>worker-&gt;vtsk</code>又会如何操作呢？且看<code>worker-&gt;vtsk</code>的初始化情况以及<code>worker-&gt;vtsk</code>的执行函数吧。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> struct vhost_worker *<span class="title">vhost_worker_create</span><span class="params">(struct vhost_dev *dev)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_worker</span> *<span class="title">worker</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_task</span> *<span class="title">vtsk</span>;</span></span><br><span class="line"><span class="keyword">char</span> name[TASK_COMM_LEN];</span><br><span class="line"><span class="keyword">int</span> ret;</span><br><span class="line">u32 id;</span><br><span class="line"></span><br><span class="line">worker = kzalloc(<span class="keyword">sizeof</span>(*worker), GFP_KERNEL_ACCOUNT);</span><br><span class="line"><span class="keyword">if</span> (!worker)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">snprintf</span>(name, <span class="keyword">sizeof</span>(name), <span class="string">"vhost-%d"</span>, current-&gt;pid);</span><br><span class="line"></span><br><span class="line">vtsk = vhost_task_create(vhost_worker, worker, name);</span><br><span class="line"><span class="keyword">if</span> (!vtsk)</span><br><span class="line"><span class="keyword">goto</span> free_worker;</span><br><span class="line"></span><br><span class="line">mutex_init(&amp;worker-&gt;mutex);</span><br><span class="line">init_llist_head(&amp;worker-&gt;work_list);</span><br><span class="line">worker-&gt;kcov_handle = kcov_common_handle();</span><br><span class="line">worker-&gt;vtsk = vtsk;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">bool</span> <span class="title">vhost_worker</span><span class="params">(<span class="keyword">void</span> *data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_worker</span> *<span class="title">worker</span> = <span class="title">data</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_work</span> *<span class="title">work</span>, *<span class="title">work_next</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">llist_node</span> *<span class="title">node</span>;</span></span><br><span class="line"></span><br><span class="line">node = llist_del_all(&amp;worker-&gt;work_list);</span><br><span class="line"><span class="keyword">if</span> (node) &#123;</span><br><span class="line">__set_current_state(TASK_RUNNING);</span><br><span class="line"></span><br><span class="line">node = llist_reverse_order(node);</span><br><span class="line"><span class="comment">/* make sure flag is seen after deletion */</span></span><br><span class="line">smp_wmb();</span><br><span class="line">llist_for_each_entry_safe(work, work_next, node, node) &#123;</span><br><span class="line">clear_bit(VHOST_WORK_QUEUED, &amp;work-&gt;flags);</span><br><span class="line">kcov_remote_start_common(worker-&gt;kcov_handle);</span><br><span class="line">work-&gt;fn(work); <span class="comment">//vq-&gt;handle_kick</span></span><br><span class="line">kcov_remote_stop();</span><br><span class="line">cond_resched();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> !!node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在初始化过程中，vhost会创建一个名为<code>vhost-$pid</code>的内核线程，其中$pid是QEMU进程的pid。该线程被称为“vhost工作线程”。<br>vhost工作线程的运行函数为<code>vhost_worker</code>，而<code>vhost_worker</code>就会触发<code>vq-&gt;handle_kick</code>的回调。</p><p>那么<code>vhost_poll</code>与<code>vq-&gt;handle_kick</code>等又是如何初始化的呢？</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vhost_dev_init</span><span class="params">(struct vhost_dev *dev,</span></span></span><br><span class="line"><span class="function"><span class="params">    struct vhost_virtqueue **vqs, <span class="keyword">int</span> nvqs,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> iov_limit, <span class="keyword">int</span> weight, <span class="keyword">int</span> byte_weight,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">bool</span> use_worker,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> (*msg_handler)</span><span class="params">(struct vhost_dev *dev, u32 asid,</span></span></span><br><span class="line">       struct vhost_iotlb_msg *msg))</span><br><span class="line">&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_virtqueue</span> *<span class="title">vq</span>;</span></span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">dev-&gt;vqs = vqs;</span><br><span class="line">dev-&gt;nvqs = nvqs;</span><br><span class="line">mutex_init(&amp;dev-&gt;mutex);</span><br><span class="line">dev-&gt;log_ctx = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;umem = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;iotlb = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;mm = <span class="literal">NULL</span>;</span><br><span class="line">dev-&gt;iov_limit = iov_limit;</span><br><span class="line">dev-&gt;weight = weight;</span><br><span class="line">dev-&gt;byte_weight = byte_weight;</span><br><span class="line">dev-&gt;use_worker = use_worker;</span><br><span class="line">dev-&gt;msg_handler = msg_handler;</span><br><span class="line">init_waitqueue_head(&amp;dev-&gt;wait);</span><br><span class="line">INIT_LIST_HEAD(&amp;dev-&gt;read_list);</span><br><span class="line">INIT_LIST_HEAD(&amp;dev-&gt;pending_list);</span><br><span class="line">spin_lock_init(&amp;dev-&gt;iotlb_lock);</span><br><span class="line">xa_init_flags(&amp;dev-&gt;worker_xa, XA_FLAGS_ALLOC);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; dev-&gt;nvqs; ++i) &#123;</span><br><span class="line">vq = dev-&gt;vqs[i];</span><br><span class="line">vq-&gt;<span class="built_in">log</span> = <span class="literal">NULL</span>;</span><br><span class="line">vq-&gt;indirect = <span class="literal">NULL</span>;</span><br><span class="line">vq-&gt;heads = <span class="literal">NULL</span>;</span><br><span class="line">vq-&gt;dev = dev;</span><br><span class="line">mutex_init(&amp;vq-&gt;mutex);</span><br><span class="line">vhost_vq_reset(dev, vq);</span><br><span class="line"><span class="keyword">if</span> (vq-&gt;handle_kick)</span><br><span class="line">vhost_poll_init(&amp;vq-&gt;poll, vq-&gt;handle_kick,</span><br><span class="line">EPOLLIN, dev, vq);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>vhost设备初始化时，会为每个vq调用<code>vhost_poll_init</code>来初始化<code>vhost_poll</code>与<code>vq-&gt;handle_kick</code>等内容。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Init poll structure */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vhost_poll_init</span><span class="params">(struct vhost_poll *poll, <span class="keyword">vhost_work_fn_t</span> fn,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">__poll_t</span> mask, struct vhost_dev *dev,</span></span></span><br><span class="line"><span class="function"><span class="params">     struct vhost_virtqueue *vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">init_waitqueue_func_entry(&amp;poll-&gt;wait, vhost_poll_wakeup); <span class="comment">//设置wait的回调，在vhost poll到ioeventfd的写后，会触发回调</span></span><br><span class="line">init_poll_funcptr(&amp;poll-&gt;table, vhost_poll_func); <span class="comment">//vhost_poll_func在vfs_poll时会触发回调，加入到file's wait queue中</span></span><br><span class="line">poll-&gt;mask = mask;</span><br><span class="line">poll-&gt;dev = dev;</span><br><span class="line">poll-&gt;wqh = <span class="literal">NULL</span>;</span><br><span class="line">poll-&gt;vq = vq;</span><br><span class="line"></span><br><span class="line">vhost_work_init(&amp;poll-&gt;work, fn); <span class="comment">//fn为vq-&gt;handle_kick，初始化为work-&gt;fn，在vhost_worker中会回调</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">vhost_poll_func</span><span class="params">(struct file *file, <span class="keyword">wait_queue_head_t</span> *wqh, <span class="comment">//在vfs_poll时会触发回调，加入到file's wait queue中</span></span></span></span><br><span class="line"><span class="function"><span class="params">    poll_table *pt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_poll</span> *<span class="title">poll</span>;</span></span><br><span class="line"></span><br><span class="line">poll = container_of(pt, struct vhost_poll, table);</span><br><span class="line">poll-&gt;wqh = wqh;</span><br><span class="line">add_wait_queue(wqh, &amp;poll-&gt;wait);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* The virtqueue structure describes a queue attached to a device. */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_virtqueue</span> &#123;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_dev</span> *<span class="title">dev</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_worker</span> __<span class="title">rcu</span> *<span class="title">worker</span>;</span></span><br><span class="line">...</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vhost_poll</span> <span class="title">poll</span>;</span></span><br><span class="line"><span class="comment">/* The routine to call when the Guest pings us, or timeout. */</span></span><br><span class="line"><span class="keyword">vhost_work_fn_t</span> handle_kick;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结下vhost侧poll ioeventfd的流程:</p><ol><li><code>vhost_poll_func</code>会让vhost poll ioeventfd，加入到file’s wait queue中</li><li>kvm写ioeventfd通知vhost</li><li>vhost回调<code>vhost_poll_wakeup</code>，将work加入到workqueue中，唤醒vhost工作线程</li><li>vhost工作线程回调<code>vq-&gt;handle_kick</code>，处理vq中的io请求</li></ol><h2 id="irqfd"><a href="#irqfd" class="headerlink" title="irqfd"></a>irqfd</h2><h3 id="qemu侧irqfd的关联"><a href="#qemu侧irqfd的关联" class="headerlink" title="qemu侧irqfd的关联"></a>qemu侧irqfd的关联</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">virtio_pci_set_guest_notifiers</span><br><span class="line">└── kvm_virtio_pci_vector_use</span><br><span class="line">    └── kvm_virtio_pci_irqfd_use</span><br><span class="line">        ├── virtio_queue_get_guest_notifier</span><br><span class="line">        └── kvm_irqchip_add_irqfd_notifier_gsi</span><br><span class="line">            └── kvm_irqchip_assign_irqfd</span><br><span class="line">                └── kvm_vm_ioctl(s, KVM_IRQFD, &amp;irqfd)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vhost_virtqueue_start</span><br><span class="line">└── vhost_virtqueue_mask</span><br><span class="line">    ├── event_notifier_get_wfd(virtio_queue_get_guest_notifier(vvq))</span><br><span class="line">    └── vhost_kernel_set_vring_call[vhost_set_vring_call]</span><br><span class="line">        └── vhost_kernel_call(dev, VHOST_SET_VRING_CALL, file)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">VirtQueue</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">        ...</span><br><span class="line">        EventNotifier guest_notifier</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由上述代码可知，qemu侧通过guest_notifier的irqfd，将vhost与kvm关联了起来；</p><ul><li>kvm负责poll irqfd，然后给vm注入中断</li><li>vhost在更新完used ring后，写irqfd来通知kvm注入中断</li></ul><h3 id="kvm侧irqfd处理"><a href="#kvm侧irqfd处理" class="headerlink" title="kvm侧irqfd处理"></a>kvm侧irqfd处理</h3><p>参考<a href="/2021/10/27/Dive-into-irqfd-KVM-side-mechanism/">Dive into irqfd(KVM side) mechanism</a>即可。</p><h3 id="vhost侧irqfd处理"><a href="#vhost侧irqfd处理" class="headerlink" title="vhost侧irqfd处理"></a>vhost侧irqfd处理</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">vhost_vring_ioctl</span><span class="params">(struct vhost_dev *d, <span class="keyword">unsigned</span> <span class="keyword">int</span> ioctl, <span class="keyword">void</span> __user *argp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">switch</span> (ioctl) &#123;</span><br><span class="line">        ...</span><br><span class="line"><span class="keyword">case</span> VHOST_SET_VRING_CALL:</span><br><span class="line"><span class="keyword">if</span> (copy_from_user(&amp;f, argp, <span class="keyword">sizeof</span> f)) &#123;</span><br><span class="line">r = -EFAULT;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">ctx = f.fd == VHOST_FILE_UNBIND ? <span class="literal">NULL</span> : eventfd_ctx_fdget(f.fd);</span><br><span class="line"><span class="keyword">if</span> (IS_ERR(ctx)) &#123;</span><br><span class="line">r = PTR_ERR(ctx);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">swap(ctx, vq-&gt;call_ctx.ctx);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">        ...</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>qemu调用<code>VHOST_SET_VRING_CALL</code>，将irqfd传递给vhost</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* This actually signals the guest, using eventfd. */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vhost_signal</span><span class="params">(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/* Signal the Guest tell them we used something up. */</span></span><br><span class="line"><span class="keyword">if</span> (vq-&gt;call_ctx.ctx &amp;&amp; vhost_notify(dev, vq))</span><br><span class="line">eventfd_signal(vq-&gt;call_ctx.ctx, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL_GPL(vhost_signal);</span><br></pre></td></tr></table></figure><p>vhost在处理完io请求，并更新used ring后，调用<code>vhost_signal</code>，触发irqfd的写；kvm poll到后，就会给vm注入中断。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将结合qemu与linux源码，解析vhost中ioeventfd与irqfd相关内容。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/tags/QEMU-KVM/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe command retry机制</title>
    <link href="http://liujunming.github.io/2024/07/07/Notes-about-NVMe-command-retry%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2024/07/07/Notes-about-NVMe-command-retry机制/</id>
    <published>2024-07-07T13:04:08.000Z</published>
    <updated>2024-07-07T13:57:13.439Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下NVMe协议中的command retry机制。<a id="more"></a><br><img src="/images/2024/07/007.jpg" alt></p><p><img src="/images/2024/07/008.jpg" alt></p><p><img src="/images/2024/07/009.jpg" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NVME_SC_DNR         = <span class="number">0x4000</span>,</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">nvme_complete_rq</span><span class="params">(struct request *req)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">switch</span> (nvme_decide_disposition(req)) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">case</span> RETRY:</span><br><span class="line">        nvme_retry_req(req);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> nvme_disposition &#123;</span><br><span class="line">    COMPLETE,</span><br><span class="line">    RETRY,</span><br><span class="line">    FAILOVER,</span><br><span class="line">    AUTHENTICATE,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">enum</span> nvme_disposition <span class="title">nvme_decide_disposition</span><span class="params">(struct request *req)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (likely(nvme_req(req)-&gt;status == <span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> COMPLETE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((nvme_req(req)-&gt;status &amp; <span class="number">0x7ff</span>) == NVME_SC_AUTH_REQUIRED)</span><br><span class="line">        <span class="keyword">return</span> AUTHENTICATE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (blk_noretry_request(req) ||</span><br><span class="line">        (nvme_req(req)-&gt;status &amp; NVME_SC_DNR) ||</span><br><span class="line">        nvme_req(req)-&gt;retries &gt;= nvme_max_retries)</span><br><span class="line">        <span class="keyword">return</span> COMPLETE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (req-&gt;cmd_flags &amp; REQ_NVME_MPATH) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nvme_is_path_error(nvme_req(req)-&gt;status) ||</span><br><span class="line">            blk_queue_dying(req-&gt;q))</span><br><span class="line">            <span class="keyword">return</span> FAILOVER;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (blk_queue_dying(req-&gt;q))</span><br><span class="line">            <span class="keyword">return</span> COMPLETE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> RETRY;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> u8 nvme_max_retries = <span class="number">5</span>;</span><br><span class="line">module_param_named(max_retries, nvme_max_retries, byte, <span class="number">0644</span>);</span><br><span class="line">MODULE_PARM_DESC(max_retries, <span class="string">"max number of retries a command may have"</span>);</span><br></pre></td></tr></table></figure><p>默认情况下，每个command最多retry5次。</p><hr><p>参考资料:</p><ol><li>NVMe 1.3 spec</li><li>Linux kernel source code</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下NVMe协议中的command retry机制。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about VIRTIO_F_EVENT_IDX feature</title>
    <link href="http://liujunming.github.io/2024/07/06/Notes-about-VIRTIO-F-EVENT-IDX-feature/"/>
    <id>http://liujunming.github.io/2024/07/06/Notes-about-VIRTIO-F-EVENT-IDX-feature/</id>
    <published>2024-07-06T05:03:38.000Z</published>
    <updated>2024-07-06T05:33:07.340Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio VIRTIO_F_EVENT_IDX feature相关notes。<a id="more"></a></p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>In many systems used and available buffer notifications involve significant overhead. To mitigate it, each VQ maintains a flag to indicate when it wants to be notified. Remember that the driver’s one is read-only by the device, and the device’s one is read-only by the driver.<br>在许多系统中，已使用和可用的缓冲区通知涉及大量开销。为了减少开销，每个 VQ 都会保留一个标志，用于指示何时需要通知。请记住，驱动程序的标志是设备只读的，而设备的标志是驱动程序只读的。</p><p>We already know all of this, and its use is pretty straightforward. The only thing you need to take care of is the asynchronous nature of this method: The side of the communication that disables or enables it can’t be sure that the other end is going to know the change, so you can miss notifications or to have more than expected.</p><p>这些我们都已经知道了，使用起来也非常简单。唯一需要注意的是该方法的异步性质： 通信的一端禁用或启用它时，无法确定另一端是否会知道这一变化，因此可能会错过通知或出现比预期更多的通知。</p><p>A more effective way of notifications toggle is enabled if the <code>VIRTIO_F_EVENT_IDX</code> feature bit is negotiated by device and driver: Instead of disable them in a binary fashion, driver and device can specify how far the other can progress before a notification is required using an specific descriptor id.<br>如果通过设备和驱动程序协商<code>VIRTIO_F_EVENT_IDX</code>功能位，就能启用更有效的通知切换方法： 与二进制禁用方式不同，驱动程序和设备可以使用特定的描述符 id 来指定对方在需要通知之前可以进行到什么程度。</p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p><img src="/images/2024/07/001.jpg" alt></p><h2 id="Used-Buffer-Notification-Suppression"><a href="#Used-Buffer-Notification-Suppression" class="headerlink" title="Used Buffer Notification Suppression"></a>Used Buffer Notification Suppression</h2><p><img src="/images/2024/07/002.jpg" alt></p><p><img src="/images/2024/07/003.jpg" alt></p><p><img src="/images/2024/07/004.jpg" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>driver往avail ring中写flags与used_event</li><li>当设备写的used_idx &gt;= used_event时，设备MUST发送中断</li><li>当设备写的used_idx &lt; used_event时，设备SHOULD NOT发送中断</li></ul><h2 id="Available-Buffer-Notification-Suppression"><a href="#Available-Buffer-Notification-Suppression" class="headerlink" title="Available Buffer Notification Suppression"></a>Available Buffer Notification Suppression</h2><p><img src="/images/2024/07/005.jpg" alt></p><p><img src="/images/2024/07/006.jpg" alt></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>设备往used ring中写flags与avail_event</li><li>当driver写的avail_idx &gt;= avail_event时，driver MUST kick</li><li>当driver写的avail_idx &lt; avail_event时，driver SHOULD NOT kick</li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.redhat.com/en/blog/virtqueues-and-virtio-ring-how-data-travels" target="_blank" rel="noopener">Virtqueues and virtio ring: How the data travels</a></li><li>virtio 1.3 spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio VIRTIO_F_EVENT_IDX feature相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>深入理解NVMe CMB机制</title>
    <link href="http://liujunming.github.io/2024/06/30/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3NVMe-CMB%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2024/06/30/深入理解NVMe-CMB机制/</id>
    <published>2024-06-30T02:00:52.000Z</published>
    <updated>2024-06-30T10:52:45.680Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下NVMe CMB机制相关notes。<a id="more"></a></p><h2 id="1-What"><a href="#1-What" class="headerlink" title="1. What"></a>1. What</h2><p>CMB（Controller Memory Buffer）是指SSD控制器内部的读写缓冲区，与HMB（Host Memory Buffer）的不同处在于所使用的内存地址位于控制器自己的内存中，而不是位于主机内存中，但它们使用队列的方式都是一样的。</p><p>NVMe CMB是NVMe SSD上的一块内存空间，可以通过PCIe MMIO BAR的方式暴露到主机内存空间中，并可由主机直接读写。这一块内存空间可以用来放置一些NVMe上特定的数据结构（如 SQ，CQ等），也可以将需要传输的数据直接放在CMB中。</p><h2 id="2-CMB-的功能"><a href="#2-CMB-的功能" class="headerlink" title="2. CMB 的功能"></a>2. CMB 的功能</h2><p>CMB 至少需要NVMe设备增加下面两个额外的寄存器，其中CMBLOC表示CMB的位置，CMBSZ表示CMB的大小，另外其中还附带了一些feature的标志位。关于这两个寄存器的细节可以阅读NVMe spec中的相关内容：</p><ul><li>3.1.3.11 Offset 38h: CMBLOC – Controller Memory Buffer Location</li><li>3.1.3.12 Offset 3Ch: CMBSZ – Controller Memory Buffer Size</li></ul><p>下图展示了CMBSZ中所包含的一些feature的介绍:</p><p><img src="/images/2024/06/016.png" alt></p><ol><li>Write Data Support (WDS): 对于将数据从主机写入到设备的命令（如Write），可以直接写入到CMB中</li><li>Read Data Support (RDS): 对于将数据从设备读取到主机的命令（如Read），可以直接从CMB中读取</li><li>PRP SGL List Support (LISTS): 可以将PRP List和SGL List放在CMB中</li><li>Completion Queue Support (CQS): 可以将CQ放在CMB中</li><li>Submission Queue Support (SQS): 可以将SQ放在CMB中</li></ol><p>PRP List的定义如下:<br><img src="/images/2024/06/017.jpg" alt></p><h2 id="3-使用CMB的例子"><a href="#3-使用CMB的例子" class="headerlink" title="3. 使用CMB的例子"></a>3. 使用CMB的例子</h2><h3 id="3-1-将CQ和SQ都放在CMB中"><a href="#3-1-将CQ和SQ都放在CMB中" class="headerlink" title="3.1 将CQ和SQ都放在CMB中"></a>3.1 将CQ和SQ都放在CMB中</h3><p>以写SQ为例，原来是主机先把请求写到内存的SQ，然后写Doorbell通知SSD，然后SSD再从内存中的SQ将命令拷贝过来。</p><p>现在是主机直接写到CMB中的SQ，然后写Doorbell通知SSD。</p><p>两个相比较，后者少了一次one read from the controller to the host，将SQ放在CMB上降低了执行命令的延迟。</p><blockquote><p>Submission Queues in host memory require the controller to perform a PCI Express read from host memory in order to fetch the queue entries. Submission Queues in controller memory enable host software to directly write the entire Submission Queue Entry to the controller’s internal memory space, avoiding one read from the controller to the host. This approach reduces latency in command execution and improves efficiency in a PCI Express fabric topology that may include multiple switches.</p></blockquote><h3 id="3-2-让CMB支持数据传输，优化NIC和NVMe-SSD之间的数据传输"><a href="#3-2-让CMB支持数据传输，优化NIC和NVMe-SSD之间的数据传输" class="headerlink" title="3.2 让CMB支持数据传输，优化NIC和NVMe SSD之间的数据传输"></a>3.2 让CMB支持数据传输，优化NIC和NVMe SSD之间的数据传输</h3><p>原本将数据从NIC发送到SSD需要从内存中转一次，现在不需要，直接发送到CMB里就好。</p><p><img src="/images/2024/06/020.jpg" alt></p><h3 id="3-2-让CMB支持数据传输，优化NVMe-SSD之间的数据传输"><a href="#3-2-让CMB支持数据传输，优化NVMe-SSD之间的数据传输" class="headerlink" title="3.2 让CMB支持数据传输，优化NVMe SSD之间的数据传输"></a>3.2 让CMB支持数据传输，优化NVMe SSD之间的数据传输</h3><p>利用了p2p功能，在NVMe设备之间直接传数据到CMB上即可，完全无需CPU和内存的介入，也不需要Root Complex参与其中。</p><p><img src="/images/2024/06/021.jpg" alt></p><h2 id="4-Spec关键notes"><a href="#4-Spec关键notes" class="headerlink" title="4. Spec关键notes"></a>4. Spec关键notes</h2><p><img src="/images/2024/06/018.jpg" alt></p><h2 id="5-p2p操作序列"><a href="#5-p2p操作序列" class="headerlink" title="5. p2p操作序列"></a>5. p2p操作序列</h2><p><img src="/images/2024/06/019.jpg" alt><br><strong>RDMA网卡利用p2p将数据写入到NVMe盘</strong></p><ol><li>driver计算出要写入到NVMe盘的数据量，从CMB中分配一段连续的buffer</li><li>RDMA利用p2p(在RDMA的MTT中，将VA映射到CMB的MMIO地址即可)将数据写入到CMB中的buffer</li><li>driver往submission queue中下发写盘的命令</li><li>driver更新SQ tail doorbell寄存器</li><li>NVMe controller根据command中的metadata与CMB buffer中的内容，将数据刷到NVMe盘中</li></ol><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>在CMB之前，像SQ、CQ、PRP List、Write Data、Read Data都是放置在HMB(DRAM)中；有了CMB之后，SQ、CQ、PRP List、Write Data、Read Data可以放置在CMB(MMIO)中。</p><p>driver为了效率，将与NVMe controller交互的信息，由DRAM移动到了MMIO(CMB)中。CMB的RDS和WDS可以支持p2p，同时SQS这些feature可以提升NVMe controller的执行效率(比如避免了NVMe controller DMA读取SQE这些信息，直接从CMB中读取即可)。</p><hr><p>参考资料:</p><ol><li>NVMe 1.3 spec</li><li><a href="https://zhuanlan.zhihu.com/p/457874205" target="_blank" rel="noopener">NVME CMB详解</a></li><li><a href="https://nvmexpress.org/wp-content/uploads/Session-2-Enabling-the-NVMe-CMB-and-PMR-Ecosystem-Eideticom-and-Mell....pdf" target="_blank" rel="noopener">Enabling the NVMe™ CMB and PMR Ecosystem</a></li><li><a href="https://0x10.sh/controller-memory-buffers" target="_blank" rel="noopener">Controller Memory Buffers</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下NVMe CMB机制相关notes。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Flow Bifurcation vs RSS</title>
    <link href="http://liujunming.github.io/2024/06/23/Flow-Bifurcation-vs-RSS/"/>
    <id>http://liujunming.github.io/2024/06/23/Flow-Bifurcation-vs-RSS/</id>
    <published>2024-06-23T12:30:14.000Z</published>
    <updated>2024-06-23T13:00:05.922Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/2024/06/23/Notes-about-Flow-Bifurcation-mechanism/">Flow Bifurcation</a>与<a href="/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/">RSS(Receive Side Scaling)</a>都会控制网络流量到指定RX Queue，那么两者的区别又是什么呢？<a id="more"></a></p><blockquote><p>RSS is trying to spread incoming packets across cores while directing packets from common flows to the same core. Unlike RSS, Intel Ethernet Flow Director is trying to establish a unique association between flows and the core with the consuming application. Indexing solely by a hash won’t do that. Distinct flows must be uniquely characterized with a high probability which cannot be accomplished by a hash alone.</p></blockquote><p>RSS只是为了load balance，无法做到queue隔离；比如dpdk的某些流量会导向RX Q1，但是如果只基于RSS的话，根据hash值的计算，内核协议栈的某些流量也会导向RX Q1；此时dpdk的网络流量就会与内核协议栈的流量导向到相同RX Queue，就无法做到queue隔离了。</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/intel-ethernet-flow-director.pdf" target="_blank" rel="noopener">Introduction to Intel® Ethernet Flow Director and Memcached Performance</a></li><li><a href="https://zhuanlan.zhihu.com/p/544596830" target="_blank" rel="noopener">浅谈RSS 和 Flow Director</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/training/setting-up-intel-ethernet-flow-director.html" target="_blank" rel="noopener">How to Set Up Intel® Ethernet Flow Director</a></li><li><a href="https://blog.csdn.net/Rong_Toa/article/details/108987658" target="_blank" rel="noopener">网卡多队列：RPS、RFS、RSS、Flow Director（DPDK支持）</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/2024/06/23/Notes-about-Flow-Bifurcation-mechanism/&quot;&gt;Flow Bifurcation&lt;/a&gt;与&lt;a href=&quot;/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/&quot;&gt;RSS(Receive Side Scaling)&lt;/a&gt;都会控制网络流量到指定RX Queue，那么两者的区别又是什么呢？
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
