<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2022-04-12T08:02:10.673Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>浅谈TLB in Intel CPU</title>
    <link href="http://liujunming.github.io/2022/04/12/%E6%B5%85%E8%B0%88TLB-in-Intel-CPU/"/>
    <id>http://liujunming.github.io/2022/04/12/浅谈TLB-in-Intel-CPU/</id>
    <published>2022-04-12T14:46:51.000Z</published>
    <updated>2022-04-12T08:02:10.673Z</updated>
    
    <content type="html"><![CDATA[<p>本文将结合SDM，浅谈下TLB。<a id="more"></a></p><h2 id="1-TLB"><a href="#1-TLB" class="headerlink" title="1. TLB"></a>1. TLB</h2><h3 id="1-1-Why"><a href="#1-1-Why" class="headerlink" title="1.1 Why"></a>1.1 Why</h3><p>没有TLB的话，每次内存寻址时，都需要访问页表（页表位于内存中），开销较大。TLB缓存了VA(Virtual Address)到PA(Physical Address)的映射，当TLB hit时，就无需从内存中访问页表了。</p><h3 id="1-2-What"><a href="#1-2-What" class="headerlink" title="1.2 What"></a>1.2 What</h3><p>The upper bits of a linear address (called the <strong>page number</strong>) determine the upper bits of the physical address (called the <strong>page frame</strong>); the lower bits of the linear address (called the page offset) determine the lower bits of the physical address. The boundary between the page number and the page offset is determined by the <strong>page size</strong>. </p><p><img src="/images/2022/04/67.PNG" alt></p><h2 id="2-全局TLB-flush"><a href="#2-全局TLB-flush" class="headerlink" title="2. 全局TLB flush"></a>2. 全局TLB flush</h2><p>每次进程切换时，需要更换CR3寄存器，flush掉全部的TLB Entries，开销较大。为此，硬件上引入了<a href="/2022/04/12/浅谈TLB-in-Intel-CPU/#3-Global-page">Global page</a>与<a href="/2022/04/12/浅谈TLB-in-Intel-CPU/#4-PCID">PCID</a>机制，避免进程切换时flush掉所有的TLB Entries。</p><h2 id="3-Global-page"><a href="#3-Global-page" class="headerlink" title="3. Global page"></a>3. Global page</h2><p>Linux内存管理中，内核空间是所有进程共享的，每个进程有自己独立的用户空间。进程切换时，是否可以不flush掉内核空间的TLB Entries呢？当然可以，Global page正是为此而生。</p><p>The Intel-64 and IA-32 architectures also allow for global pages when the PGE flag (bit 7) is 1 in CR4. If the G flag(bit 8) is 1 in a paging-structure entry that maps a page (either a PTE or a paging-structure entry in which the PS flag is 1), any TLB entry cached for a linear address using that paging-structure entry is considered to be global.</p><h2 id="4-PCID"><a href="#4-PCID" class="headerlink" title="4. PCID"></a>4. PCID</h2><p>以前，是以(VA)这个一元组为key来唯一索引TLB Entry。PCID(Process-Context Identifier)的引入，硬件以(VA,进程信息)来唯一索引TLB Entry。这样，进程切换时，无需刷掉TLB Entries了，因为不同进程的(VA,进程信息)二元组是不同的。</p><p><img src="/images/2022/04/68.PNG" alt></p><h3 id="4-1-Overlap-between-PCID-and-Global-page"><a href="#4-1-Overlap-between-PCID-and-Global-page" class="headerlink" title="4.1 Overlap between PCID and Global page"></a>4.1 Overlap between PCID and Global page</h3><p>A logical processor may use a global TLB entry to translate a linear address, even if the TLB entry is associated with a PCID different from the current PCID.</p><h2 id="5-TLB-shootdown"><a href="#5-TLB-shootdown" class="headerlink" title="5. TLB shootdown"></a>5. TLB shootdown</h2><p><img src="/images/2022/04/69.PNG" alt></p><h2 id="6-VPID"><a href="#6-VPID" class="headerlink" title="6. VPID"></a>6. VPID</h2><p>虚拟化下，以(VA,进程信息)已经不能唯一索引TLB Entry了，因为不同virtual processors间，可能产生相同的(VA,进程信息)二元组，为此，在同一个物理CPU上，不同vCPU调度时，Hypervisor需要flush掉所有的TLB Entries。为此，VPID(Virtual-Processor IDentifier)应运而生。以(VA,进程信息，virtual processor信息)这个三元组来唯一索引TLB Entry。这样，在同一个物理CPU上，不同vCPU调度时，Hypervisor无需flush掉TLB Entries。</p><p>当然，VPID与PCID的使用不是绑定的，要看具体(Hypervisor和Guest OS)的实现了。例如，Hypervisor可以使用VPID，Guest OS不使用PCID，那么，此刻硬件是以(VA,virtual processor信息)来索引TLB Entry了。在同一个物理CPU上，不同vCPU调度时，Hypervisor无需flush掉TLB Entries，但是，在Guest OS中，每次进程切换时，Guest OS需要Flush掉TLB Entries来保证正确性。</p><p><img src="/images/2022/04/70.PNG" alt></p><h2 id="7-Rethinking-Protection-Keys"><a href="#7-Rethinking-Protection-Keys" class="headerlink" title="7. Rethinking Protection Keys"></a>7. Rethinking Protection Keys</h2><p>之前已经介绍过了<a href="/2020/03/07/Introduction-to-pkeys/">PKU</a>与<a href="/2022/02/27/Introduction-to-PKS/">PKS</a>，这里，从TLB的视角，重新看下Protection Keys。</p><p>Memory Protection Keys (pkeys) are an extension to existing page-based memory permissions. Normal page permissions using page tables require expensive system calls and TLB invalidations when changing permissions. Memory Protection Keys provide a mechanism for changing protections without requiring modification of the page tables on every permission change.</p><p>Protection Keys是无需flush TLB的。留下的open是：</p><ol><li>通过Protection Keys更改permissions，最终硬件是否会更改物理TLB Entry中的access rights位呢?</li><li>如果1成立的话，对于Multiple Processors，是否需要类似于propagating这个过程呢？这个过程是由软件or硬件来完成呢？</li></ol><p>这些问题我当前并不知道答案，等待后续的更新。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将结合SDM，浅谈下TLB。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Memory Type virtualization in VT-x</title>
    <link href="http://liujunming.github.io/2022/04/10/Memory-type-virtualization-in-VT-x/"/>
    <id>http://liujunming.github.io/2022/04/10/Memory-type-virtualization-in-VT-x/</id>
    <published>2022-04-09T16:02:22.000Z</published>
    <updated>2022-04-10T03:56:09.328Z</updated>
    
    <content type="html"><![CDATA[<p>已经介绍过了native下的<a href="/2022/04/09/Memory-Cache-Control-Memory-Type/">Memory Type</a>，那么，虚拟化下的Memory Type又是何般境地呢？<a id="more"></a></p><h2 id="1-History"><a href="#1-History" class="headerlink" title="1. History"></a>1. History</h2><p>前辈们在影子页表时代探索过Memory Type的虚拟化。可以参考<a href="http://www-archive.xenproject.org/files/xensummitboston08/Cache-Virtualization.pdf" target="_blank" rel="noopener">Maintaining cache coherency</a>和<a href="https://vimeo.com/12847592" target="_blank" rel="noopener">Disheng Su: Cache Attribute Virtualization in Xen</a>。反正我也看不明白细节，但是，需要知道的是：在影子页表时代，Memory Type的虚拟化需要软件做很多事情。当来到EPT页表时代，软件需要做的事情就没有那么多了。</p><p>EPT中的Memory Type在SDM Vol3的28.3.7 EPT and Memory Typing有详细的介绍。</p><h2 id="2-EPT-and-Memory-Typing"><a href="#2-EPT-and-Memory-Typing" class="headerlink" title="2. EPT and Memory Typing"></a>2. EPT and Memory Typing</h2><p>中文翻译转载自<a href="https://tcbbd.moe/ref-and-spec/intel-sdm/sdm-vmx-ch28/" target="_blank" rel="noopener">Intel SDM Chapter 28: VMX Support for Address Translation</a>。</p><p>涉及EPT的Memory Type有两个，首先是walk EPT时，访问EPT各级页表项所采用的Memory Type：</p><ul><li>若<code>CR0.CD = 0</code>，则采用的Memory Type由EPTP的第0-2位决定，取0表示UC，取6表示WB</li><li>若<code>CR0.CD = 1</code>，则采用的Memory Type为UC</li></ul><p>其次是Guest访问内存时，采用的Memory Type，它由两个因素决定：</p><ul><li>EPT Memory Type：即EPT中最后一级页表项的第3-5位，起相当于MTRR的作用<ul><li>取0表示UC，取1表示WC，取4表示WT，取5表示WP，取6表示WB，这与MTRR中Type的含义相同</li><li>此时MTRR完全不起作用</li></ul></li><li>PAT Memory Type：<ul><li>若<code>CR0.PG = 0</code>，则PAT Memory Type为WB</li><li>若<code>CR0.PG = 1</code>，则PAT Memory Type为Guest页表翻译时根据<code>MSR[IA32_PAT]</code>确定的PAT Memory Type</li></ul></li></ul><p>最终产生的Memory Type如下：</p><ul><li>若<code>CR0.CD = 0</code><ul><li>若末级页表项的<code>IPAT = 0</code>，则Memory Type就是将EPT Memory Type当做MTRR Memory Type和PAT Memory Type合并后的结果</li><li>若末级页表项的<code>IPAT = 1</code>，则Memory Type就是EPT Memory Type</li></ul></li><li>若<code>CR0.CD = 1</code>，则Memory Type为UC</li></ul><p>IPAT即Ignore PAT(末级页表项的第6位)，如下图标注所示：<br><img src="/images/2022/04/66.PNG" alt></p><p>MTRR Memory Type和PAT Memory Type合并后的结果:<br><img src="/images/2022/04/49.PNG" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已经介绍过了native下的&lt;a href=&quot;/2022/04/09/Memory-Cache-Control-Memory-Type/&quot;&gt;Memory Type&lt;/a&gt;，那么，虚拟化下的Memory Type又是何般境地呢？
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
  </entry>
  
  <entry>
    <title>Memory Cache Control:Memory Type</title>
    <link href="http://liujunming.github.io/2022/04/09/Memory-Cache-Control-Memory-Type/"/>
    <id>http://liujunming.github.io/2022/04/09/Memory-Cache-Control-Memory-Type/</id>
    <published>2022-04-09T06:33:48.000Z</published>
    <updated>2022-04-09T15:44:05.209Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about Memory Type in SDM。<a id="more"></a><br>建议读者先看下<a href="/2019/04/20/CPU-Cache/">CPU Cache</a>与<a href="/2019/04/29/CPU-cache一致性问题/">CPU Cache一致性问题</a>。</p><h2 id="1-MESI"><a href="#1-MESI" class="headerlink" title="1. MESI"></a>1. MESI</h2><p><img src="/images/2022/04/41.PNG" alt></p><h2 id="2-Caching-Terminology"><a href="#2-Caching-Terminology" class="headerlink" title="2. Caching Terminology"></a>2. Caching Terminology</h2><p><img src="/images/2022/04/42.PNG" alt></p><p><img src="/images/2022/04/43.PNG" alt></p><h2 id="3-Methods-of-Caching-Available"><a href="#3-Methods-of-Caching-Available" class="headerlink" title="3. Methods of Caching Available"></a>3. Methods of Caching Available</h2><p><img src="/images/2022/04/44.PNG" alt><br><img src="/images/2022/04/45.PNG" alt><br><img src="/images/2022/04/46.PNG" alt></p><h3 id="3-1-Buffering-of-Write-Combining-Memory-Locations"><a href="#3-1-Buffering-of-Write-Combining-Memory-Locations" class="headerlink" title="3.1 Buffering of Write Combining Memory Locations"></a>3.1 Buffering of Write Combining Memory Locations</h3><p><img src="/images/2022/04/48.PNG" alt></p><h3 id="3-2-Choosing-a-Memory-Type"><a href="#3-2-Choosing-a-Memory-Type" class="headerlink" title="3.2 Choosing a Memory Type"></a>3.2 Choosing a Memory Type</h3><p><img src="/images/2022/04/51.PNG" alt><br><img src="/images/2022/04/52.PNG" alt></p><h2 id="4-Cache-Control-Registers-and-Bits"><a href="#4-Cache-Control-Registers-and-Bits" class="headerlink" title="4. Cache Control Registers and Bits"></a>4. Cache Control Registers and Bits</h2><p><img src="/images/2022/04/54.PNG" alt><br><img src="/images/2022/04/53.PNG" alt><br><img src="/images/2022/04/55.PNG" alt><br><img src="/images/2022/04/56.PNG" alt><br><img src="/images/2022/04/57.PNG" alt></p><h2 id="5-MTRR-Memory-Type-Range-Register"><a href="#5-MTRR-Memory-Type-Range-Register" class="headerlink" title="5. MTRR(Memory Type Range Register)"></a>5. MTRR(Memory Type Range Register)</h2><p><img src="/images/2022/04/60.PNG" alt></p><p><img src="/images/2022/04/61.PNG" alt></p><p><img src="/images/2022/04/62.PNG" alt></p><h2 id="6-PAT-Page-Attribute-Table"><a href="#6-PAT-Page-Attribute-Table" class="headerlink" title="6. PAT(Page Attribute Table)"></a>6. PAT(Page Attribute Table)</h2><p><img src="/images/2022/04/63.PNG" alt></p><p><img src="/images/2022/04/64.PNG" alt><br><img src="/images/2022/04/65.PNG" alt></p><h2 id="7-Precedence-of-Cache-Controls"><a href="#7-Precedence-of-Cache-Controls" class="headerlink" title="7. Precedence of Cache Controls"></a>7. Precedence of Cache Controls</h2><p>考虑如下情况，一个物理页面，被MTRR和PAT同时设置了不同的Memory Type，那么，最终有效的Memory Type(Effective Memory Type)是什么呢？其实就是按照一套优先级规则，推导出Effective Memory Type。规则如下：<br><img src="/images/2022/04/58.PNG" alt><br><img src="/images/2022/04/59.PNG" alt></p><h3 id="7-1-Effective-Memory-Type"><a href="#7-1-Effective-Memory-Type" class="headerlink" title="7.1 Effective Memory Type"></a>7.1 Effective Memory Type</h3><p>按照优先级规则推导出的结果如下：<br><img src="/images/2022/04/49.PNG" alt><br><img src="/images/2022/04/50.PNG" alt></p><h2 id="8-Corner-case"><a href="#8-Corner-case" class="headerlink" title="8. Corner case"></a>8. Corner case</h2><p>Mark下SDM中的Corner case吧。</p><h3 id="8-1-case1"><a href="#8-1-case1" class="headerlink" title="8.1 case1"></a>8.1 case1</h3><p><img src="/images/2022/04/47.PNG" alt></p><h3 id="8-2-case2"><a href="#8-2-case2" class="headerlink" title="8.2 case2"></a>8.2 case2</h3><p><img src="/images/2022/04/40.png" alt></p><hr><p>参考资料：</p><ol><li><a href="https://lwn.net/Articles/282250/" target="_blank" rel="noopener">Getting a handle on caching</a></li><li><a href="https://en.wikipedia.org/wiki/Write_combining" target="_blank" rel="noopener">Write combining</a></li><li><a href="https://www.kernel.org/doc/html/latest/x86/pat.html" target="_blank" rel="noopener">PAT (Page Attribute Table)</a></li><li><a href="https://wiki.osdev.org/MTRR" target="_blank" rel="noopener">osdev MTRR</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about Memory Type in SDM。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>一致性DMA映射与流式DMA映射</title>
    <link href="http://liujunming.github.io/2022/04/07/%E4%B8%80%E8%87%B4%E6%80%A7DMA%E6%98%A0%E5%B0%84%E4%B8%8E%E6%B5%81%E5%BC%8FDMA%E6%98%A0%E5%B0%84/"/>
    <id>http://liujunming.github.io/2022/04/07/一致性DMA映射与流式DMA映射/</id>
    <published>2022-04-07T12:47:53.000Z</published>
    <updated>2022-04-07T15:00:59.357Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要转载自:smc的<a href="https://zhuanlan.zhihu.com/p/109919756" target="_blank" rel="noopener">Cache和DMA一致性</a>。mark下一致性DMA映射(Consistent DMA mappings)与流式DMA映射(Streaming DMA mappings)的相关内容。<a id="more"></a></p><h2 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h2><p>我们知道DMA可以帮我们在I/O和主存之间搬运数据，且不需要CPU参与。高速缓存是CPU和主存之间的数据交互的桥梁。而DMA如果和cache之间没有任何关系的话，可能会出现数据不一致。例如，CPU修改了部分数据依然躺在cache中(采用写回机制)。DMA需要将数据从内存搬运到设备I/O上，如果DMA获取的数据是从主存那里，那么就会得到旧的数据。导致程序的不正常运行。这里告诉我们，DMA通过总线获取数据时，应该先检查cache是否命中，如果命中的话，数据应该来自cache而不是主存。但是是否需要先检查cache呢？这取决于硬件设计。</p><p><img src="/images/2022/04/37.jpg" alt></p><h3 id="1-1-Terms"><a href="#1-1-Terms" class="headerlink" title="1.1 Terms"></a>1.1 Terms</h3><p>cache hit与write hit的定义：<br><img src="/images/2022/04/39.PNG" alt></p><h2 id="2-总线监视技术"><a href="#2-总线监视技术" class="headerlink" title="2. 总线监视技术"></a>2. 总线监视技术</h2><p>什么是总线监视技术呢？其实就是为了解决以上问题提出的技术，cache控制器会监视总线上的每一条内存访问，然后检查是否命中。根据命中情况做出下一步操作。总线监视对于软件来说是透明的，软件不需要任何干涉即可避免不一致问题。但是，并不是所有的硬件都支持总线监视(<strong>x86_64硬件保证了DMA一致性</strong>)，同时操作系统应该兼容不同的硬件。因此在不支持总线监视的情况下，我们在软件上如何避免问题呢？<br>主要有两种方法：</p><ul><li>一致性DMA映射</li><li>流式DMA映射</li></ul><h2 id="3-一致性DMA映射"><a href="#3-一致性DMA映射" class="headerlink" title="3. 一致性DMA映射"></a>3. 一致性DMA映射</h2><p>当使用DMA时，需要在内存中申请一段内存当做buffer。为了避免cache的影响，可以将这段内存映射nocache，即不使用cache。映射的最小单位是4KB，因此在内存映射上至少4KB是nocahe的。这种方法简单实用，但是缺点也很明显：如果只是偶尔使用DMA，大部分都是使用数据的话，会由于nocache导致性能损失。这也是Linux系统中<code>dma_alloc_coherent()</code>接口的实现方法。</p><h2 id="4-流式DMA映射"><a href="#4-流式DMA映射" class="headerlink" title="4. 流式DMA映射"></a>4. 流式DMA映射</h2><p>为了充分使用cache带来的好处。内存映射依然采用cache的方式。但是需要格外小心。根据DMA传输方向的不同，采取不同的措施。</p><ol><li>如果DMA负责从I/O读取数据到内存(DMA Buffer)中，那么在DMA传输之前，可以invalid DMA Buffer地址范围的高速缓存。在DMA传输完成后，程序读取数据不会由于cache hit导致读取过时的数据。</li><li>如果DMA负责把内存(DMA Buffer)数据发送到I/O设备，那么在DMA传输之前，可以clean DMA Buffer地址范围的高速缓存，clean的作用是写回cache中修改的数据。在DMA传输时，不会把主存中的过时数据发送到I/O设备。</li></ol><p><strong>注意，在DMA传输没有完成期间CPU不要访问DMA Buffer</strong>。例如以上的第一种情况中，如果DMA传输期间CPU访问DMA Buffer，当DMA传输完成时。CPU读取的DMA Buffer由于cache hit导致取法获取最终的数据。同样，第二情况下，在DMA传输期间，如果CPU试图修改DMA Buffer，如果cache采用的是写回机制，那么最终写到I/O设备的数据依然是之前的旧数据。所以，这种使用方法编程开发人员应该格外小心。这也是Linux系统中流式DMA映射<code>dma_map_single()</code>接口的实现方法。</p><h3 id="4-1-DMA-Buffer-cacheline对齐要求"><a href="#4-1-DMA-Buffer-cacheline对齐要求" class="headerlink" title="4.1 DMA Buffer cacheline对齐要求"></a>4.1 DMA Buffer cacheline对齐要求</h3><p>假设我们有2个全局变量temp和buffer，buffer用作DMA缓存。初始值temp为5。temp和buffer变量毫不相关。可能buffer是当前DMA操作进程使用的变量，temp是另外一个无关进程使用的全局变量。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> temp = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">char</span> buffer[<span class="number">64</span>] = &#123; <span class="number">0</span> &#125;;</span><br></pre></td></tr></table></figure></p><p>假设，cacheline大小是64字节。那么temp变量和buffer位于同一个cacheline，buffer横跨两个cacheline。</p><p><img src="/images/2022/04/38.png" alt><br>假设现在想要启动DMA从外设读取数据到buffer中。我们进行如下操作：</p><ol><li>按照上一节的理论，我们先invalid buffer对应的2行cacheline。</li><li>启动DMA传输。</li><li>当DMA传输到buff[3]时，程序改写temp的值为6。temp的值和buffer[0]-buffer[59]的值会被缓存到cache中，并且标记dirty bit。</li><li>DMA传输还在继续，当传输到buff[50]的时候，其他程序可能读取数据导致temp变量所在的cacheline需要替换，由于cacheline是dirty的。所以cacheline的数据需要写回。此时，将temp数据写回，顺便也会将buffer[0]-buffer[59]的值写回。</li></ol><p>在第4步中，就出现了问题。由于写回导致DMA传输的部分数据(buff[3]-buffer[49])被改写(改写成了没有DMA传输前的值)。这不是我们想要的结果。因此，为了避免出现这种情况。我们应该保证DMA Buffer不会跟其他数据共享cacheline。所以我们要求DMA Buffer首地址必须cacheline对齐，并且buffer的大小也cacheline对齐。这样就不会跟其他数据共享cacheline。也就不会出现这样的问题。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>x86_64硬件保证了DMA一致性，无需考虑一致性DMA映射与流式DMA映射。</p><p>流式DMA映射根据数据方向对cache进行”flush/invalid”，既保证了数据一致性，也避免了完全关闭cache带来的性能影响。既然如此，为什么不抛弃一致性DMA映射，全面拥抱“更强大”的流式DMA映射呢？<br>考虑如下情况：当CPU和DMA需要频繁的操作一块内存区域的时候，如果采用流式DMA映射的话，需要频繁的”cache flush/invalid”操作(没有cache hit或者write hit的话，cache存在的意义就不大了)，而刷cache是比较耗时的，就会导致开销比较大。这个时候，更适合采用一致性DMA映射。</p><hr><p>参考资料：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/109919756" target="_blank" rel="noopener">Cache和DMA一致性</a></li><li><a href="https://blog.csdn.net/zifehng/article/details/120615394" target="_blank" rel="noopener">对一致性DMA映射与流式DMA映射的一些粗浅认识</a></li><li><a href="https://zhuanlan.zhihu.com/p/55289896" target="_blank" rel="noopener">对流式DMA和一致性DMA的认识</a></li><li><a href="https://www.kernel.org/doc/Documentation/DMA-API-HOWTO.txt" target="_blank" rel="noopener">Dynamic DMA mapping Guide</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要转载自:smc的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/109919756&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Cache和DMA一致性&lt;/a&gt;。mark下一致性DMA映射(Consistent DMA mappings)与流式DMA映射(Streaming DMA mappings)的相关内容。
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>每周分享第26期</title>
    <link href="http://liujunming.github.io/2022/04/05/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB%E7%AC%AC26%E6%9C%9F/"/>
    <id>http://liujunming.github.io/2022/04/05/每周分享第26期/</id>
    <published>2022-04-05T10:05:48.000Z</published>
    <updated>2022-04-07T15:00:59.358Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Universal-Chiplet-Interconnect-Express-UCIe-®"><a href="#Universal-Chiplet-Interconnect-Express-UCIe-®" class="headerlink" title="Universal Chiplet Interconnect Express (UCIe)®"></a>Universal Chiplet Interconnect Express (UCIe)®</h3><p>Universal Chiplet Interconnect Express (UCIe)® 是一种开放的行业标准互连，可在芯粒（Chiplet）之间提供高带宽、低延迟、节能且具有成本效益的封装连接。它解决了跨越云、边缘、企业、5G、汽车、高性能计算和手持领域的整个计算领域对计算、内存、存储和连接的增长需求。UCIe 提供了封装来自不同厂家芯片的能力，包括不同的晶圆厂、不同的设计和不同的封装技术。</p><p>这套标准将让不同制造商的小芯片之间的互通成为可能，允许不同厂商的芯片进行混搭。<br><a id="more"></a></p><p><a href="https://www.uciexpress.org/" target="_blank" rel="noopener">https://www.uciexpress.org/</a><br><a href="https://www.uciexpress.org/_files/ugd/0c1418_c5970a68ab214ffc97fab16d11581449.pdf" target="_blank" rel="noopener">Universal Chiplet Interconnect Express (UCIe)®: Building an open chiplet ecosystem</a><br><a href="https://www.eet-china.com/mp/a117777.html" target="_blank" rel="noopener">UCIe标准白皮书：构建开放的芯粒生态系统</a><br><a href="https://baijiahao.baidu.com/s?id=1726199707747793872&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">英特尔、AMD、Arm 等为小芯片互连制定 UCIe 标准</a></p><h3 id="sandboxie"><a href="#sandboxie" class="headerlink" title="sandboxie"></a>sandboxie</h3><p><a href="https://www.bilibili.com/video/BV1CD4y1X7hj" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1CD4y1X7hj</a></p><h3 id="王兴：绝大多数的失败，是基本功出了问题"><a href="#王兴：绝大多数的失败，是基本功出了问题" class="headerlink" title="王兴：绝大多数的失败，是基本功出了问题"></a>王兴：绝大多数的失败，是基本功出了问题</h3><p><a href="https://m.sohu.com/a/421559857_465413" target="_blank" rel="noopener">https://m.sohu.com/a/421559857_465413</a><br>本文总结了5条美团人才成长方法论，粗看不像常规动作，细看却是扎实的常识，更是基本功，触及人才培养的本质。</p><ul><li>苦练基本功</li><li>标杆学习</li><li>长线思考</li><li>结构化思考</li><li>建设性反馈</li></ul><h3 id="清华数学系学霸谈教育的意义"><a href="#清华数学系学霸谈教育的意义" class="headerlink" title="清华数学系学霸谈教育的意义"></a>清华数学系学霸谈教育的意义</h3><p><a href="https://www.bilibili.com/video/BV1y34y1b716" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1y34y1b716</a></p><p>教育的意义是什么？就是你忘掉你所有学习的东西之后，剩下的东西。能力塑造的过程：我不需要你会背中国上下五千年，但我需要你有十个小时之内，把这五千年给我学明白的能力。因为我需要你用这个能力去帮我做其他的事情。这就是我们教育的目的，授人以渔。就是你去买个菜，你要微积分干嘛，我不要你学会微积分，但你要有学会微积分的能力。</p><h3 id="职位的思考"><a href="#职位的思考" class="headerlink" title="职位的思考"></a>职位的思考</h3><p>如果你只考虑到自己的本职工作，那你的替代性是很高的。一定要超越你的职位，要有开放的心态，避免职位给你带来的思维上的短板。</p><h3 id="哲学术语"><a href="#哲学术语" class="headerlink" title="哲学术语"></a>哲学术语</h3><p>大难至易 大繁至简</p><p>Simplicity is the ultimate form of sophistication.</p><p>“大繁至简，极境至臻”，是中国人追求的境界，但要达到这样的境界却是对无数设计者最顶峰的挑战。<br><a href="https://www.xuehua.us/a/5eb57dba86ec4d1316e2918e" target="_blank" rel="noopener">https://www.xuehua.us/a/5eb57dba86ec4d1316e2918e</a></p><p>殊途同归</p><h3 id="我的-9-年开源之路：395-Patch、20-Feature，背后只有努力与热爱"><a href="#我的-9-年开源之路：395-Patch、20-Feature，背后只有努力与热爱" class="headerlink" title="我的 9 年开源之路：395 Patch、20+Feature，背后只有努力与热爱"></a>我的 9 年开源之路：395 Patch、20+Feature，背后只有努力与热爱</h3><p><a href="https://xie.infoq.cn/article/9abe905c8867f3a76c71da3f5" target="_blank" rel="noopener">https://xie.infoq.cn/article/9abe905c8867f3a76c71da3f5</a><br>腾源会也第一时间采访了李万鹏，以求向开源爱好者们传递这位开源达人成长背后的感悟及点滴。</p><h3 id="麦卡锡主义"><a href="#麦卡锡主义" class="headerlink" title="麦卡锡主义"></a>麦卡锡主义</h3><p><a href="https://www.bilibili.com/video/BV1ei4y157V4" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1ei4y157V4</a></p><p>麦卡锡主义（McCarthyism）广义上是指用大规模的宣传和不加以区分的指责，特别是没有足够证据的指控，造成对人格和名誉的诽谤。<br><a href="https://zh.wikipedia.org/wiki/%E9%BA%A6%E5%8D%A1%E9%94%A1%E4%B8%BB%E4%B9%89" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E9%BA%A6%E5%8D%A1%E9%94%A1%E4%B8%BB%E4%B9%89</a></p><h3 id="程序员做饭指南"><a href="#程序员做饭指南" class="headerlink" title="程序员做饭指南"></a>程序员做饭指南</h3><p><a href="https://github.com/Anduin2017/HowToCook" target="_blank" rel="noopener">https://github.com/Anduin2017/HowToCook</a></p><h3 id="程序员买房指南"><a href="#程序员买房指南" class="headerlink" title="程序员买房指南"></a>程序员买房指南</h3><p><a href="https://github.com/houshanren/hangzhou_house_knowledge" target="_blank" rel="noopener">https://github.com/houshanren/hangzhou_house_knowledge</a></p><h3 id="程序员“上大学”指南"><a href="#程序员“上大学”指南" class="headerlink" title="程序员“上大学”指南"></a>程序员“上大学”指南</h3><p><a href="https://github.com/PKUanonym/REKCARC-TSC-UHT" target="_blank" rel="noopener">清华大学</a><br><a href="https://github.com/tongtzeho/PKUCourse" target="_blank" rel="noopener">北京大学</a><br><a href="https://github.com/QSCTech/zju-icicles" target="_blank" rel="noopener">浙江大学</a><br><a href="https://github.com/USTC-Resource/USTC-Course" target="_blank" rel="noopener">中国科学技术大学</a><br><a href="https://github.com/sysuexam/SYSU-Exam" target="_blank" rel="noopener">中山大学</a></p><h3 id="程序员考公指南"><a href="#程序员考公指南" class="headerlink" title="程序员考公指南"></a>程序员考公指南</h3><p><a href="https://github.com/coder2gwy/coder2gwy" target="_blank" rel="noopener">https://github.com/coder2gwy/coder2gwy</a></p><h3 id="程序员“斗图”指南"><a href="#程序员“斗图”指南" class="headerlink" title="程序员“斗图”指南"></a>程序员“斗图”指南</h3><p><a href="https://github.com/getActivity/EmojiPackage" target="_blank" rel="noopener">https://github.com/getActivity/EmojiPackage</a></p><h3 id="筋膜抢"><a href="#筋膜抢" class="headerlink" title="筋膜抢"></a>筋膜抢</h3><p><a href="https://www.bilibili.com/video/BV1xA411N72M" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1xA411N72M</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Universal-Chiplet-Interconnect-Express-UCIe-®&quot;&gt;&lt;a href=&quot;#Universal-Chiplet-Interconnect-Express-UCIe-®&quot; class=&quot;headerlink&quot; title=&quot;Universal Chiplet Interconnect Express (UCIe)®&quot;&gt;&lt;/a&gt;Universal Chiplet Interconnect Express (UCIe)®&lt;/h3&gt;&lt;p&gt;Universal Chiplet Interconnect Express (UCIe)® 是一种开放的行业标准互连，可在芯粒（Chiplet）之间提供高带宽、低延迟、节能且具有成本效益的封装连接。它解决了跨越云、边缘、企业、5G、汽车、高性能计算和手持领域的整个计算领域对计算、内存、存储和连接的增长需求。UCIe 提供了封装来自不同厂家芯片的能力，包括不同的晶圆厂、不同的设计和不同的封装技术。&lt;/p&gt;
&lt;p&gt;这套标准将让不同制造商的小芯片之间的互通成为可能，允许不同厂商的芯片进行混搭。&lt;br&gt;
    
    </summary>
    
      <category term="经验" scheme="http://liujunming.github.io/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="经验" scheme="http://liujunming.github.io/tags/%E7%BB%8F%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Non-Transparent Bridge</title>
    <link href="http://liujunming.github.io/2022/04/05/Notes-about-Non-Transparent-Bridge/"/>
    <id>http://liujunming.github.io/2022/04/05/Notes-about-Non-Transparent-Bridge/</id>
    <published>2022-04-05T05:49:49.000Z</published>
    <updated>2022-04-05T06:46:16.603Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about NTB(Non-Transparent Bridge)。大部分内容转载自:<a href="https://mp.weixin.qq.com/s?__biz=MzU4MTczMDg1Nw==&amp;mid=2247484277&amp;idx=1&amp;sn=fbf2141d7207404c4c03b853d3e27bb2&amp;chksm=fd425433ca35dd25a5fc8433dd069b8e22fa6b2efae8bba2b8a2a84a2983a974cc569a287fa0&amp;mpshare=1&amp;scene=1&amp;srcid=0404PvRlkgNWWUnMFglvMpQH&amp;sharer_sharetime=1649064843142&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AQAbJioxo7WN4x2RsvEn5QY%3D&amp;acctmode=0&amp;pass_ticket=ma00ex2i0F1kMOWK2Porzd5P%2FmNQ7AfIlJ0uq9k3qPYV6Yw%2FRqAI1lQuV4GWNOx5&amp;wx_header=0#rd" target="_blank" rel="noopener">非透明桥 Non-Transparent Bridging</a>。<a id="more"></a></p><h2 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h2><p><img src="/images/2022/04/20.png" alt></p><p>问：系统A的CPU有没有可能直接访问系统B的内存和外设呢？<br>答：有！PCIe Non-Transparent Bridging功能就可以实现，也就是非透明桥！</p><p>我们首先来回顾一下PCIe系统里面，数据包的路由方式。以地址路由为例：<br><img src="/images/2022/04/21.jpg" alt></p><p>PCIe系统里面，每个设备、每个Switch，都有明确的属于自己的地址、地址范围。数据包在系统里面，根据某个确定的地址路由、寻址。每个地址具有唯一性。下面，让我们考虑两个PCIe系统的场景：<br><img src="/images/2022/04/22.jpg" alt></p><p>系统A如果需要访问到另外一个系统B的某个地址，存在两个问题：</p><ol><li>通过什么物理路径？</li><li>假设解决了问题1，同样存在地址冲突的问题：例如A系统访问地址2500h，这个地址在A系统里存在于左下方的Switch下，同时这个2500h地址也存在于B系统里的左下方的Switch下。由系统A的CPU发起的2500h地址访问，到底该访问哪一个呢？</li></ol><p>（<em>为什么两个host系统不能通过透明桥连接？PCIe数据路由是基于地址的，两个系统可能资源分配冲突，这意味着两个设备具有相同的资源分配，因此具有该地址的数据包无法正确路由</em>）</p><p><strong>解决方案是什么？当数据包通过结构从一个系统传输到另一个系统时进行地址转换，这是通过非透明桥（NT）完成的。</strong></p><p>我们来看看传统透明桥的路由：<br><img src="/images/2022/04/23.jpg" alt></p><p>①某个访问地址1500h的数据报文到达上行口P-P桥，P-P桥一看，这个地址在我的窗口范围内，向下行端口转发。<br>②图中高亮的下行端口P-P透明桥，一看，这个地址在我的桥下窗口范围内，继续向下转发。<br>③穿过透明P-P桥的数据报文，地址仍然为1500h。</p><p>对比透明桥，非透明桥的路由如下：<br><img src="/images/2022/04/24.jpg" alt></p><p>①某个访问地址Δ + 1500h的数据报文到达上行口P-P桥，P-P桥一看，这个地址在我的窗口范围内，向下行端口转发。<br>②图中高亮的下行端口P-P透明桥，一看，这个地址在我的桥下窗口范围内，继续向下转发。<br>③穿过非透明P-P桥的数据报文，<em>地址进行了翻译！从Δ +1500h变成1500h</em>。<br>④⑤是数据报文在系统B里的路由。</p><p>有意思了！非透明桥NT经过一个简单的地址翻译，把在A系统中的地址，翻译到了对应的B系统的地址！这里需要注意的是：</p><ol><li>为什么是Δ + 1500h？因为如果是1500h的话，肯定和本系统的1500h冲突。</li><li>Δ + 1500h所在的资源范围是怎么申请的？是非透明桥NT的BAR资源，向系统申请的。</li></ol><p>可以看出：非透明桥和透明桥看上去功能很像，最大的不同是：透明桥是进来什么地址，出去就是什么地址，对于桥上下两侧是“透明”的。非透明桥是有翻译功能的，可以把一个地址翻译成另一个地址。所谓非透明的部分意义也是在于此。</p><h2 id="2-地址转换与内部构造"><a href="#2-地址转换与内部构造" class="headerlink" title="2. 地址转换与内部构造"></a>2. 地址转换与内部构造</h2><p>当数据包通过结构从一个系统传输到另一个系统时进行地址转换，这是通过非透明桥（Non-Transparent）完成的。NT桥到底是怎么进行地址转换的？NT桥的内部构造又是啥样的呢？</p><p><img src="/images/2022/04/25.jpg" alt></p><p>NTB由两个PCIe设备组成，每个设备的配置方式都是Type 0，并通过桥接连接。这两个Type 0 PCI设备被称为两个非透明（NT）端点（也称为NT功能）。如下图：<br><img src="/images/2022/04/26.png" alt></p><p>既然是EP就有6个BAR空间。BAR 0 到BAR 5。所谓BAR（Base Address Register）就是每个EP设备的一个寄存器，这个寄存器会向系统申请一段一定大小的空间地址，系统所有访问这个空间地址的报文，都会被路由到这个EP来处理。</p><p>通常，BAR0是用作映射到EP设备的配置空间，访问BAR0可以映射访问所有的寄存器。而NT的BAR2 到 BAR5通常都是用于NT桥接地址转换用的。如下，我们以BAR 2为例：<br><img src="/images/2022/04/27.png" alt></p><p>NT EP向系统申请了个两段空间，一个是BAR0，所有访问BAR0地址范围的报文都将落入到EP的内存映射寄存器里面去。另一个就是BAR2，这个BAR2的地址空间，我们称为NT窗口。所有访问BAR2地址范围的报文都将进入NT，然后被地址转换！如何转换的呢？<br><img src="/images/2022/04/28.png" alt></p><p>进入NT Window的报文，会根据我们自己设置的NT桥转换基址（Translated Base Addres）做运算，运算之后的地址刚好要等于我们想访问的B系统里面的目标地址（Target Adress）。这样，即使我们加上一定的偏移（Offset），也能顺利转化为B系统里对应偏移的地址。</p><p><img src="/images/2022/04/29.png" alt></p><p>了解完大概的转换原理之后，我们跳出来再看一看这个系统图：<br><img src="/images/2022/04/30.png" alt></p><p>大家注意一下我标注的不同的颜色，从PCIe域以及地址归属来看，NT桥两边的两个EP设备，其实是分别归属于两个系统的（蓝色属于系统A，绿色属于系统B）。至于两个NT EP中间的NT Bridge，那是厂商芯片自己内部的实现。逻辑上在两个系统内都看不到。</p><p>另外，我们前面只讨论了从系统A到系统B穿过NT桥的转换，反之，从系统B到系统A类似，只不多是在B系统里面配置绿色的NT  EP设备而已。我们可以画出如下的地址转换示意图</p><p><img src="/images/2022/04/31.png" alt></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>A PCI-Express non-transparent bridge (NTB) is a point-to-point PCIe bus connecting 2 systems, providing electrical isolation between the two subsystems. A non-transparent bridge is functionally similar to a transparent bridge except that both sides of the bridge have their own independent address domains.  The host on one side of the bridge will not have the visibility of the complete memory or I/O space on the other side of the bridge.  To communicate across the non-transparent bridge, each NTB endpoint has one (or more) apertures exposed to the local system.  Writes to these apertures are mirrored to memory on the remote system.  Communications can also occur through the use of doorbell registers that initiate interrupts to the alternate domain, and scratch-pad registers accessible from both sides.</p><p><em>scratch pad</em>(A small, fast memory for the temporary storage of data)</p><p><img src="/images/2022/04/32.PNG" alt><br><img src="/images/2022/04/33.PNG" alt><br><img src="/images/2022/04/34.PNG" alt></p><p><img src="/images/2022/04/35.PNG" alt><br><img src="/images/2022/04/36.PNG" alt></p><p>再次回到文章开头。<br>问：系统A的CPU有没有可能直接访问系统B的内存和外设呢？<br>答：有！PCIe Non-Transparent Bridging功能就可以实现，也就是非透明桥！</p><hr><p>参考资料:</p><ol><li><a href="https://book.douban.com/subject/4728656/" target="_blank" rel="noopener">PCI Express 体系结构导读</a></li><li><a href="https://www.kernel.org/doc/Documentation/ntb.txt" target="_blank" rel="noopener">Kernel ntb.txt</a></li><li><a href="https://events.static.linuxfound.org/sites/events/files/slides/Linux%20NTB_0.pdf" target="_blank" rel="noopener">Linux NTB</a></li><li><a href="https://lwn.net/Articles/506761/" target="_blank" rel="noopener">PCI-Express Non-Transparent Bridge Support</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU4MTczMDg1Nw==&amp;mid=2247484277&amp;idx=1&amp;sn=fbf2141d7207404c4c03b853d3e27bb2&amp;chksm=fd425433ca35dd25a5fc8433dd069b8e22fa6b2efae8bba2b8a2a84a2983a974cc569a287fa0&amp;mpshare=1&amp;scene=1&amp;srcid=0404PvRlkgNWWUnMFglvMpQH&amp;sharer_sharetime=1649064843142&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AQAbJioxo7WN4x2RsvEn5QY%3D&amp;acctmode=0&amp;pass_ticket=ma00ex2i0F1kMOWK2Porzd5P%2FmNQ7AfIlJ0uq9k3qPYV6Yw%2FRqAI1lQuV4GWNOx5&amp;wx_header=0#rd" target="_blank" rel="noopener">非透明桥 Non-Transparent Bridging (一)</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU4MTczMDg1Nw==&amp;mid=2247484288&amp;idx=1&amp;sn=1c8dff6a288e6089e7eba49f833dfb28&amp;chksm=fd4254c6ca35ddd045368be02885b7375eb39e25ef628e9003a9d3fbc689237775327a90f59e&amp;mpshare=1&amp;scene=1&amp;srcid=04041kgNPltlFUtAEjlxCUxM&amp;sharer_sharetime=1649064850883&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AWzT7MFaawwPXbZ%2BXuad8Tc%3D&amp;acctmode=0&amp;pass_ticket=ma00ex2i0F1kMOWK2Porzd5P%2FmNQ7AfIlJ0uq9k3qPYV6Yw%2FRqAI1lQuV4GWNOx5&amp;wx_header=0#rd" target="_blank" rel="noopener">非透明桥 Non-Transparent Bridging (二)</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU4MTczMDg1Nw==&amp;mid=2247484298&amp;idx=1&amp;sn=f4c4e66c3a4faa27fd0f0157b0a0bbbe&amp;chksm=fd4254ccca35dddaf5fde69be5961d63f5ddc17a24368fe99e634bc9fe1766746e8c15cd93df&amp;mpshare=1&amp;scene=1&amp;srcid=0404eYkmrLSbk8SiK63otkGI&amp;sharer_sharetime=1649064857822&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AYWWAwuG%2Bv5c6UjdEAoiNJ8%3D&amp;acctmode=0&amp;pass_ticket=ma00ex2i0F1kMOWK2Porzd5P%2FmNQ7AfIlJ0uq9k3qPYV6Yw%2FRqAI1lQuV4GWNOx5&amp;wx_header=0#rd" target="_blank" rel="noopener">非透明桥 Non-Transparent Bridging (三)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about NTB(Non-Transparent Bridge)。大部分内容转载自:&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU4MTczMDg1Nw==&amp;amp;mid=2247484277&amp;amp;idx=1&amp;amp;sn=fbf2141d7207404c4c03b853d3e27bb2&amp;amp;chksm=fd425433ca35dd25a5fc8433dd069b8e22fa6b2efae8bba2b8a2a84a2983a974cc569a287fa0&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0404PvRlkgNWWUnMFglvMpQH&amp;amp;sharer_sharetime=1649064843142&amp;amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;amp;exportkey=AQAbJioxo7WN4x2RsvEn5QY%3D&amp;amp;acctmode=0&amp;amp;pass_ticket=ma00ex2i0F1kMOWK2Porzd5P%2FmNQ7AfIlJ0uq9k3qPYV6Yw%2FRqAI1lQuV4GWNOx5&amp;amp;wx_header=0#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;非透明桥 Non-Transparent Bridging&lt;/a&gt;。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe protocol</title>
    <link href="http://liujunming.github.io/2022/04/02/Notes-about-NVMe/"/>
    <id>http://liujunming.github.io/2022/04/02/Notes-about-NVMe/</id>
    <published>2022-04-02T09:43:16.000Z</published>
    <updated>2022-04-09T15:44:05.209Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about the NVMe protocol.<a id="more"></a></p><h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h2><p><img src="/images/2022/04/18.PNG" alt></p><p>传统的SATA硬盘只能支持一个队列，一次只能接受32条数据；而NVMe存储则支持最多64K个队列，每个队列有64K个条目。类似于跑车的例子，SATA就像只有一条车道的公路，可以容纳32辆车；而NVMe就像有6.4万条车道的公路，每条车道都能容纳6.4万辆汽车。</p><h2 id="2-Basics"><a href="#2-Basics" class="headerlink" title="2. Basics"></a>2. Basics</h2><p>Storage protocol designed for the <strong>Non-Volatile Memory</strong></p><ul><li>Defines the commands and data strutures for communication between the host and the storage device</li><li>Can operation over PCIe or Fabrics</li></ul><p><img src="/images/2022/04/15.png" alt><br><img src="/images/2022/04/16.png" alt></p><p>Here are some basic definitions in NVMe protocols. NVMe defines two main types of commands: <strong>Admin Commands</strong> and <strong>I/O Commands</strong>. In I/O operations, commands are placed by the host software into the <strong>Submission Queue (SQ)</strong>, and completion information received from SSD hardware is then placed into an associated <strong>Completion Queue (CQ)</strong> by the controller. NVMe separately designs SQ and CQ pairs for any Admin and I/O commands respectively. The host system maintains only one Admin SQ and its associated Admin CQ for the purpose of storage management and command control, while the host can maintain a maximum of 64K I/O SQs or CQs. The depth of the Admin SQ or CQ is 4K, where the Admin Queue can store at most 4096 entries, while the depth of I/O Queues is 64K. SQ and CQ should work in pairs, and normally one SQ utilizes on one CQ or multiple SQs utilize the same CQ to meet the requirements of high performances in multithread I/O processing. A SQ or CQ is a ring buffer and it is a memory area which is shared with the device that can be accessed by Direct Memory Access (DMA). Moreover, a doorbell is a register of the NVMe device controller to record the head or tail pointer of the ring buffer (SQ or CQ).</p><p><img src="/images/2022/04/19.PNG" alt></p><h2 id="3-Sequences-of-NVMe-over-PCIe"><a href="#3-Sequences-of-NVMe-over-PCIe" class="headerlink" title="3. Sequences of NVMe over PCIe"></a>3. Sequences of NVMe over PCIe</h2><p><img src="/images/2022/04/14.PNG" alt></p><p>A specific command in a NVMe IO request contains concrete read/write messages and an address pointing to the DMA buffer if the IO request is a DMA operation. Once the request is stored in a SQ, the host writes the doorbell and kicks (transfers) the request into the NVMe device so that the device can fetch I/O operations. After an IO request has been completed, the device will subsequently write the success or failure status of the request into a CQ and the device then generates an interrupt request into the host. After the host receives the interrupt and processes the completion entries, it writes to the doorbell to release the completion entries.</p><h2 id="4-NVMe-oF"><a href="#4-NVMe-oF" class="headerlink" title="4. NVMe-oF"></a>4. NVMe-oF</h2><p>NVMe over PCIe 局限在主机的本地盘使用。通过Fabrics（如RDMA或光纤通道）代替PCIe，可帮助主机访问节点外的NVMe SSD资源，NVMe-oF极大地增强了灵活性和扩展性，将NVMe低延时、高并发等特性，从服务器级别，扩展到整个数据中心级别。</p><p><img src="/images/2022/04/17.png" alt></p><p>与NVMe over PCIe相比，NVMe over RDMA在软件开销上的增加很小，可以近似地认为跨网络访问和本地访问的延迟几乎是一样的。</p><h2 id="5-Details"><a href="#5-Details" class="headerlink" title="5. Details"></a>5. Details</h2><p>最权威的资料当然是<a href="https://nvmexpress.org/specifications/" target="_blank" rel="noopener">Spec</a>。<br><a href="https://wiki.osdev.org/NVMe" target="_blank" rel="noopener">osdev</a>的总结也不错。</p><hr><p>参考资料:</p><ol><li>MDev-NVMe: A NVMe Storage Virtualization Solution with Mediated Pass-Through,ATC’18</li><li>NVMe-over-Fabrics Performance Characterization and the Path to Low-Overhead Flash Disaggregation,SYSTOR’17</li><li><a href="https://book.douban.com/subject/34815557/" target="_blank" rel="noopener">Linux开源存储全栈详解：从Ceph到容器存储</a></li><li><a href="https://jishuin.proginn.com/p/763bfbd60dcd" target="_blank" rel="noopener">深入剖析NVMe Over Fabrics</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about the NVMe protocol.
    
    </summary>
    
      <category term="存储" scheme="http://liujunming.github.io/categories/%E5%AD%98%E5%82%A8/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="存储" scheme="http://liujunming.github.io/tags/%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to GPUDirect RDMA</title>
    <link href="http://liujunming.github.io/2022/04/02/Introduction-to-GPUDirect-RDMA/"/>
    <id>http://liujunming.github.io/2022/04/02/Introduction-to-GPUDirect-RDMA/</id>
    <published>2022-04-02T00:16:16.000Z</published>
    <updated>2022-04-02T02:12:07.616Z</updated>
    
    <content type="html"><![CDATA[<p>阅读了<a href="https://developer.aliyun.com/article/603617" target="_blank" rel="noopener">浅析GPU通信技术（下）-GPUDirect RDMA</a> 一文，收获颇丰，故转载到博客中。<a id="more"></a></p><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>当前深度学习模型越来越复杂，计算数据量暴增，对于大规模深度学习训练任务，单机已经无法满足计算要求，多机多卡的分布式训练成为了必要的需求，这个时候多机间的通信成为了分布式训练性能的重要指标。</p><p>本篇文章我们就来谈谈GPUDirect RDMA技术，这是用于加速多机间GPU通信的技术。</p><h2 id="2-RDMA介绍"><a href="#2-RDMA介绍" class="headerlink" title="2. RDMA介绍"></a>2. RDMA介绍</h2><p>我们先来看看RDMA技术是什么？RDMA即Remote DMA，是Remote Direct Memory Access的英文缩写。</p><h3 id="2-1-DMA原理"><a href="#2-1-DMA原理" class="headerlink" title="2.1 DMA原理"></a>2.1 DMA原理</h3><p>在介绍RDMA之前，我们先来复习下DMA技术。</p><p>我们知道DMA（直接内存访问）技术是Offload CPU负载的一项重要技术。DMA的引入，使得原来设备内存与系统内存的数据交换必须要CPU参与，变为交给DMA控制来进行数据传输。</p><p>直接内存访问(DMA)方式，是一种完全由硬件执行I/O交换的工作方式。在这种方式中， DMA控制器从CPU完全接管对总线的控制，数据交换不经过CPU，而直接在内存和IO设备之间进行。DMA工作时，由DMA 控制器向内存发出地址和控制信号，进行地址修改，对传送字的个数计数，并且以中断方式向CPU 报告传送操作的结束。</p><p>使用DMA方式的目的是减少大批量数据传输时CPU 的开销。采用专用DMA控制器(DMAC) 生成访存地址并控制访存过程。优点有操作均由硬件电路实现，传输速度快；CPU 基本不干预，仅在初始化和结束时参与，CPU与外设并行工作，效率高。</p><h3 id="2-2-RMDA原理"><a href="#2-2-RMDA原理" class="headerlink" title="2.2 RMDA原理"></a>2.2 RMDA原理</h3><p>RDMA则是在计算机之间网络数据传输时Offload CPU负载的高吞吐、低延时通信技术。<br><img src="/images/2022/04/07.jpg" alt></p><p>如上图所示，传统的TCP/IP协议，应用程序需要要经过多层复杂的协议栈解析，才能获取到网卡中的数据包，而使用RDMA协议，应用程序可以直接旁路内核获取到网卡中的数据包。</p><p>RDMA可以简单理解为利用相关的硬件和网络技术，服务器1的网卡可以直接读写服务器2的内存，最终达到高带宽、低延迟和低资源利用率的效果。如下图所示，应用程序不需要参与数据传输过程，只需要指定内存读写地址，开启传输并等待传输完成即可。<br><img src="/images/2022/04/08.jpg" alt></p><p>在实现上，RDMA实际上是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过在网卡上将RDMA协议固化于硬件，以及支持零复制网络技术和内核内存旁路技术这两种途径来达到其高性能的远程直接数据存取的目标。</p><ol><li>零复制：零复制网络技术使网卡可以直接与应用内存相互传输数据，从而消除了在应用内存与内核之间复制数据的需要。因此，传输延迟会显著减小。</li><li>内核旁路：内核协议栈旁路技术使应用程序无需执行内核内存调用就可向网卡发送命令。在不需要任何内核内存参与的条件下，RDMA请求从用户空间发送到本地网卡并通过网络发送给远程网卡，这就减少了在处理网络传输流时内核内存空间与用户空间之间环境切换的次数。</li></ol><p>在具体的远程内存读写中，RDMA操作用于读写操作的远程虚拟内存地址包含在RDMA消息中传送，远程应用程序要做的只是在其本地网卡中注册相应的内存缓冲区。远程节点的CPU除在连接建立、注册调用等之外，在整个RDMA数据传输过程中并不提供服务，因此没有带来任何负载。</p><h3 id="2-3-RDMA实现"><a href="#2-3-RDMA实现" class="headerlink" title="2.3 RDMA实现"></a>2.3 RDMA实现</h3><p>如下图RMDA软件栈所示，目前RDMA的实现方式主要分为InfiniBand和Ethernet两种传输网络。而在以太网上，又可以根据与以太网融合的协议栈的差异分为iWARP和RoCE（包括RoCEv1和RoCEv2）。</p><p><img src="/images/2022/04/09.jpg" alt></p><p>其中，InfiniBand是最早实现RDMA的网络协议，被广泛应用到高性能计算中。但是InfiniBand和传统TCP/IP网络的差别非常大，需要专用的硬件设备，承担昂贵的价格。相比之下RoCE和iWARP的硬件成本则要低的多。</p><h2 id="3-GPUDirect-RDMA介绍"><a href="#3-GPUDirect-RDMA介绍" class="headerlink" title="3. GPUDirect RDMA介绍"></a>3. GPUDirect RDMA介绍</h2><h3 id="3-1-原理"><a href="#3-1-原理" class="headerlink" title="3.1 原理"></a>3.1 原理</h3><p>有了前文RDMA的介绍，从下图我们可以很容易明白，所谓GPUDirect RDMA，就是计算机1的GPU可以直接访问计算机2的GPU内存。而在没有这项技术之前，GPU需要先将数据从GPU内存搬移到系统内存，然后再利用RDMA传输到计算机2，计算机2的GPU还要做一次数据从系统内存到GPU内存的搬移动作。GPUDirect RDMA技术进一步减少了GPU通信的数据复制次数，进一步降低了通信延迟。<br><img src="/images/2022/04/10.jpg" alt></p><h3 id="3-2-使用"><a href="#3-2-使用" class="headerlink" title="3.2 使用"></a>3.2 使用</h3><p>需要注意的是，要想使用GPUDirect RDMA，需要保证GPU卡和RDMA网卡在同一个ROOT COMPLEX下，如下图所示：<br><img src="/images/2022/04/11.jpg" alt></p><h3 id="3-3-性能"><a href="#3-3-性能" class="headerlink" title="3.3 性能"></a>3.3 性能</h3><p>Mellanox网卡已经提供了GPUDirect RDMA的支持（既支持InfiniBand传输，也支持RoCE传输）。</p><p>下图分别是使用OSU micro-benchmarks在Mellanox的InfiniBand网卡上测试的延时和带宽数据，可以看到使用GPUDirect RDMA技术后延时大大降低，带宽则大幅提升：<br><img src="/images/2022/04/12.jpg" alt></p><p>下图是一个实际的高性能计算应用的性能数据（使用HOOMD做粒子动态仿真），可以看到随着节点增多，使用GPUDirect RDMA技术的集群的性能有明显提升，最多可以提升至2倍：<br><img src="/images/2022/04/13.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读了&lt;a href=&quot;https://developer.aliyun.com/article/603617&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;浅析GPU通信技术（下）-GPUDirect RDMA&lt;/a&gt; 一文，收获颇丰，故转载到博客中。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to VT-x Preemption Timer</title>
    <link href="http://liujunming.github.io/2022/04/01/Introduction-to-VT-x-Preemption-Timer/"/>
    <id>http://liujunming.github.io/2022/04/01/Introduction-to-VT-x-Preemption-Timer/</id>
    <published>2022-04-01T09:48:14.000Z</published>
    <updated>2022-04-01T13:08:01.255Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自:<a href="https://blog.csdn.net/xelatex_kvm/article/details/17761415" target="_blank" rel="noopener">Intel VT技术中的Preemption Timer</a>。</p><h3 id="1-什么是Preemption-Timer"><a href="#1-什么是Preemption-Timer" class="headerlink" title="1. 什么是Preemption Timer"></a>1. 什么是Preemption Timer</h3><p>Preemption Timer是一种可以周期性使VCPU触发VMExit的一种机制。即设置了Preemption Timer之后，可以使VCPU在指定的TSC cycle(注意文章最后的rate)之后产生一次VMExit。<a id="more"></a></p><p>使用Preemption Timer时需要注意下面两个问题：</p><ol><li>在旧版本的Intel CPU中Preemption Timer是不精确的。在Intel的设计中，Preemption Timer应该是严格和TSC保持一致，但是在Haswell之前的处理器并不能严格保持一致。</li><li>Preemption Timer只有在VCPU进入到non-root mode时才会开始工作，在VCPU进入VMM时或者VCPU被调度出CPU时，其值都不会变化。</li></ol><h3 id="2-如何使用Preemption-Timer"><a href="#2-如何使用Preemption-Timer" class="headerlink" title="2. 如何使用Preemption Timer"></a>2. 如何使用Preemption Timer</h3><p>Preemption Timer在VMCS中有三个域需要设置：</p><ol><li><strong>Pin-Based VM-Execution Controls</strong>,Bit 6,”Activate VMX preemption timer”: 该位如果设置为1，则打开Preemption Timer；如果为0，则下面两个域的设置均无效。</li><li><strong>VM-Exit Controls</strong>,Bit 22,”Save VMX-preemption timer value”:This control determines whether the value of the VMX-preemption timer is saved on VM exit.</li><li><strong>VMX-preemption timer value</strong>:This field contains the value that the VMX-preemption timer will use following the next VM entry with that setting. 如果设置了”Save VMX-preemption timer value”，那么在VM exit时会更新该域为新的值。</li></ol><p>和Preemption Timer相关的内容去SDM中全文搜索”Preemption Timer”。</p><p>在使用时，需要首先设置” Activate VMX preemption  timer”和 “VMX-preemption timer value”，如果需要VM exit时保存VMX-preemption timer value的话，需要设置 “Save VMX-preemption  timer  value”，这样在VCPU因为其他原因VMExit的时候不会重置VMX-preemption timer value。</p><p>注意：在由Preemption Timer Time-out产生的VMExit中，是需要重置VMX-preemption timer value的。</p><p><img src="/images/2022/04/06.PNG" alt><br>注意下这个rate。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自:&lt;a href=&quot;https://blog.csdn.net/xelatex_kvm/article/details/17761415&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Intel VT技术中的Preemption Timer&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;1-什么是Preemption-Timer&quot;&gt;&lt;a href=&quot;#1-什么是Preemption-Timer&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是Preemption Timer&quot;&gt;&lt;/a&gt;1. 什么是Preemption Timer&lt;/h3&gt;&lt;p&gt;Preemption Timer是一种可以周期性使VCPU触发VMExit的一种机制。即设置了Preemption Timer之后，可以使VCPU在指定的TSC cycle(注意文章最后的rate)之后产生一次VMExit。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVLink and NVSwitch</title>
    <link href="http://liujunming.github.io/2022/04/01/Notes-about-NVLink-and-NVSwitch/"/>
    <id>http://liujunming.github.io/2022/04/01/Notes-about-NVLink-and-NVSwitch/</id>
    <published>2022-04-01T01:37:23.000Z</published>
    <updated>2022-04-01T04:19:40.637Z</updated>
    
    <content type="html"><![CDATA[<p>Notes aout NVLink and NVSwitch。<a id="more"></a></p><h3 id="1-NVLink"><a href="#1-NVLink" class="headerlink" title="1. NVLink"></a>1. NVLink</h3><p><strong>NVLink</strong> is a wire-based serial multi-lane near-range communications link developed by Nvidia. Unlike PCI Express, a device can consist of <em>multiple NVLinks</em>, and devices use <a href="https://en.wikipedia.org/wiki/Mesh_networking" target="_blank" rel="noopener">mesh networking</a> to communicate instead of a central hub.</p><p><img src="/images/2022/04/01.png" alt></p><p><img src="/images/2022/04/02.png" alt></p><p><img src="/images/2022/04/03.PNG" alt></p><h3 id="2-NVSwitch"><a href="#2-NVSwitch" class="headerlink" title="2. NVSwitch"></a>2. NVSwitch</h3><p><img src="/images/2022/04/05.jpg" alt><br>In the above figure, GPU to GPU memory transfers via NVLink are at most two hops away – a memory request may have to be routed through the NVLink controllers on two GPUs. For example, GPU 0 may need data in GPU 5’s memory, it needs two hops (such as:GPU 0 -&gt; GPU 1 -&gt;GPU5). Each NVLink controller has a memory access latency, so each memory access latency multiplies via the number of hops is the total latency.</p><p>NVSwitch存在的作用是避免GPU和GPU之间的通信会存在多跳。</p><p><img src="/images/2022/04/04.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.cnblogs.com/kongchung/p/12945019.html" target="_blank" rel="noopener">https://www.cnblogs.com/kongchung/p/12945019.html</a></li><li><a href="https://www.nvidia.com/en-us/data-center/nvlink/" target="_blank" rel="noopener">https://www.nvidia.com/en-us/data-center/nvlink/</a></li><li><a href="https://en.wikichip.org/wiki/nvidia/nvlink" target="_blank" rel="noopener">https://en.wikichip.org/wiki/nvidia/nvlink</a></li><li><a href="https://en.wikichip.org/wiki/nvidia/nvswitch" target="_blank" rel="noopener">https://en.wikichip.org/wiki/nvidia/nvswitch</a></li><li><a href="https://en.wikipedia.org/wiki/NVLink" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/NVLink</a></li><li><a href="https://www.nextplatform.com/2018/04/13/building-bigger-faster-gpu-clusters-using-nvswitches/" target="_blank" rel="noopener">https://www.nextplatform.com/2018/04/13/building-bigger-faster-gpu-clusters-using-nvswitches/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes aout NVLink and NVSwitch。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about SIOV</title>
    <link href="http://liujunming.github.io/2022/03/31/Notes-about-SIOV/"/>
    <id>http://liujunming.github.io/2022/03/31/Notes-about-SIOV/</id>
    <published>2022-03-31T07:17:54.000Z</published>
    <updated>2022-03-31T14:33:11.078Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要转载自:</p><ol><li><a href="http://blog.chinaunix.net/uid-28541347-id-5854292.html" target="_blank" rel="noopener">Scalable IOV技术详解</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4NTcwODg4NA==&amp;mid=2247483943&amp;idx=1&amp;sn=95f267a48893894d520d370a8758f3a4&amp;chksm=cfa5831cf8d20a0a3a6bbc5f3fc6ae6c3434f02e71cf935b93d2b35bf1abc82139106b511880&amp;mpshare=1&amp;scene=1&amp;srcid=0326ubuKIgOfRoXUsG8y8NWx&amp;sharer_sharetime=1648449427829&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AcIRgEgW1RVJwfGNas9nLEM%3D&amp;acctmode=0&amp;pass_ticket=a6RZLn0bY3VEyzyN0m6198%2FdvFlHI6%2BFJg7xUTXpn1mV5TdnwOdDYHcBYHEZUhPU&amp;wx_header=0#rd" target="_blank" rel="noopener">聊聊intel平台io虚拟化技术之 SIOV</a></li><li><a href="https://01.org/blogs/ashokraj/2018/recent-enhancements-intel-virtualization-technology-directed-i/o-intel-vt-d" target="_blank" rel="noopener">RECENT ENHANCEMENTS IN INTEL® VIRTUALIZATION TECHNOLOGY FOR DIRECTED I/O (INTEL® VT-D)</a>。</li></ol><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Scalable IO Virtualization(SIOV)是IO虚拟化技术的一个演进，是SR-IOV的进一步发展。为了提升虚拟机的IO性能，intel 的VT-d解决了设备直通问题，使虚拟机可以直接访问硬件设备从而提高性能，而SR-IOV则提供了设备共享的能力，通过将设备硬件虚拟化出多个VF给不同的虚拟机使用。</p><p>首先我们回顾一下SR-IOV技术，如下图所示，SR-IOV引入了两种设备PF和VF。其中PF具有完整的PCIe功能，包括VF的管理（创建/删除/配置），VF则是一种轻量的PCIe设备，只有部分数据传输功能，不包含资源管理和配置管理。但VF也是标准的PCIe设备，既有唯一的BDF（Bus，Device，Function）来标识这个设备，拥有属于自己的PCIe配置空间。<br><img src="/images/2022/03/38.png" alt></p><h2 id="2-Limitation-of-SR-IOV"><a href="#2-Limitation-of-SR-IOV" class="headerlink" title="2. Limitation of SR-IOV"></a>2. Limitation of SR-IOV</h2><p>While SR-IOV enabled the ability to partition a device and provide direct access to VMs, it also imposed scalability limitations to modern cloud and containerized environments. For instance:</p><ul><li>Device BARs and headers must be duplicated for every VF.</li><li>PCIe limits resources such as MSI-X to maximum of 2048 vectors.</li><li>BIOS Firmware must reserve a number of resources such as MMIO ranges, bus ranges to accommodate devices of any capability to be hotplugged.</li></ul><p>SR-IOV implementations typically provide only a small number of VFs due to the above resource requirements. Typical SR-IOV devices only support 64 or less VFs per physical device. Light-weight containerized usages in modern cloud environments expect to have thousands of containers and therefore will put pressure on potentially scarce resources. In these environments, SR-IOV will not scale.</p><p>Limitations of SR-IOV based implementations include:</p><ul><li>Scalability - Unable to scale to hyperscale usages (1000+ VMs/Containers) due to cost implications for having increased memory on board and limitations on BUS numbers in certain platforms.</li><li>Flexible Resource Management - SR-IOV requires resources such as BUS numbers and MMIO ranges to use the newly created VFs. Typically, the resources are spread evenly between each of the VFs. Although it’s possible to conceive of such variable resource assignments to different VFs, it imposes hardware complexity which would increase hardware cost. For instance, being able to create a device with 2 hardware queues for one VF, and 4 queues on the same physical device for another VF is generally not implemented.</li><li>Composability - the motivation of SR-IOV is to enable direct VF pass-through. The guest driver has full control on the assigned VF device which the host/hypervisor has no insight into. This makes it difficult to perform live migration or snapshot VF device state.</li></ul><p>Even with these limitations, SR-IOV has worked well in traditional VM usage. However, this approach no longer meets the scaling requirements for containerized environments.</p><h2 id="3-Scalable-IOV"><a href="#3-Scalable-IOV" class="headerlink" title="3. Scalable IOV"></a>3. Scalable IOV</h2><p>Intel introduced the recent update to Intel® VT-d that allows for fine-grained capacity allocation. More specifically, it allows software to compose virtual devices with different capacity or capability. For instance, it’s not required to replicate hardware like SR-IOV devices. Intel® Scalable IOV allows software to compose a virtual device on demand. The virtual device provisioned via software allows most device access to be separated into slow path (configuration) and fast path (I/O). Any activity that involves configuration and control is done by software mediation. Fast path I/O is performed directly to hardware with no software intervention. This allows resources such as queues to be bundled on demand and such usage can fit either full machine virtualization or native container type usages.</p><p>Intel® Scalable IOV requires changes in the following areas:</p><ul><li>Device Support - A device should support Process Address Space ID (PASID). The PASID is a 20 bit value that is used in conjunction with the Requester ID. PASID granular resource allocation and proper isolation requirements are identified in the Intel® Scalable I/O Virtualization Technical Specification.<ul><li>The Interrupt Message Store (IMS) provides devices the flexibility to dictate how interrupts are specified without limitations on how many and where the message address and data are stored.</li></ul></li><li>Platform Support - DMA remapping hardware should support PASID granular DMA isolation capability.</li><li>System Software - Support in the Operating System to provide abstractions that allow such devices to be provisioned for a Guest OS, or native process consumption.</li></ul><p>Intel® Scalable IOV addresses the aforementioned limitations observed on PCIe* SR-IOV:</p><ul><li>Scalability - supports finer-grained device sharing. For example, on a NIC with 1024 TX/RX queue pairs, each queue pair can now be independently assignable.</li><li>Flexible Resource management - software fully manages and maps backend resources to virtual devices. This provides great flexibility for heterogeneous configurations (different resources, different capabilities, and others.)</li><li>Composability - mediation of the slow-path allows the host/hypervisor to capture the virtual device state to enable live migration or snapshot usages. Also state save/restore is required only on a small piece of device resource (queue, context, etc.), which can be implemented more easily on a device as compared to requiring the entire VF state to be migratable.</li></ul><p><img src="/images/2022/03/44.png" alt></p><p>针对SR-IOV的一些局限性，intel推出了Scalable IOV技术。它主要包含一些几个技术特性：</p><ol><li>硬件辅助的直通架构，具体来说就是<ol><li>慢速路径有软件模拟完成，所谓慢速路径一般指设备的配置，接口的管理，而快速路径则是指IO的数据传输路径。在SR-IOV中，慢速路径和快速路径都是通过硬件直通的方式完成的；</li><li>快速路径资源可以动态分配，映射；</li><li>硬件保证快速路径的资源在DMA时是完全隔离的，保证不同虚拟设备的安全隔离；</li></ol></li><li>更加细粒度的动态资源配置。具体来说就是可以按照PCIe设备上的tx/rx queue pair来切分虚拟设备，而不是VF，从而实现更细粒度的资源分配；</li><li>利用PASID（Process Address Space ID）的PCIe能力，PASID技术也是PCI协议的一个补充，它颠覆了传统通过BDF（Bus，Device，Function）来唯一标识一个PCIe设备的方式，以BDF+PASID在一个PCIe设备内细分更多的虚拟设备；</li><li>支持各种IO设备，包括网卡，存储设备，GPU，各种加速器等；</li><li>支持虚拟机，裸金属，容器等多种应用场景；</li></ol><p>以上就是Scalable IOV的主要技术特征，可以看出和SR-IOV类似，它不仅仅是PCIe设备侧的一次革新，更是硬件设备，BIOS，操作系统，hypervisor，CPU，IOMMU等整个硬件的一次革新。<br><img src="/images/2022/03/39.png" alt></p><ul><li>Over-provisioning: 两个VDEV之间的Queue资源是可以share的</li><li>Generational Compatability: vmm可以使用VDCM(Virtual Device Composition Module)在不同代的硬件设备上呈现相同的VDEV功能，这样即使在部署了不同代的SIOV设备的host之间虚拟机也能正常迁移</li></ul><h2 id="4-整体架构"><a href="#4-整体架构" class="headerlink" title="4. 整体架构"></a>4. 整体架构</h2><p>Scalable IOV的整体架构和构成如下图所示。<br><img src="/images/2022/03/40.png" alt></p><h2 id="5-硬件架构"><a href="#5-硬件架构" class="headerlink" title="5. 硬件架构"></a>5. 硬件架构</h2><p><img src="/images/2022/03/45.png" alt></p><p>SIOV 主要是以queue为粒度来给上层应用提供服务，因此设备层提出了一种叫ADI（Assignable Device Interfaces）的接口概念，这个有些类似于SR-IOV中的VF，ADI指作为一种独立的单元进行分配、配置和组织的一组后端资源。它和VF有两点不同之处：</p><ol><li>没有PCI配置空间，所有ADI设备共享PF的配置空间；</li><li>通过PASID标识，而不是BDF</li></ol><p>同时ADI作为一个可用随时分配的设备，又具备以下特点：</p><ol><li>ADI设备之间是完全隔离的，不共享任何资源；</li><li>不同的ADI设备的MMIO寄存器是以物理页为单位隔离，保证进行MMIO页映射时在不同的页，避免MMIO被不同的进程共享；</li><li>所有ADI的DMA操作通过PASID进行，因此IOMMU可以根据每个设备DMA的PASID查找不同的页表，保证物理上ADI是安全隔离的；</li><li>采用了Interrupt Message Storage（IMS）技术。其实IMS和ADI不是绑定的，ADI采用IMS是由于往往ADI设备较多，每个ADI设备的每个queue都会产生中断，为了支持大量的中断消息存储使用了IMS技术。至于IMS具体的存储格式和位置是和具体设备实现相关的。此外ADI中断不支持共享，而且只支持MSI/MSI-X，不支持lagacy中断；</li><li>每个ADI设备可以独立的进行reset操作；</li></ol><h3 id="5-1-PCI配置空间"><a href="#5-1-PCI配置空间" class="headerlink" title="5.1 PCI配置空间"></a>5.1 PCI配置空间</h3><p>对PCIe设备进行初始化和枚举时，需要配置空间能够发现设备是否支持Scalable IOV技术，intel定义了一个Designated Vendor Specific Extended Capability (DVSEC) 域用于发现和配置支持Scalable IOV技术的设备。具体如下图所示：<br><img src="/images/2022/03/41.png" alt></p><h3 id="5-2-MMIO"><a href="#5-2-MMIO" class="headerlink" title="5.2 MMIO"></a>5.2 MMIO</h3><p>ADI的MMIO，它是位于PF bar地址空间的一段连续的按页大小对齐的地址范围。每个ADI设备的MMIO是相互独立的，ADI设备的MMIO register又分为两类，一类是访问频率比较高的比如硬件层的doorbell，一类是不经常访问的或者慢路径访问的比如用来进行一些设备配置和管理等。</p><h3 id="5-3-PASID-区分来自不同ADI设备的DMA请求"><a href="#5-3-PASID-区分来自不同ADI设备的DMA请求" class="headerlink" title="5.3 PASID(区分来自不同ADI设备的DMA请求)"></a>5.3 PASID(区分来自不同ADI设备的DMA请求)</h3><p>IOMMU提供DMA remapping的操作，进行地址转换，将不同的IO设备提供的IOVA地址转换成物理地址，用于设备的DMA。在intel IOMMU中，每个IO设备通过BDF找到属于自己的页表。为了支持Scalable IOV，DMA remapping增加了PASID的支持，其多级页表也进行了重新设计，具体如下图所示：<br><img src="/images/2022/03/42.png" alt></p><h3 id="5-4-IMS"><a href="#5-4-IMS" class="headerlink" title="5.4 IMS"></a>5.4 IMS</h3><p>一个PCIe设备即使在MSI-X的情况下，它支持的最大中断数目也只能到2048，那如果一个PF上支持的ADI数量所使用的总的中断数量超过了这个limit将如何处理呢？<br>为了解决这个中断limit的问题，SIOV引入了新的中断存储机制叫IMS(Iinterrupt Message Storage)，理论上IMS在支持的中断数量是没有上限的，从实现原理上来讲其仍然是message 格式的中断触发机制，每个message有一个DWORD 大小的payload和64-bit的address。这些message存储在 IMS的table里面，这个table可以有全部缓存在硬件上，也可以全部放在host memory里面。</p><h2 id="6-软件架构"><a href="#6-软件架构" class="headerlink" title="6. 软件架构"></a>6. 软件架构</h2><p><img src="/images/2022/03/47.png" alt><br><img src="/images/2022/03/48.png" alt></p><h3 id="6-1-VDCM"><a href="#6-1-VDCM" class="headerlink" title="6.1 VDCM"></a>6.1 VDCM</h3><p>VDCM (Virtual Device Composition Module)主要负责在ADI和虚拟设备（VDEV）之间建立映射关系，处理和仿真慢速路径的操作(负责一些trap到后端的MMIO的解释执行)，另外就是ADI设备的一些操作比如Reset和配置等。</p><h3 id="6-2-VDEV"><a href="#6-2-VDEV" class="headerlink" title="6.2 VDEV"></a>6.2 VDEV</h3><p>其实上面也已经讲了它是由一个或者多个ADI设备组成，在guest里面看到的就是一个标准的PCIe 设备。每个VDEV都有虚拟的requester id, config space, memory BAR，MSI-X table等，它们都是由VDCM来进行模拟的。</p><h3 id="6-3-VDEV-MMIO-and-interrupts"><a href="#6-3-VDEV-MMIO-and-interrupts" class="headerlink" title="6.3 VDEV MMIO and interrupts"></a>6.3 VDEV MMIO and interrupts</h3><p>从上面的分析来看，VDCM在整个软件架构上扮演着非常重要的角色，下面我们结合一张图来看一下相关实现：<br><img src="/images/2022/03/46.png" alt></p><p>结合上图我们来分析一些细节的东西，比如vdev 的MMIO，中断等 。</p><h4 id="6-3-1-VDEV-MMIO"><a href="#6-3-1-VDEV-MMIO" class="headerlink" title="6.3.1 VDEV MMIO"></a>6.3.1 VDEV MMIO</h4><p>从图中可以看到VDEV MMIO实现分为三类：</p><ol><li>直接map到 ADI的 MMIO，类似SR-IOV场景下将硬件的MMIO通过EPT的方式直接让guest访问，避免大量的VM Exit；</li><li>通过VDCM 模拟的MMIO，guest里面在写这个MMIO的时候会trap到 VDCM，然后需要VDCM进行解释和模拟相关的action，通常这类MMIO是要是一些控制面的数据交互；</li><li>map到host侧的memory上，这类MMIO通常存储的是一些参数或者数据，这样就避免了在读取或者写入的时候VDCM侧的解释和指令模拟。</li></ol><h4 id="6-3-2-VDEV-interrupts"><a href="#6-3-2-VDEV-interrupts" class="headerlink" title="6.3.2 VDEV interrupts"></a>6.3.2 VDEV interrupts</h4><p>VDEV 会通过VDCM 虚拟出MSI或者MSI-X的能力呈现给guest，当guest driver 去programs MSI 或者MSI-X的时候会被VDCM截获到然后做相关的中断虚拟化操作。这里需要说明的是慢路径上的中断是可以通过VMM提供的中断注入接口来触发，而快路径或者说是数据面上的中断是通IOMMU的post interrupt来注入的。</p><h3 id="7-MISC"><a href="#7-MISC" class="headerlink" title="7. MISC"></a>7. MISC</h3><h3 id="7-1-Hardware-Assisted-Mediated-Pass-Through"><a href="#7-1-Hardware-Assisted-Mediated-Pass-Through" class="headerlink" title="7.1 Hardware-Assisted Mediated Pass-Through"></a>7.1 Hardware-Assisted Mediated Pass-Through</h3><p><img src="/images/2022/03/35.PNG" alt><br><img src="/images/2022/03/36.PNG" alt><br><img src="/images/2022/03/37.PNG" alt></p><p>另外一个视角来看SIOV：同时结合了SR-IOV和Mediated pass-through的优点。</p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4NTcwODg4NA==&amp;mid=2247483943&amp;idx=1&amp;sn=95f267a48893894d520d370a8758f3a4&amp;chksm=cfa5831cf8d20a0a3a6bbc5f3fc6ae6c3434f02e71cf935b93d2b35bf1abc82139106b511880&amp;mpshare=1&amp;scene=1&amp;srcid=0326ubuKIgOfRoXUsG8y8NWx&amp;sharer_sharetime=1648449427829&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AcIRgEgW1RVJwfGNas9nLEM%3D&amp;acctmode=0&amp;pass_ticket=a6RZLn0bY3VEyzyN0m6198%2FdvFlHI6%2BFJg7xUTXpn1mV5TdnwOdDYHcBYHEZUhPU&amp;wx_header=0#rd" target="_blank" rel="noopener">聊聊intel平台io虚拟化技术之 SIOV</a></li><li><a href="http://blog.chinaunix.net/uid-28541347-id-5854292.html" target="_blank" rel="noopener">Scalable IOV技术详解</a></li><li><a href="https://01.org/blogs/ashokraj/2018/recent-enhancements-intel-virtualization-technology-directed-i/o-intel-vt-d" target="_blank" rel="noopener">RECENT ENHANCEMENTS IN INTEL® VIRTUALIZATION TECHNOLOGY FOR DIRECTED I/O (INTEL® VT-D)</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/introducing-intel-scalable-io-virtualization.html" target="_blank" rel="noopener">Introducing Intel® Scalable I/O Virtualization</a></li><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/scalable-i-o-virtualized-servers-paper.pdf" target="_blank" rel="noopener">White Paper</a></li><li><a href="https://01.org/blogs/2019/assignable-interfaces-intel-scalable-i/o-virtualization-linux" target="_blank" rel="noopener">ASSIGNABLE INTERFACES IN INTEL® SCALABLE I/O VIRTUALIZATION IN LINUX</a></li><li><a href="https://www.opencompute.org/documents/ocp-scalable-io-virtualization-technical-specification-revision-1-v1-2-pdf" target="_blank" rel="noopener">Version 1.2 SPEC</a></li><li><a href="http://news.eeworld.com.cn/wltx/ic567173.html" target="_blank" rel="noopener">英特尔携手微软打造全新I/O虚拟化架构,提升加速器和I/O设备的可扩展性</a></li><li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/12/Hardware-Assisted-Mediated-Pass-Through-with-VFIO-Kevin-Tian-Intel.pdf" target="_blank" rel="noopener">Hardware-Assisted Mediated Pass-Through with VFIO</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Intel%C2%AE-Scalable-I_O-Virtualization_Kevin-Tian.pdf" target="_blank" rel="noopener">Intel® Scalable I/O Virtualization</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要转载自:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-28541347-id-5854292.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scalable IOV技术详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=Mzg4NTcwODg4NA==&amp;amp;mid=2247483943&amp;amp;idx=1&amp;amp;sn=95f267a48893894d520d370a8758f3a4&amp;amp;chksm=cfa5831cf8d20a0a3a6bbc5f3fc6ae6c3434f02e71cf935b93d2b35bf1abc82139106b511880&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0326ubuKIgOfRoXUsG8y8NWx&amp;amp;sharer_sharetime=1648449427829&amp;amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;amp;exportkey=AcIRgEgW1RVJwfGNas9nLEM%3D&amp;amp;acctmode=0&amp;amp;pass_ticket=a6RZLn0bY3VEyzyN0m6198%2FdvFlHI6%2BFJg7xUTXpn1mV5TdnwOdDYHcBYHEZUhPU&amp;amp;wx_header=0#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;聊聊intel平台io虚拟化技术之 SIOV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://01.org/blogs/ashokraj/2018/recent-enhancements-intel-virtualization-technology-directed-i/o-intel-vt-d&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RECENT ENHANCEMENTS IN INTEL® VIRTUALIZATION TECHNOLOGY FOR DIRECTED I/O (INTEL® VT-D)&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Misc about PCI&amp;&amp;PCIe(1)</title>
    <link href="http://liujunming.github.io/2022/03/31/Misc-about-PCI-PCIe-1/"/>
    <id>http://liujunming.github.io/2022/03/31/Misc-about-PCI-PCIe-1/</id>
    <published>2022-03-31T00:24:43.000Z</published>
    <updated>2022-04-02T13:36:36.857Z</updated>
    
    <content type="html"><![CDATA[<p>PCI和PCIe内容较杂，mark下相关的MISC。 <a id="more"></a></p><h3 id="1-Lane"><a href="#1-Lane" class="headerlink" title="1. Lane"></a>1. Lane</h3><p>中文翻译：数据通路</p><p>A PCIe connection consists of one or more data-transmission <a href="https://superuser.com/questions/843344/what-is-a-pci-express-lane" target="_blank" rel="noopener">lanes</a>, connected serially. Each lane consists of two pairs of wires, one for receiving and one for transmitting. You can have one, four, eight, or sixteen lanes in a single consumer PCIe slot–denoted as x1, x4, x8, or x16.</p><p><img src="/images/2022/03/34.jpg" alt></p><p><img src="/images/2022/03/33.PNG" alt></p><h3 id="2-Bar-size"><a href="#2-Bar-size" class="headerlink" title="2. Bar size"></a>2. Bar size</h3><p><img src="/images/2022/03/32.PNG" alt></p><p>A BAR size: it can be get by writing <strong>0FFFF FFFFH</strong> to a physical BAR register first, then read it.  Assume that the register’s value to read is <strong>A</strong>, the BAR size is:  <strong>~(A &amp; ~0FH) + 1</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCI和PCIe内容较杂，mark下相关的MISC。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Shared Virtual Memory</title>
    <link href="http://liujunming.github.io/2022/03/30/Introduction-to-Shared-Virtual-Memory/"/>
    <id>http://liujunming.github.io/2022/03/30/Introduction-to-Shared-Virtual-Memory/</id>
    <published>2022-03-30T07:15:58.000Z</published>
    <updated>2022-03-30T08:33:36.472Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要转载自：<a href="http://blog.chinaunix.net/uid-28541347-id-5854016.html" target="_blank" rel="noopener">Shared Virtual Memory（SVM）介绍</a>。<a id="more"></a></p><h3 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h3><p>Shared Virtual Addressing (SVA) allows the processor and device to use the same virtual addresses. SVA is what PCIe calls Shared Virtual Memory (SVM).<br><img src="/images/2022/03/23.PNG" alt><br><img src="/images/2022/03/24.PNG" alt></p><p>VT-d SVM: Extends root complex IOMMU to comprehend x86 page table formats<br><img src="/images/2022/03/31.PNG" alt></p><h3 id="2-History"><a href="#2-History" class="headerlink" title="2. History"></a>2. History</h3><p>共享虚拟内存（SVM）技术最初是为了解决在GPU场景下，设备（GPU）和host（CPU）之间共享内存的。目的是在设备GPU和CPU之间可以直接传递指针（地址），为了让设备可以直接使用进程空间的地址，简化编程模型。我们知道通常host侧采用的地址是主机的虚拟地址（VA），而设备侧通常使用的是物理地址（PA）或IOVA。</p><p>如下图，传统内存访问的三个途径：</p><ol><li>CPU访存通过MMU（CPU的页表）将VA转为PA访问物理地址</li><li>GPU访存通过GPU的页表访问物理地址</li><li>PCIe设备直接使用物理地址PA访问</li></ol><p><img src="/images/2022/03/25.png" alt></p><p>在引入了iommu（VT-d）后，如下图，PCIe设备也可以使用虚拟地址（IOVA）来访存了，也有设备自己对应的页表（iommu页表）完成IOVA到物理地址（PA）的映射。<br><img src="/images/2022/03/26.png" alt></p><p>这种情况下CPU进程和设备的内存通信一般采用如下流程：</p><ol><li>CPU进程分配一块内存，并采用系统调用syscall或ioctl请求内核准备接收操作</li><li>内核初始化设备的DMA操作，这里面有两种情况：一种是内核重新分配一块内核空间的内存，将其物理地址传递给设备进行DMA，另一种是如果应用程序将用户空间的内存pin住（这段内存的虚拟地址空间和物理地址空间不会发生变化）则可直接将用户空间的buffer地址传递给设备进行DMA</li><li>设备将数据DMA到内存，对应上面这里也有两种情况，如果是内核设置的内核buffer的地址，则设备会先将数据DMA到内核buffer，再由内核将数据由内核空间拷贝到用户空间的buffer（我们通常使用内核协议栈进行收发报文的应用程序就是这种），另一种如果用户空间直接将内存pin住，则设备直接将数据DMA到应用程序的buffer（我们采用DPDK收发报文就是这种）</li></ol><p><img src="/images/2022/03/27.png" alt></p><h3 id="3-引入SVM后的变化"><a href="#3-引入SVM后的变化" class="headerlink" title="3. 引入SVM后的变化"></a>3. 引入SVM后的变化</h3><p>下面我们看引入SVM后的效果，最大的区别是设备访问地址在经过iommu的DMAR转换时会参考引用CPU的mmu页表，在地址缺页时同样会产生缺页中断。为什么要这样设计呢？因为要想设备直接使用进程空间的虚拟地址可能采用的有两种方法。一种是把整个进程地址空间全部pin住，但这点一般是不现实的，除非类似DPDK应用程序全部采用静态内存，否则如果进程动态分配一个内存，那么这个地址设备是不感知的。另一种方法就是采用动态映射，就像进程访问虚拟地址一样，mmu发现缺页就会动态映射，所以从设备发来的地址请求也会经过CPU缺页处理，并将映射关系同步到iommu的页表中。<br><img src="/images/2022/03/28.png" alt></p><p>有了以上的流程，CPU和设备的内存交互流程就变成了如下图所示。主要是第三步的变化，设备直接将数据DMA到进程空间的地址，并且不需要进程pin内存，而是通过page fault触发缺页处理进行映射。<br><img src="/images/2022/03/29.png" alt></p><h3 id="4-支持SVM的条件"><a href="#4-支持SVM的条件" class="headerlink" title="4. 支持SVM的条件"></a>4. 支持SVM的条件</h3><p>那么支持SVM需要软硬件具备什么条件呢。首先是设备角度：</p><ol><li>要支持<a href="/2021/11/09/Notes%20about-PASID-Process-Address-Space-ID/">PASID</a>，因为一个设备会被多个进程访问，对应多个设备DMAR页表，需要通过PASID来区分采用哪个页表</li><li>支持<a href="/2022/03/30/Notes-about-PCIe-Page-Request-Interface/">dma page fault</a>处理，当访问的虚拟地址引发缺页时能够等待或重试</li></ol><p>从驱动角度来说，</p><ol><li>操作设备的API需要通过PASID来区分不同进程</li></ol><p><img src="/images/2022/03/30.PNG" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.kernel.org/doc/html/latest/x86/sva.html" target="_blank" rel="noopener">Shared Virtual Addressing (SVA) with ENQCMD</a></li><li><a href="https://archive.fosdem.org/2016/schedule/event/intel_svm/attachments/slides/1269/export/events/attachments/intel_svm/slides/1269/FOSDEM_2016___SVM_on_Intel_Graphics.pdf" target="_blank" rel="noopener">SVM on Intel Graphics</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2018/52/kvm-forum-vSVA-yliu-jpan-jean-eric.pdf" target="_blank" rel="noopener">Shared Virtual Addressing in KVM kvm forum 2018</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Shared-Virtual-Memory-in-KVM_Yi-Liu.pdf" target="_blank" rel="noopener">Shared Virtual Addressing in KVM</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要转载自：&lt;a href=&quot;http://blog.chinaunix.net/uid-28541347-id-5854016.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Shared Virtual Memory（SVM）介绍&lt;/a&gt;。
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to PCIe Page Request Interface</title>
    <link href="http://liujunming.github.io/2022/03/30/Notes-about-PCIe-Page-Request-Interface/"/>
    <id>http://liujunming.github.io/2022/03/30/Notes-about-PCIe-Page-Request-Interface/</id>
    <published>2022-03-30T01:45:26.000Z</published>
    <updated>2022-03-30T06:48:37.556Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录PCIe中PRI(Page Request Interface)相关知识点。<a id="more"></a></p><p>阅读本文前，读者需要对<a href="/2019/11/24/Introduction-to-PCIe-Address-Translation-Services/">ATS</a>和<a href="/2021/07/29/Notes-about-guest-memory-pinning-when-direct-assignment-of-I-0-devices/">guest memory pinning when direct assignment of I/O devices</a>有一定的了解。</p><p>PRI(Page Request Interface) allows functions(BDF中的F) to raise page faults to the IOMMU.</p><h3 id="1-DMA-Page-Fault-Support"><a href="#1-DMA-Page-Fault-Support" class="headerlink" title="1. DMA Page Fault Support"></a>1. DMA Page Fault Support</h3><p>Description from PCIe spec:<br><img src="/images/2022/03/21.PNG" alt></p><p>Description from ASPLOS ’17 paper:<br><img src="/images/2022/03/20.PNG" alt></p><h3 id="2-Page-Request-Services"><a href="#2-Page-Request-Services" class="headerlink" title="2. Page Request Services"></a>2. Page Request Services</h3><p>The general model for a page request is as follows:</p><ol><li>A Function determines that it requires access to a page for which an ATS translation is not available.</li><li>The Function causes the associated Page Request Interface to send a Page Request Message to its RC. A Page Request Message contains a page address and a Page Request Group (PRG) index. The PRG index is used to identify the transaction and is used to match requests with responses.</li><li>When the RC determines its response to the request (which will typically be to make the requested page resident), it sends a PRG Response Message back to the requesting Function.</li><li>The Function can then employ ATS to request a translation for the requested page(s). </li></ol><h3 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h3><p><img src="/images/2022/03/22.PNG" alt></p><hr><p>参考资料：</p><ol><li><a href="https://composter.com.ua/documents/ats_r1.1_26Jan09.pdf" target="_blank" rel="noopener">Address Translation Services Revision 1.1</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Shared-Virtual-Memory-in-KVM_Yi-Liu.pdf" target="_blank" rel="noopener">Shared Virtual Memory in KVM</a></li><li>Page Fault Support for Network Controllers, ASPLOS ’17</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录PCIe中PRI(Page Request Interface)相关知识点。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel I/OAT</title>
    <link href="http://liujunming.github.io/2022/03/29/Introduction-to-Intel-I-OAT/"/>
    <id>http://liujunming.github.io/2022/03/29/Introduction-to-Intel-I-OAT/</id>
    <published>2022-03-29T05:34:04.000Z</published>
    <updated>2022-03-29T12:21:10.541Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel I/OAT(I/O Acceleration Technology)相关知识点。<a id="more"></a></p><h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>Intel I/OAT is actually a set of technologies that each contributes to increased performance.</p><p>The features of Intel I/OAT enhance data acceleration across the computing platform.</p><ul><li><strong>Intel® QuickData Technology</strong> enables data copy by the chipset instead of the CPU, to move data more efficiently through the server and provide fast, scalable, and reliable throughput.</li><li><strong>Direct Cache Access (DCA)</strong> allows a capable I/O device, such as a network controller, to place data directly into CPU cache, reducing cache misses and improving application response times.</li><li><strong>Extended Message Signaled Interrupts (MSI-X)</strong> distributes I/O interrupts to multiple CPUs and cores, for higher efficiency, better CPU utilization, and higher application performance.</li><li><strong>Receive Side Coalescing (RSC)</strong> aggregates packets from the same TCP/IP flow into one larger packet, reducing per-packet processing costs for faster TCP/IP processing.</li><li><strong>Low latency interrupts</strong> tune interrupt interval times depending on the latency sensitivity of the data, using criteria such as port number or packet size, for higher processing efficiency.</li></ul><p><img src="/images/2022/03/16.PNG" alt></p><p>本文只介绍QuickData Technology和DCA。</p><h3 id="2-Intel®-QuickData-Technology"><a href="#2-Intel®-QuickData-Technology" class="headerlink" title="2. Intel® QuickData Technology"></a>2. Intel® QuickData Technology</h3><p><img src="/images/2022/03/17.PNG" alt></p><p><img src="/images/2022/03/15.PNG" alt><br><img src="/images/2022/03/19.PNG" alt></p><p><img src="/images/2022/03/18.PNG" alt></p><p><a href="https://01.org/blogs/2019/introducing-intel-data-streaming-accelerator" target="_blank" rel="noopener">Intel® DSA</a> replaces the Intel® QuickData Technology.</p><h3 id="3-Direct-Cache-Access-DCA"><a href="#3-Direct-Cache-Access-DCA" class="headerlink" title="3. Direct Cache Access (DCA)"></a>3. Direct Cache Access (DCA)</h3><p><img src="/images/2022/03/13.PNG" alt><br><img src="/images/2022/03/14.PNG" alt></p><p><a href="/2022/03/28/Introduction-to-Intel-DDIO-technology/">Introduction to Intel DDIO technology</a></p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/io/i-o-acceleration-technology-paper.html" target="_blank" rel="noopener">White Paper</a></li><li><a href="https://www.intel.com/content/www/us/en/wireless-network/accel-technology.html" target="_blank" rel="noopener">Intel® I/O Acceleration Technology</a></li><li><a href="https://www.usenix.org/system/files/atc20-farshin.pdf" target="_blank" rel="noopener">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi hundred-gigabit Networks</a></li><li><a href="https://insujang.github.io/2021-04-26/using-intel-ioat-dma/" target="_blank" rel="noopener">Using Intel IOAT DMA</a></li><li><a href="https://www.snia.org/sites/default/files/SDC/2016/presentations/persistent_memory/Tanveer_Alam_Enterprise_Storage_RAS_Augmented_native_Intel_Platform_Storage_Extensions.pdf" target="_blank" rel="noopener">Tanveer_Alam_Enterprise_Storage_RAS_Augmented_native_Intel_Platform_Storage_Extensions.pdf</a></li><li><a href="https://landley.net/kdocs/ols/2005/ols2005v1-pages-289-296.pdf" target="_blank" rel="noopener">Accelerating Network Receive Processing</a></li><li><a href="https://0xffff.one/d/934" target="_blank" rel="noopener">关于intel的IOAT技术</a></li><li><a href="http://nowlab.cse.ohio-state.edu/static/media/publications/abstract/vaidyana-cluster07.pdf" target="_blank" rel="noopener">Efficient Asynchronous Memory Copy Operations on Multi-Core Systems and I/OAT</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel I/OAT(I/O Acceleration Technology)相关知识点。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Compute Express Link (CXL)</title>
    <link href="http://liujunming.github.io/2022/03/28/Introduction-to-Compute-Express-Link-CXL/"/>
    <id>http://liujunming.github.io/2022/03/28/Introduction-to-Compute-Express-Link-CXL/</id>
    <published>2022-03-28T11:15:28.000Z</published>
    <updated>2022-03-28T15:59:00.766Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Compute Express Link (CXL)相关知识点。<a id="more"></a></p><h3 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h3><p>Compute Express Link™ (CXL™) is an industry-supported <strong>Cache-Coherent Interconnect</strong> for <em>Processors</em>, <em>Memory Expansion</em> and <em>Accelerators</em>. CXL technology maintains <strong>memory coherency</strong> between the CPU memory space and memory on attached devices, which allows resource sharing for higher performance, reduced software stack complexity, and lower overall system cost. This permits users to simply focus on target workloads as opposed to the redundant memory management hardware in their accelerators. </p><p>CXL is designed to be an industry open standard interface for <strong>high-speed communications</strong>, as accelerators are increasingly used to complement CPUs in support of emerging applications such as Artificial Intelligence and Machine Learning.</p><h3 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h3><p>通俗地说，有了CXL，Host在访问Device Memory时，可以得到像访问本地Memory一样的体验；同样的，Device访问host Memory时，也像是在访问Device Memory一样。</p><p>我们已经有了PCIe这样的高速串行总线，为什么还要再搞出一个新的CXL呢 ？主要是因为<strong>PCIe不支持cache的一致性</strong>，这会导致每次Device去访问Host上的内存时，即便已经访问了多次而且内存也没有变化的情况下，都要重新访问，这样导致性能很差。另外因为人工智能和机器学习的兴起，FPGA/GPU 卡上会有大量的内存，在不进行运算时闲置的话，会造成资源浪费。而因为PCIe不支持Cache的一致性，Host访问设备上的内存也会非常的慢(CPU访问设备的内存是不cache的，意味着这次访问完而且设备内存也没有变化的情况下，下次还要重新访问。为什么不cache 呢?因为设备的内存不能汇报自己的改变)。所以Intel就发明了CXL，它在PCIe的基础上加入了<strong>Cache一致性</strong>的支持以用来<strong>提高设备和主机之间内存互相访问的速度</strong>。</p><h3 id="3-Components"><a href="#3-Components" class="headerlink" title="3. Components"></a>3. Components</h3><p><img src="/images/2022/03/11.PNG" alt><br>CXL在PCIe 5.0的基础上复用三种类型的协议:</p><ul><li>CXL.io: Provides discovery, configuration, register access, interrupts, etc.</li><li>CXL.cache: Provides the CXL device access to the processor memory</li><li>CXL.memory: Provides the Processor access to the CXL device attached memory</li></ul><p>Each of these are illustrated in the block below.<br><img src="/images/2022/03/09.webp" alt></p><h3 id="4-Usage"><a href="#4-Usage" class="headerlink" title="4. Usage"></a>4. Usage</h3><p><img src="/images/2022/03/10.PNG" alt><br>HBM(High Bandwidth Memory)</p><h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><p><img src="/images/2022/03/12.PNG" alt></p><p>In short, CXL is an open industry standard interconnect offering <strong>high-bandwidth</strong>, <strong>low-latency</strong> connectivity between the host processor and devices including accelerators, memory expansion, and smart I/O devices. CXL utilizes the PCI Express® (PCIe®) 5.0 physical layer infrastructure and the PCIe alternate protocol to address the demanding needs of high-performance computational workloads in Artificial Intelligence, Machine Learning, communication systems, and HPC through the enablement of <strong>coherency and memory semantics</strong> across heterogeneous processing and memory systems.</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/65435956" target="_blank" rel="noopener">基于PCIe 5.0的CXL是什么？</a></li><li><a href="https://zhuanlan.zhihu.com/p/383878879" target="_blank" rel="noopener">CXL简介</a></li><li><a href="http://www.360doc.com/content/20/0519/10/36367108_913231044.shtml" target="_blank" rel="noopener">强力科普PCIe/CXL</a></li><li><a href="https://www.computeexpresslink.org/about-cxl" target="_blank" rel="noopener">About CXL™</a></li><li><a href="https://www.computeexpresslink.org/post/introduction-to-compute-express-link-cxl-the-cpu-to-device-interconnect-breakthrough" target="_blank" rel="noopener">Introduction to Compute Express Link (CXL): The CPU-To-Device Interconnect Breakthrough</a></li><li><a href="https://docs.wixstatic.com/ugd/0c1418_27f700c09d4648c4bede5dec99a20824.pdf" target="_blank" rel="noopener">Compute Express Link® (CXL):A Coherent Interface for Ultra-High-Speed Transfers</a></li><li><a href="https://docs.wixstatic.com/ugd/0c1418_d9878707bbb7427786b70c3c91d5fbd1.pdf" target="_blank" rel="noopener">Compute Express Link</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Compute Express Link (CXL)相关知识点。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel DDIO technology</title>
    <link href="http://liujunming.github.io/2022/03/28/Introduction-to-Intel-DDIO-technology/"/>
    <id>http://liujunming.github.io/2022/03/28/Introduction-to-Intel-DDIO-technology/</id>
    <published>2022-03-28T01:29:57.000Z</published>
    <updated>2022-03-28T06:24:15.604Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel DDIO(Data Direct I/O) technology。<a id="more"></a></p><h2 id="1-What-is-Intel®-Data-Direct-I-O"><a href="#1-What-is-Intel®-Data-Direct-I-O" class="headerlink" title="1. What is  Intel® Data Direct I/O?"></a>1. What is  Intel® Data Direct I/O?</h2><p>Intel® Data Direct I/O (Intel® DDIO) is a feature to be introduced on the Intel® E5 Xeon® processors. Intel’s LAN Access Division (LAD) worked for the incorporation of Intel DDIO into the Xeon E5 processor because of its benefits for LAN I/O in terms of performance and system power consumption. With Intel DDIO, Intel’s Ethernet server NICs and controllers talk directly to the processor cache without a detour via system memory. Intel DDIO makes the processor cache the primary destination and source of I/O data rather than main memory. By avoiding system memory, Intel DDIO reduces latency, increases system I/O bandwidth, and reduces power consumption due to memory reads and writes.<br><img src="/images/2022/03/06.PNG" alt></p><h2 id="2-How-does-it-work"><a href="#2-How-does-it-work" class="headerlink" title="2. How does it work?"></a>2. How does it work?</h2><p>Read and Writes的视角是NIC。<br>详情可以阅读<a href="https://blog.csdn.net/qq_40500045/article/details/109272627" target="_blank" rel="noopener">谈谈DDIO你该知道的事</a>。</p><h3 id="2-1-NIC-Reads"><a href="#2-1-NIC-Reads" class="headerlink" title="2.1 NIC Reads"></a>2.1 NIC Reads</h3><p><img src="/images/2022/03/07.PNG" alt></p><h4 id="2-1-1-Without-DDIO"><a href="#2-1-1-Without-DDIO" class="headerlink" title="2.1.1 Without DDIO"></a>2.1.1 Without DDIO</h4><ol><li>处理器更新报文和控制结构体。由于分配的缓冲区在内存中， 因此会触发一次Cache不命中，处理器把内存读取到Cache中，然后更新控制结构体和报文信息。之后通知NIC来读取报文。</li><li>NIC收到有报文需要传递到网络上的通知后，它首先需要读取控制结构体进而知道从哪里获取报文。由于之前处理器刚把该缓冲区从内存读到Cache中并且做了更新，很有可能Cache还没有来得及把更新的内容写回到内存中。因此，当NIC发起一个对内存的读请求时，很有可能这个请求会发送到Cache系统中，Cache系统会把数据写回到内存中，然后内存控制器再把数据写到PCI总线上去。因此，一个读内存的操作会产生多次内存的读写。</li></ol><h4 id="2-1-2-With-DDIO"><a href="#2-1-2-With-DDIO" class="headerlink" title="2.1.2 With DDIO"></a>2.1.2 With DDIO</h4><ol><li>处理器更新报文和控制结构体。这个步骤和没有DDIO的技术类似，但是由于DDIO的引入，处理器会开始就把内存中的缓冲区和控制结构体预取到Cache，因此减少了内存读的时间。</li><li>NIC收到有报文需要传递到网络上的通知后，通过PCI总线把控制结构体和报文送到NIC内部。利用DDIO技术，I/O访问可以直接将Cache的内容送到PCI总线上。这样，就减少了Cache写回时等待的时间。</li></ol><p>由此可以看出，由于DDIO技术的引入，网卡的读操作减少了访问内存的次数，因而提高了访问效率，减少了报文转发的延迟。在理想状况下，NIC和处理器无需访问内存，直接通过访问Cache就可以完成更新数据，把数据送到NIC内部，进而送到网络上的所有操作。</p><h3 id="2-2-NIC-Writes"><a href="#2-2-NIC-Writes" class="headerlink" title="2.2 NIC Writes"></a>2.2 NIC Writes</h3><p><img src="/images/2022/03/08.PNG" alt></p><h4 id="2-2-1-Without-DDIO"><a href="#2-2-1-Without-DDIO" class="headerlink" title="2.2.1 Without DDIO"></a>2.2.1 Without DDIO</h4><ol><li>报文和控制结构体通过PCI总线送到指定的内存中。如果该内存恰好缓存在Cache中(有可能之前处理器有对该内存进行过读写操作)，则需要等待Cache把内容先写回到内存中，然后才能把报文和控制结构体写到内存中。</li><li>运行在处理器上的驱动程序或者软件得到通知收到新报文，去内存中读取控制结构体和相应的报文，Cache不命中。之所以Cache一定不会命中，是因为即使该内存地址在Cache中，在步骤1中也被强制写回到内存中。因此，只能从内存中读取控制结构体和报文。</li></ol><h4 id="2-2-2-With-DDIO"><a href="#2-2-2-With-DDIO" class="headerlink" title="2.2.2 With DDIO"></a>2.2.2 With DDIO</h4><p>这时，报文和控制结构体通过PCI总线直接送到Cache中。这时有两种情形:</p><ol><li>a) 如果该内存恰好缓存在Cache中(有可能之前处理器有对该内存进行过读写操作)，则直接在Cache中更新内容，覆盖原有内容。<br>b) 如果该内存没有缓存在Cache中，则在最后一级Cache中分配一块区域，并相应更新Cache表，表明该内容是对应于内存中的某个地址的。</li><li>运行在处理器上的驱动或者软件被通知到有报文到达，其产生一个内存读操作，由于该内容已经在Cache中，因此直接从Cache中读。</li></ol><p>由此可以看出，DDIO技术在处理器和外设之间交换数据时，减少了处理器和外设访问内存的次数，也减少了Cache写回的等待，提高了系统的吞吐率和数据的交换延迟。</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology-brief.html" target="_blank" rel="noopener">Intel® Data Direct I/O Technology (Intel® DDIO): A Primer</a></li><li><a href="https://www.intel.com/content/www/us/en/io/data-direct-i-o-faq.html" target="_blank" rel="noopener">Intel Data Direct I/O (Intel DDIO): Frequently Asked Questions</a></li><li><a href="https://blog.csdn.net/qq_40500045/article/details/109272627" target="_blank" rel="noopener">谈谈DDIO你该知道的事</a></li><li><a href="https://www.usenix.org/system/files/atc20-farshin.pdf" target="_blank" rel="noopener">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel DDIO(Data Direct I/O) technology。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>thread core module die package</title>
    <link href="http://liujunming.github.io/2022/03/16/thread-core-module-die-package/"/>
    <id>http://liujunming.github.io/2022/03/16/thread-core-module-die-package/</id>
    <published>2022-03-16T06:42:02.000Z</published>
    <updated>2022-03-16T11:58:04.885Z</updated>
    
    <content type="html"><![CDATA[<p>Terms: thread core module die package.<a id="more"></a><br><img src="/images/2022/03/03.PNG" alt></p><p>Per SDM Vol4:<br><img src="/images/2022/03/02.PNG" alt></p><h3 id="thread"><a href="#thread" class="headerlink" title="thread"></a>thread</h3><p>Individual cores can support multiple <em>hardware threads</em> of execution. These are also known as <em>logical processors</em>. This technique has multiple names, including <em>simultaneous multithreading</em> (SMT) and <em>Hyper-Threading Technology </em>(HT).</p><h3 id="core"><a href="#core" class="headerlink" title="core"></a>core</h3><p><strong>(Physical) processor core</strong> is an independent execution unit that can run one program thread at a time in parallel with other cores.</p><h3 id="module"><a href="#module" class="headerlink" title="module"></a>module</h3><p>Intel Atom processors also have the concept of CPU modules. In these processors, two cores share a large L2 cache. The modules interface with the CPU fabric rather than the cores interfacing directly.</p><h3 id="die"><a href="#die" class="headerlink" title="die"></a>die</h3><p><strong>Processor die</strong> is a single continuous piece of semiconductor material (usually silicon). A die can contain any number of cores. Up to 15 are available on the Intel product line. Processor die is where the transistors making up the CPU actually reside.</p><p><img src="/images/2022/03/05.jpg" alt><br>One die with multiple cores</p><h3 id="package"><a href="#package" class="headerlink" title="package"></a>package</h3><p><strong>Processor package</strong> is what you get when you buy a single processor. It contains one or more dies, plastic/ceramic housing for dies and gold-plated contacts that match those on your motherboard.</p><p><img src="/images/2022/03/04.jpg" alt><br>CPU Package containing 2 separate DIEs</p><hr><p>参考资料:</p><ol><li><a href="https://link.springer.com/chapter/10.1007/978-1-4302-6638-9_2" target="_blank" rel="noopener">CPU Power Management</a></li><li><a href="https://superuser.com/questions/324284/what-is-meant-by-the-terms-cpu-core-die-and-package" target="_blank" rel="noopener">What is meant by the terms CPU, Core, Die and Package?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Terms: thread core module die package.
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>How to setup proxy for docker</title>
    <link href="http://liujunming.github.io/2022/03/15/How-to-setup-proxy-for-docker/"/>
    <id>http://liujunming.github.io/2022/03/15/How-to-setup-proxy-for-docker/</id>
    <published>2022-03-15T06:02:49.000Z</published>
    <updated>2022-03-15T08:23:12.113Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍docker中的代理设置。<a id="more"></a></p><h3 id="1-Docker-daemon"><a href="#1-Docker-daemon" class="headerlink" title="1. Docker daemon"></a>1. Docker daemon</h3><p><a href="https://stackoverflow.com/questions/23111631/cannot-download-docker-images-behind-a-proxy" target="_blank" rel="noopener">Cannot download Docker images behind a proxy</a></p><p>In order to download container images from the outside world when running commands like <code>docker pull busybox</code> ,set the proxies by:</p><ul><li>Create directory</li></ul><p><code>mkdir /etc/systemd/system/docker.service.d</code></p><ul><li>Create file</li></ul><p><code>vi /etc/systemd/system/docker.service.d/http-proxy.conf</code></p><ul><li>File content</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=&quot;HTTP_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:80/&quot;</span><br></pre></td></tr></table></figure><p>If you have internal Docker registries that you need to contact without proxying you can specify them via the <code>NO_PROXY</code> environment variable:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Environment=&quot;HTTP_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;NO_PROXY=localhost,127.0.0.0/8,docker-registry.somecorporation.com&quot;</span><br></pre></td></tr></table></figure></p><ul><li>Flush changes</li></ul><p><code>systemctl daemon-reload</code></p><ul><li>Verify that the configuration has been loaded</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show --property Environment docker</span><br><span class="line">Environment=HTTP_PROXY=http://proxy.example.com:80/</span><br><span class="line">Environment=HTTPS_PROXY=http://proxy.example.com:80/</span><br></pre></td></tr></table></figure><ul><li>Restart docker</li></ul><p><code>systemctl restart docker</code></p><h3 id="2-Running-container"><a href="#2-Running-container" class="headerlink" title="2. Running container"></a>2. Running container</h3><p>In order for a running container to access the internet you need to fix the dns names since Google’s are the default and they don’t work. We need to update the dns names by using <a href="https://docs.docker.com/engine/reference/commandline/dockerd/" target="_blank" rel="noopener">daemon configuration file</a>.</p><ul><li>Create file</li></ul><p><code>/etc/docker/daemon.json</code></p><ul><li>File content</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;dns&quot;:[</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reload daemon</li></ul><p><code>systemctl daemon-reload</code></p><ul><li>Restart docker</li></ul><p><code>systemctl restart docker</code></p><ul><li>Now you can run a container and pass the proxies like this</li></ul><p><code>docker run --env http_proxy=http://proxy.example.com:80/ --env https_proxy=http://proxy.example.com:80/ -ti ubuntu bash</code></p><h3 id="3-Building-the-container"><a href="#3-Building-the-container" class="headerlink" title="3. Building the container"></a>3. Building the container</h3><ul><li>You can use ENV in the docker file:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">FROM ubuntu:16.04</span><br><span class="line"> </span><br><span class="line">ENV http_proxy http://proxy.example.com:80/</span><br><span class="line">ENV https_proxy http://proxy.example.com:80/</span><br><span class="line">...</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://elegantinfrastructure.com/docker/ultimate-guide-to-docker-http-proxy-configuration/" target="_blank" rel="noopener">Ultimate Guide to Docker HTTP Proxy Configuration</a></li><li><a href="https://docs.docker.com/network/proxy/" target="_blank" rel="noopener">Configure Docker to use a proxy server</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍docker中的代理设置。
    
    </summary>
    
      <category term="工具" scheme="http://liujunming.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to VT-x Page-Modification Logging</title>
    <link href="http://liujunming.github.io/2022/03/02/Introduction-to-VT-x-Page-Modification-Logging/"/>
    <id>http://liujunming.github.io/2022/03/02/Introduction-to-VT-x-Page-Modification-Logging/</id>
    <published>2022-03-02T07:41:04.000Z</published>
    <updated>2022-03-02T10:39:30.417Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍VT-x中的Page-Modification Logging(PML)技术。<a id="more"></a></p><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h3><p>在没有PML前，VMM要监控GPA的修改时，需要将EPT的页面结构设置为not-present或者read-only，这样会触发许多EPT violations,开销非常大。</p><p>PML建立在CPU对EPT中的accessed与dirty标志位支持上。<br>当启用PML时，对EPT中设置了dirty标志位的写操作都会产生一条in-memory记录，报告写操作的GPA，当记录写满时，触发一次VM Exit，然后VMM就可以监控被修改的页面。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h3><p>PML is a new feature on Intel’s Boardwell server platfrom targeted to reduce overhead of dirty logging mechanism.</p><p>The specification can be found at: <a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/page-modification-logging-vmm-white-paper.pdf" target="_blank" rel="noopener">Page Modification Logging for Virtual Machine Monitor White Paper</a></p><p>Currently, dirty logging is done by write protection, which write protects guest memory, and mark dirty GFN to dirty_bitmap in subsequent write fault. This works fine, except with overhead of additional write fault for logging each dirty GFN. The overhead can be large if the write operations from guest is intensive.</p><p>PML is a hardware-assisted efficient way for dirty logging. PML logs dirty GPA automatically to a 4K PML memory buffer <strong>when CPU changes EPT table’s D-bit from 0 to 1</strong>. To do this, A new 4K PML buffer base address, and a PML index were added to VMCS. Initially PML index is set to 512 (8 bytes for each GPA), and CPU decreases PML index after logging one GPA, and eventually a PML buffer full VMEXIT happens when PML buffer is fully logged.</p><p><img src="/images/2022/03/01.PNG" alt></p><p>With PML, we don’t have to use write protection so the intensive write fault EPT violation can be avoided, with an additional PML buffer full VMEXIT for 512 dirty GPAs. Theoretically, this can reduce hypervisor overhead when guest is in dirty logging mode, and therefore more CPU cycles can be allocated to guest, so it’s expected benchmarks in guest will have better performance comparing to non-PML.</p><h3 id="3-KVM-design"><a href="#3-KVM-design" class="headerlink" title="3. KVM design"></a>3. KVM design</h3><h4 id="3-1-Enable-Disable-PML"><a href="#3-1-Enable-Disable-PML" class="headerlink" title="3.1 Enable/Disable PML"></a>3.1 Enable/Disable PML</h4><p>PML is per-vcpu (per-VMCS), while EPT table can be shared by vcpus, so we need to enable/disable PML for all vcpus of guest. A dedicated 4K page will be allocated for each vcpu when PML is enabled for that vcpu.</p><p>Currently, we choose to always enable PML for guest, which means we enables PML when creating VCPU, and never disable it during guest’s life time. This avoids the complicated logic to enable PML by demand when guest is running. And to eliminate potential unnecessary GPA logging in non-dirty logging mode, we set D-bit manually for the slots with dirty logging disabled(<a href="https://lore.kernel.org/kvm/1422413668-3509-4-git-send-email-kai.huang@linux.intel.com/" target="_blank" rel="noopener">KVM: MMU: Explicitly set D-bit for writable spte.</a>).</p><h4 id="3-2-Flush-PML-buffer"><a href="#3-2-Flush-PML-buffer" class="headerlink" title="3.2 Flush PML buffer"></a>3.2 Flush PML buffer</h4><p>When userspace querys dirty_bitmap, it’s possible that there are GPAs logged in vcpu’s PML buffer, but as PML buffer is not full, so no VMEXIT happens. In this case, we’d better to manually flush PML buffer for all vcpus and update the dirty GPAs to dirty_bitmap.</p><p>We do PML buffer flush at the beginning of each VMEXIT, this makes dirty_bitmap more updated, and also makes logic of flushing PML buffer for all vcpus easier– we only need to kick all vcpus out of guest and PML buffer for each vcpu will be flushed automatically.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.</span></span><br><span class="line"><span class="comment"> * Called before reporting dirty_bitmap to userspace.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">kvm_flush_pml_buffers</span><span class="params">(struct kvm *kvm)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_vcpu</span> *<span class="title">vcpu</span>;</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * We only need to kick vcpu out of guest mode here, as PML buffer</span></span><br><span class="line"><span class="comment"> * is flushed at beginning of all VMEXITs, and it's obvious that only</span></span><br><span class="line"><span class="comment"> * vcpus running in guest are possible to have unflushed GPAs in PML</span></span><br><span class="line"><span class="comment"> * buffer.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">kvm_for_each_vcpu(i, vcpu, kvm)</span><br><span class="line">kvm_vcpu_kick(vcpu);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/page-modification-logging-vmm-white-paper.pdf" target="_blank" rel="noopener">Page Modification Logging for Virtual Machine Monitor White Paper</a></li><li><a href="https://lore.kernel.org/kvm/1422413668-3509-1-git-send-email-kai.huang@linux.intel.com/" target="_blank" rel="noopener">KVM: VMX: Page Modification Logging (PML) support</a></li><li><a href="https://diting0x.github.io/20170821/intel-pml/" target="_blank" rel="noopener">Intel VT 页面修改记录(PML)</a></li><li><a href="https://arxiv.org/pdf/2001.09991.pdf" target="_blank" rel="noopener">Intel Page Modification Logging, a hardware virtualization feature: study and improvement for virtual machine working set estimation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍VT-x中的Page-Modification Logging(PML)技术。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
  </entry>
  
</feed>
