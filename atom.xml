<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2023-05-28T07:33:49.280Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes about Linux swiotlb技术</title>
    <link href="http://liujunming.github.io/2023/05/28/Notes-about-Linux-swiotlb%E6%8A%80%E6%9C%AF/"/>
    <id>http://liujunming.github.io/2023/05/28/Notes-about-Linux-swiotlb技术/</id>
    <published>2023-05-28T06:18:14.000Z</published>
    <updated>2023-05-28T07:33:49.280Z</updated>
    
    <content type="html"><![CDATA[<p>本文将总结下Linux swiotlb技术。<a id="more"></a></p><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><img src="/images/2023/05/52.jpg" alt></p><h3 id="2-bounce-buffer"><a href="#2-bounce-buffer" class="headerlink" title="2. bounce buffer"></a>2. bounce buffer</h3><p><img src="/images/2023/05/53.jpg" alt><br>If the requested DMA operation is a DMA read (the device reads from memory DMA_TO_DEVICE), the data is copied from the original buffer to the bounce buffer, and the adapter reads it from the bounce buffer’s memory location. If the requested DMA operation is a write, the data is written(the device writes to memory DMA_FROM_DEVICE) by the adapter to the bounce buffer, and then copied to the original buffer.</p><p><img src="/images/2023/05/54.jpg" alt><br><img src="/images/2023/05/55.jpg" alt></p><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>理解mmap时、同步时、unmap时的内存copy即可。<br>为了简单起见，只讨论DMA_FROM_DEVICE与DMA_TO_DEVICE这两个case。</p><ol><li>mmap时，建立原始物理地址与swiotlb buffer的映射，对于DMA_TO_DEVICE，需要将数据从原始物理地址处拷贝到swiotlb buffer</li><li>当设备往swiotlb buffer写入后，driver CPU需要读DMA内存时，需要sync下，将数据从swiotlb buffer拷贝到原始物理地址处</li><li>driver CPU更改原始物理地址内存，当需要DMA_TO_DEVICE时，需要sync下，将数据从原始物理地址处拷贝到swiotlb buffer</li><li>unmap时，解除原始物理地址与swiotlb buffer的映射，对于DMA_FROM_DEVICE，需要将数据从swiotlb buffer拷贝到原始物理地址处</li></ol><hr><p>参考资料:</p><ol><li><a href="https://research.ibm.com/haifa/dept/stt/pubs/utilizing-iommus-ols06.pdf" target="_blank" rel="noopener">Utilizing IOMMUs for Virtualization in Linux and Xen</a></li><li><a href="https://blog.csdn.net/liuhangtiant/article/details/87825466" target="_blank" rel="noopener">Linux swiotlb技术解析</a></li><li><a href="https://blog.csdn.net/qq_34719392/article/details/114873284" target="_blank" rel="noopener">Linux x86-64 IOMMU详解（二）——SWIOTLB（软件IOMMU）</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将总结下Linux swiotlb技术。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Notes about Linux NAPI</title>
    <link href="http://liujunming.github.io/2023/05/28/Notes-about-Linux-NAPI/"/>
    <id>http://liujunming.github.io/2023/05/28/Notes-about-Linux-NAPI/</id>
    <published>2023-05-28T03:48:45.000Z</published>
    <updated>2023-05-28T04:55:05.588Z</updated>
    
    <content type="html"><![CDATA[<p>本文将学习下Linux网络收包的NAPI机制。<a id="more"></a></p><h3 id="What"><a href="#What" class="headerlink" title="What"></a>What</h3><p>NAPI (“new API,” though it is not so new anymore) is an interrupt mitigation mechanism used with network devices. </p><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><blockquote><p>随着网络带宽的发展，网速越来越快，之前的中断收包模式已经无法适应目前千兆，万兆的带宽了。如果每个数据包大小等于MTU大小(1460字节)。当驱动以千兆网速收包时，CPU将每秒被中断91829次。过多的中断会引起一个问题，CPU一直陷入硬中断而没有时间来处理别的事情了。为了解决这个问题，内核引入了NAPI机制。</p></blockquote><blockquote><p>NAPI就是混合中断和轮询的方式来收包，当有中断来了，驱动关闭中断，通知内核收包，内核软中断轮询当前网卡，在规定时间尽可能多的收包。时间用尽或者没有数据可收，内核再次开启中断，准备下一次收包。</p></blockquote><p>When network traffic is heavy, the kernel can safely predict that incoming packets will be available anytime it gets around to looking, so there is no need to have the adapter interrupting it (possibly thousands of times per second) to tell it about those packets. So a NAPI-compliant driver will turn off the packet receive interrupt and provide a <code>poll()</code> method to the kernel. When the kernel is ready to deal with more packets, <code>poll()</code> will be called with a maximum number of packets it is allowed to feed into the kernel; it should process up to that many packets and quit.</p><h3 id="Full-Picture"><a href="#Full-Picture" class="headerlink" title="Full Picture"></a>Full Picture</h3><p><img src="/images/2023/05/51.png" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/images/2023/05/50.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://lwn.net/Articles/214457/" target="_blank" rel="noopener">Reworking NAPI</a></li><li><a href="https://lwn.net/Articles/244640/" target="_blank" rel="noopener">Newer, newer NAPI</a></li><li><a href="https://docs.kernel.org/networking/napi.html" target="_blank" rel="noopener">NAPI</a></li><li><a href="https://blog.csdn.net/Rong_Toa/article/details/109401935" target="_blank" rel="noopener">Linux网络协议栈：NAPI机制与处理流程分析（图解）</a></li><li><a href="https://zhuanlan.zhihu.com/p/610334133" target="_blank" rel="noopener">NAPI 内核机制与驱动实现</a></li><li><a href="https://wenfh2020.com/2021/12/29/kernel-tcp-receive/" target="_blank" rel="noopener">Linux 网络数据接收流程（TCP）- NAPI</a></li><li><a href="https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf" target="_blank" rel="noopener">Red Hat Enterprise Linux Network Performance Tuning Guide</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将学习下Linux网络收包的NAPI机制。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about SMAP and SMEP</title>
    <link href="http://liujunming.github.io/2023/05/27/Notes-about-SMAP-and-SMEP/"/>
    <id>http://liujunming.github.io/2023/05/27/Notes-about-SMAP-and-SMEP/</id>
    <published>2023-05-27T07:54:33.000Z</published>
    <updated>2023-05-27T10:16:36.435Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下SMAP(Supervisor Memory Access Protection) and SMEP(Supervisor Memory Execute Protection)相关notes，参考kernel版本为<a href="https://elixir.bootlin.com/linux/v6.3/source" target="_blank" rel="noopener">v6.3</a>。<a id="more"></a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>SMEP prevents the kernel running in ring 0 from executing code which is user accessible. SMAP prevents the kernel from accessing userspace memory while the AC flag in the RFLAGS register is clear. These features can help harden the kernel against exploitation and prevent certain kinds of memory corruption.</p><h3 id="Description-from-SDM"><a href="#Description-from-SDM" class="headerlink" title="Description from SDM"></a>Description from SDM</h3><p><img src="/images/2023/05/45.jpg" alt><br><img src="/images/2023/05/46.jpg" alt><br><img src="/images/2023/05/47.jpg" alt></p><p><img src="/images/2023/05/48.jpg" alt><br><img src="/images/2023/05/49.jpg" alt></p><h3 id="copy-to-user"><a href="#copy-to-user" class="headerlink" title="copy_to_user"></a>copy_to_user</h3><p>由于内核空间与用户空间的内存不能直接互访，因此需要借助内核函数<code>copy_to_user</code>完成内核空间到用户空间的复制。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * igb_ptp_get_ts_config - get hardware time stamping config</span></span><br><span class="line"><span class="comment"> * @netdev: netdev struct</span></span><br><span class="line"><span class="comment"> * @ifr: interface struct</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Get the hwtstamp_config settings to return to the user. Rather than attempt</span></span><br><span class="line"><span class="comment"> * to deconstruct the settings from the registers, just return a shadow copy</span></span><br><span class="line"><span class="comment"> * of the last known settings.</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">igb_ptp_get_ts_config</span><span class="params">(struct net_device *netdev, struct ifreq *ifr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">igb_adapter</span> *<span class="title">adapter</span> = <span class="title">netdev_priv</span>(<span class="title">netdev</span>);</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">hwtstamp_config</span> *<span class="title">config</span> = &amp;<span class="title">adapter</span>-&gt;<span class="title">tstamp_config</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> copy_to_user(ifr-&gt;ifr_data, config, <span class="keyword">sizeof</span>(*config)) ?</span><br><span class="line">        -EFAULT : <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">copy_to_user</span><br><span class="line">└── _copy_to_user</span><br><span class="line">    └── raw_copy_to_user</span><br><span class="line">        └── copy_user_generic</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> __always_inline __must_check <span class="keyword">unsigned</span> <span class="keyword">long</span></span><br><span class="line">copy_user_generic(<span class="keyword">void</span> *to, <span class="keyword">const</span> <span class="keyword">void</span> *from, <span class="keyword">unsigned</span> len)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> ret;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * If CPU has ERMS feature, use copy_user_enhanced_fast_string.</span></span><br><span class="line"><span class="comment">     * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.</span></span><br><span class="line"><span class="comment">     * Otherwise, use copy_user_generic_unrolled.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    alternative_call_2(copy_user_generic_unrolled,</span><br><span class="line">             copy_user_generic_string,</span><br><span class="line">             X86_FEATURE_REP_GOOD,</span><br><span class="line">             copy_user_enhanced_fast_string,</span><br><span class="line">             X86_FEATURE_ERMS,</span><br><span class="line">             ASM_OUTPUT2(<span class="string">"=a"</span> (ret), <span class="string">"=D"</span> (to), <span class="string">"=S"</span> (from),</span><br><span class="line">                     <span class="string">"=d"</span> (len)),</span><br><span class="line">             <span class="string">"1"</span> (to), <span class="string">"2"</span> (from), <span class="string">"3"</span> (len)</span><br><span class="line">             : <span class="string">"memory"</span>, <span class="string">"rcx"</span>, <span class="string">"r8"</span>, <span class="string">"r9"</span>, <span class="string">"r10"</span>, <span class="string">"r11"</span>);</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">SYM_FUNC_START(copy_user_generic_string)</span><br><span class="line">    ASM_STAC</span><br><span class="line">    cmpl $<span class="number">8</span>,%edx</span><br><span class="line">    jb <span class="number">2f</span>       <span class="comment">/* less than 8 bytes, go to byte copy loop */</span></span><br><span class="line">    ALIGN_DESTINATION</span><br><span class="line">    movl %edx,%ecx</span><br><span class="line">    shrl $<span class="number">3</span>,%ecx</span><br><span class="line">    andl $<span class="number">7</span>,%edx</span><br><span class="line"><span class="number">1</span>:  rep movsq</span><br><span class="line"><span class="number">2</span>:  movl %edx,%ecx</span><br><span class="line"><span class="number">3</span>:  rep movsb</span><br><span class="line">    xorl %eax,%eax</span><br><span class="line">    ASM_CLAC</span><br><span class="line">    RET</span><br><span class="line"></span><br><span class="line"><span class="number">11</span>: leal (%rdx,%rcx,<span class="number">8</span>),%ecx</span><br><span class="line"><span class="number">12</span>: movl %ecx,%edx      <span class="comment">/* ecx is zerorest also */</span></span><br><span class="line">    jmp .Lcopy_user_handle_tail</span><br><span class="line"></span><br><span class="line">    _ASM_EXTABLE_CPY(<span class="number">1b</span>, <span class="number">11b</span>)</span><br><span class="line">    _ASM_EXTABLE_CPY(<span class="number">3b</span>, <span class="number">12b</span>)</span><br><span class="line">SYM_FUNC_END(copy_user_generic_string)</span><br><span class="line">EXPORT_SYMBOL(copy_user_generic_string)</span><br></pre></td></tr></table></figure><p>从copy_user_generic_string的第2行可知，在内核态往用户态复制内存前，需要运行<code>STAC</code>指令；从第13行可知，在内核态往用户态复制内存后，需要运行<code>CLAC</code>指令。</p><hr><p>参考资料:</p><ol><li><a href="https://wiki.osdev.org/Supervisor_Memory_Protection" target="_blank" rel="noopener">Supervisor Memory Protection</a></li><li><a href="https://lore.kernel.org/all/1348256595-29119-9-git-send-email-hpa@linux.intel.com/" target="_blank" rel="noopener">x86, smap: Add STAC and CLAC instructions to control user space access</a></li><li><a href="https://blog.csdn.net/Haomione/article/details/122217131" target="_blank" rel="noopener">copy_to_user/copy_from_user参数解析</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下SMAP(Supervisor Memory Access Protection) and SMEP(Supervisor Memory Execute Protection)相关notes，参考kernel版本为&lt;a href=&quot;https://elixir.bootlin.com/linux/v6.3/source&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;v6.3&lt;/a&gt;。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about iommu=pt kernel parameter</title>
    <link href="http://liujunming.github.io/2023/05/21/Notes-about-iommu-pt-kernel-parameter/"/>
    <id>http://liujunming.github.io/2023/05/21/Notes-about-iommu-pt-kernel-parameter/</id>
    <published>2023-05-21T07:22:58.000Z</published>
    <updated>2023-05-21T13:02:58.896Z</updated>
    
    <content type="html"><![CDATA[<p>当使用KVM pass-thru设备时，通常会设置<code>intel_iommu=on iommu=pt</code>内核参数，其中<code>intel_iommu=on</code>就是使能intel iommu，本文将介绍<code>iommu=pt</code>。<a id="more"></a></p><p>本文参考的内核版本是<a href="https://elixir.bootlin.com/linux/v5.0/source" target="_blank" rel="noopener">v5.0</a>。</p><p>identity mapping指的是iova与hpa 1:1映射。</p><h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h2><p>The <code>pt</code> option only enables IOMMU translation for devices used in pass-thru ,doesn’t enable IOMMU translation for host used devices ,and this will improve performance for host PCIe devices (which are not pass-thru to a VM).</p><p>内核的注释：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This variable becomes 1 if iommu=pt is passed on the kernel command line.</span></span><br><span class="line"><span class="comment"> * If this variable is 1, IOMMU implementations do no DMA translation for</span></span><br><span class="line"><span class="comment"> * devices and allow every device to access to whole physical memory. This is</span></span><br><span class="line"><span class="comment"> * useful if a user wants to use an IOMMU only for KVM device assignment to</span></span><br><span class="line"><span class="comment"> * guests and not for driver dma translation.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> iommu_pass_through __read_mostly = <span class="number">1</span>;</span><br></pre></td></tr></table></figure></p><h2 id="2-源码解析"><a href="#2-源码解析" class="headerlink" title="2. 源码解析"></a>2. 源码解析</h2><h3 id="2-1-pt-option解析"><a href="#2-1-pt-option解析" class="headerlink" title="2.1 pt option解析"></a>2.1 <code>pt</code> option解析</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> __<span class="function">init <span class="keyword">int</span> <span class="title">iommu_setup</span><span class="params">(<span class="keyword">char</span> *p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">strncmp</span>(p, <span class="string">"pt"</span>, <span class="number">2</span>))</span><br><span class="line">        iommu_pass_through = <span class="number">1</span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-init-dmars"><a href="#2-2-init-dmars" class="headerlink" title="2.2 init_dmars"></a>2.2 init_dmars</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> IDENTMAP_ALL        1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This domain is a statically identity mapping domain.</span></span><br><span class="line"><span class="comment"> *  1. This domain creats a static 1:1 mapping to all usable memory.</span></span><br><span class="line"><span class="comment"> *  2. It maps to each iommu if successful.</span></span><br><span class="line"><span class="comment"> *  3. Each iommu maps to this domain if successful.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">dmar_domain</span> *<span class="title">si_domain</span>;</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> hw_pass_through = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __<span class="function">init <span class="title">init_dmars</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (!ecap_pass_through(iommu-&gt;ecap))</span><br><span class="line">        hw_pass_through = <span class="number">0</span>;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (iommu_pass_through)</span><br><span class="line">        iommu_identity_mapping |= IDENTMAP_ALL;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (iommu_identity_mapping) &#123;</span><br><span class="line">        ret = si_domain_init(hw_pass_through);</span><br><span class="line">        <span class="keyword">if</span> (ret)</span><br><span class="line">            <span class="keyword">goto</span> free_iommu;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (iommu_identity_mapping) &#123;</span><br><span class="line">        ret = iommu_prepare_static_identity_mapping(hw_pass_through);</span><br><span class="line">        <span class="keyword">if</span> (ret) &#123;</span><br><span class="line">            pr_crit(<span class="string">"Failed to setup IOMMU pass-through\n"</span>);</span><br><span class="line">            <span class="keyword">goto</span> free_iommu;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>#define ecap_pass_through(e)    ((e &gt;&gt; 6) &amp; 0x1)</code></p><p><code>ecap_pass_through(iommu-&gt;ecap)</code>的含义是检查Extended Capability Register的<code>PT</code> field。<br><img src="/images/2023/05/43.jpg" alt><br>如果Hardware supports pass-through translation type,那么<code>hw_pass_through</code>为1；否则<code>hw_pass_through</code>为0。</p><p>当<code>iommu_pass_through</code>被设置时，<code>iommu_identity_mapping</code>也会被设置。接着会依次调用<code>si_domain_init</code>与<code>iommu_prepare_static_identity_mapping</code>。</p><h3 id="2-3-si-domain-init"><a href="#2-3-si-domain-init" class="headerlink" title="2.3 si_domain_init"></a>2.3 si_domain_init</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __<span class="function">init <span class="title">si_domain_init</span><span class="params">(<span class="keyword">int</span> hw)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> nid, ret = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    si_domain = alloc_domain(DOMAIN_FLAG_STATIC_IDENTITY);</span><br><span class="line">    <span class="keyword">if</span> (!si_domain)</span><br><span class="line">        <span class="keyword">return</span> -EFAULT;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (md_domain_init(si_domain, DEFAULT_DOMAIN_ADDRESS_WIDTH)) &#123;</span><br><span class="line">        domain_exit(si_domain);</span><br><span class="line">        <span class="keyword">return</span> -EFAULT;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    pr_debug(<span class="string">"Identity mapping domain allocated\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (hw)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    for_each_online_node(nid) &#123; <span class="comment">//迭代所有的活动结点(针对NUMA)</span></span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">long</span> start_pfn, end_pfn;</span><br><span class="line">        <span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">        for_each_mem_pfn_range(i, nid, &amp;start_pfn, &amp;end_pfn, <span class="literal">NULL</span>) &#123;</span><br><span class="line">            ret = iommu_domain_identity_map(si_domain, <span class="comment">//iova与hpa 1:1映射</span></span><br><span class="line">                    PFN_PHYS(start_pfn), PFN_PHYS(end_pfn));</span><br><span class="line">            <span class="keyword">if</span> (ret)</span><br><span class="line">                <span class="keyword">return</span> ret;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上述代码可知，当<code>hw_pass_through</code>为1时，无需建立iova与hpa 1:1映射的iommu页表；否则需要对all usable memory建立iova与hpa 1:1映射的iommu页表。</p><h3 id="2-4-iommu-prepare-static-identity-mapping"><a href="#2-4-iommu-prepare-static-identity-mapping" class="headerlink" title="2.4 iommu_prepare_static_identity_mapping"></a>2.4 iommu_prepare_static_identity_mapping</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iommu_prepare_static_identity_mapping</span><br><span class="line">└── dev_prepare_static_identity_mapping</span><br><span class="line">    └── domain_add_dev_info</span><br><span class="line">        └── dmar_insert_one_dev_info</span><br><span class="line">            └── domain_context_mapping</span><br><span class="line">                └── domain_context_mapping_one</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">domain_context_mapping_one</span><span class="params">(struct dmar_domain *domain,</span></span></span><br><span class="line"><span class="function"><span class="params">                                      struct intel_iommu *iommu,</span></span></span><br><span class="line"><span class="function"><span class="params">                                      u8 bus, u8 devfn)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 设置translation type 为 pass through</span></span><br><span class="line">    <span class="keyword">if</span> (hw_pass_through &amp;&amp; domain_type_is_si(domain))</span><br><span class="line">        translation = CONTEXT_TT_PASS_THROUGH;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 获取这个设备在contex table表里面的地址</span></span><br><span class="line">    context = iommu_context_addr(iommu, bus, devfn, <span class="number">1</span>);</span><br><span class="line">    ...</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">dma_pte</span> *<span class="title">pgd</span> = <span class="title">domain</span>-&gt;<span class="title">pgd</span>;</span> <span class="comment">// iova页表基址</span></span><br><span class="line">    <span class="keyword">int</span> agaw;</span><br><span class="line"></span><br><span class="line">    context_set_domain_id(context, did);</span><br><span class="line">    <span class="comment">// 设置转换类型</span></span><br><span class="line">    context_set_translation_type(context, translation);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 下面代码可以看出pass through模式不会设置iova页表地址</span></span><br><span class="line">    <span class="keyword">if</span> (translation != CONTEXT_TT_PASS_THROUGH) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 非pass through模式下需要设置iova页表的基地址</span></span><br><span class="line">        context_set_address_root(context, virt_to_phys(pgd));</span><br><span class="line">        context_set_address_width(context, agaw);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * In pass through mode, AW must be programmed to</span></span><br><span class="line"><span class="comment">         * indicate the largest AGAW value supported by</span></span><br><span class="line"><span class="comment">         * hardware. And ASR is ignored by hardware.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        context_set_address_width(context, iommu-&gt;msagaw);</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>#define CONTEXT_TT_PASS_THROUGH 2</code><br><img src="/images/2023/05/44.jpg" alt><br>因此<code>CONTEXT_TT_PASS_THROUGH</code>为10b，即是2。</p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>配置了<code>iommu=pt</code>就会实现identity mapping:</p><ul><li>如果Hardware supports pass-through translation type，则配置pass-through translation type即可实现identity mapping，此时无需配置iommu页表;</li><li>如果Hardware doesn’t support pass-through translation type，则需要配置iommu页表，使得iova与hpa 1:1映射。</li></ul><p>当<code>hw_pass_through</code>=0时，依然要走iommu页表，因此性能是不如<code>hw_pass_through</code>=1的。</p><hr><p>参考资料:</p><ol><li><a href="http://blog.chinaunix.net/uid-28541347-id-5868588.html" target="_blank" rel="noopener">iommu passthrough分析</a></li><li>Intel VT-d spec</li><li><a href="https://zhuanlan.zhihu.com/p/365408539" target="_blank" rel="noopener">IOMMU(二)-从配置说起</a></li><li><a href="https://pve.proxmox.com/wiki/PCI_Passthrough#PT_Mode" target="_blank" rel="noopener">PCI_Passthrough PT Mode</a></li><li><a href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/installation_guide/appe-configuring_a_hypervisor_host_for_pci_passthrough" target="_blank" rel="noopener">Configuring a Host for PCI Passthrough</a></li><li><a href="https://mp.weixin.qq.com/s/6OK4e-m_NRn4vdl3p4sbTw" target="_blank" rel="noopener">深入了解iommu系列一：iommu硬件架构和驱动初始化</a></li><li><a href="https://zhuanlan.zhihu.com/p/479963917" target="_blank" rel="noopener">深入了解iommu系列二:iommu 工作原理解析之dma remapping</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当使用KVM pass-thru设备时，通常会设置&lt;code&gt;intel_iommu=on iommu=pt&lt;/code&gt;内核参数，其中&lt;code&gt;intel_iommu=on&lt;/code&gt;就是使能intel iommu，本文将介绍&lt;code&gt;iommu=pt&lt;/code&gt;。
    
    </summary>
    
      <category term="IOMMU" scheme="http://liujunming.github.io/categories/IOMMU/"/>
    
    
      <category term="IOMMU" scheme="http://liujunming.github.io/tags/IOMMU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Speed Up Boot-up Time for Guest in Alibaba Cloud</title>
    <link href="http://liujunming.github.io/2023/05/14/Notes-about-Speed-Up-Boot-up-Time-for-Guest-in-Alibaba-Cloud/"/>
    <id>http://liujunming.github.io/2023/05/14/Notes-about-Speed-Up-Boot-up-Time-for-Guest-in-Alibaba-Cloud/</id>
    <published>2023-05-14T04:31:55.000Z</published>
    <updated>2023-05-14T10:01:50.931Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about <a href="https://static.sched.com/hosted_files/kvmforum2020/51/The%20Practice%20Method%20to%20Speed%20Up%2010x%20Boot-up%20Time%20for%20Guest%20in%20Alibaba%20Cloud.pdf" target="_blank" rel="noopener">Speed Up Boot-up Time for Guest in Alibaba Cloud</a>。</p><p>Motivation: 当有<a href="/2021/07/29/Notes-about-guest-memory-pinning-when-direct-assignment-of-I-0-devices/">pass-thru设备</a>，虚拟机启动时，需要分配好全部内存;当虚拟机内存较大时，开机时间较长。<br>Idea: 利用气球驱动，guest刚启动时，气球充气，Only map necessary memory first；guest启动后，气球放气，将内存还给guest。这样在guest启动过程中，只需map必要内存，无需map所有内存，即可达到Speed Up Boot-up Time for Guest的目的。<br><a id="more"></a></p><p><img src="/images/2023/05/39.jpg" alt><br><img src="/images/2023/05/40.jpg" alt><br><img src="/images/2023/05/42.jpg" alt><br><img src="/images/2023/05/41.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about &lt;a href=&quot;https://static.sched.com/hosted_files/kvmforum2020/51/The%20Practice%20Method%20to%20Speed%20Up%2010x%20Boot-up%20Time%20for%20Guest%20in%20Alibaba%20Cloud.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Speed Up Boot-up Time for Guest in Alibaba Cloud&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Motivation: 当有&lt;a href=&quot;/2021/07/29/Notes-about-guest-memory-pinning-when-direct-assignment-of-I-0-devices/&quot;&gt;pass-thru设备&lt;/a&gt;，虚拟机启动时，需要分配好全部内存;当虚拟机内存较大时，开机时间较长。&lt;br&gt;Idea: 利用气球驱动，guest刚启动时，气球充气，Only map necessary memory first；guest启动后，气球放气，将内存还给guest。这样在guest启动过程中，只需map必要内存，无需map所有内存，即可达到Speed Up Boot-up Time for Guest的目的。&lt;br&gt;
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Crystal Beach DMA(CBDMA)</title>
    <link href="http://liujunming.github.io/2023/05/14/Notes-about-Crystal-Beach-DMA-CBDMA/"/>
    <id>http://liujunming.github.io/2023/05/14/Notes-about-Crystal-Beach-DMA-CBDMA/</id>
    <published>2023-05-14T03:23:54.000Z</published>
    <updated>2023-05-14T04:16:06.272Z</updated>
    
    <content type="html"><![CDATA[<p>Crystal Beach DMA(CBDMA)其实就是<a href="/2022/03/29/Introduction-to-Intel-I-OAT/#2-Intel®-QuickData-Technology">Intel® QuickData Technology</a>，说白了就是offload memory copy to DMA engine，<a href="/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/">DSA</a>代替了该技术。<a id="more"></a></p><p><img src="/images/2023/05/37.jpg" alt></p><p><img src="/images/2023/05/38.jpg" alt></p><p>FAST ‘23 paper <a href="https://www.usenix.org/conference/fast23/presentation/su" target="_blank" rel="noopener">Revitalizing the Forgotten On-Chip DMA to Expedite Data Movement in NVM-based Storage Systems</a>也是使用了CBDMA来offload memory copy。</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/133489817" target="_blank" rel="noopener">NFV加速利器，CPU中的CBDMA引擎</a></li><li><a href="https://www.dpdk.org/wp-content/uploads/sites/35/2018/12/JiayuHu_Accelerating_paravirtio_with_CBDMA.pdf" target="_blank" rel="noopener">Accelerating Para-Virtual I/O with CBDMA</a></li><li><a href="https://static.sched.com/hosted_files/dpdkbordeaux2019/09/Asynchronous%20CBDMA%20Enqueue%20Framework%20for%20vHost-User.pdf" target="_blank" rel="noopener">Asynchronous CBDMA Enqueue Framework for vHost-User</a></li><li><a href="https://insujang.github.io/2021-04-26/using-intel-ioat-dma/" target="_blank" rel="noopener">Using Intel IOAT DMA</a></li><li><a href="https://www.intel.com/content/www/us/en/wireless-network/accel-technology.html" target="_blank" rel="noopener">Intel® I/O Acceleration Technology</a></li><li><a href="https://www.usenix.org/conference/fast23/presentation/su" target="_blank" rel="noopener">Revitalizing the Forgotten On-Chip DMA to Expedite Data Movement in NVM-based Storage Systems</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Crystal Beach DMA(CBDMA)其实就是&lt;a href=&quot;/2022/03/29/Introduction-to-Intel-I-OAT/#2-Intel®-QuickData-Technology&quot;&gt;Intel® QuickData Technology&lt;/a&gt;，说白了就是offload memory copy to DMA engine，&lt;a href=&quot;/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/&quot;&gt;DSA&lt;/a&gt;代替了该技术。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about VT-d Virtual Command Support</title>
    <link href="http://liujunming.github.io/2023/05/13/Notes-about-VT-d-Virtual-Command-Support/"/>
    <id>http://liujunming.github.io/2023/05/13/Notes-about-VT-d-Virtual-Command-Support/</id>
    <published>2023-05-12T23:41:02.000Z</published>
    <updated>2023-05-13T03:45:24.725Z</updated>
    
    <content type="html"><![CDATA[<p>Virtual Command Support (VCS) - Virtual register intended to help support virtualization of the IOMMU. Unlike an SR-IOV device where an entire device is exposed to a guest, the new model creates device instances using PASID. This requires the PASID to be a flat global space which <strong>requires the guest and host PASIDs to be the same</strong>. Only virtual IOMMUs exposed to a guest would enumerate this capability. <strong>It provides an interface to for the host to control allocation of PASIDs in a guest OS</strong>.<a id="more"></a></p><p><img src="/images/2023/05/36.jpg" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Virtual command interface for enlightened pasid management. */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_CMD_ALLOC          0x1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_CMD_FREE           0x2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_VRSP_IP            0x1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_VRSP_SC(e)         (((e) &amp; 0xff) &gt;&gt; 1)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_VRSP_SC_SUCCESS        0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_VRSP_SC_NO_PASID_AVAIL 16</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_VRSP_SC_INVALID_PASID  16</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_VRSP_RESULT_PASID(e)   (((e) &gt;&gt; 16) &amp; 0xfffff)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VCMD_CMD_OPERAND(e)     ((e) &lt;&lt; 16)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">vcmd_alloc_pasid</span><span class="params">(struct intel_iommu *iommu, u32 *pasid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> flags;</span><br><span class="line">    u8 status_code;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    u64 res;</span><br><span class="line"></span><br><span class="line">    raw_spin_lock_irqsave(&amp;iommu-&gt;register_lock, flags);</span><br><span class="line">    dmar_writeq(iommu-&gt;reg + DMAR_VCMD_REG, VCMD_CMD_ALLOC);</span><br><span class="line">    IOMMU_WAIT_OP(iommu, DMAR_VCRSP_REG, dmar_readq,</span><br><span class="line">              !(res &amp; VCMD_VRSP_IP), res);</span><br><span class="line">    raw_spin_unlock_irqrestore(&amp;iommu-&gt;register_lock, flags);</span><br><span class="line"></span><br><span class="line">    status_code = VCMD_VRSP_SC(res);</span><br><span class="line">    <span class="keyword">switch</span> (status_code) &#123;</span><br><span class="line">    <span class="keyword">case</span> VCMD_VRSP_SC_SUCCESS:</span><br><span class="line">        *pasid = VCMD_VRSP_RESULT_PASID(res);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> VCMD_VRSP_SC_NO_PASID_AVAIL:</span><br><span class="line">        pr_info(<span class="string">"IOMMU: %s: No PASID available\n"</span>, iommu-&gt;name);</span><br><span class="line">        ret = -ENOSPC;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        ret = -ENODEV;</span><br><span class="line">        pr_warn(<span class="string">"IOMMU: %s: Unexpected error code %d\n"</span>,</span><br><span class="line">            iommu-&gt;name, status_code);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vcmd_free_pasid</span><span class="params">(struct intel_iommu *iommu, u32 pasid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> flags;</span><br><span class="line">    u8 status_code;</span><br><span class="line">    u64 res;</span><br><span class="line"></span><br><span class="line">    raw_spin_lock_irqsave(&amp;iommu-&gt;register_lock, flags);</span><br><span class="line">    dmar_writeq(iommu-&gt;reg + DMAR_VCMD_REG,</span><br><span class="line">            VCMD_CMD_OPERAND(pasid) | VCMD_CMD_FREE);</span><br><span class="line">    IOMMU_WAIT_OP(iommu, DMAR_VCRSP_REG, dmar_readq,</span><br><span class="line">              !(res &amp; VCMD_VRSP_IP), res);</span><br><span class="line">    raw_spin_unlock_irqrestore(&amp;iommu-&gt;register_lock, flags);</span><br><span class="line"></span><br><span class="line">    status_code = VCMD_VRSP_SC(res);</span><br><span class="line">    <span class="keyword">switch</span> (status_code) &#123;</span><br><span class="line">    <span class="keyword">case</span> VCMD_VRSP_SC_SUCCESS:</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> VCMD_VRSP_SC_INVALID_PASID:</span><br><span class="line">        pr_info(<span class="string">"IOMMU: %s: Invalid PASID\n"</span>, iommu-&gt;name);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        pr_warn(<span class="string">"IOMMU: %s: Unexpected error code %d\n"</span>,</span><br><span class="line">            iommu-&gt;name, status_code);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>流程主要分为如下两个步骤:</p><ol><li><p>guest分配pasid时，写vIOMMU的vcmd寄存器，此时会trap下来，host会将分配好的host paisd传给guest，这样guest与host的pasid就一样了。</p></li><li><p>当guest配置WQ Configuration register(MMIO寄存器)的PASID field时需要trap下来，hypervisor会检查guest的pasid与host的pasid是否一致，如果一致，那么hypervisor会将这个host PASID写入物理WQ Configuration register的PASID field。</p></li></ol><hr><p>参考资料:</p><ol><li><a href="https://01.org/blogs/ashokraj/2018/recent-enhancements-intel-virtualization-technology-directed-i/o-intel-vt-d" target="_blank" rel="noopener">RECENT ENHANCEMENTS IN INTEL® VIRTUALIZATION TECHNOLOGY FOR DIRECTED I/O (INTEL® VT-D)</a></li><li><a href="https://elixir.bootlin.com/linux/v6.3/source" target="_blank" rel="noopener">Linux kernel v6.3</a></li><li>Intel VT-d spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Virtual Command Support (VCS) - Virtual register intended to help support virtualization of the IOMMU. Unlike an SR-IOV device where an entire device is exposed to a guest, the new model creates device instances using PASID. This requires the PASID to be a flat global space which &lt;strong&gt;requires the guest and host PASIDs to be the same&lt;/strong&gt;. Only virtual IOMMUs exposed to a guest would enumerate this capability. &lt;strong&gt;It provides an interface to for the host to control allocation of PASIDs in a guest OS&lt;/strong&gt;.
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Shared Virtual Memory virtualization</title>
    <link href="http://liujunming.github.io/2023/05/07/Notes-about-Shared-Virtual-Memory-virtualization/"/>
    <id>http://liujunming.github.io/2023/05/07/Notes-about-Shared-Virtual-Memory-virtualization/</id>
    <published>2023-05-07T07:37:36.000Z</published>
    <updated>2023-05-07T11:32:15.311Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍下Shared Virtual Memory virtualization相关内容。<a id="more"></a></p><h2 id="1-Prerequisite"><a href="#1-Prerequisite" class="headerlink" title="1. Prerequisite"></a>1. Prerequisite</h2><ul><li><a href="/2022/03/30/Introduction-to-Shared-Virtual-Memory/">Shared Virtual Memory</a></li><li>VT-d基础知识(需要阅读VT-d spec)<ul><li>Legacy Mode Address Translation</li><li><a href="/2022/05/14/浅谈IOMMU-pageing-structures/">Scalable Mode Address Translation</a><ul><li>First-Stage Translation</li><li>Second-Stage Translation</li><li>Nested Translation</li><li>Pass-through Translation<br><img src="/images/2023/05/24.png" alt><br><img src="/images/2023/05/34.jpg" alt></li></ul></li></ul></li></ul><h2 id="2-Enable-SVM-in-VM"><a href="#2-Enable-SVM-in-VM" class="headerlink" title="2. Enable SVM in VM"></a>2. Enable SVM in VM</h2><p><img src="/images/2023/05/25.jpg" alt></p><p><img src="/images/2023/05/26.jpg" alt></p><p><img src="/images/2023/05/27.jpg" alt></p><p><img src="/images/2023/05/28.jpg" alt></p><p><img src="/images/2023/05/32.jpg" alt></p><p><img src="/images/2023/05/33.jpg" alt></p><p><img src="/images/2023/05/29.jpg" alt></p><p><img src="/images/2023/05/30.jpg" alt></p><p><img src="/images/2023/05/31.jpg" alt></p><p><img src="/images/2023/05/35.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Shared-Virtual-Memory-in-KVM_Yi-Liu.pdf" target="_blank" rel="noopener">Shared Virtual Memory in KVM</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2018/52/kvm-forum-vSVA-yliu-jpan-jean-eric.pdf" target="_blank" rel="noopener">Shared Virtual Addressing in KVM</a></li><li><a href="https://www.youtube.com/watch?v=Kq_nfGK5MwQ" target="_blank" rel="noopener">Video for SVM in KVM forum</a></li><li><a href="http://blog.chinaunix.net/uid-28541347-id-5854016.html" target="_blank" rel="noopener">Shared Virtual Memory（SVM）介绍</a></li><li><a href="https://www.cnblogs.com/shaohef/p/12079657.html" target="_blank" rel="noopener">Shared Virtual Memory (SVM) Functions</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍下Shared Virtual Memory virtualization相关内容。
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
  </entry>
  
  <entry>
    <title>DSA dedicated work queue virtualization</title>
    <link href="http://liujunming.github.io/2023/05/07/DSA-dedicated-work-queue-virtualization/"/>
    <id>http://liujunming.github.io/2023/05/07/DSA-dedicated-work-queue-virtualization/</id>
    <published>2023-05-07T00:13:11.000Z</published>
    <updated>2023-05-07T11:24:27.729Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/2023/05/04/Scalable-Work-Submission-in-Device-Virtualization/">Scalable Work Submission in Device Virtualization</a>介绍了<a href="/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/">DSA</a> shared work queue的virtualization，本文将介绍DSA dedicated work queue的virtualization。<a id="more"></a></p><p>本文将带着如下两个问题进行讨论：<br>Q1: 虚拟机用dedicated work queue时，要使用pasid吗？<br>Q2: 如果guest使用pasid，那pasid翻译该如何操作?</p><h3 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h3><p><img src="/images/2023/05/22.png" alt><br>从DSA的spec中可知，当使用dedicated work queue时，pasid是一个可选项。因此，当虚拟机使用dedicated work queue时，可以使用pasid，也可以不使用pasid。</p><h3 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h3><p>To submit work to a Dedicated Work Queue, software uses a 64-byte memory write transaction with write atomicity.</p><p>On Intel CPUs, work submission to a DWQ(Dedicated Work Queue) is performed using the MOVDIR64B instruction, which generates a non-torn 64-byte write. </p><p>If the PASID capability is enabled, the WQ(Work Queue) PASID Enable field of the WQ Configuration register controls whether PASID is used for each DWQ. Since the MOVDIR64B instruction does not fill in the PASID as the ENQCMD or ENQCMDS instructions do, the PASID field in the descriptor is ignored. When PASID is enabled for a DWQ, the device uses the WQ PASID field of the WQ Configuration register to do address translation. The WQ PASID field must be set by the driver before enabling a work queue in dedicated mode.</p><p><img src="/images/2023/05/23.png" alt></p><p><strong>dedicated work queue不会共享，所以MOVDIR64B就不用写入pasid了</strong>。</p><p>当guest配置WQ Configuration register(MMIO寄存器)的PASID field时需要trap下来，hypervisor分配一个host的PASID，然后将这个host PASID写入物理WQ Configuration register的PASID field。这样即可完成pasid虚拟化。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/2023/05/04/Scalable-Work-Submission-in-Device-Virtualization/&quot;&gt;Scalable Work Submission in Device Virtualization&lt;/a&gt;介绍了&lt;a href=&quot;/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/&quot;&gt;DSA&lt;/a&gt; shared work queue的virtualization，本文将介绍DSA dedicated work queue的virtualization。
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
  </entry>
  
  <entry>
    <title>Scalable Work Submission in Device Virtualization</title>
    <link href="http://liujunming.github.io/2023/05/04/Scalable-Work-Submission-in-Device-Virtualization/"/>
    <id>http://liujunming.github.io/2023/05/04/Scalable-Work-Submission-in-Device-Virtualization/</id>
    <published>2023-05-04T05:47:12.000Z</published>
    <updated>2023-05-04T11:43:41.306Z</updated>
    
    <content type="html"><![CDATA[<p>本文将以<a href="/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/">Intel Data Streaming Accelerator</a>为例，讲解DMWr (Deferrable Memory Write) TLP、ENQCMD/ENQCMDS指令、ENQCMD Virtualization、<a href="/2022/03/30/Introduction-to-Shared-Virtual-Memory/">SVA</a> Work Submission In Guest相关内容。<a id="more"></a></p><h2 id="1-DMWr-TLP"><a href="#1-DMWr-TLP" class="headerlink" title="1. DMWr TLP"></a>1. DMWr TLP</h2><h3 id="1-1-What"><a href="#1-1-What" class="headerlink" title="1.1 What"></a>1.1 What</h3><p>Deferrable Memory Write (DMWr) transactions are a new type of TLP supported by the PCI Specifications. This new feature allows the completer to return an acknowledgement to the requester of the DMWr transaction and provides the completer a mechanism to temporarily refuse the request.</p><p>The Deferrable Memory Write (DMWr) is an Optional Non-Posted Request that enables a scalable high-performance mechanism to implement shared work queues and similar capabilities. With DMWr, devices can have a single shared work queue and accept work items from multiple non-cooperating software agents in a non-blocking way.</p><p>读完上述定义后，或许对DMWr的理解不够深刻，接下来我们将以DSA的SWQ(Shared Work Queue)为例，阐述下为什么要有DMWr。</p><h3 id="1-2-Why"><a href="#1-2-Why" class="headerlink" title="1.2 Why"></a>1.2 Why</h3><p><img src="/images/2023/05/04.png" alt></p><p><img src="/images/2023/05/05.png" alt></p><p><img src="/images/2023/05/07.png" alt></p><p><img src="/images/2023/05/06.png" alt></p><blockquote><p>DMWr is a 64-byte non-posted write that waits for a response from the device before completing. The device returns Success if the descriptor is accepted into the work queue, or Retry if the descriptor is not accepted due to WQ capacity or QoS. </p></blockquote><p>正常写mmio寄存器是posted tlp，也就是说completer不会给requester返回报文。<br>DMWr是non-posted write tlp，这也为retry带来了可能！</p><h2 id="2-ENQCMD-ENQCMDS指令"><a href="#2-ENQCMD-ENQCMDS指令" class="headerlink" title="2. ENQCMD/ENQCMDS指令"></a>2. ENQCMD/ENQCMDS指令</h2><p>On Intel CPUs, DMWr is generated using the <code>ENQCMD</code> or <code>ENQCMDS</code> instructions. The <code>ENQCMD</code> and <code>ENQCMDS</code> instructions return the status of the command submission in <code>EFLAGS.ZF</code> flag; 0 indicates Success, and 1 indicates Retry.</p><p><img src="/images/2023/05/08.jpg" alt><br>SDM vol2中有这两个指令的详细描述。</p><p><img src="/images/2023/05/12.jpg" alt></p><p><img src="/images/2023/05/09.jpg" alt><br>ENQCMD 中destination offset参数的含义： enqueue registers, which are special device registers accessed using memory-mapped I/O (MMIO). 说白了，offset就是MMIO enqueue registers的location！</p><p><img src="/images/2023/05/13.jpg" alt></p><p><img src="/images/2023/05/14.jpg" alt></p><h3 id="2-1-Example-in-DSA"><a href="#2-1-Example-in-DSA" class="headerlink" title="2.1 Example in DSA"></a>2.1 Example in DSA</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">inline</span>  <span class="keyword">unsigned</span> <span class="keyword">int</span></span><br><span class="line">enqcmd(<span class="keyword">void</span> *dst, <span class="keyword">const</span> <span class="keyword">void</span> *src)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">uint8_t</span> retry;</span><br><span class="line">    <span class="function"><span class="keyword">asm</span> <span class="title">volatile</span><span class="params">(<span class="string">".byte 0xf2, 0x0f, 0x38, 0xf8, 0x02\t\n"</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="string">"setz %0\t\n"</span></span></span></span><br><span class="line">                 : "=r"(retry) : "a" (dst), "d" (src));</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span>)retry;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">while</span> (enqcmd(wq_portal, &amp;desc) &amp;&amp; enq_retry++ &lt; ENQ_RETRY_MAX) ;</span><br><span class="line"><span class="keyword">if</span> (enq_retry == ENQ_RETRY_MAX) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"ENQCMD retry limit exceeded\n"</span>);</span><br><span class="line">    rc = EXIT_FAILURE;</span><br><span class="line">    <span class="keyword">goto</span> done;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><a href="https://github.com/RaymondHuang210129/Intel-DSA-Experiments/blob/master/intel_dsa_sample.c" target="_blank" rel="noopener">https://github.com/RaymondHuang210129/Intel-DSA-Experiments/blob/master/intel_dsa_sample.c</a></p><h2 id="3-Scalability-In-Device-Virtualization"><a href="#3-Scalability-In-Device-Virtualization" class="headerlink" title="3. Scalability In Device Virtualization"></a>3. Scalability In Device Virtualization</h2><p><img src="/images/2023/05/16.jpg" alt><br><img src="/images/2023/05/17.jpg" alt><br><img src="/images/2023/05/18.jpg" alt><br><img src="/images/2023/05/19.jpg" alt></p><h2 id="4-ENQCMD-Virtualization"><a href="#4-ENQCMD-Virtualization" class="headerlink" title="4. ENQCMD Virtualization"></a>4. ENQCMD Virtualization</h2><p><img src="/images/2023/05/10.png" alt></p><p><img src="/images/2023/05/11.png" alt><br>sdm vol3  搜索ENQCMD即可！</p><p><img src="/images/2023/05/20.jpg" alt><br><img src="/images/2023/05/21.jpg" alt></p><h2 id="5-SVA-Work-Submission-In-Guest"><a href="#5-SVA-Work-Submission-In-Guest" class="headerlink" title="5. SVA Work Submission In Guest"></a>5. SVA Work Submission In Guest</h2><p><img src="/images/2023/05/15.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://static.sched.com/hosted_files/kvmforum2020/22/Scalable_Work_Submission_In_Device_Virtualization.pdf" target="_blank" rel="noopener">Scalable Work Submission in Device Virtualization</a></li><li><a href="https://blog.csdn.net/weixin_40357487/article/details/123339073" target="_blank" rel="noopener">PCIe 6.0 新特性 - DMWr (Deferrable Memory Write) 详解</a></li><li><a href="https://www.intel.com/content/www/us/en/docs/programmable/683501/22-2-6-0-0/deferrable-memory-write-dmwr.html" target="_blank" rel="noopener">Deferrable Memory Write (DMWr)</a></li><li><a href="https://www.freepatentsonline.com/y2020/0004703.html" target="_blank" rel="noopener">NON-POSTED WRITE TRANSACTIONS</a></li><li>Intel SDM</li><li>Intel Data Streaming Accelerator spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将以&lt;a href=&quot;/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/&quot;&gt;Intel Data Streaming Accelerator&lt;/a&gt;为例，讲解DMWr (Deferrable Memory Write) TLP、ENQCMD/ENQCMDS指令、ENQCMD Virtualization、&lt;a href=&quot;/2022/03/30/Introduction-to-Shared-Virtual-Memory/&quot;&gt;SVA&lt;/a&gt; Work Submission In Guest相关内容。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NBD（Network Block Device)</title>
    <link href="http://liujunming.github.io/2023/05/04/Notes-about-NBD%EF%BC%88Network-Block-Device/"/>
    <id>http://liujunming.github.io/2023/05/04/Notes-about-NBD（Network-Block-Device/</id>
    <published>2023-05-04T05:29:23.000Z</published>
    <updated>2023-05-04T05:37:17.282Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/50460919" target="_blank" rel="noopener">NBD（Network Block Device）简介及基本使用</a></p><p>NBD指的是Network Block Device，正如其名字的意思，NBD让用户可以通过网络访问到某个块设备，或者镜像文件。<a id="more"></a></p><p><img src="/images/2023/05/03.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/50460919&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NBD（Network Block Device）简介及基本使用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;NBD指的是Network Block Device，正如其名字的意思，NBD让用户可以通过网络访问到某个块设备，或者镜像文件。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Notes about协程</title>
    <link href="http://liujunming.github.io/2023/05/02/Notes-about%E5%8D%8F%E7%A8%8B/"/>
    <id>http://liujunming.github.io/2023/05/02/Notes-about协程/</id>
    <published>2023-05-02T06:23:49.000Z</published>
    <updated>2023-05-02T12:22:15.347Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下协程(Coroutines)相关notes。<a id="more"></a></p><h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><h3 id="1-1-Why"><a href="#1-1-Why" class="headerlink" title="1.1 Why?"></a>1.1 Why?</h3><p><a href="/pdf/什么是协程.pdf">什么是协程</a></p><p>vs多线程：<br>操作系统在线程等待IO的时候，会阻塞当前线程，切换到其它线程，这样在当前线程等待IO的过程中，其它线程可以继续执行。当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题。<strong>一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间</strong>。</p><p>协程刚好可以解决上述2个问题。协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。<strong>协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程</strong>，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。</p><h3 id="1-2-What"><a href="#1-2-What" class="headerlink" title="1.2 What"></a>1.2 What</h3><p><img src="/images/2023/05/02.png" alt></p><blockquote><p>协程本质上和单线程+状态机是等价的，只是用协程的话，协程负责来保存状态，开发起来方便些(不用自己写那个状态机)。</p></blockquote><h3 id="1-3-When"><a href="#1-3-When" class="headerlink" title="1.3 When"></a>1.3 When</h3><p>在有大量IO操作业务的情况下，我们采用协程替换线程，可以到达很好的效果，一是降低了系统内存，二是减少了系统切换开销，因此系统的性能也会提升。</p><p>在协程中尽量不要调用阻塞IO的方法，比如打印，读取文件，Socket接口等，除非改为异步调用的方式，并且协程只有在IO密集型的任务中才会发挥作用。</p><p><strong>协程只有和异步IO结合起来才能发挥出最大的威力</strong>。</p><h2 id="2-QEMU中的协程"><a href="#2-QEMU中的协程" class="headerlink" title="2. QEMU中的协程"></a>2. QEMU中的协程</h2><h3 id="2-1-为什么qemu要使用协程"><a href="#2-1-为什么qemu要使用协程" class="headerlink" title="2.1 为什么qemu要使用协程"></a>2.1 为什么qemu要使用协程</h3><p><a href="https://lore.kernel.org/qemu-devel/1311672077-4592-1-git-send-email-stefanha@linux.vnet.ibm.com/" target="_blank" rel="noopener">Coroutines for better asynchronous programming</a></p><p>仔细阅读<a href="http://blog.vmsplice.net/2014/01/coroutines-in-qemu-basics.html" target="_blank" rel="noopener">Coroutines in QEMU: The basics</a> <em>Callback hell in event-driven programs</em>即可。The coroutine version is much easier to understand because the code is sequential. Under the hood the coroutine version returns back to the event loop just like the callback version. Therefore the code still uses the event loop but it can be written like a sequential program.</p><blockquote><p>Coroutines make it possible to write sequential code that is actually executed across multiple iterations of the event loop. This is useful for code that needs to perform blocking I/O and would quickly become messy if split into a chain of callback functions. </p></blockquote><h3 id="2-2-The-QEMU-coroutine-API"><a href="#2-2-The-QEMU-coroutine-API" class="headerlink" title="2.2 The QEMU coroutine API"></a>2.2 The QEMU coroutine API</h3><p>The coroutine API is documented in <a href="https://gitlab.com/qemu-project/qemu/-/blob/stable-6.0/include/qemu/coroutine.h" target="_blank" rel="noopener">include/qemu/coroutine.h</a>. The main functions are:</p><h4 id="2-2-1-create-coroutine"><a href="#2-2-1-create-coroutine" class="headerlink" title="2.2.1 create coroutine"></a>2.2.1 create coroutine</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Coroutine entry point</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * When the coroutine is entered for the first time, opaque is passed in as an</span></span><br><span class="line"><span class="comment"> * argument.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * When this function returns, the coroutine is destroyed automatically and</span></span><br><span class="line"><span class="comment"> * execution continues in the caller who last entered the coroutine.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="keyword">void</span> coroutine_fn <span class="title">CoroutineEntry</span><span class="params">(<span class="keyword">void</span> *opaque)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a new coroutine</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Use qemu_coroutine_enter() to actually transfer control to the coroutine.</span></span><br><span class="line"><span class="comment"> * The opaque argument is passed as the argument to the entry point.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">Coroutine *<span class="title">qemu_coroutine_create</span><span class="params">(CoroutineEntry *entry, <span class="keyword">void</span> *opaque)</span></span>;</span><br></pre></td></tr></table></figure><p>When a new coroutine is started, it will begin executing the entry function. The caller can pass an opaque pointer to data needed by the coroutine.</p><h4 id="2-2-2-execute-coroutine"><a href="#2-2-2-execute-coroutine" class="headerlink" title="2.2.2 execute coroutine"></a>2.2.2 execute coroutine</h4><p>The new coroutine is executed by calling <code>qemu_coroutine_enter</code>:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Transfer control to a coroutine</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qemu_coroutine_enter</span><span class="params">(Coroutine *coroutine)</span></span>;</span><br></pre></td></tr></table></figure></p><h4 id="2-2-3-yield-coroutine"><a href="#2-2-3-yield-coroutine" class="headerlink" title="2.2.3 yield coroutine"></a>2.2.3 yield coroutine</h4><p>If the coroutine needs to wait for an event such as I/O completion or user input, it calls <code>qemu_coroutine_yield</code>:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Transfer control back to a coroutine's caller</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This function does not return until the coroutine is re-entered using</span></span><br><span class="line"><span class="comment"> * qemu_coroutine_enter().</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> coroutine_fn <span class="title">qemu_coroutine_yield</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The yield function transfers control back to the <code>qemu_coroutine_enter</code> caller. The coroutine can be re-entered at a later point in time by calling <code>qemu_coroutine_enter</code>, for example, when an I/O request has completed.</p><hr><p>参考资料:</p><ol><li><a href="http://blog.vmsplice.net/2014/01/coroutines-in-qemu-basics.html" target="_blank" rel="noopener">Coroutines in QEMU: The basics</a></li><li><a href="https://royhunter.github.io/2016/06/24/qemu-coroutine/" target="_blank" rel="noopener">QEMU中的协程—qemu-coroutine</a></li><li><a href="https://zhuanlan.zhihu.com/p/172471249" target="_blank" rel="noopener">什么是协程？</a></li><li><a href="https://mp.weixin.qq.com/s/SyWjLg3lYx3pIJQfEtik8Q" target="_blank" rel="noopener">​浅谈协程</a></li><li><a href="https://mp.weixin.qq.com/s/IO4ynnKEfy2Rt-Me7EIeqg" target="_blank" rel="noopener">当谈论协程时，我们在谈论什么</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下协程(Coroutines)相关notes。
    
    </summary>
    
      <category term="操作系统" scheme="http://liujunming.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="QEMU" scheme="http://liujunming.github.io/tags/QEMU/"/>
    
      <category term="操作系统" scheme="http://liujunming.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>如何mount虚拟机镜像</title>
    <link href="http://liujunming.github.io/2023/05/01/%E5%A6%82%E4%BD%95mount%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"/>
    <id>http://liujunming.github.io/2023/05/01/如何mount虚拟机镜像/</id>
    <published>2023-05-01T11:49:38.000Z</published>
    <updated>2023-05-01T12:36:17.731Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍下mount虚拟机镜像的方法。 <a id="more"></a></p><h3 id="losetup"><a href="#losetup" class="headerlink" title="losetup"></a>losetup</h3><p>losetup只能mount raw格式的镜像。</p><p>To check what is the first usable loop device, run<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">losetup -f</span><br></pre></td></tr></table></figure></p><p>After that, use the output of that command to link the disk image to the loop device file (using root privileges):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">losetup -P /dev/loopX example.img</span><br></pre></td></tr></table></figure></p><p>The -P flag searches through the image for partitions, which you need to mount.</p><p>After that, create the folder named example and run the command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/loopXpY example</span><br></pre></td></tr></table></figure></p><p>The disk image should now be mounted in that directory. Depending on the Y variable, the right partition was mounted.</p><h3 id="qemu-nbd"><a href="#qemu-nbd" class="headerlink" title="qemu-nbd"></a>qemu-nbd</h3><p>Export a QEMU disk image using the NBD protocol.</p><p>qemu-nbd可以mount多种格式的虚拟机镜像，因此适用范围比losetup要广！</p><ul><li><p>Enable NBD on the host</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe nbd max_part=8</span><br></pre></td></tr></table></figure></li><li><p>Connect the QCOW2 as a network block device</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-nbd -c /dev/nbd0 /var/lib/vz/images/100/vm-100-disk-1.qcow2</span><br></pre></td></tr></table></figure></li><li><p>List partitions inside the QCOW2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fdisk /dev/nbd0 -l</span><br></pre></td></tr></table></figure></li><li><p>Mount the partition from the VM</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/nbd0p1 /mnt/somepoint/</span><br></pre></td></tr></table></figure></li><li><p>After you’re done, unmount and disconnect</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">umount /mnt/somepoint/</span><br><span class="line">qemu-nbd -d /dev/nbd0</span><br><span class="line">rmmod nbd</span><br></pre></td></tr></table></figure></li></ul><hr><p>参考资料:</p><ol><li><a href="https://iaguozhi.github.io/blogs/Change-vm-kernel-by-qemu-nbd.html" target="_blank" rel="noopener">记录一次将虚拟机kernel写坏之后的修复过程</a></li><li><a href="https://www.qemu.org/docs/master/tools/qemu-nbd.html" target="_blank" rel="noopener">QEMU Disk Network Block Device Server</a></li><li><a href="https://eloydegen.com/blog/posts/losetup/" target="_blank" rel="noopener">Mount disk images using losetup</a></li><li><a href="https://www.man7.org/linux/man-pages/man8/losetup.8.html" target="_blank" rel="noopener">man losetup</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍下mount虚拟机镜像的方法。
    
    </summary>
    
      <category term="工具" scheme="http://liujunming.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Notes about GPU Direct Storage</title>
    <link href="http://liujunming.github.io/2023/05/01/Notes-about-GPU-Direct-Storage/"/>
    <id>http://liujunming.github.io/2023/05/01/Notes-about-GPU-Direct-Storage/</id>
    <published>2023-05-01T07:32:26.000Z</published>
    <updated>2023-05-01T11:29:26.325Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下GPU Direct Storage相关notes。<a id="more"></a></p><blockquote><p>从IO读取链路来看，NVMe控制器通过DMA引擎将硬盘数据直接写入GPU显存，避免了主机内存和CPU的参与，从而实现CPU和主存的IO旁路，使IO吞吐能力不在受限于系统总线的带宽压力。</p></blockquote><p>说白了，就是支持NVMe与GPU的PCIe p2p，不过只支持NVMe到GPU的方向。</p><p><img src="/images/2023/05/01.png" alt></p><p><strong>GPUDirect Storage</strong> enables a direct data path between local or remote storage, such as NVMe or NVMe over Fabric (NVMe-oF), and GPU memory. It avoids extra copies through a bounce buffer in the CPU’s memory, enabling a direct memory access (DMA) engine near the NIC or storage to move data on a direct path into or out of GPU memory — all without burdening the CPU.</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/509396439" target="_blank" rel="noopener">GPU Direct Storage</a></li><li><a href="https://developer.nvidia.com/gpudirect" target="_blank" rel="noopener">NVIDIA GPUDirect</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下GPU Direct Storage相关notes。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about hyper_dmabuf</title>
    <link href="http://liujunming.github.io/2023/04/30/Notes-about-hyper-dmabuf/"/>
    <id>http://liujunming.github.io/2023/04/30/Notes-about-hyper-dmabuf/</id>
    <published>2023-04-30T12:26:38.000Z</published>
    <updated>2023-04-30T13:22:49.838Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下hyper_dmabuf相关notes。<a id="more"></a><br><img src="/images/2023/04/20.png" alt></p><p>Hyper_DMABUF driver is a Linux device driver running on multiple Virtual achines (VMs), which expands DMA-BUF sharing capability to the VM environment where multiple different OS instances need to share same physical data without data-copy across VMs.</p><p>To share a DMA_BUF across VMs, an instance of the Hyper_DMABUF drv on the exporting VM (so called, “exporter”) imports a local DMA_BUF from the original producer of the buffer, then re-exports it with an unique ID, hyper_dmabuf_id for the buffer to the importing VM (so called, “importer”).</p><p>Another instance of the Hyper_DMABUF driver on importer registers a hyper_dmabuf_id together with reference information for the shared physical pages associated with the DMA_BUF to its database when the export happens.</p><p>The actual mapping of the DMA_BUF on the importer’s side is done by the Hyper_DMABUF driver when user space issues the IOCTL command to access the shared DMA_BUF. The Hyper_DMABUF driver works as both an importing and exporting driver as is, that is, no special configuration is required. Consequently, only a single module per VM is needed to enable cross-VM DMA_BUF exchange.</p><hr><p>参考资料:</p><ol><li><a href="https://projectacrn.github.io/1.6.1/developer-guides/hld/hld-APL_GVT-g.html#hyper-dma-buffer-sharing" target="_blank" rel="noopener">Hyper DMA Buffer Sharing</a></li><li><a href="https://github.com/downor/linux_hyper_dmabuf/blob/hyper_dmabuf_integration_v4/Documentation/hyper-dmabuf-sharing.txt" target="_blank" rel="noopener">Linux Hyper DMABUF Driver</a></li><li><a href="https://lists.freedesktop.org/archives/dri-devel/2017-December/160709.html" target="_blank" rel="noopener">hyper_dmabuf: initial working version of hyper_dmabuf drv</a></li><li><a href="https://www.phoronix.com/news/Intel-Hyper-DMA-BUF" target="_blank" rel="noopener">Intel Introduces “Hyper DMA-BUF” To Exchange Buffers Between VMs</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下hyper_dmabuf相关notes。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Notes about dma_buf</title>
    <link href="http://liujunming.github.io/2023/04/30/Notes-about-dma-buf/"/>
    <id>http://liujunming.github.io/2023/04/30/Notes-about-dma-buf/</id>
    <published>2023-04-30T06:25:25.000Z</published>
    <updated>2023-04-30T08:16:16.512Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下dma_buf相关notes。<a id="more"></a></p><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><p><img src="/images/2023/04/17.jpg" alt><br>以摄像头采集数据，GPU显示数据为例。摄像头设备将数据DMA到内存中后，GPU需要将这些DMA内存进行显示，也就是说摄像头DMA的输出数据是GPU的DMA输入数据。如果没有DMA buffer sharing机制，则需要将摄像头的DMA数据拷贝一份以搬到GPU的DMA数据中，因此存在内存copy的开销！<br>dma_buf则提供了一套统一框架，可以实现不同device的驱动之间DMA buffer的sharing，同时还允许userspace mmap共享的DMA buffer！</p><h3 id="What"><a href="#What" class="headerlink" title="What"></a>What</h3><p><img src="/images/2023/04/19.jpg" alt></p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p><img src="/images/2023/04/18.jpg" alt></p><p><img src="/images/2023/04/14.png" alt></p><h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><p><img src="/images/2023/04/13.png" alt></p><p><img src="/images/2023/04/15.png" alt><br><img src="/images/2023/04/16.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://blog.csdn.net/hexiaolong2009/article/details/102596744" target="_blank" rel="noopener">dma-buf 由浅入深（一） —— 最简单的 dma-buf 驱动程序</a></li><li><a href="https://github.com/hexiaolong2008/sample-code/tree/master/dma-buf/08" target="_blank" rel="noopener">dma-buf</a></li><li><a href="https://saiyn.github.io/homepage/2018/04/18/linux-kernel-dmabuf/" target="_blank" rel="noopener">Linux内核笔记之DMA_BUF</a></li><li><a href="https://elinux.org/images/a/a8/DMA_Buffer_Sharing-_An_Introduction.pdf" target="_blank" rel="noopener">DMA Buffer Sharing Framework:An Introduction</a></li><li><a href="https://www.openfabrics.org/wp-content/uploads/2020-workshop-presentations/303.-OFI-GPU-DMA-BUF-OFA2020v2.pdf" target="_blank" rel="noopener">RDMA WITH GPU MEMORY VIA DMA-BUF</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg2OTc0ODAzMw==&amp;mid=2247502900&amp;idx=1&amp;sn=dd73aae7e7b296317fbff613d475888e&amp;chksm=ce9ad01af9ed590c71f309a8b4ba4bad72dda1d75f9af5d153caf5dd11e229aa75c8507685c7&amp;mpshare=1&amp;scene=1&amp;srcid=0403TMj3qA6LY1DDtxctawJO&amp;sharer_sharetime=1648995232072&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AZdgR1ASyNPvcNHeaNH3PpE%3D&amp;acctmode=0&amp;pass_ticket=bxkMR5mJMnjqkgrSRKMG4Na40WpTHdV%2FfvZCJEtYhn3FUItw%2FA0ZMr0FE2oTAbbL&amp;wx_header=0#rd" target="_blank" rel="noopener">dma-buf学习分享</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下dma_buf相关notes。
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about TSO、GSO、LRO、GRO</title>
    <link href="http://liujunming.github.io/2023/04/23/Notes-about-TSO%E3%80%81GSO%E3%80%81LRO%E3%80%81GRO/"/>
    <id>http://liujunming.github.io/2023/04/23/Notes-about-TSO、GSO、LRO、GRO/</id>
    <published>2023-04-23T09:21:32.000Z</published>
    <updated>2023-04-23T10:35:19.351Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自<a href="https://mp.weixin.qq.com/s/jtKtdtSS15dRaivtdcAbQQ" target="_blank" rel="noopener">图解网络Offload</a>，将介绍TSO、GSO、LRO、GRO相关笔记。<a id="more"></a></p><p>网络应用程序如果要发送很大的数据包，经过内核协议栈的时，大包会被分片成多个不超过MTU长度的包。这个分片比较费CPU资源。Offload技术可以把这些分片和合并的工作进行优化处理，也可以直接Offload到网卡上。</p><h3 id="MTU"><a href="#MTU" class="headerlink" title="MTU"></a>MTU</h3><p>MTU是一个二层的概念，即最大传输单元（Maximum Transmission Unit，MTU），它不包含二层以太网头尾数据。网卡发送数据包的大小都是限制在MTU内的。<br><img src="/images/2023/04/06.png" alt></p><p>Offload涉及到四个概念：TSO、GSO、LRO、GRO。（当然还有UDP的UFO，以及一些checksum的Offload，在这里不讨论。）</p><h3 id="TSO"><a href="#TSO" class="headerlink" title="TSO"></a>TSO</h3><p>TSO(TCP Segmentation Offload)是一种利用网卡对大数据包进行分片，从而减小CPU负荷的一种技术。其作用通过两个图来对比：</p><p>TSO off和GSO off 状态数据包的发送过程：<br><img src="/images/2023/04/07.png" alt></p><p>TSO on状态数据包的发送过程：<br><img src="/images/2023/04/08.png" alt><br>一个大的网络包直到进入网卡内部后才由网卡进行了分片。</p><h3 id="GSO"><a href="#GSO" class="headerlink" title="GSO"></a>GSO</h3><p>GSO（Generic Segmentation Offload）是延缓分片技术。它比TSO更通用，原因在于它不需要硬件的支持就可以进行分片。</p><p>其过程是：首先查询网卡是否支持TSO功能，如果硬件支持TSO则使用网卡的硬件分片能力执行分片；如果网卡不支持 TSO 功能，则将分片的执行，延缓到了将数据推送到网卡的前一刻执行。</p><p>网卡关闭TSO时，GSO on状态数据包的发送过程：<br><img src="/images/2023/04/09.png" alt></p><p>一个大的网络包直到进入网卡前的最后一步才进行了分片。</p><p>TSO和GSO对应数据发送过程，对应数据接收过程的是LRO和GRO。</p><h3 id="LRO"><a href="#LRO" class="headerlink" title="LRO"></a>LRO</h3><p>LRO（Large Receive Offload）是将网卡接收到的多个数据包合并成一个大的数据包，然后再传递给网络协议栈处理的技术。这样可以提高系统接收数据包的能力，减轻CPU负载。</p><p>LRO off和GRO off 状态数据包的接收过程：<br><img src="/images/2023/04/10.png" alt></p><p>LRO on状态数据包的接收过程：<br><img src="/images/2023/04/11.png" alt><br>数据一进入网卡立刻进行了合并。</p><h3 id="GRO"><a href="#GRO" class="headerlink" title="GRO"></a>GRO</h3><p>GRO（Generic Receive Offload）是LRO 的软件实现，只是GRO 的合并条件更加的严格和灵活。</p><p>GRO on状态数据包的接收过程：<br><img src="/images/2023/04/12.png" alt></p><p>以上的网络offload是网络协议栈配合网卡完成的，在现在的很多智能网卡上可以直接offload整个网络协议栈，即把网络协议的处理放到了智能网卡上。</p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s/jtKtdtSS15dRaivtdcAbQQ" target="_blank" rel="noopener">图解网络Offload</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自&lt;a href=&quot;https://mp.weixin.qq.com/s/jtKtdtSS15dRaivtdcAbQQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;图解网络Offload&lt;/a&gt;，将介绍TSO、GSO、LRO、GRO相关笔记。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about 零拷贝技术splice</title>
    <link href="http://liujunming.github.io/2023/04/23/Notes-about-%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AFsplice/"/>
    <id>http://liujunming.github.io/2023/04/23/Notes-about-零拷贝技术splice/</id>
    <published>2023-04-23T06:22:39.000Z</published>
    <updated>2023-04-23T06:32:45.106Z</updated>
    
    <content type="html"><![CDATA[<p>转载自:<a href="https://mp.weixin.qq.com/s?__biz=MzA3NzYzODg1OA==&amp;mid=2648466923&amp;idx=1&amp;sn=acf2fb71a960f3831f9b98657b39d4ce&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">一文读懂零拷贝技术｜splice使用</a>。<a id="more"></a><br>服务端要向客户端连接发送一个文件，一般过程如下：</p><ul><li>服务端首先调用 <code>read()</code> 函数读取文件内容。</li><li>服务端通过调用 <code>write()</code>/<code>send()</code> 函数将文件内容发送给客户端连接。</li></ul><p>上面过程如下图所示：</p><p><img src="/images/2023/04/04.jpeg" alt></p><p>从上图可以看出，在发送文件的过程中，首先需要将文件页缓存（Page Cache）从内核态复制到用户态缓存中，然后再从用户态缓存复制到客户端的 Socket 缓冲区中。</p><p>其实在上面的过程中，复制文件数据到用户态缓存这个操作是多余的，我们完全可以直接把文件页缓存的数据复制到 Socket 缓冲区即可，这样就可以减少一次拷贝数据的操作。</p><p>为了实现这样的功能，内核提供了一个名为 <code>splice()</code>的系统调用，使用 <code>splice()</code>系统调用可以避免从内核态拷贝数据到用户态。</p><blockquote><p>不需要将内核态的数据拷贝到用户态缓存的技术被称为：<code>零拷贝技术</code>。</p></blockquote><p><img src="/images/2023/04/05.jpeg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s?__biz=MzA3NzYzODg1OA==&amp;mid=2648466923&amp;idx=1&amp;sn=acf2fb71a960f3831f9b98657b39d4ce&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">一文读懂零拷贝技术｜splice使用</a></li><li><a href="https://mp.weixin.qq.com/s/vN1VIgX73arke4put_cyRg" target="_blank" rel="noopener">一文读懂零拷贝技术｜splice原理与实现</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转载自:&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzA3NzYzODg1OA==&amp;amp;mid=2648466923&amp;amp;idx=1&amp;amp;sn=acf2fb71a960f3831f9b98657b39d4ce&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;一文读懂零拷贝技术｜splice使用&lt;/a&gt;。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux IOMMU bypass method</title>
    <link href="http://liujunming.github.io/2023/04/22/Linux-IOMMU-bypass-method/"/>
    <id>http://liujunming.github.io/2023/04/22/Linux-IOMMU-bypass-method/</id>
    <published>2023-04-22T08:45:06.000Z</published>
    <updated>2023-04-22T09:30:38.240Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2023/04/01.jpg" alt><br><a id="more"></a><br><img src="/images/2023/04/03.jpg" alt></p><p><img src="/images/2023/04/02.jpg" alt><br>将AT字段设置为10b即可bypass IOMMU。</p><p><a href="/2019/11/24/Introduction-to-PCIe-Address-Translation-Services/">ATC</a>的深入理解：如果设备在ATC中找到对应的映射entry后，会将TLP AT字段设置为10b，并将TLP中的address字段设置为翻译后的地址。</p><hr><p>参考资料:</p><ol><li><a href="https://pdfs.semanticscholar.org/b450/50db1fb770a07bc60c66d3532ee4d1949ccb.pdf" target="_blank" rel="noopener">Thunderclap:Exploring Vulnerabilities in Operating System IOMMU Protection via DMA from Untrustworthy Peripherals</a></li><li>PCIe spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2023/04/01.jpg&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="IOMMU" scheme="http://liujunming.github.io/categories/IOMMU/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
      <category term="IOMMU" scheme="http://liujunming.github.io/tags/IOMMU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Root Complex</title>
    <link href="http://liujunming.github.io/2023/02/12/Notes-about-root-complex/"/>
    <id>http://liujunming.github.io/2023/02/12/Notes-about-root-complex/</id>
    <published>2023-02-12T09:36:29.000Z</published>
    <updated>2023-02-12T10:19:59.731Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Root Complex(RC)相关笔记。<br><img src="/images/2023/02/06.png" alt></p><a id="more"></a><p>The Root Complex is an entity that includes a Host Bridge and one or more root ports.<br><img src="/images/2023/02/07.png" alt></p><p>当CPU读写pcie设备的MMIO BAR时，RC的Host Bridge将processor transactions转换为TLP。因此当host bridge发现CPU访问的物理地址区间是MMIO时，会让目标EP(End Point)所属的root port发送memory read/write TLP，经过路由，最终TLP会下发到目标EP。</p><hr><p>参考资料:</p><ol><li><a href="http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-1/" target="_blank" rel="noopener">Down to the TLP: How PCI express devices talk</a></li><li><a href="https://astralvx.com/introduction-to-pcie/" target="_blank" rel="noopener">Introduction to PCIe</a></li><li><a href="https://zhuanlan.zhihu.com/p/32786076" target="_blank" rel="noopener">使用Xilinx IP核进行PCIE开发学习笔记</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Root Complex(RC)相关笔记。&lt;br&gt;&lt;img src=&quot;/images/2023/02/06.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
</feed>
