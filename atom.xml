<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2024-05-04T09:52:49.445Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>notes about MPTCP</title>
    <link href="http://liujunming.github.io/2024/05/04/notes-about-MPTCP/"/>
    <id>http://liujunming.github.io/2024/05/04/notes-about-MPTCP/</id>
    <published>2024-05-04T08:52:30.000Z</published>
    <updated>2024-05-04T09:52:49.445Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Multipath TCP (MPTCP)相关notes。<a id="more"></a></p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>Aims at allowing a TCP connection to use multiple paths to maximize throughput and increase redundancy.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Hosts connected to the Internet or within a data center environment are often connected by multiple paths. However, when TCP is used for data transport, communication is restricted to a single network path. It is possible that some paths between the two hosts are congested, whereas alternate paths are underutilized. A more efficient use of network resources is possible if these multiple paths are used concurrently. In addition, the use of multiple connections enhances the user experience, because it provides higher throughput and improved resilience against network failures.<br>连接到互联网或数据中心环境中的主机通常由多条路径连接。然而，当使用 TCP 进行数据传输时，通信仅限于一条网络路径。两台主机之间的某些路径可能会出现拥塞，而备用路径的利用率却很低。如果同时使用这些多路径，就能更有效地利用网络资源。此外，多连接的使用还能增强用户体验，因为它提供了更高的吞吐量，并提高了对网络故障的恢复能力。</p><p>MPTCP is a set of extensions to regular TCP that enables a single data flow to be separated and carried across multiple connections.<br>MPTCP 是对普通 TCP 的一系列扩展，可将单个数据流分离并在多个连接中传输。</p><p>As shown in this diagram, MPTCP is able to separate the 9mbps flow into three different sub-flows on the sender node, which is subsequently aggregated back into the original data flow on the receiving node.<br>如图所示，MPTCP 能够在发送节点上将 9mbps 的数据流分离成三个不同的子数据流，然后在接收节点上汇总成原始数据流。</p><p><img src="/images/2024/05/004.avif" alt></p><p>The data that enters the MPTCP connection acts exactly as it does through a regular TCP connection; the transmitted data has guaranteed an in-order delivery. Since MPTCP adjusts the network stack and operates within the transport layer, it is used transparently by the application.<br>进入 MPTCP 连接的数据与通过普通 TCP 连接的数据完全相同；传输的数据保证按顺序传送。由于 MPTCP 调整了网络堆栈并在传输层内运行，因此应用程序可以透明地使用它。</p><p><img src="/images/2024/05/005.avif" alt></p><h2 id="Simplified-description"><a href="#Simplified-description" class="headerlink" title="Simplified description"></a>Simplified description</h2><p>Difference between TCP and MPTCP:<br><img src="/images/2024/05/003.png" alt></p><p>The core idea of multipath TCP is to define a way to build a connection between two hosts and not between two interfaces (as standard TCP does).</p><p>For instance, Alice has a smartphone with 3G and WiFi interfaces (with IP addresses 10.11.12.13 and 10.11.12.14) and Bob has a computer with an Ethernet interface (with IP address 20.21.22.23).</p><p>In standard TCP, the connection should be established between two IP addresses. Each TCP connection is identified by a four-tuple (source and destination addresses and ports). Given this restriction, an application can only create one TCP connection through a single link. Multipath TCP allows the connection to use several paths simultaneously. For this, Multipath TCP creates one TCP connection, called subflow, over each path that needs to be used.</p><hr><p>参考资料:</p><ol><li><a href="https://www.wikiwand.com/en/Multipath_TCP" target="_blank" rel="noopener">https://www.wikiwand.com/en/Multipath_TCP</a></li><li><a href="https://datatracker.ietf.org/doc/html/rfc8684" target="_blank" rel="noopener">https://datatracker.ietf.org/doc/html/rfc8684</a></li><li><a href="https://www.cisco.com/c/en/us/support/docs/ip/transmission-control-protocol-tcp/116519-technote-mptcp-00.html" target="_blank" rel="noopener">MPTCP and Product Support Overview</a></li><li><a href="https://www.usenix.org/system/files/login/articles/login1210_bonaventure.pdf" target="_blank" rel="noopener">An Overview of Multipath TCP</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Multipath TCP (MPTCP)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about virtio per-virtqueue reset</title>
    <link href="http://liujunming.github.io/2024/05/03/Notes-about-virtio-per-virtqueue-reset/"/>
    <id>http://liujunming.github.io/2024/05/03/Notes-about-virtio-per-virtqueue-reset/</id>
    <published>2024-05-03T10:47:16.000Z</published>
    <updated>2024-05-03T12:10:53.338Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio per-virtqueue reset特性的相关notes，主要内容转载自<a href="https://developer.aliyun.com/article/996430" target="_blank" rel="noopener">virtio 1.2 来了！</a>。<a id="more"></a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>引入这个特性的目的是解决virtio-net不支持队列级别的reset操作的问题。这在很多现代化的网卡中是一个比较常见的功能, 是实现很多功能的基础能力, 为了让 virtio-net 支持更多能力, 这个特性的引入是必须的。但是 Per-virtqueue reset 并不只限于 virtio-net 这一种设备，它是一个 virtio 的基础能力，相信其它的 virtio 设备也会慢慢支持这个 feature。</p><h2 id="Implementation-Process"><a href="#Implementation-Process" class="headerlink" title="Implementation Process"></a>Implementation Process</h2><p>Per-virtqueue reset 由 driver 针对某一个队列发起，基于某一种 transport(比如 PCIe) 通知 device。device 停止使用队列，driver 在 reset 之后可以重新 re-enable 队列。virtio spec 定义了这个过程中详细的交互流程和信息。<br>以下是 virtio spec 中定义的详细流程：</p><ul><li>driver 基于 transport 通知 device 某个指定的队列要 reset。</li><li>device 收到请求之后设置 reset 状态为 1，停止此队列的所有操作，包括中断等，并设置队列的所有的状态到初始值。在 device 完成 reset 操作之前，返回给 driver 的 reset 状态都是 1，直到 reset 操作完成。reset 完成之后 reset 及 enable 的值都要设置成 0。</li><li>driver 在检查到队列的 reset 状态变成 0 之后，就表示device reset 操作已经完成了。这个时候开始，driver 就可以安全地回收队列占用的相关资源了。</li></ul><p>到此 driver 对于队列的 reset 操作就已经完成了。</p><ul><li>之后 virtio driver 可选地进行 re-enable 操作，在操作的过程中，driver 可以给 device 新的参数来 re-enable 这个队列。比如新的队列大小。</li></ul><p>以上是一个完整的 reset &amp; re-enable 的过程，理论上 re-enable 是可选的。</p><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>对于现代的很多硬件设备来讲，对于队列进行 reset 是一个比较常见的功能，所以这个功能的引入让 virtio 设备更加现代化。早期 virtio 的出现是伴随着高性能的需求而来的，我们原来更加关注它在性能上的基本功能，一些高级功能并不重视。per-virtqueue reset 让 virtio 对于队列的使用更加灵活，譬如我们可以基于 per-vertqueue reset 实现下面两个功能：</p><ol><li>调整virtio-net 网卡队列的ring size。在 virtio-net 的场景下，基于 per-virtqueue reset 我们可以实现网卡队列 ring size 的调整。目前一般的网卡都支持使用<code>ethtool -G eth0 rx &lt;new size&gt; tx &lt;new size&gt;</code>来调整队列的大小，但是原来的 virtio-net 一直是不支持这样一个简单的功能的，现在基于 per-virtqueue reset，我们很快就可以在 Linux 下使用这个命令来调整队列的大小。</li><li>支持AF_XDP，扩展云上应用的边界。除了应用于上述简单的场景之外，我们还可以在更高级的场景应用到这个功能。<strong>per-virtqueue reset 也可以视作一种资源的快速回收机制</strong>。比如在 virtio-net 的情况下，我们必须要等待收到新的数据包或者硬件完成数据包的发送才能完成对于 buffer 资源的回收。而现在基于 per-virtqueue reset，driver 可以不用被动地等待而是可以主动调用 reset 快速地让 device 释放对于某个队列上的 buffer 资源的占用，实现资源的快速回收。这可以让 virtio-net 支持 AF_XDP 这样的高级功能，实现在 linux 内核框架下的高性能收发包。</li></ol><h2 id="Spec-Details"><a href="#Spec-Details" class="headerlink" title="Spec Details"></a>Spec Details</h2><p>per-virtqueue reset的细节请参考<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">virtio 1.2 spec</a>。</p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html#x1-280001" target="_blank" rel="noopener">2.6.1 Virtqueue Reset</a></p><blockquote><p><strong>VIRTIO_F_RING_RESET(40)</strong> This feature indicates that the driver can reset a queue individually.</p></blockquote><blockquote><p>2.6.1.1.2 Driver Requirements: Virtqueue Reset<br>After the driver tells the device to reset a queue, the driver MUST verify that the queue has actually been reset.<br>After the queue has been successfully reset, the driver MAY release any resource associated with that virtqueue.</p></blockquote><blockquote><p>2.6.1.2.1 Device Requirements: Virtqueue Re-enable<br>The device MUST observe any queue configuration that may have been changed by the driver, like the maximum queue size.</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">virtio_pci_common_cfg</span> &#123;</span> </span><br><span class="line">        ...</span><br><span class="line"> </span><br><span class="line">        <span class="comment">/* About a specific virtqueue. */</span> </span><br><span class="line">        le16 queue_select;              <span class="comment">/* read-write */</span> </span><br><span class="line">        le16 queue_size;                <span class="comment">/* read-write */</span> </span><br><span class="line">        le16 queue_msix_vector;         <span class="comment">/* read-write */</span> </span><br><span class="line">        le16 queue_enable;              <span class="comment">/* read-write */</span> </span><br><span class="line">        ...</span><br><span class="line">        le16 queue_reset;               <span class="comment">/* read-write */</span> </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><blockquote><p><strong>queue_size</strong><br>Queue Size. On reset, specifies the maximum queue size supported by the device. This can be modified by the driver to reduce memory requirements.</p><p><strong>queue_reset</strong><br>The driver uses this to selectively reset the queue.</p></blockquote><blockquote><p><strong>4.1.4.3.1 Device Requirements: Common configuration structure layout</strong><br>If VIRTIO_F_RING_RESET has been negotiated, the device MUST present a 0 in queue_reset on reset.<br>If VIRTIO_F_RING_RESET has been negotiated, the device MUST present a 0 in queue_reset after the virtqueue is enabled with queue_enable.<br>The device MUST reset the queue when 1 is written to queue_reset. The device MUST continue to present 1 in queue_reset as long as the queue reset is ongoing. The device MUST present 0 in both queue_reset and queue_enable when queue reset has completed.</p></blockquote><blockquote><p><strong>4.1.4.3.2 Driver Requirements: Common configuration structure layout</strong><br>If VIRTIO_F_RING_RESET has been negotiated, after the driver writes 1 to queue_reset to reset the queue, the driver MUST NOT consider queue reset to be complete until it reads back 0 in queue_reset. The driver MAY re-enable the queue by writing 1 to queue_enable after ensuring that other virtqueue fields have been set up correctly. The driver MAY set driver-writeable queue configuration values to different values than those that were used before the queue reset.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://www.alibabacloud.com/blog/virtio-1-2-is-coming_599615" target="_blank" rel="noopener">Virtio 1.2 is Coming!</a></li><li><a href="https://developer.aliyun.com/article/996430" target="_blank" rel="noopener">virtio 1.2 来了！</a></li><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html" target="_blank" rel="noopener">virtio 1.2 spec</a></li><li><a href="https://lore.kernel.org/kvm/20220801063902.129329-1-xuanzhuo@linux.alibaba.com/" target="_blank" rel="noopener">virtio pci support VIRTIO_F_RING_RESET</a></li><li><a href="https://lists.oasis-open.org/archives/virtio-dev/202111/msg00013.html" target="_blank" rel="noopener">virtio: pci support virtqueue reset</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio per-virtqueue reset特性的相关notes，主要内容转载自&lt;a href=&quot;https://developer.aliyun.com/article/996430&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;virtio 1.2 来了！&lt;/a&gt;。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about AF_XDP</title>
    <link href="http://liujunming.github.io/2024/05/03/Notes-about-AF-XDP/"/>
    <id>http://liujunming.github.io/2024/05/03/Notes-about-AF-XDP/</id>
    <published>2024-05-03T01:05:40.000Z</published>
    <updated>2024-05-03T09:34:14.955Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要转载自<a href="https://hpnpl.net/posts/recapituatling-af-xdp/" target="_blank" rel="noopener">Recapitulating AF_XDP</a>。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>this time I will talk about a pretty awesome feature in the Linux kernel: AF_XDP. Please keep in mind, that this is a summary and explanation in my own words, and it’s not intended to fully cover all technical depths. The focus lies on understanding AF_XDP’s core concepts, learn how to use it, and what to consider while using it.<br>这次，我将谈谈 Linux 内核中一个非常棒的功能： AF_XDP。请记住，这只是用我自己的话进行的总结和解释，并不打算完全涵盖所有的技术深度。重点在于理解AF_XDP的核心概念，学习如何使用它，以及使用时需要注意的事项。</p><p>The official <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html?highlight=af_xdp" target="_blank" rel="noopener">kernel documentation</a> describes AF_XDP as “an address family that is optimized for high performance packet processing”. Why do we need an additional address family (or in other words an additional type of network socket)? Reading this sentence from the doc implies, that the existing address families are not suitable for high performance networking. And that’s exactly the case. While the Linux networking stack does a really good job abstracting layers from applications, it suffers performance due to exactly these abstractions. That’s why other libraries like DPDK completely bypass the kernel networking stack with their so called <code>Poll Mode Drivers</code> (PMD). This is very, very fast, reaching line rate for 100Gbit/s NICs. But this performance comes with some drawbacks: DPDK code is difficult to maintain, there is no chance to benefit from any kernel functionality (e.g. existing networking drivers), the number of supported NICs is limited and smaller than by the kernel, and PMD drivers completely block each used core to 100%.</p><p>内核官方文档将 AF_XDP 描述为 “为高性能数据包处理而优化的address family”。为什么我们需要一个额外的address family（或者换句话说，一个额外的网络套接字类型）？从文档中的这句话可以看出，现有的address families并不适合高性能网络。事实正是如此。虽然Linux网络协议栈在应用程序抽象层方面做得非常好，但正是由于这些抽象，它的性能受到了影响。这就是为什么其他库（如 DPDK）会通过所谓的轮询模式驱动程序（PMD）完全绕过内核网络协议栈。这样做的速度非常非常快，可以达到 100Gbit/s 网卡的速度。但这种性能也有一些缺点： DPDK 代码难以维护，无法从任何内核功能（如现有的网络驱动程序）中获益，支持的网卡数量有限，比内核还少，而且PMD 驱动程序将每个使用的core完全阻塞到100%(笔者注：PMD比较占用CPU资源)。</p><p>Consequently, getting some functionality in the Linux kernel that allows high-performance packet processing sounds pretty awesome. At first, there is one important thing to name which sometimes confuses people: AF_XDP is not a kernel bypass, like DPDK, it’s a fastpath inside the kernel. This means, e.g. normal kernel networking drivers are used. After clarifying this important difference, let’s dig into AF_XDP to see how it works and what we need to consider.<br>因此，在 Linux内核中加入一些允许高性能数据包处理的功能听起来非常棒。首先，有一点很重要，有时会让人感到困惑： AF_XDP 并不是像 DPDK 那样的绕过内核，而是内核中的fastpath。这意味着，例如，会使用正常的内核网络驱动程序。在澄清了这一重要区别之后，让我们深入了解 AF_XDP，看看它是如何工作的，以及我们需要考虑的事项。</p><p>Note: For deep explanations of all used concepts, please visit the <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html" target="_blank" rel="noopener">kernel documentation</a>, it’s really great! A complete working program and tutorial can be found <a href="https://github.com/xdp-project/xdp-tutorial/tree/master/advanced03-AF_XDP" target="_blank" rel="noopener">here</a>.<br>注：要深入了解所有使用的概念，请访问<a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html" target="_blank" rel="noopener">内核文档</a>，它真的很棒！在<a href="https://github.com/xdp-project/xdp-tutorial/tree/master/advanced03-AF_XDP" target="_blank" rel="noopener">这里</a>可以找到完整运行的程序和教程。</p><h2 id="Data-Flow-eBPF-and-XDP"><a href="#Data-Flow-eBPF-and-XDP" class="headerlink" title="Data Flow: eBPF and XDP"></a>Data Flow: eBPF and XDP</h2><p>In the mentioned kernel documentation, the authors assume that the reader is familiar with bpf and xdp, otherwise pointing to <a href="https://docs.cilium.io/en/latest/bpf/" target="_blank" rel="noopener">cilium docs</a> as reference. However, I think it’s important to mention how these two things work together with AF_XDP, to understand how AF_XDP differs from e.g. DPDK. XDP itself is a way to bypass the normal networking stack (not the whole kernel) to achieve high performance packet processing speeds. eBPF is used to run verified code in the kernel on a set of different events, called hooks. One of these hooks is the XDP hook. An eBPF program using the XDP hook gets called for every incoming packet arriving at the driver (if the driver supports running eBPF), getting a reference to the raw packet representation. The eBPF program can now perform different tasks with the packet, like modifying it, dropping it, passing it to the network stack, sending it back to the NIC or redirecting it. In our AF_XDP case, the redirecting (XDP_REDIRECT) is the most important action, because it allows to send packets directly to userspace. The following figure shows the flow of packets using a normal socket and AF_XDP.</p><p>在提到的内核文档中，作者假定读者熟悉bpf和xdp，否则会将<a href="https://docs.cilium.io/en/latest/bpf/" target="_blank" rel="noopener">cilium文档</a>作为参考。不过，我认为有必要提及这两样东西如何与 AF_XDP 协同工作，以了解 AF_XDP 与 DPDK 等的不同之处。XDP本身是一种绕过普通网络堆栈（而非整个内核）以实现高性能数据包处理速度的方法。eBPF用于在内核中不同的事件（称为钩子）运行验证代码。其中一个钩子就是 XDP 钩子。使用 XDP 钩子的 eBPF 程序会调用到达驱动程序的每个传入数据包（如果驱动程序支持运行 eBPF），并获得原始数据包的引用。现在，eBPF程序可以对数据包执行不同的任务，如修改、丢弃、传递给网络协议栈、发送回 NIC 或重定向。在我们的 AF_XDP 案例中，重定向（XDP_REDIRECT）是最重要的操作，因为它允许将数据包直接发送到用户空间。下图显示了使用普通套接字和 AF_XDP 的数据包流程。</p><p><img src="/images/2024/05/001.jpeg" alt></p><p>After being received by the NIC, the first layer the packets pass is the networking driver. In the driver, applications may load eBPF programs using the XDP hook to perform the actions explained above. In AF_XDP, the eBPF program redirects the packet to a particular XDP socket that was created in userspace. Bypassing the Linux networking (Traffic control, IP, TCP/UDP, etc), the userspace application can now handle the packets without further actions performed in the kernel. If the driver supports ZEROCOPY, the packets are written directly into address space of the application, otherwise, one copy operation needs to be performed. In contrast to AF_XDP, packets targeted to normal sockets (UDP/TCP) traverse the networking stack. They can either be passed to the stack using XDP_PASS or there is no eBPF program using the XDP hook, and packets are forwarded directly to the networking stack.<br>数据包被网卡接收后，首先经过的一层是网络驱动程序。在驱动程序中，应用程序可使用 XDP 钩子加载 eBPF 程序，以执行上述操作。在 AF_XDP 中，eBPF 程序会将数据包重定向到在用户空间创建的特定 XDP socket。绕过 Linux 网络（Traffic control、IP、TCP/UDP等），用户空间应用程序现在可以处理数据包，而无需在内核中执行进一步操作。如果驱动程序支持 ZEROCOPY，数据包就会直接写入应用程序的地址空间，否则就需要执行一次复制操作。与 AF_XDP 不同，针对普通套接字（UDP/TCP）的数据包会走网络协议栈。这些数据包可以使用XDP_PASS传递到协议栈，或者不使用 XDP 钩子的 eBPF 程序，直接转发到网络协议栈。</p><p>Now let’s consider the backwards direction. In AF_XDP, packets can be passed directly to the NIC driver by passing a block of memory containing them to the driver, which then processes them and send them to the NIC. On the other hand, normal sockets send packets using syscalls like <code>sendto</code>, where the packets traverse the whole networking stack backwards. On the outgoing side, there is no XDP hook that can be attached using eBPF, so no further packet processing here.</p><p>现在，让我们考虑一下相反方向(笔者注：发包)。在 AF_XDP 中，数据包可以直接传递给网卡驱动程序，方法是将包含数据包的内存块传递给驱动程序，然后由驱动程序处理数据包并将其发送给网卡。另一方面，普通套接字使用<code>sendto</code>等系统调用发送数据包，数据包会经过整个网络协议栈。发包时，没有可以使用 eBPF attached的 XDP 钩子，因此这里没有进一步的数据包处理。</p><p>Note: Please consider that there are some SmartNICs that also support running XDP programs directly on the NIC. However, this is not the common case, therefor the driver mode is focused here.<br>注：请注意，有些 SmartNIC 也支持直接在 NIC 上运行 XDP程序。不过，这种情况并不常见，因此这里主要介绍驱动程序模式。</p><h2 id="Structure-and-Concepts"><a href="#Structure-and-Concepts" class="headerlink" title="Structure and Concepts"></a>Structure and Concepts</h2><p>In the previous section, we saw how packets flow until they arrive at our application. So now let’s look at how AF_XDP sockets read and write packets from/to the NIC driver. AF_XDP works in a completely different way from what we already know about socket programming. The setup of the socket is quite similar, but reading and writing from/to the NIC differs a lot. In AF_XDP, you create a UMEM region, and you have four rings assigned to the UMEM: RX, TX, completion and fill ring. Wow, sounds really complication. But trust me, it’s not. UMEM is basically just an area of continuos virtual memory, divided into equal-sized frames. The mentioned 4 rings contain pointers to particular offsets in the UMEM. To understand the rings, let’s consider an example, shown in the next figure.<br>在上一节中，我们了解了数据包在到达应用程序之前是如何流动的。现在我们来看看 AF_XDP 套接字是如何从网卡驱动程序读写数据包的。AF_XDP的工作方式与我们已经了解的套接字编程完全不同。套接字的设置非常相似，但从 NIC 读取和向 NIC 写入数据却有很大不同。在 AF_XDP 中，您需要创建一个 UMEM 区域，并为UMEM分配四个环：RX、TX、completion和fill环。听起来真复杂。但相信我，其实并不复杂。UMEM 基本上只是一个连续虚拟内存区域，被划分为大小相等的帧。上述 4 个环包含指向 UMEM中特定偏移量的指针。为了理解这些环，让我们看一个例子，如下图所示。</p><p><img src="/images/2024/05/002.jpeg" alt></p><p>This figure covers the reading of packets from the driver. So we produce UMEM addresses to the fill ring, meaning we put some slots of our UMEM into the fill ring (1). Afterwards, we notify the kernel: Hey, there are entries in our fill ring, please write arriving packets there. After passing the fill ring (2) and the rx ring (3) to the kernel, the kernel writes packets at the slots we produced beforehand (4) to the rx ring. We can now fetch new packets using the rx ring, after the kernel gives us back both rings (5) (6). The rx ring contains packet descriptors in the slots we passed via the fill ring to the kernel, in case there were packets that arrived. Great, we can now handle all of our packets, and then start again putting some references in the fill ring, and continue the same reading packets from the NIC.</p><p>To send packets via the NIC, the remaining two rings are used, in a similar way seen before on the receive side. We produce packet descriptors to the tx ring, meaning we put some references to our UMEM into the tx ring. Once we filled the ring, we pass it to the kernel. After the kernel transferred the packets, the respective references are filled into the completion ring and our application can reuse the slots in the UMEM.</p><p>In summary, using AF_XDP, we get a pretty awesome tradeoff between using existing code of the kernel (NIC drivers) and gain high performance for packet processing. I hope this article gives you at least an idea of how AF_XDP works.</p><h2 id="4个ring的总结"><a href="#4个ring的总结" class="headerlink" title="4个ring的总结"></a>4个ring的总结</h2><p>对于fill ring、completion ring、rx ring、tx ring的总结:</p><ul><li>fill ring与rx ring配合，用于收包<ul><li>fill ring(生产者是用户态程序，消费者是内核态中的XDP程序)类比于virtio-net收包时的avail ring</li><li>rx ring(生产者是XDP程序，消费者是用户态程序)类比于virtio-net收包时的used ring</li></ul></li><li>completion ring与tx ring配合，用于发包<ul><li>tx ring(生产者是用户态程序，消费者是XDP程序)类比于virtio-net发包时的avail ring</li><li>completion ring(生产者是XDP程序，消费者是用户态程序)类比于virtio-net发包时的used ring</li></ul></li><li>The UMEM uses two rings: FILL and COMPLETION. Each socket associated with the UMEM must have an RX queue, TX queue or both. Say, that there is a setup with four sockets (all doing TX and RX). Then there will be one FILL ring, one COMPLETION ring, four TX rings and four RX rings.</li><li>The rings are head(producer)/tail(consumer) based rings. A producer writes the data ring at the index pointed out by struct xdp_ring producer member, and increasing the producer index. A consumer reads the data ring at the index pointed out by struct xdp_ring consumer member, and increasing the consumer index.</li></ul><hr><p>参考资料:</p><ol><li><a href="https://rexrock.github.io/post/af_xdp1/" target="_blank" rel="noopener">AF_XDP技术详解</a></li><li><a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html?highlight=af_xdp" target="_blank" rel="noopener">https://www.kernel.org/doc/html/latest/networking/af_xdp.html?highlight=af_xdp</a></li><li><a href="https://docs.cilium.io/en/latest/bpf/" target="_blank" rel="noopener">https://docs.cilium.io/en/latest/bpf/</a></li><li><a href="https://www.youtube.com/watch?v=9bbdhnbVbDk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=9bbdhnbVbDk</a></li><li><a href="https://www.youtube.com/watch?v=Gv-nG6F_09I&amp;t=1417s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=Gv-nG6F_09I&amp;t=1417s</a></li><li><a href="http://vger.kernel.org/lpc_net2018_talks/lpc18_paper_af_xdp_perf-v2.pdf" target="_blank" rel="noopener">http://vger.kernel.org/lpc_net2018_talks/lpc18_paper_af_xdp_perf-v2.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要转载自&lt;a href=&quot;https://hpnpl.net/posts/recapituatling-af-xdp/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Recapitulating AF_XDP&lt;/a&gt;。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>notes about AMD spec</title>
    <link href="http://liujunming.github.io/2024/04/21/notes-about-AMD-spec/"/>
    <id>http://liujunming.github.io/2024/04/21/notes-about-AMD-spec/</id>
    <published>2024-04-21T12:44:30.000Z</published>
    <updated>2024-04-22T14:19:53.653Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下AMD spec相关信息。<a id="more"></a>本文将随着笔者的研发经验的积累持续更新。</p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><h4 id="搜索网站"><a href="#搜索网站" class="headerlink" title="搜索网站"></a>搜索网站</h4><p><a href="https://www.amd.com/en/search/documentation/hub.html#sortCriteria=%40amd_release_date%20descending" target="_blank" rel="noopener">https://www.amd.com/en/search/documentation/hub.html#sortCriteria=%40amd_release_date%20descending</a></p><h4 id="搜索技巧"><a href="#搜索技巧" class="headerlink" title="搜索技巧"></a>搜索技巧</h4><p>作为虚拟化开发人员，Programmer References和Specifications这两个Document Type用的比较多。<br><img src="/images/2024/04/015.jpg" alt></p><h3 id="APM"><a href="#APM" class="headerlink" title="APM"></a>APM</h3><p>APM(Architecture Programmer’s Manual)与Intel的SDM对应。</p><ul><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/24592.pdf" target="_blank" rel="noopener">APM Vol1</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/24593.pdf" target="_blank" rel="noopener">APM Vol2</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/24594.pdf" target="_blank" rel="noopener">APM Vol3</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/26568.pdf" target="_blank" rel="noopener">APM Vol4</a></li><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/26569.pdf" target="_blank" rel="noopener">APM Vol5</a></li></ul><h3 id="IOMMU"><a href="#IOMMU" class="headerlink" title="IOMMU"></a>IOMMU</h3><ul><li><a href="https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/specifications/48882_IOMMU.pdf" target="_blank" rel="noopener">AMD I/O Virtualization Technology (IOMMU) Specification, 48882</a></li></ul><h3 id="MISC"><a href="#MISC" class="headerlink" title="MISC"></a>MISC</h3><p><img src="/images/2024/04/016.jpg" alt></p><ul><li><a href="https://kib.kiev.ua/x86docs/AMD/" target="_blank" rel="noopener">https://kib.kiev.ua/x86docs/AMD/</a></li><li><a href="https://lore.kernel.org/kvm/?q=24593" target="_blank" rel="noopener">https://lore.kernel.org/kvm/?q=24593</a></li><li><a href="https://lore.kernel.org/kvm/?q=APM" target="_blank" rel="noopener">https://lore.kernel.org/kvm/?q=APM</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下AMD spec相关信息。
    
    </summary>
    
      <category term="AMD" scheme="http://liujunming.github.io/categories/AMD/"/>
    
    
      <category term="AMD" scheme="http://liujunming.github.io/tags/AMD/"/>
    
  </entry>
  
  <entry>
    <title>深入理解intel vmfunc指令</title>
    <link href="http://liujunming.github.io/2024/04/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3intel-vmfunc%E6%8C%87%E4%BB%A4/"/>
    <id>http://liujunming.github.io/2024/04/21/深入理解intel-vmfunc指令/</id>
    <published>2024-04-21T11:31:37.000Z</published>
    <updated>2024-04-21T12:28:28.358Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>VMFUNC is an Intel hardware instruction that allows software in non-root mode (in both kernel and user modes) to invoke a VM function. VM functions are processor features managed by the hypervisor. EPTP (the pointer to an EPT) switching is one of these VM functions<a id="more"></a>, which allows the guest to load a new value for the EPTP from an EPTP list stored in the Virtual Machine Control Structure (VMCS) configured by the hypervisor. The new EPT then translates subsequent guest physical addresses (GPA) to host physical addresses (HPA). The EPTP list can hold at most 512 EPTP entries. The typical usage of EPTP switching is to create multiple domains for one physical address space and these domains usually have different memory mappings and privileges. With the Virtual Processor ID (VPID) feature enabled, the VMFUNC instruction does not flush TLB. </p><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><p><img src="/images/2024/04/010.jpg" alt></p><p><img src="/images/2024/04/011.jpg" alt></p><p><img src="/images/2024/04/012.jpg" alt></p><p><img src="/images/2024/04/013.jpg" alt></p><p><img src="/images/2024/04/014.jpg" alt></p><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>Q: TLB是gva-&gt;hpa的映射，EPTP切换之后，此时HPA可能会发生变化，那么vmfunc需要flush TLB吗？</p><p>A: The logical processor starts creating and using guest-physical and combined mappings associated with the new value of bits 51:12 of EPTP; the combined mappings created and used are associated with the current VPID and PCID (these are not changed by VMFUNC).<br>当enable VPID时，硬件会重新creating mappings！所以是由硬件保证了TLB的正确性，无需flush TLB!<br>笔者猜测硬件的行为：执行vmfunc命令时，处理器会检查TLB中当前VPID和PCID的相关entry，然后更新TLB中的相关entry(比如会更新HPA的值或者权限)。</p><hr><p>参考资料:</p><ol><li><a href="https://www.cse.unsw.edu.au/~cs9242/19/exam/paper1.pdf" target="_blank" rel="noopener">SkyBridge: Fast and Secure Inter-Process Communication for Microkernels</a></li><li>Intel SDM Vol3</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h3&gt;&lt;p&gt;VMFUNC is an Intel hardware instruction that allows software in non-root mode (in both kernel and user modes) to invoke a VM function. VM functions are processor features managed by the hypervisor. EPTP (the pointer to an EPT) switching is one of these VM functions
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Cuckoo hashing</title>
    <link href="http://liujunming.github.io/2024/04/14/Notes-about-Cuckoo-hashing/"/>
    <id>http://liujunming.github.io/2024/04/14/Notes-about-Cuckoo-hashing/</id>
    <published>2024-04-14T00:02:43.000Z</published>
    <updated>2024-04-14T10:25:23.037Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Cuckoo hashing相关notes。<a id="more"></a></p><h2 id="1-Why"><a href="#1-Why" class="headerlink" title="1. Why"></a>1. Why</h2><p>Cuckoo hashing is a scheme for resolving hash collisions of values of hash functions in a table(解决表中哈希函数值的哈希碰撞问题), with worst-case constant lookup time. </p><h2 id="2-What"><a href="#2-What" class="headerlink" title="2. What"></a>2. What</h2><p>The name derives from the behavior of some species of cuckoo, where the cuckoo chick pushes the other eggs or young out of the nest when it hatches(这一名称源于某些种类杜鹃的行为，即杜鹃雏鸟在孵化时会将其他蛋或幼鸟推出巢外); analogously, inserting a new key into a cuckoo hashing table may push an older key to a different location in the table.</p><h2 id="3-How"><a href="#3-How" class="headerlink" title="3. How"></a>3. How</h2><p>In the basic variant of Cuckoo hashing we use two hash tables <strong>T1</strong> and <strong>T2</strong> of equal size, and we index them with the hash functions <strong>h1</strong>, respectively <strong>h2</strong>. Here are the main operations:</p><h3 id="3-1-Search"><a href="#3-1-Search" class="headerlink" title="3.1 Search"></a>3.1 Search</h3><p>Search couldn’t be easier: an element <strong>x</strong> can exist in one of two locations: in <strong>T1</strong> at position <strong>h1(x)</strong> or in <strong>T2</strong> at position <strong>h2(x)</strong>. We can check both locations in constant time.</p><h3 id="3-2-Delete"><a href="#3-2-Delete" class="headerlink" title="3.2 Delete"></a>3.2 Delete</h3><p>Delete is similarly easy: we look at the two possible locations, and if the element is there, we delete it.</p><h3 id="3-3-Insert"><a href="#3-3-Insert" class="headerlink" title="3.3 Insert"></a>3.3 Insert</h3><p>Insert is a bit trickier: we put <strong>x</strong> in <strong>T1[h1(x)]</strong>. If there was some element <strong>y</strong> stored in that location, <strong>y</strong> must be evicted (thus the name “cuckoo” hashing). We put <strong>y</strong> in its other valid location <strong>T2[h2(y)]</strong>. If that location is occupied by some element <strong>z</strong>, we have to evict <strong>z</strong> and insert it in its other valid location <strong>T1[h1(z)]</strong>. We continue like this until we find an empty place and the process finishes, or until we give up because because we ran into a loop. If the latter happens, we conclude that insert failed, we stop and we rehash everything with new hash functions (increasing the table sizes if the tables are getting too full).</p><h2 id="4-算法细节"><a href="#4-算法细节" class="headerlink" title="4. 算法细节"></a>4. 算法细节</h2><p><img src="/images/2024/04/006.jpg" alt><br>图(a)解析：想插入元素x，用哈希函数h1(x)计算出下标i1，发现巢穴中i1已经存在元素y，于是用x踢开y；y用哈希函数h2(x)计算出下一个下标，发现存在元素z，于是用y踢开z；z用哈希函数h1(x)计算出下一个下标，填充进去，算法结束。</p><p>注意：在T1的元素发生哈希冲突时，用h2(x)去计算下一个下标、在T2的元素发生哈希冲突时，用h1(x)去计算下一个下标。</p><p>图(b)解析：想插入元素x，x踢出y，y踢出z，z踢出u，u踢出v，v踢出x，形成死循环，在T1插入失败；转而在T2插入，x踢出t，t踢出s，s踢出x，形成死循环，在T2插入失败，需要rehash。</p><p><img src="/images/2024/04/008.svg" alt></p><p>Insert算法与上面的图(b)相比，在T1插入失败就会rehash，不会继续尝试往T2中插入；这属于实现细节问题，无需纠结。</p><p><img src="/images/2024/04/009.svg" alt></p><h2 id="5-布谷鸟哈希表-vs-桶链式哈希表"><a href="#5-布谷鸟哈希表-vs-桶链式哈希表" class="headerlink" title="5. 布谷鸟哈希表 vs 桶链式哈希表"></a>5. 布谷鸟哈希表 vs 桶链式哈希表</h2><p><img src="/images/2024/04/007.jpg" alt></p><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>cuckoo hashing适合空间需求量大，对读性能要求高，对写性能相对低，操作比例读为主写为辅的场景。理由是基于Cuckoo hashing的优点和缺点。</p><h3 id="6-1-优点"><a href="#6-1-优点" class="headerlink" title="6.1 优点"></a>6.1 优点</h3><ul><li><p>哈希表本身的空间利用率高:当表宽为1(也就是只使用1个hash表)时，最大load factor（多少空间被利用）大概率能到50%；实验显示表宽大于4以后，load factor大概率能到95%</p></li><li><p>查询可以使用两次读完成:表宽不大的情况下，两次（可并行）的cacheline read就能完成一个查询。相比之下查询链式哈希表可能有多次pointer dereference，查询时间方差会大过cuckoo hashing</p></li></ul><h3 id="6-2-缺点"><a href="#6-2-缺点" class="headerlink" title="6.2 缺点"></a>6.2 缺点</h3><ul><li><p>插入操作的复杂度大:当表接近其最大load factor时，会有很多次kick out操作才能完成一次插入</p></li><li><p>读写的高并发算法复杂:不像链式哈希表，每个bucket放一把读写锁就可以实现细粒度的读写并发，cuckoo hashing的写会涉及到多个且事先不预知的bucket，抢锁会复杂</p></li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.lkozma.net/cuckoo_hashing_visualization/" target="_blank" rel="noopener">Cuckoo Hashing Visualization</a></li><li><a href="https://www.di.ens.fr/~vergnaud/algo0910/Cuckoo.pdf" target="_blank" rel="noopener">Cuckoo hashing paper</a></li><li><a href="https://www.wikiwand.com/en/Cuckoo_hashing" target="_blank" rel="noopener">Wikipedia</a></li><li><a href="https://www.baeldung.com/cs/cuckoo-hashing" target="_blank" rel="noopener">baeldung:Cuckoo Hashing</a></li><li><a href="https://zhuanlan.zhihu.com/p/442498412" target="_blank" rel="noopener">布谷鸟哈希和布谷鸟过滤器</a></li><li><a href="https://zhuanlan.zhihu.com/p/594818514" target="_blank" rel="noopener">布谷鸟哈希（Cuckoo hash）</a></li><li><a href="https://www.zhihu.com/question/28544694?utm_id=0" target="_blank" rel="noopener">Cuckoo hashing主要适合在哪些场景使用?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Cuckoo hashing相关notes。
    
    </summary>
    
      <category term="数据结构" scheme="http://liujunming.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据结构" scheme="http://liujunming.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about linux DAX</title>
    <link href="http://liujunming.github.io/2024/04/13/Notes-about-linux-DAX/"/>
    <id>http://liujunming.github.io/2024/04/13/Notes-about-linux-DAX/</id>
    <published>2024-04-13T08:22:43.000Z</published>
    <updated>2024-04-13T08:40:31.063Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下DAX(Direct Access)相关notes。</p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>Direct Access (DAX) enables direct access to files stored in persistent memory or on a block device. Without DAX support in a file system, the page cache is generally used to buffer reads and writes to files, and requires an extra copy operation.</p><p>DAX removes the extra copy operation by performing reads and writes directly to the storage device. <a id="more"></a></p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>The page cache is usually used to buffer reads and writes to files. It is also used to provide the pages which are mapped into userspace by a call to mmap.</p><p>For block devices that are memory-like, the page cache pages would be unnecessary copies of the original storage. The DAX code removes the extra copy by performing reads and writes directly to the storage device. For file mappings, the storage device is mapped directly into userspace.</p><hr><p>参考资料:</p><ol><li><a href="https://docs.kernel.org/filesystems/dax.html" target="_blank" rel="noopener">Direct Access for Files</a></li><li><a href="https://kb.pmem.io/faq/100000008-What-is-DAX/" target="_blank" rel="noopener">What is Direct-Access (DAX)?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下DAX(Direct Access)相关notes。&lt;/p&gt;
&lt;h2 id=&quot;What&quot;&gt;&lt;a href=&quot;#What&quot; class=&quot;headerlink&quot; title=&quot;What&quot;&gt;&lt;/a&gt;What&lt;/h2&gt;&lt;p&gt;Direct Access (DAX) enables direct access to files stored in persistent memory or on a block device. Without DAX support in a file system, the page cache is generally used to buffer reads and writes to files, and requires an extra copy operation.&lt;/p&gt;
&lt;p&gt;DAX removes the extra copy operation by performing reads and writes directly to the storage device.
    
    </summary>
    
      <category term="文件系统" scheme="http://liujunming.github.io/categories/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>How to pin file page cache</title>
    <link href="http://liujunming.github.io/2024/04/13/How-to-pin-file-page-cache/"/>
    <id>http://liujunming.github.io/2024/04/13/How-to-pin-file-page-cache/</id>
    <published>2024-04-13T01:31:09.000Z</published>
    <updated>2024-04-13T02:12:12.487Z</updated>
    
    <content type="html"><![CDATA[<p>本文将会介绍Linux中pin住文件page cache的方法。<a id="more"></a></p><h3 id="vmtouch"><a href="#vmtouch" class="headerlink" title="vmtouch"></a>vmtouch</h3><blockquote><p>-t<br>Touch virtual memory pages. Reads a byte from each page of the file. If the page is not resident in memory, a page fault will be generated and the page will be read from disk into the file system’s memory cache.<br>-l<br>Lock pages into physical memory. This option works the same as “-t” except it calls mlock(2) on all the memory mappings and doesn’t close the descriptors when finished. The files will be locked in physical memory until the vmtouch process is killed.</p></blockquote><h3 id="page-cache"><a href="#page-cache" class="headerlink" title="page cache"></a>page cache</h3><p><img src="/images/2024/04/005.png" alt></p><p>For read reuqest:</p><ol><li><p>When a user-space application wants to read data from disks, it asks the kernel for data using special system calls such as <code>read()</code>, <code>pread()</code>, <code>vread()</code>, <code>mmap()</code>, <code>sendfile()</code>, etc.</p></li><li><p>Linux kernel, in turn, checks whether the pages are present in Page Cache and immediately returns them to the caller if so. As you can see kernel has made 0 disk operations in this case.</p></li><li><p>If there are no such pages in Page Cache, the kernel must load them from disks. In order to do that, it has to find a place in Page Cache for the requested pages. A memory reclaim process must be performed if there is no free memory (in the caller’s cgroup or system). Afterward, kernel schedules a read disk IO operation, stores the target pages in the memory, and finally returns the requested data from Page Cache to the target process. Starting from this moment, any future requests to read this part of the file (no matter from which process or cgroup) will be handled by Page Cache without any disk IO operation until these pages have not been evicted.</p></li></ol><p>由此可知，<code>mmap()</code> file reads也是会使用到page cache的。</p><h3 id="mlock"><a href="#mlock" class="headerlink" title="mlock"></a>mlock</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">mlock</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> addr[.len], <span class="keyword">size_t</span> len)</span></span>;</span><br></pre></td></tr></table></figure><blockquote><p>mlock(), mlock2(), and mlockall() lock part or all of the calling process’s virtual address space into RAM, preventing that memory from being paged to the swap area.</p></blockquote><p>实际上内核对mlock锁住的页面主要做了两步比较重要的操作:</p><ol><li>调用mlock的时候就将所需要的物理页面准备好</li><li>内存回收时，当扫描到相关的物理页面时，将其放入不可回收的lru链表</li></ol><p>第一步保证访问的虚拟地址对应的物理页面在内存中，第二步保证了锁住的页面不会被回收。</p><h3 id="vmtouch-source-code"><a href="#vmtouch-source-code" class="headerlink" title="vmtouch source code"></a>vmtouch source code</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/hoytech/vmtouch/blob/master/vmtouch.c</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vmtouch_file</span><span class="params">(<span class="keyword">char</span> *path)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    fd = open(path, open_flags, <span class="number">0</span>);</span><br><span class="line">    ...</span><br><span class="line">    mem = mmap(<span class="literal">NULL</span>, len_of_range, PROT_READ, MAP_SHARED, fd, offset);</span><br><span class="line">    ...</span><br><span class="line">    mlock(mem, len_of_range)</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现其实也比较简单，基于<code>mmap</code>和<code>mlock</code>即可！</p><hr><p>参考资料:</p><ol><li><a href="https://github.com/hoytech/vmtouch/blob/master/vmtouch.c" target="_blank" rel="noopener">vmtouch.c</a></li><li><a href="https://linux.die.net/man/8/vmtouch" target="_blank" rel="noopener">man vmtouch</a></li><li><a href="https://man7.org/linux/man-pages/man2/mlock.2.html" target="_blank" rel="noopener">man mlock</a></li><li><a href="https://biriukov.dev/docs/page-cache/2-essential-page-cache-theory/" target="_blank" rel="noopener">Essential Page Cache theory</a></li><li><a href="https://biriukov.dev/docs/page-cache/3-page-cache-and-basic-file-operations/" target="_blank" rel="noopener">Page Cache and basic file operations</a></li><li><a href="https://blog.csdn.net/feelabclihu/article/details/123288206" target="_blank" rel="noopener">mlock锁原理剖析</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将会介绍Linux中pin住文件page cache的方法。
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Intel new feature:user-timer events</title>
    <link href="http://liujunming.github.io/2024/04/06/Intel-new-feature-user-timer-events/"/>
    <id>http://liujunming.github.io/2024/04/06/Intel-new-feature-user-timer-events/</id>
    <published>2024-04-06T08:33:33.000Z</published>
    <updated>2024-04-06T10:23:39.180Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录下<a href="https://cdrdv2.intel.com/v1/dl/getContent/671368" target="_blank" rel="noopener">Intel® Architecture Instruction Set Extensions Programming Reference</a>中的user-timer events技术。笔者特意裁剪了本文相关的描述: <a href="/pdf/user-timer events.pdf">user-timer events</a><a id="more"></a></p><p>The feature defines a new 64-bit value called the <strong>user deadline</strong>. Software may read and write the user deadline. When the user deadline is not zero, a user-timer event becomes pending when the logical processor’s timestamp counter (TSC) is greater than or equal to the user deadline.</p><p>A pending user-timer event is processed by the processor when CPL = 3 and certain other conditions apply. When processed, the event results in a user interrupt with the <strong>user-timer vector</strong>. (Software may read and write the user-timer vector). Specifically, the processor sets the bit in the UIRR (user interrupt request register) corresponding to the user timer vector. The processing also clears the user deadline, ensuring that there will be no subsequent user-timer events until software writes the user deadline again.</p><h2 id="User-Deadline"><a href="#User-Deadline" class="headerlink" title="User Deadline"></a>User Deadline</h2><p>A logical processor that supports user-timer events supports a 64-bit value called the <strong>user deadline</strong>. If the user deadline is non-zero, the logical processor pends a user-timer event when the timestamp counter (TSC) reaches or exceeds the user deadline.</p><p>Software can write the user deadline using instructions. The processor enforces the following:</p><ul><li>Writing zero to the user deadline disables user-timer events and cancels any that were pending. As a result, no user-timer event is pending after zero is written to the user deadline.</li><li>If software writes the user deadline with a non-zero value that is less than the TSC, a user-timer event will be pending upon completion of that write.</li><li>If software writes the user deadline with a non-zero value that is greater than that of the TSC, no user-timer event will be pending after the write until the TSC reaches the new user deadline.</li><li>A logical processor processes a pending user-timer event under certain conditions; The logical processor clears the user deadline after pending a user-timer event.</li></ul><h2 id="User-Timer-Architectural-State"><a href="#User-Timer-Architectural-State" class="headerlink" title="User Timer: Architectural State"></a>User Timer: Architectural State</h2><p>The user-timer architecture defines a new MSR, IA32_UINTR_TIMER. This MSR can be accessed using MSR index 1B00H.</p><p>The IA32_UINTR_TIMER MSR has the following format:</p><ul><li>Bits 5:0 are the user-timer vector. Processing of a user-timer event results in the pending of a user interrupt with this vector.</li><li>Bits 63:6 are the upper 56 bits of the user deadline.</li></ul><p>Note that no bits are reserved in the MSR and that writes to the MSR will not fault due to the value of the instruction’s source operand. The IA32_UINTR_TIMER MSR can be accessed via the following instructions: RDMSR, RDMSRLIST, URDMSR, UWRMSR, WRMSR, WRMSRLIST, and WRMSRNS.</p><p>If the IA32_UINTR_TIMER MSR is written with value X, the user-timer vector gets value X &amp; 3FH; the user deadline gets value X &amp; ~3FH.</p><p>If the user-timer vector is V (0 ≤ V ≤ 63) and the user deadline is D, a read from the IA32_UINTR_TIMER MSR return value (D &amp; ~3FH) | V.</p><h2 id="Pending-and-Processing-of-User-Timer-Events"><a href="#Pending-and-Processing-of-User-Timer-Events" class="headerlink" title="Pending and Processing of User-Timer Events"></a>Pending and Processing of User-Timer Events</h2><p>There is a user-timer event pending whenever the user deadline is non-zero and is less than or equal to the value of the timestamp counter (TSC).</p><p>If CR4.UINTR = 1, a logical processor processes a pending user-timer event at an instruction boundary at which the following conditions all hold1: (1) IA32_EFER.LMA = CS.L = 1 (the logical processor is in 64-bit mode); (2) CPL = 3; (3) UIF = 1; and (4) the logical processor is not in the shutdown state or in the wait-for-SIPI state.</p><p>When the conditions just identified hold, the logical processor processes a user-timer event. User-timer events have priority just above that of user-interrupt delivery. If the logical processor was in a state entered using the TPAUSE and UMWAIT instructions, it first wakes up from that state and becomes active.<br>当上述条件成立时，logical processor就会处理用户定时器事件。用户定时器事件的优先级仅高于user-interrupt delivery。如果logical processor处于使用TPAUSE和UMWAIT指令进入的状态，它将首先从该状态中唤醒并进入活动状态。</p><p>The following pseudocode details the processing of a user-timer event:</p><ul><li>UIRR[UserTimerVector] := 1;</li><li>recognize a pending user interrupt;// may be delivered immediately after processing </li><li>IA32_UINTR_TIMER := 0;// clears the deadline and the vector</li></ul><h2 id="VMX-support"><a href="#VMX-support" class="headerlink" title="VMX support"></a>VMX support</h2><p>One new 64-bit VM-execution control field is defined called the <strong>virtual user-timer control</strong>. It can be accessed with the encoding pair 2050H/2051H.</p><p>Software can read and write the IA32_UINTR_TIMER MSR using certain instructions. The operation of those instructions is changed when they are executed in VMX non-root operation:</p><ul><li>Any read from the IA32_UINTR_TIMER MSR (e.g., by RDMSR) returns the value of the virtual user-timer control.</li><li>Any write to the IA32_UINTR_TIMER MSR (e.g., by WRMSR) is treated as follows:<ul><li>The source operand is written to the virtual user-timer control (updating the VMCS).</li><li>Bits 5:0 of the source operand are written to the user-timer vector.</li><li>If bits 63:6 of the source operand are zero, the user deadline (the value that actually controls when hardware generates a user time event) is cleared to 0.</li><li>If bits 63:6 of the source operand are not all zero, the user deadline is computed as follows. The source operand (with the low 6 bits cleared) is interpreted as a virtual user deadline. The processor converts that value to the actual user deadline based on the current configuration of TSC offsetting and TSC scaling.</li><li>Following such a write, the value of the IA32_UINTR_TIMER MSR (e.g., as would be observed following a subsequent VM exit) is such that bits 63:6 contain the actual user deadline (not the virtual user deadline), while bits 5:0 contain the user-timer vector.</li></ul></li></ul><h2 id="相关指令汇总"><a href="#相关指令汇总" class="headerlink" title="相关指令汇总"></a>相关指令汇总</h2><ul><li>RDMSR</li><li>RDMSRLIST</li><li>URDMSR</li><li>UWRMSR</li><li>WRMSR</li><li>WRMSRLIST</li><li>WRMSRNS</li><li>TPAUSE</li><li>UMWAIT</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>笔者认为user-timer就是TSC deadline模式的lapic timer在user interrupt中的拓展。而user-timer虚拟化的核心就是IA32_UINTR_TIMER MSR的虚拟化。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录下&lt;a href=&quot;https://cdrdv2.intel.com/v1/dl/getContent/671368&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Intel® Architecture Instruction Set Extensions Programming Reference&lt;/a&gt;中的user-timer events技术。笔者特意裁剪了本文相关的描述: &lt;a href=&quot;/pdf/user-timer events.pdf&quot;&gt;user-timer events&lt;/a&gt;
    
    </summary>
    
      <category term="Time" scheme="http://liujunming.github.io/categories/Time/"/>
    
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
  </entry>
  
  <entry>
    <title>Intel new feature:monitorless MWAIT</title>
    <link href="http://liujunming.github.io/2024/04/05/Intel-new-feature-monitorless-MWAIT/"/>
    <id>http://liujunming.github.io/2024/04/05/Intel-new-feature-monitorless-MWAIT/</id>
    <published>2024-04-05T03:37:30.000Z</published>
    <updated>2024-04-05T13:56:00.935Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel的新feature:<a href="/pdf/monitorless MWAIT.pdf">monitorless MWAIT</a>。<a id="more"></a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Prior this feature, execution of the MWAIT instruction causes a logical processor to suspend execution and enter an implementation-dependent optimized state only if the MONITOR instruction was executed previously, specifying an address range to monitor, and there have been no stores to that address range since MONITOR executed. The logical processor leaves the optimized state and resumes execution when there is a write to the monitored address range.</p><p>This existing functionality <strong>supports software that seeks to suspend execution until an event associated with a write to the monitored address range</strong>. For example, that range may contain the head pointer of a work queue that is written when there is work for the suspended logical processor.</p><p>It is possible that software may wish to suspend execution with no requirement to resume execution in response to a memory write. Such software is not well served by the existing MWAIT instruction since it must incur the overhead of monitoring some (irrelevant) address range and may resume execution earlier than intended following a memory write.<br>软件可能希望暂停执行，而不要求在内存写入后恢复执行。现有的 MWAIT 指令并不能很好地满足这类软件的需求，因为它必须承担监控某些（不相关的）地址范围的开销，而且在内存写入后，可能会比预定时间提前恢复执行。</p><p>Monitorless MWAIT enhances the MWAIT instruction by allowing suspension of execution without monitoring an address range.<br>Monitorless MWAIT允许在不监控地址范围的情况下暂停执行，从而增强了 MWAIT 指令。</p><p>The feature is defined with an enumeration independent of that of existing MONITOR/MWAIT. That allows a VMM to virtualize monitorless MWAIT without having to virtualize the address-range monitoring of the existing feature.<br>该特性是用一个独立于现有MONITOR/MWAIT的枚举定义的。这使得 VMM 可以虚拟化monitorless MWAIT，而无需虚拟化现有功能的地址范围监控。</p><h3 id="Q-amp-amp-A"><a href="#Q-amp-amp-A" class="headerlink" title="Q &amp;&amp; A"></a>Q &amp;&amp; A</h3><p>Q: 使用了monitorless MWAIT后，logical processor什么时候可以resume execution呢？<br>A: spec中其实已经给出了答案:<br><img src="/images/2024/04/002.jpg" alt></p><ul><li>an NMI or SMI</li><li>a debug exception</li><li>a machine check exception</li><li>the BINIT# signal</li><li>the INIT# signal</li><li>the RESET# signal</li><li>an external interrupt if ECX[0] = 1</li></ul><p>Q: monitorless MWAIT与hlt的区别在哪里？<br>A: 与hlt相比，monitorless MWAIT resume execution的delay更低(因为monitorless MWAIT的节能程度不如hlt)，同时monitorless MWAIT还可以精细地指定C-state及Sub C-state。<br><img src="/images/2024/04/003.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="/2020/05/01/Introduction-to-halt-pause-monitor-mwait-instruction/">Introduction to hlt/pause/monitor/mwait instruction</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel的新feature:&lt;a href=&quot;/pdf/monitorless MWAIT.pdf&quot;&gt;monitorless MWAIT&lt;/a&gt;。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Intel new feature:APIC-timer virtualization</title>
    <link href="http://liujunming.github.io/2024/04/04/Intel-new-feature-APIC-timer-virtualization/"/>
    <id>http://liujunming.github.io/2024/04/04/Intel-new-feature-APIC-timer-virtualization/</id>
    <published>2024-04-04T10:40:40.000Z</published>
    <updated>2024-04-06T10:23:35.592Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录下<a href="https://cdrdv2.intel.com/v1/dl/getContent/671368" target="_blank" rel="noopener">Intel® Architecture Instruction Set Extensions Programming Reference</a>中的APIC-timer virtualization技术。笔者特意裁剪了本文相关的描述: <a href="/pdf/APIC-timer virtualization.pdf">APIC-timer virtualization</a><a id="more"></a></p><h2 id="1-overview"><a href="#1-overview" class="headerlink" title="1. overview"></a>1. overview</h2><p>The new feature virtualizes the TSC-deadline mode of the APIC timer. When this mode is active, software can program the APIC timer with a deadline written to the IA32_TSC_DEADLINE MSR. A timer interrupt becomes pending when the logical processor’s timestamp counter (TSC) is greater or equal to the deadline.</p><p>APIC-timer virtualization operates in conjunction with the existing virtual-interrupt delivery feature. With that feature, a virtual-machine monitor (VMM) establishes a virtual-APIC page in memory for each virtual logical processor (vCPU). A logical processor uses this page to virtualize certain aspects of APIC operation for the vCPU.</p><p>The feature is based on new guest-timer hardware that introduces two new architectural features: <strong>guest-timer events</strong> and a <strong>guest deadline</strong>. With APIC-timer virtualization, guest writes to the IA32_TSC_DEADLINE MSR do not interact with the APIC (or its timer) but instead establish a guest deadline to arm the guest-timer hardware. When a logical processor’s TSC is greater than or equal to the guest deadline(shadow context), a guest-timer event becomes pending. (笔者注:硬件)Processing of a guest-timer event updates the virtual-APIC page to record the fact that a new virtual interrupt is pending.</p><h2 id="2-guest-timer-hardware"><a href="#2-guest-timer-hardware" class="headerlink" title="2. guest-timer hardware"></a>2. guest-timer hardware</h2><p>A logical processor supports APIC-timer virtualization using new guest-timer hardware. Software controls this hardware using an unsigned 64-bit value called the <strong>guest deadline</strong>. (There is a separate guest deadline for each logical processor.) If the guest deadline is non-zero, a guest-timer event will be pending when the timestamp counter (TSC) reaches or exceeds the guest deadline.</p><h2 id="3-changes-to-vmx-non-root-operation"><a href="#3-changes-to-vmx-non-root-operation" class="headerlink" title="3. changes to vmx non-root operation"></a>3. changes to vmx non-root operation</h2><p>The 1-setting of the “APIC-timer virtualization” VM-execution control changes how a logical processor responds to accesses to the IA32_TSC_DEADLINE MSR.</p><h3 id="3-1-Accesses-to-the-IA32-TSC-DEADLINE-MSR"><a href="#3-1-Accesses-to-the-IA32-TSC-DEADLINE-MSR" class="headerlink" title="3.1 Accesses to the IA32_TSC_DEADLINE MSR"></a>3.1 Accesses to the IA32_TSC_DEADLINE MSR</h3><p>If the “APIC-timer virtualization” VM-execution control is 1, the operation of reads and writes to the<br>IA32_TSC_DEADLINE MSR (MSR 6E0H) is modified:</p><ul><li>Any read from the IA32_TSC_DEADLINE MSR (e.g., by RDMSR) that does not cause a fault or a VM exit returns the value of the guest deadline shadow (from the VMCS).</li><li>Any write to the IA32_TSC_DEADLINE MSR (e.g., by WRMSR) that does not cause a fault or a VM exit is treated as follows:<ul><li>The source operand is written to the guest deadline shadow (updating the VMCS).</li><li>If the source operand is zero, the guest deadline (<strong>the value that controls when hardware generates a guest time event</strong>) is cleared to 0.</li><li>If the source operand is not zero, the guest deadline is computed as follows. The source operand is interpreted as a virtual deadline. The processor converts that value to the actual guest deadline based on the current configuration of TSC offsetting and TSC scaling.</li></ul></li></ul><p>Note that when the “APIC-timer virtualization” VM-execution control is 1, such writes do not change the value of the IA32_TSC_DEADLINE MSR nor do they interact with the APIC timer in any way.</p><h3 id="3-2-Processing-of-Guest-Timer-Events"><a href="#3-2-Processing-of-Guest-Timer-Events" class="headerlink" title="3.2 Processing of Guest-Timer Events"></a>3.2 Processing of Guest-Timer Events</h3><p>Processing of a guest-timer event updates the virtual-APIC page to cause a virtual timer interrupt to become pending. Specifically, the logical processor performs the following steps:</p><ul><li>V := virtual timer vector;</li><li>VIRR[V] := 1;// update virtual IRR field on virtual-APIC page</li><li>RVI := max{RVI, V};// update guest interrupt status field in VMCS</li><li>evaluate pending virtual interrupts;// a virtual interrupt may be delivered immediately after this processing </li><li>Guest deadline := 0;</li><li>Guest deadline shadow := 0;</li></ul><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><ul><li>guest 在non-root mode写IA32_TSC_DEADLINE MSR时，无需发生VM Exit，硬件会更新guest deadline shadow(virtual context)，同时也会更新guest deadline(shadow context)；</li><li>硬件会利用guest deadline(<strong>the value that controls when hardware generates a guest time event</strong>)与physical TSC相比，当physical timestap大于等于guest deadline时，就会给non-root mode的vCPU注入timer中断</li><li>给vCPU注入的timer中断vector由Virtual timer vector决定</li></ul><p><img src="/images/2024/04/001.jpg" alt></p><p>利用<a href="/2021/03/20/虚拟化学习心得-three-context/">three context思想</a>以及理解上述三个VMCS fields的作用，即可对APIC-timer virtualization有深入的理解。</p><h2 id="5-Q-amp-A"><a href="#5-Q-amp-A" class="headerlink" title="5. Q &amp; A"></a>5. Q &amp; A</h2><p>Q: hypervisor如何获知vCPU的timer中断vector的呢？<br>A: guest配置LVT的Timer时，会发生VM Exit，这样hypervisor就可以知道vCPU的timer中断vector，然后将Virtual timer vector这一VMCS field设置为vCPU的timer中断vector即可！<br><img src="/images/2024/04/004.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录下&lt;a href=&quot;https://cdrdv2.intel.com/v1/dl/getContent/671368&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Intel® Architecture Instruction Set Extensions Programming Reference&lt;/a&gt;中的APIC-timer virtualization技术。笔者特意裁剪了本文相关的描述: &lt;a href=&quot;/pdf/APIC-timer virtualization.pdf&quot;&gt;APIC-timer virtualization&lt;/a&gt;
    
    </summary>
    
      <category term="Time" scheme="http://liujunming.github.io/categories/Time/"/>
    
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
  </entry>
  
  <entry>
    <title>Notes about vhost-user-nvme</title>
    <link href="http://liujunming.github.io/2024/03/31/Notes-about-vhost-user-nvme/"/>
    <id>http://liujunming.github.io/2024/03/31/Notes-about-vhost-user-nvme/</id>
    <published>2024-03-31T04:21:45.000Z</published>
    <updated>2024-03-31T05:34:46.391Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍下vhost-user-nvme(SPDK Vhost-NVMe)技术。</p><p>The SPDK Vhost-NVMe target combines NVMe 1.3 new feature as well as vhost-user technology to accelerate NVMe IOs inside Guest VM. It uses NVMe as the paravirtualization protocol between Guest and SPDK Vhost-NVMe target. Also, no special paravirtualization driver is required inside Guest.<a id="more"></a></p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p><img src="/images/2024/03/018.jpg" alt></p><p><img src="/images/2024/03/019.jpg" alt></p><p><img src="/images/2024/03/020.jpg" alt></p><p><img src="/images/2024/03/021.jpg" alt></p><h2 id="SPDK-vhost-solution"><a href="#SPDK-vhost-solution" class="headerlink" title="SPDK vhost solution"></a>SPDK vhost solution</h2><p>Combine virtio and NVMe to inform a uniform SPDK vhost solution(结合virtio和NVMe形成统一的SPDK vhost解决方案)<br><img src="/images/2024/03/022.jpg" alt></p><p><img src="/images/2024/03/023.jpg" alt></p><p><img src="/images/2024/03/016.jpg" alt></p><p><img src="/images/2024/03/017.jpg" alt></p><p><img src="/images/2024/03/024.jpg" alt></p><p><img src="/images/2024/03/014.jpg" alt></p><p><img src="/images/2024/03/025.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/11/Accelerating-NVMe-I_Os-in-Virtual-Machine-via-SPDK-vhost_-Solution-Ziye-Yang-_-Changpeng-Liu-Intel.pdf" target="_blank" rel="noopener">Accelerating NVMe I/Os in Virtual Machine via SPDK vhost* Solution</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2018/75/KVM_Forum_26_Oct_2018_Vhost-NVMe.pdf" target="_blank" rel="noopener">slides: SPDK vhost Target: A Practical Solution to Accelerate Storage I/Os</a></li><li><a href="https://www.youtube.com/watch?v=paTvtJ6JdAc" target="_blank" rel="noopener">video: SPDK vhost Target: A Practical Solution to Accelerate Storage I/Os</a></li><li><a href="https://patchwork.kernel.org/project/qemu-devel/patch/1516003315-17878-2-git-send-email-changpeng.liu@intel.com/" target="_blank" rel="noopener">block/NVMe: introduce a new vhost NVMe host device to QEMU</a></li><li><a href="https://www.youtube.com/watch?v=y2vXN10AveM" target="_blank" rel="noopener">Vhost-NVMe: A New Virtualization Solution to Accelerate Guest NVMe IOs</a></li><li><a href="https://ieeexplore.ieee.org/document/8567374" target="_blank" rel="noopener">SPDK Vhost-NVMe: Accelerating I/Os in Virtual Machines on NVMe SSDs via User Space Vhost Target</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍下vhost-user-nvme(SPDK Vhost-NVMe)技术。&lt;/p&gt;
&lt;p&gt;The SPDK Vhost-NVMe target combines NVMe 1.3 new feature as well as vhost-user technology to accelerate NVMe IOs inside Guest VM. It uses NVMe as the paravirtualization protocol between Guest and SPDK Vhost-NVMe target. Also, no special paravirtualization driver is required inside Guest.
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe Shadow doorbell buffer</title>
    <link href="http://liujunming.github.io/2024/03/30/Notes-about-NVMe-Shadow-doorbell-buffer/"/>
    <id>http://liujunming.github.io/2024/03/30/Notes-about-NVMe-Shadow-doorbell-buffer/</id>
    <published>2024-03-30T08:20:37.000Z</published>
    <updated>2024-03-30T10:56:43.201Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍下NVMe中的Shadow doorbell buffer机制。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2024/03/014.jpg" alt><br>The guest driver only writes to the MMIO register when EventIdx has been reached.  This eliminates some MMIO writes.</p><h2 id="Identify"><a href="#Identify" class="headerlink" title="Identify"></a>Identify</h2><p><img src="/images/2024/03/015.jpg" alt></p><p>可以参考<a href="https://github.com/qemu/qemu/commit/3f7fe8de3d4" target="_blank" rel="noopener">hw/nvme: Implement shadow doorbell buffer support</a>中的<code>NVME_OACS_DBBUF = 1 &lt;&lt; 8</code>。</p><h2 id="Doorbell-Buffer-Config-command"><a href="#Doorbell-Buffer-Config-command" class="headerlink" title="Doorbell Buffer Config command"></a>Doorbell Buffer Config command</h2><p><img src="/images/2024/03/012.jpg" alt></p><blockquote><p>doorbell values are written by the nvme driver (guest OS) and the event index is written by the virtual device (host OS).</p></blockquote><p>The Doorbell Buffer Config admin command is implemented for the guest to enable shadow doobell buffer. When this feature is enabled, each SQ/CQ is associated with two buffers, i.e., Shadow Doorbell buffer and EventIdx buffer. According to the Spec, each queue’s doorbell register is only updated when the Shadow Doorbell buffer value changes from being less than or equal to the value of the corresponding EventIdx buffer entry to being greater than that value. Therefore, the number of MMIO’s on the doorbell registers is greatly reduced.</p><h2 id="Updating-Controller-Doorbell-Registers-using-a-Shadow-Doorbell-Buffer"><a href="#Updating-Controller-Doorbell-Registers-using-a-Shadow-Doorbell-Buffer" class="headerlink" title="Updating Controller Doorbell Registers using a Shadow Doorbell Buffer"></a>Updating Controller Doorbell Registers using a Shadow Doorbell Buffer</h2><p><img src="/images/2024/03/013.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://nvmexpress.org/wp-content/uploads/NVM_Express_Revision_1.3.pdf" target="_blank" rel="noopener">https://nvmexpress.org/wp-content/uploads/NVM_Express_Revision_1.3.pdf</a></li><li><a href="https://patchwork.kernel.org/project/qemu-devel/patch/1516003315-17878-2-git-send-email-changpeng.liu@intel.com/#21421425" target="_blank" rel="noopener">block/NVMe: introduce a new vhost NVMe host device to QEMU</a></li><li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=f9f38e33389" target="_blank" rel="noopener">nvme: improve performance for virtual NVMe devices</a></li><li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/11/Accelerating-NVMe-I_Os-in-Virtual-Machine-via-SPDK-vhost_-Solution-Ziye-Yang-_-Changpeng-Liu-Intel.pdf" target="_blank" rel="noopener">Accelerating NVMe I/Os in Virtual Machine via SPDK vhost* Solution</a></li><li><a href="https://lore.kernel.org/all/20220616123408.3306055-1-fanjinhao21s@ict.ac.cn/" target="_blank" rel="noopener">hw/nvme: Add shadow doorbell buffer support</a></li><li><a href="https://github.com/qemu/qemu/commit/3f7fe8de3d4" target="_blank" rel="noopener">hw/nvme: Implement shadow doorbell buffer support</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍下NVMe中的Shadow doorbell buffer机制。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>(译)QEMU Internals: vhost architecture</title>
    <link href="http://liujunming.github.io/2024/03/24/QEMU-Internals-vhost-architecture/"/>
    <id>http://liujunming.github.io/2024/03/24/QEMU-Internals-vhost-architecture/</id>
    <published>2024-03-24T10:27:39.000Z</published>
    <updated>2024-03-24T10:34:07.875Z</updated>
    
    <content type="html"><![CDATA[<p>原文链接: <a href="https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html" target="_blank" rel="noopener">https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html</a></p><p>This post explains how vhost provides in-kernel virtio devices for KVM. I have been hacking on vhost-scsi and have answered questions about ioeventfd, irqfd, and vhost recently, so I thought this would be a useful QEMU Internals post.<br>这篇文章介绍了 vhost 如何为 KVM 提供内核 virtio 设备。我最近一直在研究 vhost-scsi，并回答了有关 ioeventfd、irqfd 和 vhost 的问题，因此我认为这将是一篇有用的 QEMU Internals 帖子。<a id="more"></a></p><h2 id="Vhost-overview"><a href="#Vhost-overview" class="headerlink" title="Vhost overview"></a>Vhost overview</h2><p>The vhost drivers in Linux provide in-kernel virtio device emulation. Normally the QEMU userspace process emulates I/O accesses from the guest. Vhost puts virtio emulation code into the kernel, taking QEMU userspace out of the picture. This allows device emulation code to directly call into kernel subsystems instead of performing system calls from userspace.<br>Linux 中的 vhost 驱动程序提供内核 virtio 设备模拟。通常，QEMU 用户空间进程会模拟guest的 I/O 访问。Vhost 将 virtio 仿真代码放入内核，将 QEMU 用户空间排除在外。这允许设备仿真代码直接调用内核子系统，而不是从用户空间执行系统调用。</p><p>The vhost-net driver emulates the virtio-net network card in the host kernel. Vhost-net is the oldest vhost device and the only one which is available in mainline Linux. Experimental vhost-blk and vhost-scsi devices have also been developed.<br>vhost-net 驱动程序在主机内核中模拟 virtio-net 网卡。vhost-net 是最古老的 vhost 设备，也是主线 Linux 中唯一可用的设备。此外，还开发了试验性的 vhost-blk 和 vhost-scsi 设备。</p><p>In Linux 3.0 the vhost code lives in drivers/vhost/. Common code that is used by all devices is in drivers/vhost/vhost.c. This includes the virtio vring access functions which all virtio devices need in order to communicate with the guest. The vhost-net code lives in drivers/vhost/net.c.<br>在 Linux 3.0 中，vhost 代码位于 drivers/vhost/。所有设备都要使用的通用代码位于 drivers/vhost/vhost.c 中。其中包括 virtio vring 访问函数，所有 virtio 设备都需要这些函数才能与guest通信。vhost-net 代码位于 drivers/vhost/net.c 中。</p><h2 id="The-vhost-driver-model"><a href="#The-vhost-driver-model" class="headerlink" title="The vhost driver model"></a>The vhost driver model</h2><p>The vhost-net driver creates a /dev/vhost-net character device on the host. This character device serves as the interface for configuring the vhost-net instance.<br>vhost-net 驱动程序会在主机上创建一个 /dev/vhost-net 字符设备。该字符设备是配置 vhost-net 实例的接口。</p><p>When QEMU is launched with -netdev tap,vhost=on it opens /dev/vhost-net and initializes the vhost-net instance with several ioctl(2) calls. These are necessary to associate the QEMU process with the vhost-net instance, prepare for virtio feature negotiation, and pass the guest physical memory mapping to the vhost-net driver.<br>当使用 -netdev tap,vhost=on 启动 QEMU 时，它会打开 /dev/vhost-net 并通过几个 ioctl(2) 调用初始化 vhost-net 实例。这些调用对于将 QEMU 进程与 vhost-net 实例关联、准备 virtio 功能协商以及将guest物理内存映射传递给 vhost-net 驱动程序都是必要的。</p><p>During initialization the vhost driver creates a kernel thread called vhost-$pid, where $pid is the QEMU process pid. This thread is called the “vhost worker thread”. The job of the worker thread is to handle I/O events and perform the device emulation.<br>在初始化过程中，vhost 驱动程序会创建一个名为 vhost-$pid 的内核线程，其中 $pid 是 QEMU 进程的 pid。该线程被称为 “vhost 工作线程”。工作线程的任务是处理 I/O 事件和执行设备仿真。</p><p><img src="/images/2024/03/010.png" alt></p><h2 id="In-kernel-virtio-emulation"><a href="#In-kernel-virtio-emulation" class="headerlink" title="In-kernel virtio emulation"></a>In-kernel virtio emulation</h2><p>Vhost does not emulate a complete virtio PCI adapter. Instead it restricts itself to virtqueue operations only. QEMU is still used to perform virtio feature negotiation and live migration, for example. This means a vhost driver is not a self-contained virtio device implementation, it depends on userspace to handle the control plane while the data plane is done in-kernel.<br>Vhost 不会模拟完整的 virtio PCI 适配器。相反，它仅限于进行 virtqueue 操作。例如，QEMU 仍用于执行 virtio 功能协商和热迁移。这意味着 vhost 驱动程序不是独立的 virtio 设备实现，它依赖用户空间来处理控制面，而数据面则在内核中完成。</p><p>The vhost worker thread waits for virtqueue kicks and then handles buffers that have been placed on the virtqueue. In vhost-net this means taking packets from the tx virtqueue and transmitting them over the tap file descriptor.<br>vhost 工作线程会等待 virtqueue kicks，然后处理放在 virtqueue 上的缓冲区。在 vhost-net 中，这意味着从 tx virtqueue 获取数据包并通过 tap 文件描述符传输。</p><p>File descriptor polling is also done by the vhost worker thread. In vhost-net the worker thread wakes up when packets come in over the tap file descriptor and it places them into the rx virtqueue so the guest can receive them.<br>文件描述符轮询也由 vhost 工作线程完成。在 vhost-net 中，当数据包通过 tap 文件描述符进入时，工作线程就会被唤醒，并将数据包放入 rx virtqueue，这样guest就能接收到这些数据包。</p><h2 id="Vhost-as-a-userspace-interface"><a href="#Vhost-as-a-userspace-interface" class="headerlink" title="Vhost as a userspace interface"></a>Vhost as a userspace interface</h2><p>One surprising aspect of the vhost architecture is that it is not tied to KVM in any way. Vhost is a userspace interface and has no dependency on the KVM kernel module. This means other userspace code, like libpcap, could in theory use vhost devices if they find them convenient high-performance I/O interfaces.<br>vhost 架构令人惊讶的一点是，它与 KVM 没有任何关联。Vhost 是一个用户空间接口，不依赖于 KVM 内核模块。这意味着其他用户空间代码（如 libpcap）如果发现 vhost 设备是方便的高性能 I/O 接口，理论上也可以使用 vhost 设备。</p><p>When a guest kicks the host because it has placed buffers onto a virtqueue, there needs to be a way to signal the vhost worker thread that there is work to do. Since vhost does not depend on the KVM kernel module they cannot communicate directly. Instead vhost instances are set up with an eventfd file descriptor which the vhost worker thread watches for activity. The KVM kernel module has a feature known as ioeventfd for taking an eventfd and hooking it up to a particular guest I/O exit. QEMU userspace registers an ioeventfd for the VIRTIO_PCI_QUEUE_NOTIFY hardware register access which kicks the virtqueue. This is how the vhost worker thread gets notified by the KVM kernel module when the guest kicks the virtqueue.<br>当guest因为在 virtqueue 上放置了buffers而kick主机时，需要有一种方法来向 vhost 工作线程发出有工作要做的信号。由于 vhost 并不依赖于 KVM 内核模块，因此它们无法直接通信。相反，vhost 实例会设置一个 eventfd 文件描述符，由 vhost 工作线程监视其活动。KVM 内核模块有一个名为 ioeventfd 的功能，用于获取 eventfd 并将其连接到特定的guest I/O VM exit。QEMU 用户空间会为 VIRTIO_PCI_QUEUE_NOTIFY 硬件寄存器访问注册一个 ioeventfd，从而kick virtqueue。这样，当 guest kick virtqueue 时，vhost 工作线程就会收到 KVM 内核模块的通知。</p><p>On the return trip from the vhost worker thread to interrupting the guest a similar approach is used. Vhost takes a “call” file descriptor which it will write to in order to kick the guest. The KVM kernel module has a feature called irqfd which allows an eventfd to trigger guest interrupts. QEMU userspace registers an irqfd for the virtio PCI device interrupt and hands it to the vhost instance. This is how the vhost worker thread can interrupt the guest.<br>在从 vhost 工作线程返回到中断guest的过程中，也使用了类似的方法。Vhost 会获取一个 “call “文件描述符，并写入该文件描述符以通知guest。KVM 内核模块有一个名为 irqfd 的功能，允许 eventfd 触发guest中断。QEMU 用户空间为 virtio PCI 设备中断注册了一个 irqfd，并将其交给 vhost 实例。这就是 vhost 工作线程中断guest的方式。</p><p>In the end the vhost instance only knows about the guest memory mapping, a kick eventfd, and a call eventfd.<br>最终，vhost 实例只知道 guest 内存映射、ioeventfd 和 irqfd。</p><h2 id="Where-to-find-out-more"><a href="#Where-to-find-out-more" class="headerlink" title="Where to find out more"></a>Where to find out more</h2><p>Here are the main points to begin exploring the code:<br>以下是开始探索代码的要点：</p><ul><li>drivers/vhost/vhost.c - common vhost driver code</li><li>drivers/vhost/net.c - vhost-net driver</li><li>virt/kvm/eventfd.c - ioeventfd and irqfd</li></ul><p>The QEMU userspace code shows how to initialize the vhost instance:<br>QEMU 用户空间代码显示了如何初始化 vhost 实例:</p><ul><li>hw/vhost.c - common vhost initialization code</li><li>hw/vhost_net.c - vhost-net initialization</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文链接: &lt;a href=&quot;https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This post explains how vhost provides in-kernel virtio devices for KVM. I have been hacking on vhost-scsi and have answered questions about ioeventfd, irqfd, and vhost recently, so I thought this would be a useful QEMU Internals post.&lt;br&gt;这篇文章介绍了 vhost 如何为 KVM 提供内核 virtio 设备。我最近一直在研究 vhost-scsi，并回答了有关 ioeventfd、irqfd 和 vhost 的问题，因此我认为这将是一篇有用的 QEMU Internals 帖子。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about vhost-pci</title>
    <link href="http://liujunming.github.io/2024/03/09/Notes-about-vhost-pci/"/>
    <id>http://liujunming.github.io/2024/03/09/Notes-about-vhost-pci/</id>
    <published>2024-03-09T12:27:55.000Z</published>
    <updated>2024-03-09T12:40:38.797Z</updated>
    
    <content type="html"><![CDATA[<p>vhost-pci的详细介绍可以参考<a href="https://www.linux-kvm.org/images/5/55/02x07A-Wei_Wang-Design_of-Vhost-pci.pdf" target="_blank" rel="noopener">Design of Vhost-pci slides</a>与<a href="https://www.youtube.com/watch?v=xITj0qsaSJQ" target="_blank" rel="noopener">video</a>，本文主要mark下相关notes。<a id="more"></a></p><h2 id="Usage-and-Motivation"><a href="#Usage-and-Motivation" class="headerlink" title="Usage and Motivation"></a>Usage and Motivation</h2><p><img src="/images/2024/03/007.jpg" alt></p><p><img src="/images/2024/03/008.jpg" alt></p><p><img src="/images/2024/03/009.jpg" alt></p><p>如上图所示，通过network packet的数据流向对比，可以显示vhost-pci的作用:<strong>high performance inter-VM communication schemes</strong>，vhost-pci机制可以让network packet直接从一个vm传输到另外一个vm中，无需经过vSwitch的中转。</p><h2 id="Vhost-pci-Design-Details"><a href="#Vhost-pci-Design-Details" class="headerlink" title="Vhost-pci Design Details"></a>Vhost-pci Design Details</h2><p><img src="/images/2024/03/001.jpg" alt></p><p><img src="/images/2024/03/005.jpg" alt></p><p><img src="/images/2024/03/006.jpg" alt></p><h2 id="vhost-pci-vs-virtio-vhost-user"><a href="#vhost-pci-vs-virtio-vhost-user" class="headerlink" title="vhost-pci vs virtio-vhost-user"></a>vhost-pci vs virtio-vhost-user</h2><p><img src="/images/2024/03/002.jpg" alt><br>详细内容可以参考<a href="https://lore.kernel.org/qemu-devel/20180110161438.GA28096@stefanha-x1.localdomain/" target="_blank" rel="noopener">vhost-pci and virtio-vhost-user</a>。</p><hr><p>参考资料:</p><ol><li><a href="https://www.linux-kvm.org/images/5/55/02x07A-Wei_Wang-Design_of-Vhost-pci.pdf" target="_blank" rel="noopener">Design of Vhost-pci slides</a></li><li><a href="https://www.youtube.com/watch?v=xITj0qsaSJQ" target="_blank" rel="noopener">Design of Vhost-pci video</a></li><li><a href="https://archive.fosdem.org/2018/schedule/event/virtio/attachments/slides/2167/export/events/attachments/virtio/slides/2167/fosdem_virtio1_1.pdf" target="_blank" rel="noopener">What’s new in Virtio 1.1?</a></li><li><a href="https://static.sched.com/hosted_files/dpdkuserspace22/93/DPDK22_virtualization_of_DPDK_applications_using_virtio_vhost_user.pdf" target="_blank" rel="noopener">Virtualization of DPDK applications using virtio-vhost-user</a></li><li><a href="https://lore.kernel.org/qemu-devel/1494578148-102868-1-git-send-email-wei.w.wang@intel.com/" target="_blank" rel="noopener">Vhost-pci for inter-VM communication</a></li><li><a href="https://lore.kernel.org/qemu-devel/20180110161438.GA28096@stefanha-x1.localdomain/" target="_blank" rel="noopener">vhost-pci and virtio-vhost-user</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;vhost-pci的详细介绍可以参考&lt;a href=&quot;https://www.linux-kvm.org/images/5/55/02x07A-Wei_Wang-Design_of-Vhost-pci.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Design of Vhost-pci slides&lt;/a&gt;与&lt;a href=&quot;https://www.youtube.com/watch?v=xITj0qsaSJQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;video&lt;/a&gt;，本文主要mark下相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about VirtioVhostUser</title>
    <link href="http://liujunming.github.io/2024/03/03/Notes-about-VirtioVhostUser/"/>
    <id>http://liujunming.github.io/2024/03/03/Notes-about-VirtioVhostUser/</id>
    <published>2024-03-03T12:11:04.000Z</published>
    <updated>2024-03-09T10:07:30.162Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA</a><a id="more"></a></p><h2 id="1-What"><a href="#1-What" class="headerlink" title="1. What"></a>1. What</h2><p>The virtio-vhost-user device lets guests act as vhost device backends so that virtual network switches and storage appliance VMs can provide virtio devices to other guests.</p><p>virtio-vhost-user 设备可让客户机充当 vhost 设备后端，这样虚拟网络交换机和存储设备虚拟机就能为其他客户机提供 virtio 设备。</p><p>virtio-vhost-user was inspired by vhost-pci by Wei Wang and Zhiyong Yang.</p><p>virtio-vhost-user 的灵感来源于Wei Wang和Zhiyong Yang的 vhost-pci。</p><h2 id="2-Use-cases"><a href="#2-Use-cases" class="headerlink" title="2. Use cases"></a>2. Use cases</h2><h3 id="2-1-Appliances-for-cloud-environments"><a href="#2-1-Appliances-for-cloud-environments" class="headerlink" title="2.1 Appliances for cloud environments"></a>2.1 Appliances for cloud environments</h3><p><strong>2.1 用于云环境的设备</strong></p><p>In cloud environments everything is a guest. It is not possible for users to run vhost-user processes on the host. This precludes high-performance vhost-user appliances from running in cloud environments.</p><p>在云环境中，一切都是客户机。用户不可能在主机上运行 vhost-user进程。这使得高性能 vhost-user 设备无法在云环境中运行。</p><p>virtio-vhost-user allows vhost-user appliances to be shipped as virtual machine images. They can provide I/O services directly to other guests instead of going through an extra layer of device emulation like a host network switch:</p><p>virtio-vhost-user 允许 vhost-user 设备作为虚拟机镜像发布。它们可以直接向其他客户机提供 I/O 服务，而无需通过额外的设备仿真层（如主机网络交换机）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    Traditional Appliance VMs       virtio-vhost-user Appliance VMs</span><br><span class="line">+-------------+   +-------------+  +-------------+   +-------------+</span><br><span class="line">|     VM1     |   |     VM2     |  |     VM1     |   |     VM2     |</span><br><span class="line">|  Appliance  |   |   Consumer  |  |  Appliance  |   |   Consumer  |</span><br><span class="line">|      ^      |   |      ^      |  |      &lt;------+---+------&gt;      |</span><br><span class="line">+------|------+---+------|------+  +-------------+---+-------------+</span><br><span class="line">|      +-----------------+      |  |                               |</span><br><span class="line">|             Host              |  |             Host              |</span><br><span class="line">+-------------------------------+  +-------------------------------+</span><br></pre></td></tr></table></figure></p><h3 id="2-2-Exitless-VM-to-VM-communication"><a href="#2-2-Exitless-VM-to-VM-communication" class="headerlink" title="2.2 Exitless VM-to-VM communication"></a>2.2 Exitless VM-to-VM communication</h3><p>Once the vhost-user session has been established all vring activity can be performed by poll mode drivers in shared memory. This eliminates vmexits in the data path so that the highest possible VM-to-VM communication performance can be achieved.</p><p>一旦 vhost-user 会话建立，所有 vring 活动都可由共享内存中的轮询模式驱动程序执行。这样就消除了数据路径中的 vmexits，从而实现尽可能高的VM-to-VM 通信性能。</p><p>Even when interrupts are necessary, virtio-vhost-user can use lightweight vmexits thanks to ioeventfd instead of exiting to host userspace. This ensures that VM-to-VM communication bypasses device emulation in QEMU.</p><p>即使需要中断(笔者注: virtio前端驱动kick需要发生VM Exit)，virtio-vhost-user 也可以通过 ioeventfd 使用轻量级 vmexits，而不是退出到主机用户空间。这可确保VM-to-VM 通信绕过 QEMU 中的设备仿真。</p><h2 id="3-How-it-works"><a href="#3-How-it-works" class="headerlink" title="3. How it works"></a>3. How it works</h2><p>Virtio devices were originally emulated inside the QEMU host userspace process. Later on, vhost allowed a subset of a virtio device, called the vhost device backend, to be implement inside the host kernel. vhost-user then allowed vhost device backends to reside in host userspace processes instead.</p><p>Virtio 设备最初是在 QEMU 主机用户空间进程内仿真的。后来，vhost 允许一部分virtio 设备（称为 vhost 设备后端）在主机内核中实现。vhost-user 允许 vhost 设备后端驻留在主机用户空间进程中。</p><p>virtio-vhost-user takes this one step further by moving the vhost device backend into a guest. It works by tunneling the vhost-user protocol over a new virtio device type called virtio-vhost-user.</p><p>virtio-vhost-user 在此基础上更进一步，将 vhost 设备后端移至客户机中。它的工作原理是在名为 virtio-vhost-user 的新 virtio 设备类型上传输 vhost-user 协议。</p><p>The following diagram shows how two guests communicate:</p><p>下图显示了两个客户机的通信方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+-------------+                     +-------------+</span><br><span class="line">|     VM1     |                     |     VM2     |</span><br><span class="line">|             |                     |             |</span><br><span class="line">|    vhost    |    shared memory    |             |</span><br><span class="line">|   device    | +-----------------&gt; |             |</span><br><span class="line">|   backend   |                     |             |</span><br><span class="line">|             |                     | virtio-net  |</span><br><span class="line">+-------------+                     +-------------+</span><br><span class="line">|             |                     |             |</span><br><span class="line">|  virtio-    |  vhost-user socket  |             |</span><br><span class="line">| vhost-user  | &lt;-----------------&gt; | vhost-user  |</span><br><span class="line">|    QEMU     |                     |    QEMU     |</span><br><span class="line">+-------------+                     +-------------+</span><br></pre></td></tr></table></figure></p><p>VM2 sees a regular virtio-net device. VM2’s QEMU uses the existing vhost-user feature as if it were talking to a host userspace vhost-user backend.</p><p>VM2 看到的是普通的 virtio-net 设备。VM2 的 QEMU 使用现有的 vhost-user 功能，就像与主机用户空间 vhost-user 后端对话一样。</p><p>VM1’s QEMU tunnels the vhost-user protocol messages from VM1’s QEMU to the new virtio-vhost-user device so that guest software in VM1 can act as the vhost-user backend.</p><p>VM1 的 QEMU 将 vhost-user 协议信息从 VM1 的 QEMU 隧道(笔者注:可以类比于网络中的隧道技术，a method for transporting data across a network using protocols that are not supported by that network)传输到新的 virtio-vhost-user 设备，这样 VM1 中的客户机软件就可以充当 vhost-user 后端。</p><p>It is possible to reuse existing vhost-user backend software with virtio-vhost-user since they use the same vhost-user protocol messages. A driver is required for the virtio-vhost-user PCI device that carries the message instead of the usual vhost-user UNIX domain socket. The driver can be implemented in a guest userspace process using Linux vfio-pci but guest kernel driver implementation would also be also possible.</p><p>由于 virtio-vhost-user 使用相同的 vhost-user 协议信息，因此可以重新使用现有的 vhost-user 后端软件。virtio-vhost-user PCI 设备需要一个驱动程序来传输信息，而不是通常的 vhost-user UNIX 域套接字。该驱动程序可在客户机用户空间进程中使用 Linux vfio-pci 实现，也可在客户机内核驱动程序中实现。</p><p>The vhost device backend vrings are accessed through shared memory and do not require vhost-user message exchanges in the data path. No vmexits are taken when poll mode drivers are used. Even when interrupts are used, QEMU is not involved in the data path because ioeventfd lightweight vmexits are taken.</p><p>vhost 设备后端 vrings 通过共享内存访问，不需要在数据路径中进行 vhost 用户信息交换。使用轮询模式驱动程序时，不会出现 vmexits。即使使用中断(笔者注: virtio前端驱动kick需要发生VM Exit)，QEMU 也不会参与数据路径，因为会使用 ioeventfd 轻量级 vmexits。</p><p>All vhost device types work with virtio-vhost-user, including net, scsi, and blk.</p><p>所有 vhost 设备类型都能与 virtio-vhost-user 一起使用，包括 net、scsi 和 blk。</p><h2 id="4-DPDK使用案例"><a href="#4-DPDK使用案例" class="headerlink" title="4. DPDK使用案例"></a>4. DPDK使用案例</h2><p>下面截取了DPDK中VirtioVhostUser的使用案例:</p><p><img src="/images/2024/03/003.png" alt></p><p><img src="/images/2024/03/004.png" alt></p><p>slides中的Memory region I/O in device，笔者的理解就是VVU(VirtioVhostUser) device的MMIO寄存器。</p><hr><p>参考资料:</p><ol><li><a href="https://wiki.qemu.org/Features/VirtioVhostUser" target="_blank" rel="noopener">https://wiki.qemu.org/Features/VirtioVhostUser</a></li><li><a href="https://archive.fosdem.org/2018/schedule/event/virtio/attachments/slides/2167/export/events/attachments/virtio/slides/2167/fosdem_virtio1_1.pdf" target="_blank" rel="noopener">https://archive.fosdem.org/2018/schedule/event/virtio/attachments/slides/2167/export/events/attachments/virtio/slides/2167/fosdem_virtio1_1.pdf</a></li><li><a href="https://static.sched.com/hosted_files/dpdkuserspace22/93/DPDK22_virtualization_of_DPDK_applications_using_virtio_vhost_user.pdf" target="_blank" rel="noopener">https://static.sched.com/hosted_files/dpdkuserspace22/93/DPDK22_virtualization_of_DPDK_applications_using_virtio_vhost_user.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA&lt;/a&gt;
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Share Virtual Address</title>
    <link href="http://liujunming.github.io/2024/02/25/Notes-about-Share-Virtual-Address/"/>
    <id>http://liujunming.github.io/2024/02/25/Notes-about-Share-Virtual-Address/</id>
    <published>2024-02-25T12:49:48.000Z</published>
    <updated>2024-02-25T13:15:05.974Z</updated>
    
    <content type="html"><![CDATA[<p>之前记录过<a href="/2022/03/30/Introduction-to-Shared-Virtual-Memory/">Introduction to Shared Virtual Memory</a>，但主要偏向于Intel的SVA方案。本文主要是mark下多个架构下的SVA方案。<a id="more"></a></p><p><img src="/images/2024/02/001.jpg" alt></p><p><img src="/images/2024/02/002.jpg" alt></p><p>Shared Virtual Addressing (SVA) is the ability to share process address spaces with devices. It is called “SVM” (Shared Virtual Memory) by OpenCL and some IOMMU architectures, but since that abbreviation is already used for AMD virtualisation in Linux (Secure Virtual Machine), we prefer the less ambiguous “SVA”.<br><a href="https://lwn.net/Articles/747230/" target="_blank" rel="noopener">Shared Virtual Addressing for the IOMMU</a></p><p><img src="/images/2024/02/003.jpg" alt></p><p><img src="/images/2024/02/004.jpg" alt></p><p><img src="/images/2024/02/005.jpg" alt></p><p>cc:Cache Coherent</p><p><img src="/images/2024/02/006.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://soft.cs.tsinghua.edu.cn/os2atc2018/ppt/osd5.pdf" target="_blank" rel="noopener">SVA：基于异构系统的内存管理技术</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Shared-Virtual-Addressing_Yisheng-Xie-_-Bob-Liu.pdf" target="_blank" rel="noopener">Share Virtual Address</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前记录过&lt;a href=&quot;/2022/03/30/Introduction-to-Shared-Virtual-Memory/&quot;&gt;Introduction to Shared Virtual Memory&lt;/a&gt;，但主要偏向于Intel的SVA方案。本文主要是mark下多个架构下的SVA方案。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>深入理解VMDq(Virtual Machine Device Queue)</title>
    <link href="http://liujunming.github.io/2024/01/27/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3VMDq-Virtual-Machine-Device-Queue/"/>
    <id>http://liujunming.github.io/2024/01/27/深入理解VMDq-Virtual-Machine-Device-Queue/</id>
    <published>2024-01-27T12:31:01.000Z</published>
    <updated>2024-01-28T09:06:50.969Z</updated>
    
    <content type="html"><![CDATA[<p>本文将深入探究VMDq(Virtual Machine Device Queue)相关内容。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>强烈建议观看视频<a href="https://www.youtube.com/watch?v=QvKXbpV6WXk" target="_blank" rel="noopener">Intel VMDq Explanation</a>，非常清晰地介绍了VMDq，下面主要是视频中的概要总结，将以Without VMDq与With VMDq来做对比。</p><h3 id="Without-VMDq"><a href="#Without-VMDq" class="headerlink" title="Without VMDq"></a>Without VMDq</h3><p>A single core(you have one core that’s actually in charge of handling every packet before it determines which other core interrupt for the action, copy the data to the target vm) cannot keep up with 10 Gbps of data. Most packets coming in require two interrupts, one for the core assigned to handle Ethernet interrupts, followed by an interrupt of the core processing the VM where the packet is targetd for.</p><h3 id="With-VMDq"><a href="#With-VMDq" class="headerlink" title="With VMDq"></a>With VMDq</h3><p>Receive Path</p><ul><li>Data packets for different VMs get sorted at the Ethernet silicon based on MAC address/VLAN tags</li><li>Sorted data packets get parsed to the respective VMs</li><li>Data packets being received by respective VMs</li></ul><p>Reduces overhead and increases throughput by sorting packets with the Intel Ethernet Controller and spreading the workload amongst multiple CPU cores.</p><h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><p>这节主要是mark下<a href="https://www.intel.sg/content/dam/www/public/us/en/documents/white-papers/vmdq-technology-paper.pdf" target="_blank" rel="noopener">Intel® VMDq Technology white paper</a>中的关键notes。</p><p><img src="/images/2024/01/009.jpg" alt></p><p><img src="/images/2024/01/010.jpg" alt></p><h2 id="VMDq-vs-SR-IOV"><a href="#VMDq-vs-SR-IOV" class="headerlink" title="VMDq vs SR-IOV"></a>VMDq vs SR-IOV</h2><ul><li><p>VMDq<br>VMM在服务器的物理网卡中为每个虚机分配一个独立的队列，这样虚机出来的流量可以直接经过软件交换机发送到指定队列上，软件交换机无需进行排序和路由操作。<br>但是，VMM和虚拟交换机仍然需要将网络流量在VMDq和虚机之间进行复制。</p></li><li><p>SR-IOV<br>对于SR-IOV来说，则更加彻底，它通过创建不同虚拟功能（VF）的方式，呈现给虚拟机的就是独立的网卡，因此，虚拟机直接跟网卡通信，不需要经过软件交换机，VF和VM之间通过DMA进行高速数据传输，SR-IOV的性能是最好的。</p></li></ul><p><img src="/images/2024/01/011.jpeg" alt></p><p>Unlike SR-IOV, which exposes a complete device interface to the virtual machine guest, VMDq only provides network queues to the virtual machine guest.</p><p>VMDq只是一个过渡性的技术，当前已经被SR-IOV所替代。</p><hr><p>参考资料:</p><ol><li><a href="https://www.youtube.com/watch?v=QvKXbpV6WXk" target="_blank" rel="noopener">Intel VMDq Explanation</a></li><li><a href="https://www.intel.sg/content/dam/www/public/us/en/documents/white-papers/vmdq-technology-paper.pdf" target="_blank" rel="noopener">Intel® VMDq Technology white paper</a></li><li><a href="https://blog.csdn.net/yeasy/article/details/39178335" target="_blank" rel="noopener">网卡虚拟化技术：VMDq和SR-IOV</a></li><li>CompSC: live migration with pass-through devices</li><li><a href="https://forum.huawei.com/enterprise/en/what-is-the-difference-between-sr-iov-and-vmdq/thread/667229636603559936-667213860102352896" target="_blank" rel="noopener">What is the difference between SR-IOV and VMDQ?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将深入探究VMDq(Virtual Machine Device Queue)相关内容。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Live Migration with SR-IOV Pass-through on KVM Forum 2015</title>
    <link href="http://liujunming.github.io/2024/01/27/Live-Migration-with-SR-IOV-Pass-through-on-KVM-Forum-2015/"/>
    <id>http://liujunming.github.io/2024/01/27/Live-Migration-with-SR-IOV-Pass-through-on-KVM-Forum-2015/</id>
    <published>2024-01-27T12:00:36.000Z</published>
    <updated>2024-01-27T12:22:05.235Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about Live migration with SR-IOV pass-through by Weidong Han on KVM Forum 2015.<a id="more"></a></p><ul><li><a href="https://www.linux-kvm.org/images/9/9a/03x07-Juniper-Weidong_Han-LiveMigrationWithSR-IOVPass-through.pdf" target="_blank" rel="noopener">slides</a></li><li><a href="https://www.youtube.com/watch?v=vnwEnzVp9Zo" target="_blank" rel="noopener">video</a></li></ul><p><img src="/images/2024/01/005.jpg" alt></p><p>iproute2 is a collection of userspace utilities for controlling and monitoring various aspects of networking in the Linux kernel, including routing, network interfaces, tunnels, traffic control, and network-related device drivers.</p><p><img src="/images/2024/01/008.jpg" alt></p><p><img src="/images/2024/01/006.jpg" alt></p><p><img src="/images/2024/01/007.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.wikiwand.com/en/Iproute2" target="_blank" rel="noopener">https://www.wikiwand.com/en/Iproute2</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about Live migration with SR-IOV pass-through by Weidong Han on KVM Forum 2015.
    
    </summary>
    
      <category term="live migration" scheme="http://liujunming.github.io/categories/live-migration/"/>
    
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
  </entry>
  
  <entry>
    <title>notes:Live migration with pass-through device for Linux VM</title>
    <link href="http://liujunming.github.io/2024/01/21/notes-Live-migration-with-pass-through-device-for-Linux-VM/"/>
    <id>http://liujunming.github.io/2024/01/21/notes-Live-migration-with-pass-through-device-for-Linux-VM/</id>
    <published>2024-01-21T04:29:09.000Z</published>
    <updated>2024-01-21T04:40:21.839Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/liujunming/paper_reading_notes/issues/54" target="_blank" rel="noopener">Live migration with pass-through device for Linux VM</a><a id="more"></a></p><p><img src="/images/2024/01/004.jpg" alt></p><p>通过Linux bounding机制实现，将直通网络设备和PV设备通过bounding机制绑定为一张网卡，做热迁移时切换到PV设备。</p><p><strong>Bonded Interface</strong>用于将多个网络接口聚合成一个逻辑上的”bonded”接口。可用于故障备份或负载均衡等场景。</p><p><img src="/images/2024/01/003.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://calinyara.github.io/technology/2019/08/22/vnet_interface.html" target="_blank" rel="noopener">虚拟网络设备简介</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/liujunming/paper_reading_notes/issues/54&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Live migration with pass-through device for Linux VM&lt;/a&gt;
    
    </summary>
    
      <category term="live migration" scheme="http://liujunming.github.io/categories/live-migration/"/>
    
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
  </entry>
  
</feed>
