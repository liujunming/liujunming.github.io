<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2025-11-02T11:31:56.396Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes about virtio-9p</title>
    <link href="http://liujunming.github.io/2025/11/02/Notes-about-virtio-9p/"/>
    <id>http://liujunming.github.io/2025/11/02/Notes-about-virtio-9p/</id>
    <published>2025-11-02T09:23:44.000Z</published>
    <updated>2025-11-02T11:31:56.396Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio-9p的相关notes。<a id="more"></a></p><h2 id="什么是9P？"><a href="#什么是9P？" class="headerlink" title="什么是9P？"></a>什么是9P？</h2><p>9P(Plan 9 File System Protocol)是一种网络文件系统协议。它最初是为贝尔实验室的Plan 9 from Bell Labs分布式操作系统而设计的。</p><p>其核心思想是：<strong>“将所有资源都表示为文件”</strong>。在 Plan 9 中，不仅是磁盘上的数据，包括进程、设备、网络接口、甚至窗口系统接口，都被统一抽象为文件系统中的文件。9P 就是用来访问这些”文件”的协议。</p><h2 id="9P核心特性与设计哲学"><a href="#9P核心特性与设计哲学" class="headerlink" title="9P核心特性与设计哲学"></a>9P核心特性与设计哲学</h2><h3 id="简单与精简"><a href="#简单与精简" class="headerlink" title="简单与精简"></a>简单与精简</h3><ul><li>9P 的消息类型非常少（大约 10 种左右，如 <code>Tversion</code>, <code>Tauth</code>, <code>Tattach</code>, <code>Tclunk</code>, <code>Twalk</code>, <code>Topen</code>, <code>Tread</code>, <code>Twrite</code>, <code>Tstat</code>, <code>Twstat</code>），协议本身简洁明了。</li><li>这种简单性使得其实现轻量，开销小。</li></ul><h3 id="分层与状态性"><a href="#分层与状态性" class="headerlink" title="分层与状态性"></a>分层与状态性</h3><ul><li>9P 是一个有状态的协议。客户端首先通过 <code>Tattach</code> 消息与服务器建立一个”会话”，获取一个根文件句柄（<code>fid</code>）。</li><li>随后，客户端通过 <code>Twalk</code> 消息在这个根句柄的基础上”遍历”路径，逐步获取目标文件的句柄。</li><li>这种基于 <code>fid</code> 的交互模式，非常类似于在本地文件系统中使用文件描述符。</li></ul><h3 id="传输无关性"><a href="#传输无关性" class="headerlink" title="传输无关性"></a>传输无关性</h3><ul><li>9P 协议定义的是消息格式和语义，并不关心底层传输介质。</li><li>它可以运行在多种传输层之上，最常见的是 TCP/IP，也可以是 Unix Domain Socket，甚至是 serial port connections。</li></ul><p><strong>在虚拟化场景下，virtio-9p就是这个传输通道！</strong>而virtio-9p就是本文要介绍的内容。 </p><h2 id="virtio-9p-Architecture"><a href="#virtio-9p-Architecture" class="headerlink" title="virtio-9p Architecture"></a>virtio-9p Architecture</h2><p><img src="/images/2025/11/007.png" alt></p><p>The following figure shows the basic structure of the 9pfs implementation in QEMU.</p><p><img src="/images/2025/11/006.png" alt></p><p>The 9P transport driver is the communication channel between host system and guest system. </p><p>virtio transport driver: The 9p virtio transport driver uses e.g. a virtual PCI device and ontop the virtio protocol to transfer the 9p messages between clients (guest systems) and 9p server (host system). <a href="https://gitlab.com/qemu-project/qemu/-/blob/master/hw/9pfs/virtio-9p-device.c" target="_blank" rel="noopener">virtio-9p-device.c</a></p><hr><p>参考资料:</p><ol><li><a href="https://www.kernel.org/doc/ols/2010/ols2010-pages-109-120.pdf" target="_blank" rel="noopener">VirtFS—A virtualization aware File System pass-through</a></li><li><a href="https://wiki.qemu.org/Documentation/9p" target="_blank" rel="noopener">Documentation/9p</a></li><li>deepseek prompt:简要介绍下9P (protocol)</li><li><a href="https://www.wikiwand.com/en/articles/9P_(protocol)" target="_blank" rel="noopener">https://www.wikiwand.com/en/articles/9P_(protocol)</a></li><li><a href="https://gitlab.com/qemu-project/qemu/-/blob/master/hw/9pfs/virtio-9p-device.c" target="_blank" rel="noopener">virtio-9p-device.c</a></li><li><a href="https://www.kernel.org/doc/Documentation/filesystems/9p.txt" target="_blank" rel="noopener">v9fs: Plan 9 Resource Sharing for Linux</a></li><li><a href="https://github.com/torvalds/linux/tree/master/fs/9p" target="_blank" rel="noopener">linux/fs/9p</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio-9p的相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes about gVisor</title>
    <link href="http://liujunming.github.io/2025/11/02/Notes-about-gVisor/"/>
    <id>http://liujunming.github.io/2025/11/02/Notes-about-gVisor/</id>
    <published>2025-11-02T05:41:03.000Z</published>
    <updated>2025-11-02T07:35:40.715Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下gVisor的相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>gVisor provides a strong layer of isolation between running applications and the host operating system. It is an application kernel that implements a Linux-like interface. Unlike Linux, it is written in a memory-safe language (Go) and runs in userspace.</p><h2 id="How-is-this-different"><a href="#How-is-this-different" class="headerlink" title="How is this different?"></a>How is this different?</h2><h3 id="Machine-level-virtualization"><a href="#Machine-level-virtualization" class="headerlink" title="Machine-level virtualization"></a>Machine-level virtualization</h3><p>such as KVM and Xen, exposes virtualized hardware to a guest kernel via a Virtual Machine Monitor (VMM). This virtualized hardware is generally enlightened (paravirtualized) and additional mechanisms can be used to improve the visibility between the guest and host (e.g. balloon drivers, paravirtualized spinlocks). Running containers in distinct virtual machines can provide great isolation, compatibility and performance, but for containers it often requires additional proxies and agents, and may require a larger resource footprint and slower start-up times.</p><p><img src="/images/2025/11/003.png" alt></p><h3 id="Rule-based-execution"><a href="#Rule-based-execution" class="headerlink" title="Rule-based execution"></a>Rule-based execution</h3><p>such as <a href="https://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt" target="_blank" rel="noopener">seccomp</a>, <a href="https://github.com/selinuxproject" target="_blank" rel="noopener">SELinux</a> and <a href="https://wiki.ubuntu.com/AppArmor" target="_blank" rel="noopener">AppArmor</a>, allows the specification of a fine-grained security policy for an application or container. These schemes typically rely on hooks implemented inside the host kernel to enforce the rules. If the surface can be made small enough, then this is an excellent way to sandbox applications and maintain native performance. However, in practice it can be extremely difficult (if not impossible) to reliably define a policy for arbitrary, previously unknown applications, making this approach challenging to apply universally.</p><p><img src="/images/2025/11/004.png" alt></p><h3 id="gVisor"><a href="#gVisor" class="headerlink" title="gVisor"></a>gVisor</h3><p>provides a third isolation mechanism, distinct from those above.</p><p>gVisor intercepts application system calls and acts as the guest kernel, without the need for translation through virtualized hardware. gVisor may be thought of as either a merged guest kernel and VMM, or as seccomp on steroids(强化版). This architecture allows it to provide a flexible resource footprint (i.e. one based on threads and memory mappings, not fixed guest physical resources) while also lowering the fixed costs of virtualization. However, this comes at the price of reduced application compatibility and higher per-system call overhead.</p><p><img src="/images/2025/11/005.png" alt></p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>Google gVisor is a sandboxed container runtime that uses para-virtualization to isolate containerized applications from the host system without the heavy-weight resource allocation that comes with full virtual machines. It implements a user space kernel, <code>Sentry</code>, that is written in the Go Language and runs in a restricted <a href="/2025/04/05/Notes-about-Seccomp-filter/">seccomp</a> container. </p><p><img src="/images/2025/11/002.png" alt></p><p>Figure 2 shows gVisor’s architecture. All syscalls made by the application are redirected into the Sentry, which implements most system call functionality itself for the 237 syscalls it supports. Sentry makes calls to 53 host syscalls to support its operations. This prevents the application from having any direct interaction with the host through syscalls. gVisor supports two methods of redirecting syscalls: <code>ptrace-mode</code> uses ptrace in the Linux kernel to forward syscalls to the sentry and <code>KVM-mode</code> uses KVM to trap syscalls before they hit the Linux kernel so they can be forwarded to the sentry. KVM-mode performs better than ptrace for many workloads and has several benefits over the ptrace platform according to the gVisor documentation.</p><p>gVisor starts a Gofer process with each container that provides the Sentry with access to file system resources. Thus, a compromised Sentry cannot directly read or write any files. A writable tmpfs can be overlaid on the entire file system to provide complete isolation from the host file system. To enable sharing between the running containers and with the host, a shared file access mode may be used.</p><p>gVisor has its own user-space networking stack written in Go called <code>netstack</code>. The Sentry uses netstack to handle almost all networking, including TCP connection state, control messages, and packet assembly, rather than relying on kernel code that shares much more state across containers. gVisor also provides an option to use host networking for higher performance</p><hr><p>参考资料:</p><ol><li><a href="https://gvisor.dev/docs/" target="_blank" rel="noopener">What is gVisor?</a></li><li><a href="https://pages.cs.wisc.edu/~swift/papers/vee20-isolation.pdf" target="_blank" rel="noopener">Blending Containers and Virtual Machines:A Study of Firecracker and gVisor</a></li><li><a href="https://ipads.se.sjtu.edu.cn/_media/pub/members/paper.pdf" target="_blank" rel="noopener">A Hardware-Software Co-Design for Efficient Secure Containers</a></li><li><a href="https://mp.weixin.qq.com/s/b8KFCv4jwdzcq4T44566HA" target="_blank" rel="noopener">Google 发布 gVisor – 容器沙箱运行时</a></li><li><a href="https://www.usenix.org/system/files/hotcloud19-paper-young.pdf" target="_blank" rel="noopener">The True Cost of Containing: A gVisor Case Study</a></li><li><a href="https://gvisor.dev/blog/2023/04/28/systrap-release/" target="_blank" rel="noopener">Releasing Systrap - A high-performance gVisor platform</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下gVisor的相关notes。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Skip List</title>
    <link href="http://liujunming.github.io/2025/11/01/Notes-about-Skip-List/"/>
    <id>http://liujunming.github.io/2025/11/01/Notes-about-Skip-List/</id>
    <published>2025-11-01T14:40:28.000Z</published>
    <updated>2025-11-01T14:56:43.624Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Skip List的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>可以看下视频<a href="https://www.youtube.com/watch?v=ol-FaNLXlR0" target="_blank" rel="noopener">Skip List Explained</a>。</p><h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>跳跃表的核心思想非常直观：<strong>为有序的链表增加多级“索引”</strong>，来加速查找过程。它是对“二分查找”思想的一种链表实现。</p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>对于一个有序链表，查找一个元素只能从头到尾遍历，时间复杂度是 O(n)，效率很低。我们希望能像数组的二分查找一样，实现 O(log n) 的查找效率，但链表不支持随机访问。</p><p>跳跃表通过“空间换时间”的策略，完美地解决了这个问题。</p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>跳跃表由多层链表组成:</p><ol><li>最底层（第0层）：是一个包含所有元素的有序单链表</li><li>第1层：是底层链表的一个“索引”子集，它只包含底层中的部分节点，节点密度更低</li><li>第2层：是第1层索引的索引，节点更稀疏</li><li>以此类推… 直到最顶层</li></ol><p>每一层都是一个有序的链表，高层链表是低层链表的“快速通道”。</p><p><img src="/images/2025/11/001.png" alt></p><hr><p>参考资料:</p><ol><li>deepseek prompt:简要介绍下跳跃表这个数据结构</li><li><a href="https://www.youtube.com/watch?v=ol-FaNLXlR0" target="_blank" rel="noopener">Skip List Explained</a></li><li><a href="https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/skiplists.pdf" target="_blank" rel="noopener">Skip Lists CMSC 420</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Skip List的相关notes。
    
    </summary>
    
      <category term="数据结构" scheme="http://liujunming.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据结构" scheme="http://liujunming.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about CRIU(Checkpoint/Restore In Userspace)</title>
    <link href="http://liujunming.github.io/2025/10/01/Notes-about-CRIU-Checkpoint-Restore-In-Userspace/"/>
    <id>http://liujunming.github.io/2025/10/01/Notes-about-CRIU-Checkpoint-Restore-In-Userspace/</id>
    <published>2025-10-01T01:19:54.000Z</published>
    <updated>2025-10-01T13:36:03.255Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下CRIU(Checkpoint/Restore In Userspace)的相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>CRIU(Checkpoint/Restore In Userspace) is a Linux software. It can freeze a running container (or an individual application) and checkpoint its state to disk. The data saved can be used to restore the application and run it exactly as it was during the time of the freeze. Using this functionality, application or container live migration, snapshots, remote debugging, and <a href="https://criu.org/Usage_scenarios" target="_blank" rel="noopener">many other things</a> are now possible.</p><p><img src="/images/2025/10/001.png" alt></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CRIU is a software framework for transparent checkpointing and restoring of Linux processes. It enables live migration, snapshots, or accelerated start-up of processes and containers. To extract the user-space application state, CRIU uses conventional debugging mechanisms, like <a href="http://man7.org/linux/man-pages/man2/ptrace.2.html" target="_blank" rel="noopener">ptrace</a>. However, to extract the state of process-specific kernel objects, CRIU depends on special Linux kernel interfaces.</p><p><img src="/images/2025/10/003.png" alt></p><p>During recovery, CRIU runs inside the target process and recreates all OS objects on behalf of the target. This way, CRIU utilises the available OS mechanisms to run most of the recovery without the need for significant kernel modifications. Finally, CRIU removes any traces of itself from the process.</p><p>CRIU can also <a href="https://lwn.net/Articles/495304/" target="_blank" rel="noopener">restore the state of TCP connections</a>, a crucial feature for live migration of distributed applications. The Linux kernel introduced a new TCP connection state, <code>TCP_REPAIR</code>, for that purpose. In this state, a user-level process can modify the send and receive message queues, get and set the message sequence numbers and timestamps, or open and close a connection without notifying the other side.</p><h2 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h2><p>CRIU checkpoint与restore时需要考虑的进程状态:</p><ul><li>All of the threads running within the process, where they are executing, their priority, and their signal handling state.</li><li>A complete memory map of the process: which mappings exist at which addresses and the protections that apply to each.</li><li>A list of the process’s open files, including the actual files that have been opened, whether each was opened for read, write, or append access, the current file position, and the file-descriptor number.</li><li>Every open network connection, who the peer is, which protocol is in use, and any in-transit data.</li><li>The configuration of the namespaces in which the process is running.</li><li>Active file notifications, terminal configurations, active timers, and no end of other details.</li></ul><h2 id="CRIU-for-RDMA-and-GPU"><a href="#CRIU-for-RDMA-and-GPU" class="headerlink" title="CRIU for RDMA and GPU"></a>CRIU for RDMA and GPU</h2><ul><li><a href="https://www.usenix.org/system/files/atc21-planeta.pdf" target="_blank" rel="noopener">MigrOS: Transparent Live-Migration Support for Containerised RDMA Applications</a></li><li><a href="https://developer.nvidia.com/blog/checkpointing-cuda-applications-with-criu/" target="_blank" rel="noopener">Checkpointing CUDA Applications with CRIU</a></li><li><a href="https://arxiv.org/pdf/2502.16631" target="_blank" rel="noopener">CRIUgpu</a></li></ul><p>更多学术paper，可以参考<a href="https://criu.org/Academic_Research" target="_blank" rel="noopener">Academic Research</a></p><h2 id="CRIB"><a href="#CRIB" class="headerlink" title="CRIB"></a>CRIB</h2><ul><li><a href="https://lwn.net/Articles/984313/" target="_blank" rel="noopener">CRIB: checkpoint/restore in BPF</a></li><li><a href="https://mp.weixin.qq.com/s/XtCebIlGOvugvP35R3pNpw" target="_blank" rel="noopener">CRIB: checkpoint/restore in BPF</a></li><li><a href="https://lpc.events/event/18/contributions/1812/" target="_blank" rel="noopener">Checkpoint/Restore In eBPF (CRIB)</a></li></ul><p>CRIB (Checkpoint/Restore In eBPF) is a proposed eBPF-based system for checkpointing and restoring process state, offering improved performance, flexibility, and extensibility over traditional Linux kernel methods like <code>procfs</code>. It consists of user-space programs, eBPF programs, and kernel functions to facilitate efficient dumping and restoring of process data directly within the kernel.</p><p><img src="/images/2025/10/002.png" alt></p><p>CRIB is an innovative approach to process checkpointing and restoring, designed to address the limitations of existing kernel mechanisms. It leverages the power of eBPF to provide a more performant, flexible, and extensible solution for managing process states. </p><hr><p>参考资料:</p><ol><li><a href="https://criu.org/Main_Page" target="_blank" rel="noopener">https://criu.org/Main_Page</a></li><li><a href="https://github.com/checkpoint-restore/criu" target="_blank" rel="noopener">source code</a></li><li><a href="https://www.usenix.org/system/files/atc21-planeta.pdf" target="_blank" rel="noopener">MigrOS: Transparent Live-Migration Support for Containerised RDMA Applications</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下CRIU(Checkpoint/Restore In Userspace)的相关notes。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Notes about CUDA Dynamic Parallelism</title>
    <link href="http://liujunming.github.io/2025/09/21/Notes-about-CUDA-Dynamic-Parallelism/"/>
    <id>http://liujunming.github.io/2025/09/21/Notes-about-CUDA-Dynamic-Parallelism/</id>
    <published>2025-09-21T12:45:49.000Z</published>
    <updated>2025-09-21T13:27:55.592Z</updated>
    
    <content type="html"><![CDATA[<p>CUDA Dynamic Parallelism(也简称DP，动态并行)是CUDA编程模型的一个高级特性，它允许<strong>GPU内核（Kernel）在设备端自行创建和启动新的内核</strong>，而无需CPU的干预。<a id="more"></a></p><h2 id="为什么引入Dynamic-Parallelism？"><a href="#为什么引入Dynamic-Parallelism？" class="headerlink" title="为什么引入Dynamic Parallelism？"></a>为什么引入Dynamic Parallelism？</h2><p>传统 CUDA 模型要求所有内核（Kernel）必须由 CPU 预先启动，在面对递归算法、自适应计算、不规则数据结构等场景时存在显著局限性。</p><p>CUDA Dynamic Parallelism就是来解决这个问题的。</p><h2 id="DP基本概念"><a href="#DP基本概念" class="headerlink" title="DP基本概念"></a>DP基本概念</h2><p>CUDA DP是CUDA编程模型的一个扩展，CUDA5.0引入的关键特性。允许CUDA内核直接在GPU上创建和同步新的工作（即启动新的内核），实现运行时动态任务生成与调度，从而实现更灵活的并行计算模式。</p><p>与静态并行相比，DP 的核心创新在于：</p><ol><li>任务生成的本地化：任务调度逻辑完全在 GPU 上执行，无需 CPU 介入</li><li>动态资源分配：根据运行时数据特征实时调整并行粒度</li><li>算法自然映射：支持递归、条件分支等不规则控制流的并行化</li></ol><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><ol><li>父内核启动：整个过程仍然始于CPU。CPU启动一个普通的内核（称为父内核）</li><li>设备端启动：在父内核的执行过程中，GPU上的线程可以根据其计算得到的结果，使用标准的语法来启动一个新的子内核</li><li>设备端同步：父内核中的线程可以使用 <code>cudaDeviceSynchronize()</code> 来等待其启动的所有子内核执行完毕，然后再继续自己的工作</li><li>层次结构：子内核还可以继续启动自己的子内核，从而形成一个嵌套的、层次化的内核启动结构。CUDA确保了这种嵌套执行的正确性</li></ol><h2 id="DP核心优势"><a href="#DP核心优势" class="headerlink" title="DP核心优势"></a>DP核心优势</h2><ol><li>消除主机端瓶颈: 在传统模式中，每次内核启动需经历 “CPU 准备参数→PCIe 传输→GPU 执行” 的固定流程。DP 将任务启动逻辑迁移至设备端，避免了频繁的主机 - 设备同步。例如，在递归计算中，子内核启动延迟从 PCIe 级（微秒级）降至 GPU 内核内函数调用级（纳秒级）。</li><li>细粒度任务调度: 线程可根据局部数据特征动态决定是否启动子任务。如在自适应网格计算中，仅当网格单元误差超过阈值时才启动子内核进行细化，实现计算资源的精准分配。</li><li>支持复杂算法结构: 传统CUDA难以高效实现的递归算法（如快速傅里叶变换、光线追踪的光线分叉），可通过DP直接映射为内核层级调用，避免繁琐的迭代化改造。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CUDA Dynamic Parallelism 是一项强大的技术，它通过允许GPU内核自主启动新工作，深化了GPU的独立计算能力。它主要解决了减少CPU-GPU通信瓶颈和简化复杂算法实现的问题，但随之而来的是需要开发者对开销和复杂性进行仔细权衡。在合适的场景下使用，它能带来显著的性能提升和编程便利性。</p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s/teAouWA11-uJ5N3MKEyXcQ" target="_blank" rel="noopener">NVIDIA | BaM与CUDA Dynamic Parallelism的核心区别</a></li><li>deepseek prompt:简要介绍下cuda的Dynamic Parallelism技术</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CUDA Dynamic Parallelism(也简称DP，动态并行)是CUDA编程模型的一个高级特性，它允许&lt;strong&gt;GPU内核（Kernel）在设备端自行创建和启动新的内核&lt;/strong&gt;，而无需CPU的干预。
    
    </summary>
    
      <category term="GPU" scheme="http://liujunming.github.io/categories/GPU/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="CUDA" scheme="http://liujunming.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel Advanced Matrix Extensions (AMX)</title>
    <link href="http://liujunming.github.io/2025/09/20/Notes-about-Intel-Advanced-Matrix-Extensions-AMX/"/>
    <id>http://liujunming.github.io/2025/09/20/Notes-about-Intel-Advanced-Matrix-Extensions-AMX/</id>
    <published>2025-09-20T09:18:05.000Z</published>
    <updated>2025-09-20T10:19:11.388Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel Advanced Matrix Extensions (AMX)的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>To significantly improve the throughput of the CPU for machine learning (ML) applications, Intel has integrated AMX, an on-chip matrix-multiplication(矩阵乘法) accelerator, along with Instruction Set Architecture (ISA) support, starting from the 4th generation Xeon CPUs (SPR), released in 2023. </p><p><img src="/images/2025/09/013.png" alt></p><p>The above Figure depicts the accelerator architecture consisting of two core components: (1) a 2D array of registers (tiles) and (2) Tile matrix multiply unit (TMUL), which are designed to support INT8 and BF16 formats. The tiles store sub-arrays of matrices and the TMUL, a 2D array of multiply-add units, operate on these tiles. The 2D structure of TMUL delivers significantly higher operations/cycle through large tile-based matrix computation, which operates with greater parallelism than 1D compute engines such as AVX engines. The CPU dispatches AMX instructions, such as tile load/store and accelerator commands, to the multi-cycle AMX units. The accelerator’s memory accesses remain coherent with the CPU’s memory accesses.</p><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p>AMX 通过其两个核心组件协同工作来加速矩阵运算:</p><ul><li>Tile:这是一组 8 个二维寄存器（TMM0-TMM7），每个大小为 1 KB，专门用于存储较大的数据块（矩阵）</li><li>TMUL(Tile Matrix Multiply Unit，Tile矩阵乘法单元):这是一个专用的矩阵乘法加速引擎，直接与 Tile 寄存器连接，用于执行 AI 计算中至关重要的矩阵乘法运算</li></ul><p>AMX 支持 BF16 和 INT8 两种数据类型。BF16 在保持足够精度的同时提高了计算效率，适用于训练和推理；INT8 则主要用于推理场景，以进一步追求速度和能效。</p><h2 id="XSAVE的支持"><a href="#XSAVE的支持" class="headerlink" title="XSAVE的支持"></a>XSAVE的支持</h2><p>Tile的状态分为两部分: TILECFG和TILEDATA</p><p><img src="/images/2025/09/014.png" alt></p><p><img src="/images/2025/09/015.png" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/qemu/qemu/blob/master/target/i386/cpu.h</span></span><br><span class="line"><span class="comment">/* Ext. save area 17: AMX XTILECFG state */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">XSaveXTILECFG</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint8_t</span> xtilecfg[<span class="number">64</span>];</span><br><span class="line">&#125; XSaveXTILECFG;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Ext. save area 18: AMX XTILEDATA state */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">XSaveXTILEDATA</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint8_t</span> xtiledata[<span class="number">8</span>][<span class="number">1024</span>];</span><br><span class="line">&#125; XSaveXTILEDATA;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Intel AMX 是 Intel内置在其现代服务器 CPU 中的专用矩阵计算加速硬件，它通过专门的寄存器和执行单元，大幅提升了 CPU 执行 AI 训练和推理任务（尤其是矩阵乘法）的效率。对于希望利用现有服务器基础设施进行高效 AI 计算、同时控制成本和复杂性的企业来说，AMX 提供了一项值得关注的技术。</p><hr><p>参考资料:</p><ol><li>Intel SDM vol1</li><li>LIA: A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading(ISCA’25)</li><li>deepseek prompt:简要介绍下Intel的amx技术</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel Advanced Matrix Extensions (AMX)的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel Processor Trace (IPT)</title>
    <link href="http://liujunming.github.io/2025/09/14/Notes-about-Intel-Processor-Trace-Intel-PT/"/>
    <id>http://liujunming.github.io/2025/09/14/Notes-about-Intel-Processor-Trace-Intel-PT/</id>
    <published>2025-09-14T12:47:52.000Z</published>
    <updated>2025-09-21T02:13:43.460Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel Processor Trace (IPT)的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/09/009.png" alt></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Currently, there are several control flow tracing mechanisms in hardware, each representing a different set of tradeoffs (as shown in Table 1). Branch Trace Store (BTS) can capture all control transfer events (e.g., call, return and all types of jumps) to a memory-resident BTS buffer. Each record contains the addresses of source and target of the branch instruction, thus there is no need to decode it. However, BTS introduces very high overhead during tracing and is inflexible due to the lack of event filtering mechanisms. While Last Brach Record (LBR) has some support of event filtering (e.g., filtering out conditional branches), it can only record 16 or 32 most recent branch pairs (source and target) into a register stack. Though it incurs very low tracing overhead, it can hardly provide precise protection.</p><p><img src="/images/2025/09/010.png" alt></p><p>Due to the capability of dynamically tracing control flow, BTS and LBR have been exploited to defend against ROP like attacks, which, however, either incur high overhead (those using BTS) or sacrifice security due to imprecise tracing(LBR).</p><p>IPT能以比BTS低得多的性能开销（通常在 5% 左右或更低），持续、近乎无遗漏地记录程序执行的控制流。它在很多场景下正在替代 BTS。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>IPT is introduced in Intel Core M and 5th generation Intel Core processors. Each CPU core has its own IPT hardware that generates trace information of running programs in the form of <code>packets</code>. IPT configuration can only be done by the privileged agents (e.g., OS) using certain model-specific registers (MSRs). The traced packets are written to the pre-configured memory buffer in a compressed form to minimize the output bandwidth and reduce the tracing overhead. The software decoder can decode the traced packets based on pre-defined format, with the extra information like the program binaries, as well as some runtime data provided by the control agent, to precisely reconstruct the program flow. Thanks to the aggressive compression of traces, it can collect more control flow tracing information including control flow, execution modes, and timings than BTS, yet incurring much less tracing overhead compared to BTS. This, however, also incurs orders of magnitude slower decoding speed than tracing.</p><p><img src="/images/2025/09/011.png" alt></p><p><img src="/images/2025/09/012.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://ipads.se.sjtu.edu.cn/zh/publications/LiuHPCA17.pdf" target="_blank" rel="noopener">Transparent and Efficient CFI Enforcement with Intel Processor Trace</a></li><li><a href="https://halobates.de/blog/p/406" target="_blank" rel="noopener">Intel Processor Trace resources</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel Processor Trace (IPT)的相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PMU(Performance Monitoring Unit)</title>
    <link href="http://liujunming.github.io/2025/09/14/Notes-about-PMU-Performance-Monitoring-Unit/"/>
    <id>http://liujunming.github.io/2025/09/14/Notes-about-PMU-Performance-Monitoring-Unit/</id>
    <published>2025-09-14T10:48:56.000Z</published>
    <updated>2025-09-14T12:35:31.177Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PMU(Performance Monitoring Unit)的相关notes，内容源于DSN’12 paper，时效性可能不太准确。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>There are generally two working modes of PMUs: interrupt-based mode and precision mode. In the first mode, a counter will automatically increase and generate an interrupt when it has reached a predefined threshold (i.e., event-based sampling) or predefined time has elapsed (i.e., time-based sampling). This is the basic performance counter mode, which supports most types of events, but lacks precise instruction pointer information, resulting in that the reported IP (instruction pointer) is up to tens of instructions away from the instruction causing the event, due to the out-of-order execution in modern processors. For example, according to AMD’s manual, the reported IP may be up to 72 instructions away from the actual IP causing the event.</p><p>To improve the precision and flexibility of PMUs,most commodity processors also support a precise mode of performance monitoring, including the Precise Event-Based Sampling (PEBS), Branch Trace Store (BTS),Last Branch Record (LBR) and Event Filtering (EF). Currently, most existing commodity processors support parts of the features mentioned above.</p><h2 id="Precise-Performance-Counter"><a href="#Precise-Performance-Counter" class="headerlink" title="Precise Performance Counter"></a>Precise Performance Counter</h2><p>In PEBS, the samples of performance counters are written into a preregistered memory region. When the memory region is nearly full, an interrupt is generated to trigger the handler. By batching the samples and processing them together, this mechanism improves the performance of monitoring significantly. Meanwhile, thanks to the <em>atomic-freeze</em> feature, the IP addresses recorded in traces are exactly the ones causing the event.<br><img src="/images/2025/09/008.png" alt></p><h2 id="Branch-Trace-Store"><a href="#Branch-Trace-Store" class="headerlink" title="Branch Trace Store"></a>Branch Trace Store</h2><p>Intel’s BTS mechanism provides the capability of capturing all control transfer events and saving the events in a memory-resident BTS buffer. The events include all types of jump, call, return, interrupt and exception. The recorded information includes the addresses of branch source and target. Thus, it enables the monitoring of the whole control flow of an application. Similar as PEBS, the branch trace is also recorded in a pre-registered memory region, which makes the batching processing possible.</p><h2 id="Last-Branch-Record"><a href="#Last-Branch-Record" class="headerlink" title="Last Branch Record"></a>Last Branch Record</h2><p>LBR in Intel Core and Core i7, records the most recent branches into a register stack. This mechanism records similar data as in BTS. It records the source address and target address of each branch, thus provides the ability to trace the control flow of a program as well. However, due to the small size of the register stack (e.g., Intel Core has 4 pairs, Core i7 has 16 pairs), previous samples may be overwritten by upcoming samples during monitoring.</p><p><img src="/images/2025/09/007.png" alt></p><p>可以把 LBR 想象成一个容量很小的高速暂存器，只记录最近发生的几次分支，但查看速度极快，几乎不影响当前工作。而 BTS 则像一个庞大的流水账本，忠实记录下所有的分支，但往账本上写字的过程本身会消耗一些精力。</p><h2 id="Event-Filtering"><a href="#Event-Filtering" class="headerlink" title="Event Filtering"></a>Event Filtering</h2><p>The Event Filtering mechanism provides additional constraints to record events. It is used to filter events not concerned with. For example, latency constraints can be applied in Itanium2’s cache related events, which only count on high latency cache misses. Further, constraints such as “do not capture conditional branches”, “do not capture near return branches” are generally available on recent processors, which support LBR such as Intel Core i7. However, this mechanism is currently only available in LBR, control transfers recorded in BTS lack this type of filtering support.</p><h2 id="Conditional-Counting"><a href="#Conditional-Counting" class="headerlink" title="Conditional Counting"></a>Conditional Counting</h2><p>To separate user-level events from kernel-level ones, PMUs also support conditional event counting: they only increment counter while the processor is running at a specific privilege level (e.g. user, kernel or both). Further, to isolate possible interferences in performance counters among multiple processes/threads, operating systems are usually enhanced by saving and restoring performance counters during context switches.</p><hr><p>参考资料:</p><ol><li><a href="https://ipads.se.sjtu.edu.cn/_media/publications/cfimon-dsn12.pdf" target="_blank" rel="noopener">CFIMon: Detecting Violation of Control Flow Integrity using Performance Counters</a></li><li><a href="https://llvm.org/devmtg/2024-04/slides/TechnicalTalks/Xiao-EnablingHW-BasedPGO.pdf" target="_blank" rel="noopener">Enabling HW-based PGO</a></li><li>deepssek prompt: pmu的pebs与lbr，如何理解</li><li>deepssek prompt: Intel Branch Trace Store与Last Branch Record的区别</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PMU(Performance Monitoring Unit)的相关notes，内容源于DSN’12 paper，时效性可能不太准确。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about qemu pxb(pci expander bridge)</title>
    <link href="http://liujunming.github.io/2025/09/13/Notes-about-qemu-pxb-pci-expander-bridge/"/>
    <id>http://liujunming.github.io/2025/09/13/Notes-about-qemu-pxb-pci-expander-bridge/</id>
    <published>2025-09-13T11:16:54.000Z</published>
    <updated>2025-09-13T11:40:19.492Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下qemu pxb(pci expander bridge)的相关notes。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>PXB is a “light-weight” host bridge whose purpose is to enable the main host bridge to support multiple PCI root buses.</p><p>As oposed to PCI-2-PCI bridge’s secondary bus, PXB’s bus is a primary bus and can be associated with a NUMA node(different from the main host bridge) allowing the guest OS to recognize the proximity of a pass-through device to other resources as RAM and CPUs.</p><p>The PXB is composed from:</p><ul><li>A primary PCI bus (can be associated with a NUMA node)<br>Acts like a normal pci bus and from the functionality point<br>of view is an “expansion” of the bus behind the<br>main host bridge.</li><li>A pci-2-pci bridge behind the primary PCI bus where the actual<br>devices will be attached.</li><li>A host-bridge PCI device<br>Situated on the bus behind the main host bridge, allows<br>the BIOS to configure the bus number and IO/mem resources.<br>It does not have its own config/data register for configuration<br>cycles, this being handled by the main host bridge.</li></ul><ul><li>A host-bridge sysbus to comply with QEMU current design.</li></ul><h2 id="PCI-Root-Bus"><a href="#PCI-Root-Bus" class="headerlink" title="PCI Root Bus"></a>PCI Root Bus</h2><p>This section demonstrates how to create extra PCI root buses through the “light-weight” PXB (PCI Expander Bridge) host bridge. It is “pxb” in QEMU command line. It is implemented only for i440fx and can be placed only on bus 0.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -machine pc,accel=kvm -vnc :8 -smp 4 -m 4096M \</span><br><span class="line">-net nic -net user,hostfwd=tcp::5028-:22 \</span><br><span class="line">-hda ol8.qcow2 -serial stdio \</span><br><span class="line">-device pxb,id=bridge1,bus=pci.0,bus_nr=3 \</span><br><span class="line">-device virtio-scsi-pci,bus=bridge1,addr=0x3 \</span><br><span class="line">-device pxb,id=bridge2,bus=pci.0,bus_nr=8 \</span><br><span class="line">-device virtio-scsi-pci,bus=bridge2,addr=0x3 \</span><br><span class="line">-device virtio-scsi-pci,bus=bridge2,addr=0x4</span><br></pre></td></tr></table></figure></p><p>The above QEMU command line creates two extra PCI root buses. The first root bus (04:00.0) has one virtio-scsi-pci HBA (04:03.0), and the second root bus (09:00.0) has two virtio-scsi-pci HBAs (09:03.0 and 09:04.0).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci</span><br><span class="line">00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)</span><br><span class="line">00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]</span><br><span class="line">00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]</span><br><span class="line">00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)</span><br><span class="line">00:02.0 VGA compatible controller: Device 1234:1111 (rev 02)</span><br><span class="line">00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 03)</span><br><span class="line">00:04.0 Host bridge: Red Hat, Inc. QEMU PCI Expander bridge</span><br><span class="line">00:05.0 Host bridge: Red Hat, Inc. QEMU PCI Expander bridge</span><br><span class="line">03:00.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge</span><br><span class="line">04:03.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI</span><br><span class="line">08:00.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge</span><br><span class="line">09:03.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI</span><br><span class="line">09:04.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI</span><br></pre></td></tr></table></figure></p><p>The below <code>lspci</code> output and figure depict the PCI bus topology for this example.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci -t</span><br><span class="line">-+-[0000:08]---00.0-[09]--+-03.0</span><br><span class="line"> |                        \-04.0</span><br><span class="line"> +-[0000:03]---00.0-[04]----03.0</span><br><span class="line"> \-[0000:00]-+-00.0</span><br><span class="line">             +-01.0</span><br><span class="line">             +-01.1</span><br><span class="line">             +-01.3</span><br><span class="line">             +-02.0</span><br><span class="line">             +-03.0</span><br><span class="line">             +-04.0</span><br><span class="line">             \-05.0</span><br></pre></td></tr></table></figure><p><img src="/images/2025/09/005.avif" alt></p><h2 id="PCIe-Root-Complex"><a href="#PCIe-Root-Complex" class="headerlink" title="PCIe Root Complex"></a>PCIe Root Complex</h2><p>This section demonstrates how to create extra PCIe root buses through extra Root Complexes. According to QEMU source code, PCIe features are supported only by ‘q35’ machine type on x86 architecture and the ‘virt’ machine type on AArch64. The root complex is created by using “pxb-pcie” on the QEMU command line.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -machine q35,accel=kvm -vnc :8 -smp 4 -m 4096M \</span><br><span class="line">-net nic -net user,hostfwd=tcp::5028-:22 \</span><br><span class="line">-hda ol8.qcow2 -serial stdio \</span><br><span class="line">-device pxb-pcie,id=pcie.1,bus_nr=2,bus=pcie.0 \</span><br><span class="line">-device ioh3420,id=pcie_port1,bus=pcie.1,chassis=1 \</span><br><span class="line">-device virtio-scsi-pci,bus=pcie_port1 \</span><br><span class="line">-device ioh3420,id=pcie_port2,bus=pcie.1,chassis=2 \</span><br><span class="line">-device virtio-scsi-pci,bus=pcie_port2 \</span><br><span class="line">-device pxb-pcie,id=pcie.2,bus_nr=8,bus=pcie.0 \</span><br><span class="line">-device ioh3420,id=pcie_port3,bus=pcie.2,chassis=3 \</span><br><span class="line">-device virtio-scsi-pci,bus=pcie_port3</span><br></pre></td></tr></table></figure></p><p>The above QEMU command line creates two extra PCIe root complexes. The first root complex has one virtio-scsi-pci HBA (09:00.0), and the second has two virtio-scsi-pci HBAs (03:00.0 and 04:00.0).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci</span><br><span class="line">00:00.0 Host bridge: Intel Corporation 82G33/G31/P35/P31 Express DRAM Controller</span><br><span class="line">00:01.0 VGA compatible controller: Device 1234:1111 (rev 02)</span><br><span class="line">00:02.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection</span><br><span class="line">00:03.0 Host bridge: Red Hat, Inc. QEMU PCIe Expander bridge</span><br><span class="line">00:04.0 Host bridge: Red Hat, Inc. QEMU PCIe Expander bridge</span><br><span class="line">00:1f.0 ISA bridge: Intel Corporation 82801IB (ICH9) LPC Interface Controller (rev 02)</span><br><span class="line">00:1f.2 SATA controller: Intel Corporation 82801IR/IO/IH (ICH9R/DO/DH) 6 port SATA Controller [AHCI mode] (rev 02)</span><br><span class="line">00:1f.3 SMBus: Intel Corporation 82801I (ICH9 Family) SMBus Controller (rev 02)</span><br><span class="line">02:00.0 PCI bridge: Intel Corporation 7500/5520/5500/X58 I/O Hub PCI Express Root Port 0 (rev 02)</span><br><span class="line">02:01.0 PCI bridge: Intel Corporation 7500/5520/5500/X58 I/O Hub PCI Express Root Port 0 (rev 02)</span><br><span class="line">03:00.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI (rev 01)</span><br><span class="line">04:00.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI (rev 01)</span><br><span class="line">08:00.0 PCI bridge: Intel Corporation 7500/5520/5500/X58 I/O Hub PCI Express Root Port 0 (rev 02)</span><br><span class="line">09:00.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI (rev 01)</span><br></pre></td></tr></table></figure></p><p>The below <code>lspci</code> output and figure depict the PCIe topology for this example.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@vm ~]# lspci -t</span><br><span class="line">-+-[0000:08]---00.0-[09]----00.0</span><br><span class="line"> +-[0000:02]-+-00.0-[03]----00.0</span><br><span class="line"> |           \-01.0-[04]----00.0</span><br><span class="line"> \-[0000:00]-+-00.0</span><br><span class="line">             +-01.0</span><br><span class="line">             +-02.0</span><br><span class="line">             +-03.0</span><br><span class="line">             +-04.0</span><br><span class="line">             +-1f.0</span><br><span class="line">             +-1f.2</span><br><span class="line">             \-1f.3</span><br></pre></td></tr></table></figure></p><p><img src="/images/2025/09/006.avif" alt></p><hr><p>参考资料:</p><ol><li><a href="https://github.com/qemu/qemu/blob/master/docs/pci_expander_bridge.txt" target="_blank" rel="noopener">qemu/docs/pci_expander_bridge.txt</a></li><li><a href="https://blogs.oracle.com/linux/post/a-study-of-the-linux-kernel-pci-subsystem-with-qemu" target="_blank" rel="noopener">A study of the Linux kernel PCI subsystem with QEMU</a></li><li><a href="https://mail.coreboot.org/archives/list/seabios@seabios.org/message/7QA3NUQ7PMU445BWS6FGI54CFWED66GM/" target="_blank" rel="noopener">[PATCH RFC V2 12/17] hw/pci: introduce PCI Expander Bridge (PXB)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下qemu pxb(pci expander bridge)的相关notes。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="QEMU" scheme="http://liujunming.github.io/tags/QEMU/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ARM pointer authentication</title>
    <link href="http://liujunming.github.io/2025/09/07/Notes-about-ARM-pointer-authentication/"/>
    <id>http://liujunming.github.io/2025/09/07/Notes-about-ARM-pointer-authentication/</id>
    <published>2025-09-07T00:21:35.000Z</published>
    <updated>2025-09-07T01:31:56.528Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下ARM pointer authentication的相关notes。<a id="more"></a></p><h2 id="核心目的"><a href="#核心目的" class="headerlink" title="核心目的"></a>核心目的</h2><p>Pointer Authentication(PA)的核心目的是验证指针(如返回地址、函数指针等)是否被恶意篡改。它通过为指针添加加密认证码(PAC, Pointer Authentication Code)并在使用前验证其有效性来实现。</p><p><img src="/images/2025/09/001.png" alt></p><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><ol><li>生成PAC(签名)：<ul><li>使用一个密钥(CPU寄存器存储)、指针的虚拟地址和额外的上下文信息(如堆栈指针)，通过加密算法(ARM选择的是QARMA算法)为指针生成一个短小的PAC</li><li>PAC被存储在指针值中未使用的最高位中。因为64位架构中，虚拟地址通常不会全部使用(如48位)，高位空间是空闲的</li></ul></li><li>验证与恢复(验证)：<ul><li>在使用指针前(如函数返回前)，CPU会执行验证指令(如<code>AUT*</code>)</li><li>系统会使用相同的密钥和上下文重新计算PAC，并与指针中存储的PAC进行比较</li><li>如果匹配，则移除PAC，恢复原始指针，程序正常执行</li><li>如果不匹配，则指针会被置为一个非法值，使用时通常会触发一个异常(如段错误)，从而阻止攻击</li></ul></li></ol><h2 id="Pointer-Authentication-in-ARMv8-3-A"><a href="#Pointer-Authentication-in-ARMv8-3-A" class="headerlink" title="Pointer Authentication in ARMv8.3-A"></a>Pointer Authentication in ARMv8.3-A</h2><p><img src="/images/2025/09/003.png" alt></p><p><img src="/images/2025/09/004.png" alt></p><p>PA is intended for checking the integrity of pointers with minimal size and performance impact. It is available when the processor executes in 64-bit ARM state(AArch64). PA adds instructions for creating and authenticating pointer authentication codes (PACs). The PAC is a tweakable message authentication code (MAC) calculated over the pointer value and a 64-bit modifier as the tweak. Different combinations of key and modifier pairs allow domain separation among different classes of authenticated pointers. This prevents authenticated pointer values from being arbitrarily interchangeable with one another.</p><p>PA provides five different keys for PAC generation: two for code pointers, two for data pointers, and one for generic use. The keys are stored in hardware registers configured to be accessible only from a higher privilege level: e.g., the kernel maintains the keys for a user space process, generating keys for each process at process <code>exec</code>. The keys remain constant throughout the process lifetime, whereas the modifier is given in an instruction-specific register operand on each PAC creation and authentication (i.e., MAC verification). Thus it can be used to describe the run-time context in which the pointer is created and used. The modifier value is not necessarily confidential but ideally such that it 1) precisely describes the context of use in which the pointer is valid, and 2) cannot be influenced by the attacker</p><h2 id="Example-PA-based-return-address-signing"><a href="#Example-PA-based-return-address-signing" class="headerlink" title="Example: PA-based return address signing"></a>Example: PA-based return address signing</h2><p><img src="/images/2025/09/002.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.usenix.org/conference/usenixsecurity19/presentation/liljestrand" target="_blank" rel="noopener">PAC it up: Towards Pointer Integrity using ARM Pointer Authentication</a></li><li>deepseek prompt:简要介绍下arm Pointer Authentication</li><li><a href="https://learn.arm.com/learning-paths/servers-and-cloud-computing/pac/pac/" target="_blank" rel="noopener">Pointer Authentication on Arm</a></li><li><a href="https://lwn.net/Articles/718888/" target="_blank" rel="noopener">https://lwn.net/Articles/718888/</a></li><li><a href="https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/pointer-auth-v7.pdf" target="_blank" rel="noopener">Pointer Authentication on ARMv8.3: Design and Analysis of the New Software Security Instructions</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下ARM pointer authentication的相关notes。
    
    </summary>
    
      <category term="ARM" scheme="http://liujunming.github.io/categories/ARM/"/>
    
    
      <category term="ARM" scheme="http://liujunming.github.io/tags/ARM/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Linux kernel 内存分配函数</title>
    <link href="http://liujunming.github.io/2025/08/17/Notes-about-Linux-kernel-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%87%BD%E6%95%B0/"/>
    <id>http://liujunming.github.io/2025/08/17/Notes-about-Linux-kernel-内存分配函数/</id>
    <published>2025-08-17T09:24:12.000Z</published>
    <updated>2025-08-24T12:26:57.280Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux kernel内存分配函数的相关notes。<a id="more"></a></p><h2 id="内核函数"><a href="#内核函数" class="headerlink" title="内核函数"></a>内核函数</h2><h3 id="kmalloc系列"><a href="#kmalloc系列" class="headerlink" title="kmalloc系列"></a>kmalloc系列</h3><p>kmalloc() 申请的内存位于物理内存映射区域，而且在物理上也是连续的，它们与真实的物理地址只有一个固定的偏移，因而存在较简单的转换关系。</p><ul><li>kmalloc</li><li>kzalloc</li><li>kcalloc</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kzalloc - allocate memory. The memory is set to zero.</span></span><br><span class="line"><span class="comment"> * @size: how many bytes of memory are required.</span></span><br><span class="line"><span class="comment"> * @flags: the type of memory to allocate (see kmalloc).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kzalloc</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kmalloc(size, flags | __GFP_ZERO);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kcalloc - allocate memory for an array. The memory is set to zero.</span></span><br><span class="line"><span class="comment"> * @n: number of elements.</span></span><br><span class="line"><span class="comment"> * @size: element size.</span></span><br><span class="line"><span class="comment"> * @flags: the type of memory to allocate (see kmalloc).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kcalloc</span><span class="params">(<span class="keyword">size_t</span> n, <span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kmalloc_array(n, size, flags | __GFP_ZERO);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="vmalloc系列"><a href="#vmalloc系列" class="headerlink" title="vmalloc系列"></a>vmalloc系列</h3><p>物理地址不连续：通过映射非连续的物理页帧实现。<br>虚拟地址连续：虚拟地址空间是连续的。<br>逐页映射: 用 <code>alloc_page()</code> 从伙伴系统获取多个不连续的物理页帧。 修改内核页表，将这些物理页映射到连续的虚拟地址空间（<code>VMALLOC_START</code> ~ <code>VMALLOC_END</code>）。</p><ul><li>vmalloc</li><li>vzalloc</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *vmalloc  -  allocate virtually contiguous memory</span></span><br><span class="line"><span class="comment"> *@size:allocation size</span></span><br><span class="line"><span class="comment"> *Allocate enough pages to cover @size from the page level</span></span><br><span class="line"><span class="comment"> *allocator and map them into contiguous kernel virtual space.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *For tight control over page level allocator and protection flags</span></span><br><span class="line"><span class="comment"> *use __vmalloc() instead.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">vmalloc</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> __vmalloc_node_flags(size, NUMA_NO_NODE,</span><br><span class="line">    GFP_KERNEL);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *vzalloc - allocate virtually contiguous memory with zero fill</span></span><br><span class="line"><span class="comment"> *@size:allocation size</span></span><br><span class="line"><span class="comment"> *Allocate enough pages to cover @size from the page level</span></span><br><span class="line"><span class="comment"> *allocator and map them into contiguous kernel virtual space.</span></span><br><span class="line"><span class="comment"> *The memory allocated is set to zero.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *For tight control over page level allocator and protection flags</span></span><br><span class="line"><span class="comment"> *use __vmalloc() instead.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">vzalloc</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> __vmalloc_node_flags(size, NUMA_NO_NODE,</span><br><span class="line">GFP_KERNEL | __GFP_ZERO);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="kvzalloc系列"><a href="#kvzalloc系列" class="headerlink" title="kvzalloc系列"></a>kvzalloc系列</h3><ul><li>kvzalloc</li><li>kvcalloc</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kvzalloc</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kvmalloc(size, flags | __GFP_ZERO);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> *<span class="title">kvmalloc</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> kvmalloc_node(size, flags, NUMA_NO_NODE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kvmalloc_node - attempt to allocate physically contiguous memory, but upon</span></span><br><span class="line"><span class="comment"> * failure, fall back to non-contiguous (vmalloc) allocation.</span></span><br><span class="line"><span class="comment"> * @size: size of the request.</span></span><br><span class="line"><span class="comment"> * @flags: gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.</span></span><br><span class="line"><span class="comment"> * @node: numa node to allocate from</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Uses kmalloc to get the memory but if the allocation fails then falls back</span></span><br><span class="line"><span class="comment"> * to the vmalloc allocator. Use kvfree for freeing the memory.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported.</span></span><br><span class="line"><span class="comment"> * __GFP_RETRY_MAYFAIL is supported, and it should be used only if kmalloc is</span></span><br><span class="line"><span class="comment"> * preferable to the vmalloc fallback, due to visible performance drawbacks.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Please note that any use of gfp flags outside of GFP_KERNEL is careful to not</span></span><br><span class="line"><span class="comment"> * fall back to vmalloc.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">kvmalloc_node</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">gfp_t</span> flags, <span class="keyword">int</span> node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">gfp_t</span> kmalloc_flags = flags;</span><br><span class="line"><span class="keyword">void</span> *ret;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * vmalloc uses GFP_KERNEL for some internal allocations (e.g page tables)</span></span><br><span class="line"><span class="comment"> * so the given set of flags has to be compatible.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> ((flags &amp; GFP_KERNEL) != GFP_KERNEL)</span><br><span class="line"><span class="keyword">return</span> kmalloc_node(size, flags, node);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * We want to attempt a large physically contiguous block first because</span></span><br><span class="line"><span class="comment"> * it is less likely to fragment multiple larger blocks and therefore</span></span><br><span class="line"><span class="comment"> * contribute to a long term fragmentation less than vmalloc fallback.</span></span><br><span class="line"><span class="comment"> * However make sure that larger requests are not too disruptive - no</span></span><br><span class="line"><span class="comment"> * OOM killer and no allocation failure warnings as we have a fallback.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (size &gt; PAGE_SIZE) &#123;</span><br><span class="line">kmalloc_flags |= __GFP_NOWARN;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!(kmalloc_flags &amp; __GFP_RETRY_MAYFAIL))</span><br><span class="line">kmalloc_flags |= __GFP_NORETRY;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ret = kmalloc_node(size, kmalloc_flags, node);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * It doesn't really make sense to fallback to vmalloc for sub page</span></span><br><span class="line"><span class="comment"> * requests</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (ret || size &lt;= PAGE_SIZE)</span><br><span class="line"><span class="keyword">return</span> ret;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> __vmalloc_node_flags_caller(size, node, flags,</span><br><span class="line">__builtin_return_address(<span class="number">0</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="物理地址获取"><a href="#物理地址获取" class="headerlink" title="物理地址获取"></a>物理地址获取</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> phys_addr_t <span class="title">kvm_kaddr_to_phys</span><span class="params">(<span class="keyword">void</span> *kaddr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!is_vmalloc_addr(kaddr)) &#123;</span><br><span class="line"><span class="keyword">return</span> __pa(kaddr);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> page_to_phys(vmalloc_to_page(kaddr)) +</span><br><span class="line">       offset_in_page(kaddr);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note that the virtual address may come from different kernel memory zones, including the <code>vmalloc region</code> or the <code>direct memory region</code>. Checking whether the a virtual address belongs to the vmalloc region by invoking the <code>is_vmalloc_addr</code> function. If so, the <code>vmalloc_to_page</code> function is used to get the corresponding physical page structure; otherwise, the <code>virt_to_page</code> function is used to obtain the right page.</p><hr><p>参考资料:</p><ol><li><a href="https://elixir.bootlin.com/linux/v4.19/" target="_blank" rel="noopener">https://elixir.bootlin.com/linux/v4.19/</a></li><li><a href="https://stackoverflow.com/questions/5153173/what-are-differences-between-kmalloc-kcalloc-vmalloc-and-kzalloc" target="_blank" rel="noopener">what are differences between kmalloc() kcalloc() vmalloc() and kzalloc()?</a></li><li><a href="https://zhuanlan.zhihu.com/p/470783392" target="_blank" rel="noopener">带你看懂Linux内核空间内存申请函数kmalloc.、kzalloc、 vmalloc的区别（一篇就够了）</a></li><li><a href="https://blog.csdn.net/sinat_37817094/article/details/149230334" target="_blank" rel="noopener">傻傻分不清楚 kmalloc、vmalloc和malloc之间有什么区别以及实现上的差异</a></li><li><a href="https://www.usenix.org/system/files/atc19-hu.pdf" target="_blank" rel="noopener">QZFS: QAT Accelerated Compression in File System for Application Agnostic and Cost Efficient Data Storage</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux kernel内存分配函数的相关notes。
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about GPUDirect Async family</title>
    <link href="http://liujunming.github.io/2025/07/05/Notes-about-GPUDirect-Async-family/"/>
    <id>http://liujunming.github.io/2025/07/05/Notes-about-GPUDirect-Async-family/</id>
    <published>2025-07-05T03:24:40.000Z</published>
    <updated>2025-07-05T07:31:12.661Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下GPUDirect Async family技术的相关notes。<a id="more"></a></p><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><ul><li><a href="/2022/04/02/Introduction-to-GPUDirect-RDMA/">GPUDirect RDMA</a></li><li><a href="/2023/05/01/Notes-about-GPU-Direct-Storage/">GPU Direct Storage</a></li><li><a href="/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/">IBGDA</a></li></ul><h2 id="GPUDirect-Async-Kernel-Initiated-Network"><a href="#GPUDirect-Async-Kernel-Initiated-Network" class="headerlink" title="GPUDirect Async Kernel-Initiated Network"></a>GPUDirect Async Kernel-Initiated Network</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>为了让GPU和网卡并行起来，CPU仍然扮演了厚重的调度角色，而且GPU空转时间比较长。</p><p><img src="/images/2025/07/004.png" alt></p><p>能否把控制面也offload一部分？于是乎GPUDirect Async Kernel-Initiated Network概念被提了出来。</p><p>GPUDirect Async Kernel-Initiated Network整体的逻辑如下:<br><img src="/images/2025/07/005.png" alt></p><p>GPUDirect Async Kernel-Initiated Network消除了CPU在通信控制路径中的作用。</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>The following is an example diagram of a CPU-centric approach:<br><img src="/images/2025/07/006.webp" alt></p><p>The following is an example diagram of a GPU-centric approach:<br><img src="/images/2025/07/007.webp" alt></p><p>在此处，网卡既可以是RDMA网卡，也可以是Ethernet网卡。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/images/2025/07/008.png" alt></p><ol><li>对于GDR，GPUDirect Async Kernel-Initiated Network可以offload critical控制路径到GPU中(也就是<a href="/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/">IBGDA</a>)</li><li>对于GPU与Ethernet网卡的p2p，GPUDirect Async Kernel-Initiated Network可以offload critical控制路径到GPU中</li></ol><h2 id="GPUDirect-Async-Kernel-Initiated-Storage"><a href="#GPUDirect-Async-Kernel-Initiated-Storage" class="headerlink" title="GPUDirect Async Kernel Initiated Storage"></a>GPUDirect Async Kernel Initiated Storage</h2><p><a href="https://mp.weixin.qq.com/s/l0PxXtDV8trFyi87Ucerww" target="_blank" rel="noopener">BaM</a>是首个以加速器为中心的方法，使GPU能够按需访问存储在内存或存储设备中的数据，而无需依赖CPU来发起或触发这些访问。</p><h3 id="BaM架构"><a href="#BaM架构" class="headerlink" title="BaM架构"></a>BaM架构</h3><p>BaM的<strong>设计目标</strong>是为GPU线程提供高效的存储访问抽象，以便其能够按需、细粒度且高吞吐量地访问存储设备，同时提升存储访问性能。为此，如下图所示，BaM在GPU内存中配置了专门的存储I/O队列和缓冲区，并借助GPU的内存映射功能，将存储DB reg映射到GPU地址空间。</p><p><img src="/images/2025/07/003.webp" alt></p><h3 id="与传统GDS对比"><a href="#与传统GDS对比" class="headerlink" title="与传统GDS对比"></a>与传统GDS对比</h3><p><img src="/images/2025/07/001.webp" alt></p><p>与传统的存储数据访问模式（GDS）相比，BaM带来了显著的变革，使GPU线程能够直接访问存储，从而实现了细粒度的计算与I/O重叠。这一设计理念带来了多方面的优势。</p><ul><li>首先，减少CPU-GPU同步开销，以及GPU内核的启动频率，从而消除了每次数据访问时CPU的启动和调度需求。</li><li>其次，降低I/O放大开销，由于CPU调度通常以大块数据任务为单位，而非GPU实际所需的随机数据，BaM中GPU线程仅在需要时才获取，因此有效避免了IO放大问题。</li><li>最后，简化编程并隐藏延迟，过去，为了处理不同规模的数据，开发人员可能需要计算应用层面的复杂数据分块和分割策略；而Bam允许程序员通过数组抽象自然地访问数据，并<u>利用GPU线程在大规模数据集上的并行性来隐藏存储访问延迟</u>，从而简化了编程逻辑。</li></ul><h3 id="BaM与NVIDIA-GDS的性能对比"><a href="#BaM与NVIDIA-GDS的性能对比" class="headerlink" title="BaM与NVIDIA GDS的性能对比"></a>BaM与NVIDIA GDS的性能对比</h3><p><img src="/images/2025/07/002.webp" alt></p><p>BaM与NVIDIA GDS的性能对比显示：当访问粒度小于32KB时，受传统 CPU 软件栈开销限制，GDS无法使PCIe接口饱和。相比之下，BaM 即使在4KB的I/O粒度下也能使接口饱和（约25GBps）。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>对于GDS，GPUDirect Async Kernel-Initiated Storage可以offload critical控制路径到GPU中</p><h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><p>GPU正朝着更高自主性和异步性的趋势发展。GPUDirect Async技术族在将数据从内存或存储直接移动到GPU内存时，可加速控制路径。</p><hr><p>参考资料:</p><ol><li><a href="https://downloads.openfabrics.org/ofv/ofv_presentation_GPU.pdf" target="_blank" rel="noopener">OFVWG:GPUDirect and PeerDirect</a></li><li><a href="https://docs.nvidia.com/doca/sdk/doca+gpunetio/index.html" target="_blank" rel="noopener">DOCA GPUNetIO</a></li><li><a href="https://joyxu.github.io/2022/06/06/gpu-direct/" target="_blank" rel="noopener">gpu-direct</a></li><li><a href="http://blog.chinaunix.net/uid-28541347-id-5886592.html" target="_blank" rel="noopener">GPU Direct相关技术和原理</a></li><li>GPU-Initiated On-Demand High-Throughput Storage Access in the BaM System Architecture(ASPLOS’23)</li><li><a href="http://nvidia.zhidx.com/index.php?m=content&amp;c=index&amp;a=show&amp;catid=6&amp;id=3190" target="_blank" rel="noopener">使用 NVIDIA DOCA GPUNetIO 实现实时网络处理功能</a></li><li><a href="https://github.com/NVIDIA/nccl/issues/1380" target="_blank" rel="noopener">Does NCCL support DOCA GPUNetIO?</a></li><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0743731517303386" target="_blank" rel="noopener">GPUDirect Async: Exploring GPU synchronous communication techniques for InfiniBand clusters</a></li><li><a href="https://zhuanlan.zhihu.com/p/430101220" target="_blank" rel="noopener">【研究综述】浅谈GPU通信和PCIe P2P DMA</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下GPUDirect Async family技术的相关notes。
    
    </summary>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/categories/AI-Infra/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/tags/AI-Infra/"/>
    
  </entry>
  
  <entry>
    <title>Notes about GPU vs NPU</title>
    <link href="http://liujunming.github.io/2025/06/08/Notes-about-GPU-vs-NPU/"/>
    <id>http://liujunming.github.io/2025/06/08/Notes-about-GPU-vs-NPU/</id>
    <published>2025-06-08T01:50:07.000Z</published>
    <updated>2025-06-08T02:29:39.524Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下GPU vs NPU(Neural Processing Unit)的相关notes，建议先看下科普视频<a href="https://www.bilibili.com/video/BV1mFQgY5EfM/" target="_blank" rel="noopener">比GPU更快！NPU是如何实现AI加速的？</a>。<a id="more"></a></p><h2 id="GPU-vs-NPU"><a href="#GPU-vs-NPU" class="headerlink" title="GPU vs NPU"></a>GPU vs NPU</h2><p>NPU内部的基本单元还是乘加器，只是改变了数据的流动方式，不再需要运回缓存，从而提高了运算效率。</p><p>NPUs are specialized hardware accelerators that excel at performing neural network computations efficiently. In contrast to general-purpose processors, NPUs are specifically optimized to meet the unique requirements of neural networks. They offer capabilities such as massive parallelism, high-speed data processing and matrix computation. These features are essential for handling intensive computations involved in deep learning algorithms.</p><p>To meet these requirements, NPU adopts several special hardware structures. <strong>One key unit for the modern NPU is the matrix calculation unit (MCU)</strong> like tube, systolic array, etc. With these matrix units, <u>NPU can execute the matrix calculation like multiplication and convolution within one operation</u>. Some NPUs also have other dedicated units for some specialized operations like sparse matrix calculation, activation function, etc.</p><p>Besides the matrix unit, <strong>NPUs often adopt a near data computing (NDC) architecture to minimize data retrieval overhead</strong>. For example, weights in neural networks are pre-stored in the SRAM/scratchpad near the matrix unit, allowing for quick access during computations. This reduces latency and energy consumption by eliminating the need to retrieve weights from main memory for each task. <strong>NDC optimizes data flow and improves computational efficiency by minimizing memory access bottlenecks</strong>.</p><p>Furthermore, <u>NPUs also leverage multi-core architectures with a Network-on-Chip (NoC) network to further parallelize data computation</u>. <strong>The NoC network allows for direct data transfer among NPU cores without the need for additional memory load/store instructions</strong>.</p><h2 id="CUDA-Cores-vs-Tensor-Cores"><a href="#CUDA-Cores-vs-Tensor-Cores" class="headerlink" title="CUDA Cores vs Tensor Cores"></a>CUDA Cores vs Tensor Cores</h2><p>A100/H100等GPU，不仅仅有通用的CUDA Cores，还集成了NPU中的Cores。</p><p><u>CUDA cores</u> are responsible for <strong>general-purpose</strong> processing tasks in GPUs, which handle a wide range of instructions including integer operations, floating-point operations, load/store operations, etc. CUDA cores execute scalar (or vector) instructions operating on individual (or vector) data elements. </p><p><u>Tensor cores</u> are <strong>specialized hardware</strong> designed for accelerating matrix multiplication. Tensor cores have 16.0×/14.8× higher FLOPS than CUDA cores on A100/H100 GPUs. Besides, Tensor cores work at a coarse-grained granularity, e.g. performing a matrix multiplication between two FP16 matrices of shape 16 × 16 and 16 × 8 with a single <em>mma</em> (matrix multiply and accumulate) instruction.</p><hr><p>参考资料:</p><ol><li><a href="https://ipads.se.sjtu.edu.cn/_media/publications/feng-isca24.pdf" target="_blank" rel="noopener">sNPU: Trusted Execution Environments on Integrated NPUs</a></li><li><a href="https://www.usenix.org/system/files/atc24-xia.pdf" target="_blank" rel="noopener">Quant-LLM: Accelerating the Serving of Large Language Models via FP6-Centric Algorithm-System Co-Design on Modern GPUs</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下GPU vs NPU(Neural Processing Unit)的相关notes，建议先看下科普视频&lt;a href=&quot;https://www.bilibili.com/video/BV1mFQgY5EfM/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;比GPU更快！NPU是如何实现AI加速的？&lt;/a&gt;。
    
    </summary>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/categories/AI-Infra/"/>
    
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="AI Infra" scheme="http://liujunming.github.io/tags/AI-Infra/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PIPT,VIVT and VIPT</title>
    <link href="http://liujunming.github.io/2025/06/07/Notes-about-PIPT-VIPT-and-VIVT/"/>
    <id>http://liujunming.github.io/2025/06/07/Notes-about-PIPT-VIPT-and-VIVT/</id>
    <published>2025-06-07T09:08:24.000Z</published>
    <updated>2025-06-08T01:44:25.222Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Cache缓存设计策略中的PIPT(Physically-Indexed, Physically-Tagged)、VIVT(Virtually-Indexed, Virtually-Tagged)和VIPT(Virtually-Indexed, Physically-Tagged)的相关notes。<a id="more"></a></p><h2 id="Basic-knowledge"><a href="#Basic-knowledge" class="headerlink" title="Basic knowledge"></a>Basic knowledge</h2><p>Cache 本质就是一个硬件 hash 表（Tag RAM + DATA RAM），一般来说 Cache 的大小指的是DATA RAM 的大小，并不包括 Tag RAM 的物理空间。如下图所示，以一个 32 位内存地址为例，其中 [0, 4] 属于 Offset 字段，[5, 12] 属于 Index 字段，[13, 31] 属于 Tag 字段。</p><p><img src="/images/2025/06/009.png" alt></p><h2 id="PIPT"><a href="#PIPT" class="headerlink" title="PIPT"></a>PIPT</h2><p><img src="/images/2025/06/010.png" alt></p><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ol><li>CPU 发出虚拟地址 (VA)</li><li>MMU (内存管理单元) 将 VA 翻译成物理地址 (PA)（通过查询 TLB 或页表）</li><li>使用 PA 的一部分索引缓存</li><li>使用 PA 的剩余部分作为标签，与索引找到的缓存行中的标签进行比较</li><li>如果标签匹配且有效位为真，则缓存命中，数据返回给 CPU；否则缓存未命中</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>速度慢，缓存查找必须等待MMU完成虚拟地址到物理地址的转换后才能开始，这增加了访问latency。</p><h2 id="VIVT"><a href="#VIVT" class="headerlink" title="VIVT"></a>VIVT</h2><h3 id="工作流程-1"><a href="#工作流程-1" class="headerlink" title="工作流程"></a>工作流程</h3><ol><li>CPU 发出虚拟地址 (VA)</li><li>同时：<ul><li>使用 VA 的一部分直接索引缓存，使用 VA 的剩余部分作为标签，与索引找到的缓存行中的标签进行比较</li><li>MMU 将 VA 翻译成物理地址 (PA)</li></ul></li><li>如果标签匹配、有效位为真，并且 MMU 转换成功（地址有效），则缓存命中，数据返回给 CPU；否则缓存未命中</li></ol><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>歧义问题： 不同的物理地址（不同的数据）可能具有相同的虚拟地址索引和标签部分（如果映射不同）。这会导致一个缓存行错误地对应多个物理地址的数据。</li><li>同名问题： 同一个物理地址（同一份数据）被映射到不同的虚拟地址时，会在缓存中产生多个副本（不同虚拟标签）。修改一个副本不会自动更新另一个副本，导致数据不一致。</li><li>需要操作系统干预： 内核在切换进程上下文或修改页表映射时，必须清空（flush）整个或部分缓存以避免歧义和同名问题，开销巨大。</li></ul><h2 id="VIPT"><a href="#VIPT" class="headerlink" title="VIPT"></a>VIPT</h2><p><img src="/images/2025/06/012.png" alt></p><p><img src="/images/2025/06/011.png" alt></p><h3 id="工作流程-2"><a href="#工作流程-2" class="headerlink" title="工作流程"></a>工作流程</h3><ol><li>CPU 发出虚拟地址 (VA)</li><li>同时：<ul><li>使用 VA 的一部分直接索引缓存</li><li>MMU (通过 TLB) 将 VA 翻译成物理地址 (PA)</li></ul></li><li>使用 PA 的（相应）部分作为标签，与步骤2中索引找到的缓存行中的标签进行比较</li><li>如果标签匹配且有效位为真，则缓存命中，数据返回给 CPU；否则缓存未命中</li></ol><hr><p>参考资料:</p><ol><li><a href="https://www.youtube.com/watch?v=3sX5obQCHNA" target="_blank" rel="noopener">Virtual Memory: 13 TLBs and Caches</a></li><li><a href="https://mp.weixin.qq.com/s/cUShsE0FWGnQWWQn8B4W_Q" target="_blank" rel="noopener">GPU TLB 与 MIG 切分：真的完全物理隔离吗？</a></li><li><a href="https://courses.physics.illinois.edu/ece411/fa2022/slides/lect08_l.pptx" target="_blank" rel="noopener">Caches: Policies and Interactions with VM</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Cache缓存设计策略中的PIPT(Physically-Indexed, Physically-Tagged)、VIVT(Virtually-Indexed, Virtually-Tagged)和VIPT(Virtually-Indexed, Physically-Tagged)的相关notes。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe reservation机制</title>
    <link href="http://liujunming.github.io/2025/06/02/Notes-about-NVMe-reservation%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2025/06/02/Notes-about-NVMe-reservation机制/</id>
    <published>2025-06-02T00:31:40.000Z</published>
    <updated>2025-06-02T05:23:30.037Z</updated>
    
    <content type="html"><![CDATA[<p>对于NVMe共享盘，是通过NVMe reservation机制来实现的。可以将NVMe reservation机制类比于读写锁。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2025/06/003.png" alt></p><p>reservation的意义在于，提供一个机制，避免多个host访问一个共享的namespace出现冲突。reservation操作的对象是namespace，而不是ssd。</p><p>reservation是<u>管理共享者的访问权限</u>；用户可以用共享，但是不用reservation，reservation是一个可选项。</p><blockquote><p>Multi-Attach enabled io2 volumes support NVMe reservations, which is a set of industry-standard storage fencing protocols. These protocols enable you to create and manage reservations that control and coordinate access from multiple instances to a shared volume. Reservations are used by shared storage applications to ensure data consistency.</p></blockquote><p><a href="https://docs.aws.amazon.com/ebs/latest/userguide/nvme-reservations.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/ebs/latest/userguide/nvme-reservations.html</a></p><h2 id="How-to-identify-reservation-capability"><a href="#How-to-identify-reservation-capability" class="headerlink" title="How to identify reservation capability"></a>How to identify reservation capability</h2><p><img src="/images/2025/06/001.png" alt></p><p><img src="/images/2025/06/002.png" alt></p><h2 id="Reservation-Register"><a href="#Reservation-Register" class="headerlink" title="Reservation Register"></a>Reservation Register</h2><p>The Reservation Register command is used to <u>register</u>, <u>unregister</u>, or <u>replace</u> a reservation key.</p><h2 id="Reservation-Report"><a href="#Reservation-Report" class="headerlink" title="Reservation Report"></a>Reservation Report</h2><p>The Reservation Report command returns a Reservation Status data structure to memory that describes the registration and reservation status of a namespace.</p><h2 id="Reservation-Acquire"><a href="#Reservation-Acquire" class="headerlink" title="Reservation Acquire"></a>Reservation Acquire</h2><p>The Reservation Acquire command is used to acquire a reservation on a namespace, preempt a reservation held on a namespace, and abort a reservation held on a namespace.</p><p>要想获得对Namespace的全部访问权限，Host需要下发Reservation Acquire命令.</p><p>一个Namespace同时只能接收一个Reservation，在Host A已经占用namespace时，如果Host B发送Reservation Acquire命令，该命令会被SSD abort。</p><p><img src="/images/2025/06/007.png" alt></p><p><img src="/images/2025/06/008.png" alt></p><p>在NVMe里，有3类角色：</p><ul><li>Reservation Holder – 当前获得namespace使用权Host</li><li>Registrant – 所有获得Reservation Key的Host</li><li>Non-Registrant – 其他Host</li></ul><p>Reservation Holder，有6种Reservation模式:</p><ul><li>Write Exclusive — 除了Reservation Holder，其他Host都不能写该Namespace</li><li>Exclusive Access — 除了Reservation Holder，其他Host都不能访问该Namespace</li><li>Write Exclusive – Registrant Only — 除了Reservation Holder和一个Registrant，其他Host都不能写该Namespace</li><li>Exclusive Access – Registrant Only –除了Reservation Holder和一个Registrant，其他Host都不能访问该Namespace</li><li>Write Exclusive – All Registrant Only –除了Reservation Holder和Registrants，其他Host不能访问该Namespace</li><li>Exclusive Access – All Registrant Only –除了Reservation Holder和Registrants，其他Host不能访问该Namespace</li></ul><p>在Host A是Reservation Holder的情况下，Host B也有方式把namespace的使用权夺过来，具体方式是下发Reservation Acquire命令, 把Reservation Acquire Action字段设置为001b，同时在Current Reservation Key字段设置正确的Key。只要当前Host A的reservation type不是”Write Exclusive – All Registrants”或者“Exclusive Access – All Registrants”, SSD就会注销Host A，释放其使用权，并将Host B设置为新的Reservation Holder。</p><h2 id="Reservation-Release"><a href="#Reservation-Release" class="headerlink" title="Reservation Release"></a>Reservation Release</h2><p>The Reservation Release command is used to release or clear a reservation held on a namespace.</p><p>Host A即使被Host B抢走了Namespace的使用权，但是只要保持了Registrant身份，仍然可以下发Reservation Release命令，将Reservation Release Action字段设置为001，同时在Current Reservation Key字段设置正确的Key，就可以将所有注册为该Namespace registrant的Host全部注销掉。</p><h2 id="Reservation-Notification"><a href="#Reservation-Notification" class="headerlink" title="Reservation Notification"></a>Reservation Notification</h2><p><img src="/images/2025/06/004.png" alt></p><p>在registration preempted, reservation released, and reservation preempted这三种情况下，如果没有禁止掉Reservation Notification的话，设备就会基于Admin queue的AER(Async Event Request)发送Reservation Notification，告诉driver发生了registration preempted, reservation released, or reservation preempted事件。</p><p>Reservation Notification AER的相关描述:<br><img src="/images/2025/06/005.png" alt></p><p><img src="/images/2025/06/006.png" alt></p><hr><p>参考资料:</p><ol><li>NVMe spec 1.4</li><li><a href="https://www.cnblogs.com/FireLife-Cheng/p/16248247.html" target="_blank" rel="noopener">NVM Feature— Reservation(NVME 学习笔记五)</a></li><li><a href="http://www.ssdfans.com/?p=97147" target="_blank" rel="noopener">蛋蛋读剩的NVMe之一：NVMe Reservation</a></li><li><a href="https://help.aliyun.com/zh/ecs/user-guide/enable-multi-attach" target="_blank" rel="noopener">通过多重挂载功能将单块云盘挂载至多台ECS实例</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于NVMe共享盘，是通过NVMe reservation机制来实现的。可以将NVMe reservation机制类比于读写锁。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>深入理解virtio packed virtqueue机制</title>
    <link href="http://liujunming.github.io/2025/06/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3virtio-packed-virtqueue%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2025/06/01/深入理解virtio-packed-virtqueue机制/</id>
    <published>2025-06-01T01:02:37.000Z</published>
    <updated>2025-06-01T07:57:29.074Z</updated>
    
    <content type="html"><![CDATA[<p>本文将基于第一性原理，深入解析virtio packed virtqueue机制。<a id="more"></a></p><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><p>读者需要对split virtqueue有深刻的理解。 </p><h2 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h2><ol><li>本文将virtio spec中标准的”Driver Ring Wrap Counter”统称为avail_wrap_counter</li><li>本文将virtio spec中标准的”Device Ring Wrap Counter”统称为used_wrap_counter</li></ol><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h3 id="For-software-backends"><a href="#For-software-backends" class="headerlink" title="For software backends"></a>For software backends</h3><ul><li>Bad cache utilization, several cache misses per request<ul><li>metadata is scattered into several places </li><li>descriptor chain is not contiguous in memory</li><li>cache contention in many places</li></ul></li></ul><h3 id="For-hardware-implementation"><a href="#For-hardware-implementation" class="headerlink" title="For hardware implementation"></a>For hardware implementation</h3><ul><li>several PCIe transactions per descriptor</li></ul><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>packed virtqueue将virtio1.0中的desc ring，avail ring，used ring三个ring打包成一个desc ring了。</p><p><img src="/images/2025/05/015.png" alt></p><h3 id="相对split-desc去掉了next字段"><a href="#相对split-desc去掉了next字段" class="headerlink" title="相对split desc去掉了next字段"></a>相对split desc去掉了next字段</h3><p>在split desc中next字段是记录一个desc chain中的下一个desc idx使用的，通常配合flags这样使用：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ((descs[idx].flags &amp; VRING_DESC_F_NEXT) == <span class="number">1</span>)</span><br><span class="line">        nextdesc = descs[ descs[idx].next];</span><br></pre></td></tr></table></figure></p><p>但是在<strong>packed desc ring中一个desc chain一定是相邻的</strong>（可以理解为链表变为了数组），所以next字段就用不上了，上面获取nextdesc的方式可以转化为如下方式：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ((descs[idx].flags &amp; VRING_DESC_F_NEXT) == <span class="number">1</span>)</span><br><span class="line">        nextdesc = descs[++idx];</span><br></pre></td></tr></table></figure></p><h3 id="flags字段的变化"><a href="#flags字段的变化" class="headerlink" title="flags字段的变化"></a>flags字段的变化</h3><p>为了仅仅使用desc ring就能标记avail ring与used ring信息，packed virtqueue引入了avail_wrap_counter与used_wrap_counter这两个flag。相对split desc，flags字段仍然保留，但是其取值增加了，因为要把三个ring合一，每个desc就需要更多的信息表明身份（是used还是avail）。在原有flags的基础上增加了两个flag：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VRING_DESC_F_AVAIL          (1ULL &lt;&lt; 7)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VRING_DESC_F_USED (1ULL &lt;&lt; 15)</span></span><br></pre></td></tr></table></figure></p><p>avail desc：当desc flags关于VRING_DESC_F_AVAIL的设置和avail_wrap_counter同步，且VRING_DESC_F_USED的设置和avail_wrap_counter相反时，表示desc为avail desc。例如avail_wrap_counter为1时，flags应该设置VRING_DESC_F_AVAIL|~VRING_DESC_F_USED，当avail_wrap_counter为0时，flags应该设置~VRING_DESC_F_AVAIL|VRING_DESC_F_USED。</p><p>used desc：当desc flags关于VRING_DESC_F_USED的设置和used_wrap_counter同步，且VRING_DESC_F_AVAIL的设置也和used_wrap_counter同步时，表示desc为used desc。例如used_wrap_counter为1时，flags应该设置VRING_DESC_F_AVAIL|VRING_DESC_F_USED，当used_wrap_counter为0时，flags应该设置~VRING_DESC_F_AVAIL|~VRING_DESC_F_USED。</p><p>综上可以看出，avail desc的两个flag总是相反的（只能设置一个），而used desc的两个flag总是相同的，要么都设置，要么都不设置。</p><h3 id="相对split-desc增加了id字段"><a href="#相对split-desc增加了id字段" class="headerlink" title="相对split desc增加了id字段"></a>相对split desc增加了id字段</h3><p>这个id比较特殊，他是buffer id，注意不是desc的下标idx。</p><h3 id="核心要点"><a href="#核心要点" class="headerlink" title="核心要点"></a>核心要点</h3><p>last_used_idx &lt;= used_idx  &lt;= last_avail_idx &lt;= avail_idx</p><ol><li>avail_idx/avail_wrap_counter会由driver的局部变量维护</li><li>last_avail_idx/对应时刻的avail_wrap_counter会由device的局部变量维护</li><li>used_idx/used_wrap_counter会由device的局部变量维护</li><li>last_used_idx/对应时刻的used_wrap_counter会由driver的局部变量维护</li></ol><h2 id="Placing-Available-Buffers-Into-The-Descriptor-Ring"><a href="#Placing-Available-Buffers-Into-The-Descriptor-Ring" class="headerlink" title="Placing Available Buffers Into The Descriptor Ring"></a>Placing Available Buffers Into The Descriptor Ring</h2><p>For each buffer element, b:</p><ol><li>Get the next descriptor table entry, d</li><li>Get the next free buffer id value</li><li>Set <code>d.addr</code> to the physical address of the start of b</li><li>Set <code>d.len</code> to the length of b.</li><li>Set <code>d.id</code> to the buffer id</li><li>Calculate the flags as follows:<ol><li>If b is device-writable, set the VIRTQ_DESC_F_WRITE bit to 1, otherwise 0</li><li>Set the VIRTQ_DESC_F_AVAIL bit to the current value of the Driver Ring Wrap Counter</li><li>Set the VIRTQ_DESC_F_USED bit to inverse value</li></ol></li><li>Perform a memory barrier to ensure that the descriptor has been initialized</li><li>Set <code>d.flags</code> to the calculated flags value</li><li>If d is the last descriptor in the ring, toggle the Driver Ring Wrap Counter</li><li>Otherwise, increment d to point at the next descriptor</li></ol><p>This makes a single descriptor buffer available. However, in general the driver MAY make use of a batch of descriptors as part of a single request. In that case, <u>it defers updating the descriptor flags for the first descriptor (and the previous memory barrier) until after the rest of the descriptors have been initialized</u>.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Note: vq-&gt;avail_wrap_count is initialized to 1 */</span> </span><br><span class="line"><span class="comment">/* Note: vq-&gt;sgs is an array same size as the ring */</span> </span><br><span class="line"> </span><br><span class="line">id = alloc_id(vq); </span><br><span class="line"> </span><br><span class="line">first = vq-&gt;next_avail; </span><br><span class="line">sgs = <span class="number">0</span>; </span><br><span class="line"><span class="keyword">for</span> (each buffer element b) &#123;</span><br><span class="line">        sgs++; </span><br><span class="line"> </span><br><span class="line">        vq-&gt;desc[vq-&gt;next_avail].address = get_addr(b); </span><br><span class="line">        vq-&gt;desc[vq-&gt;next_avail].len = get_len(b); </span><br><span class="line"> </span><br><span class="line">        avail = vq-&gt;avail_wrap_count ? VIRTQ_DESC_F_AVAIL : <span class="number">0</span>; </span><br><span class="line">        used = !vq-&gt;avail_wrap_count ? VIRTQ_DESC_F_USED : <span class="number">0</span>; </span><br><span class="line">        f = get_flags(b) | avail | used; </span><br><span class="line">        <span class="keyword">if</span> (b is <span class="keyword">not</span> the last buffer element) &#123; </span><br><span class="line">                f |= VIRTQ_DESC_F_NEXT; </span><br><span class="line">        &#125; </span><br><span class="line"></span><br><span class="line">        vq-&gt;desc[vq-&gt;next_avail].id = id; </span><br><span class="line"> </span><br><span class="line">        <span class="comment">/* Don’t mark the 1st descriptor available until all of them are ready. */</span> </span><br><span class="line">        <span class="keyword">if</span> (vq-&gt;next_avail == first) &#123; </span><br><span class="line">                flags = f; </span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">                vq-&gt;desc[vq-&gt;next_avail].flags = f;</span><br><span class="line">                </span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        vq-&gt;next_avail++; </span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> (vq-&gt;next_avail &gt;= vq-&gt;size) &#123; </span><br><span class="line">                vq-&gt;next_avail = <span class="number">0</span>; </span><br><span class="line">                vq-&gt;avail_wrap_count \^= <span class="number">1</span>; </span><br><span class="line">        &#125; </span><br><span class="line">&#125; </span><br><span class="line">vq-&gt;sgs[id] = sgs; </span><br><span class="line">write_memory_barrier(); </span><br><span class="line">vq-&gt;desc[first].flags = flags; </span><br><span class="line"> </span><br><span class="line">memory_barrier(); </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> (vq-&gt;device_event.flags != RING_EVENT_FLAGS_DISABLE) &#123; </span><br><span class="line">        notify_device(vq); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Don’t mark the 1st descriptor available until all of them are ready<br>原因: The driver always makes the first descriptor in the list available after the rest of the list has been written out into the ring. This guarantees that the device will never observe a partial scatter/gather list in the ring.</p><h2 id="Device-writes-used-descriptor"><a href="#Device-writes-used-descriptor" class="headerlink" title="Device writes used descriptor"></a>Device writes used descriptor</h2><blockquote><p>The device only writes out a single used descriptor for the whole list. It then skips forward according to the number of descriptors in the list. The driver needs to keep track of the size of the list corresponding to each buffer ID, to be able to skip to where the next used descriptor is written by the device.</p></blockquote><p>When the device has finished processing the buffer, it writes a used device descriptor including the Buffer ID into the Descriptor Ring (overwriting a driver descriptor previously made available), and sends a used event notification.</p><p>对于一个Descriptor chain，device仅仅往第一个desc中写入used_wrap_counter，而不是往Descriptor chain中的所有desc中写入used_wrap_counter。这样在hardware实现的packed virtqueue中，可以减少设备侧的TLP交互次数。</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>接下来的案例以一个desc table大小为4的vring为例，展示packed virtqueue的流程与相关rule的原因。Flag中的A表示VIRTQ_DESC_F_AVAIL，U表示VIRTQ_DESC_F_USED，N表示VIRTQ_DESC_F_NEXT。</p><h3 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h3><p>step1: 初始化<br><img src="/images/2025/05/017.png" alt></p><p>step2: driver生产Available Buffers<br><img src="/images/2025/05/018.png" alt></p><p>step3: device拉取Available Buffers<br><img src="/images/2025/05/019.png" alt></p><p>step4: device生产used Buffers<br><img src="/images/2025/05/020.png" alt><br>对于buffer id 0，device只更新了desc 0的flag(A|U)。</p><p>step5: driver消费used Buffers<br><img src="/images/2025/05/021.png" alt><br>driver知道buffer id 0的size为3，因此更新last_used_idx为3。VIRTQ_DESC_F_NEXT is reserved in used descriptors, and should be ignored by drivers.</p><p>step6: driver生产Available Buffers<br><img src="/images/2025/05/022.png" alt></p><p>step7: device拉取Available Buffers<br><img src="/images/2025/05/023.png" alt><br>基于avail_wrap_counter的值，device可以知道desc 1(flag为A|~U|N)为上一轮的avail desc，而非本轮的avail desc。</p><h3 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h3><p>step1: 初始化<br><img src="/images/2025/05/017.png" alt></p><p>step2: driver生产Available Buffers<br><img src="/images/2025/05/024.png" alt></p><p>step3: device拉取Available Buffers<br><img src="/images/2025/05/025.png" alt></p><p>step4: device生产used Buffers<br><img src="/images/2025/05/026.png" alt><br>device只更新了desc 0的flag(A|U)，更新了desc 0的id为1。In a used descriptor, Element Address is unused. Element Length specifies the length of the buffer that has been initialized (written to) by the device. Element Length is reserved for used descriptors without the VIRTQ_DESC_F_WRITE flag, and is ignored by drivers.</p><p>step5: driver消费used Buffers<br><img src="/images/2025/05/027.png" alt><br>driver知道buffer id 1的size为2，因此更新last_used_idx为2。</p><h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_packed_desc</span> &#123;</span></span><br><span class="line"><span class="comment">/* Buffer Address. */</span></span><br><span class="line">__le64 addr;</span><br><span class="line"><span class="comment">/* Buffer Length. */</span></span><br><span class="line">__le32 len;</span><br><span class="line"><span class="comment">/* Buffer ID. */</span></span><br><span class="line">__le16 id;</span><br><span class="line"><span class="comment">/* The flags depending on descriptor type. */</span></span><br><span class="line">__le16 flags;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><a href="https://elixir.bootlin.com/linux/v6.14/source/drivers/virtio/virtio_ring.c" target="_blank" rel="noopener">https://elixir.bootlin.com/linux/v6.14/source/drivers/virtio/virtio_ring.c</a><br><a href="https://elixir.bootlin.com/qemu/v8.0.0/source/hw/virtio/virtio.c" target="_blank" rel="noopener">https://elixir.bootlin.com/qemu/v8.0.0/source/hw/virtio/virtio.c</a></p><h3 id="driver往Descriptor-Ring生产Available-Buffers"><a href="#driver往Descriptor-Ring生产Available-Buffers" class="headerlink" title="driver往Descriptor Ring生产Available Buffers"></a>driver往Descriptor Ring生产Available Buffers</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_virtqueue_packed</span> &#123;</span></span><br><span class="line"><span class="comment">/* Actual memory layout for this queue. */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> num;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_packed_desc</span> *<span class="title">desc</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_packed_desc_event</span> *<span class="title">driver</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_packed_desc_event</span> *<span class="title">device</span>;</span></span><br><span class="line">&#125; vring;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Driver ring wrap counter. */</span></span><br><span class="line"><span class="keyword">bool</span> avail_wrap_counter;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Avail used flags. */</span></span><br><span class="line">u16 avail_used_flags;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Index of the next avail descriptor. */</span></span><br><span class="line">u16 next_avail_idx;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_desc_state_packed</span> &#123;</span></span><br><span class="line"><span class="keyword">void</span> *data;<span class="comment">/* Data for callback. */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Indirect desc table and extra table, if any. These two will be</span></span><br><span class="line"><span class="comment"> * allocated together. So we won't stress more to the memory allocator.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_packed_desc</span> *<span class="title">indir_desc</span>;</span></span><br><span class="line">u16 num;<span class="comment">/* Descriptor list length. */</span></span><br><span class="line">u16 last;<span class="comment">/* The last desc state in a list. */</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">virtqueue_add_packed</span><span class="params">(struct virtqueue *_vq,</span></span></span><br><span class="line"><span class="function"><span class="params">       struct scatterlist *sgs[],</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">unsigned</span> <span class="keyword">int</span> total_sg,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">unsigned</span> <span class="keyword">int</span> out_sgs,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">unsigned</span> <span class="keyword">int</span> in_sgs,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">void</span> *data,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">void</span> *ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">bool</span> premapped,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">gfp_t</span> gfp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        ...</span><br><span class="line">head = vq-&gt;packed.next_avail_idx;</span><br><span class="line">avail_used_flags = vq-&gt;packed.avail_used_flags;</span><br><span class="line"></span><br><span class="line">WARN_ON_ONCE(total_sg &gt; vq-&gt;packed.vring.num &amp;&amp; !vq-&gt;indirect);</span><br><span class="line"></span><br><span class="line">desc = vq-&gt;packed.vring.desc;</span><br><span class="line">i = head;</span><br><span class="line">descs_used = total_sg;</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">id = vq-&gt;free_head;</span><br><span class="line">BUG_ON(id == vq-&gt;packed.vring.num);</span><br><span class="line"></span><br><span class="line">curr = id;</span><br><span class="line">c = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (n = <span class="number">0</span>; n &lt; out_sgs + in_sgs; n++) &#123;</span><br><span class="line"><span class="keyword">for</span> (sg = sgs[n]; sg; sg = sg_next(sg)) &#123;</span><br><span class="line"><span class="keyword">dma_addr_t</span> addr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vring_map_one_sg(vq, sg, n &lt; out_sgs ?</span><br><span class="line">     DMA_TO_DEVICE : DMA_FROM_DEVICE,</span><br><span class="line">     &amp;addr, &amp;len, premapped))</span><br><span class="line"><span class="keyword">goto</span> unmap_release;</span><br><span class="line"></span><br><span class="line">flags = cpu_to_le16(vq-&gt;packed.avail_used_flags |</span><br><span class="line">    (++c == total_sg ? <span class="number">0</span> : VRING_DESC_F_NEXT) |</span><br><span class="line">    (n &lt; out_sgs ? <span class="number">0</span> : VRING_DESC_F_WRITE));</span><br><span class="line"><span class="keyword">if</span> (i == head)</span><br><span class="line">head_flags = flags;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">desc[i].flags = flags;</span><br><span class="line"></span><br><span class="line">desc[i].addr = cpu_to_le64(addr);</span><br><span class="line">desc[i].len = cpu_to_le32(len);</span><br><span class="line">desc[i].id = cpu_to_le16(id);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (unlikely(vq-&gt;use_dma_api)) &#123;</span><br><span class="line">vq-&gt;packed.desc_extra[curr].addr = premapped ?</span><br><span class="line">DMA_MAPPING_ERROR : addr;</span><br><span class="line">vq-&gt;packed.desc_extra[curr].len = len;</span><br><span class="line">vq-&gt;packed.desc_extra[curr].flags =</span><br><span class="line">le16_to_cpu(flags);</span><br><span class="line">&#125;</span><br><span class="line">prev = curr;</span><br><span class="line">curr = vq-&gt;packed.desc_extra[curr].next;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ((unlikely(++i &gt;= vq-&gt;packed.vring.num))) &#123;</span><br><span class="line">i = <span class="number">0</span>;</span><br><span class="line">vq-&gt;packed.avail_used_flags ^=</span><br><span class="line"><span class="number">1</span> &lt;&lt; VRING_PACKED_DESC_F_AVAIL |</span><br><span class="line"><span class="number">1</span> &lt;&lt; VRING_PACKED_DESC_F_USED;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (i &lt;= head)</span><br><span class="line">vq-&gt;packed.avail_wrap_counter ^= <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* We're using some buffers from the free list. */</span></span><br><span class="line">vq-&gt;vq.num_free -= descs_used;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Update free pointer */</span></span><br><span class="line">vq-&gt;packed.next_avail_idx = i;</span><br><span class="line">vq-&gt;free_head = curr;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Store token. */</span></span><br><span class="line">vq-&gt;packed.desc_state[id].num = descs_used;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，<code>vq-&gt;packed.avail_wrap_counter</code>是driver侧维护的avail_wrap_counter，<code>vq-&gt;packed.next_avail_idx</code>是driver侧维护的avail_idx，<code>vq-&gt;packed.desc_state[id].num</code>是driver侧维护的size of the list corresponding to buffer <code>id</code>。</p><h3 id="device从Descriptor-Ring消费Available-Buffers"><a href="#device从Descriptor-Ring消费Available-Buffers" class="headerlink" title="device从Descriptor Ring消费Available Buffers"></a>device从Descriptor Ring消费Available Buffers</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> *<span class="title">virtqueue_packed_pop</span><span class="params">(VirtQueue *vq, <span class="keyword">size_t</span> sz)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    vq-&gt;last_avail_idx += elem-&gt;ndescs;</span><br><span class="line">    vq-&gt;inuse += elem-&gt;ndescs;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (vq-&gt;last_avail_idx &gt;= vq-&gt;vring.num) &#123;</span><br><span class="line">        vq-&gt;last_avail_idx -= vq-&gt;vring.num;</span><br><span class="line">        vq-&gt;last_avail_wrap_counter ^= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，<code>vq-&gt;last_avail_idx</code>是device侧维护的last_avail_idx，<code>vq-&gt;last_avail_wrap_counter</code>是device侧维护的last_avail_idx时刻对应的avail_wrap_counter。</p><h3 id="device往Descriptor-Ring生产used-Buffers"><a href="#device往Descriptor-Ring生产used-Buffers" class="headerlink" title="device往Descriptor Ring生产used Buffers"></a>device往Descriptor Ring生产used Buffers</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">virtqueue_packed_flush</span><span class="params">(VirtQueue *vq, <span class="keyword">unsigned</span> <span class="keyword">int</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    vq-&gt;inuse -= ndescs;</span><br><span class="line">    vq-&gt;used_idx += ndescs;</span><br><span class="line">    <span class="keyword">if</span> (vq-&gt;used_idx &gt;= vq-&gt;vring.num) &#123;</span><br><span class="line">        vq-&gt;used_idx -= vq-&gt;vring.num;</span><br><span class="line">        vq-&gt;used_wrap_counter ^= <span class="number">1</span>;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，<code>vq-&gt;used_idx</code>是device侧维护的used_idx，<code>vq-&gt;used_wrap_counter</code>是device侧维护的used_wrap_counter。</p><h3 id="driver从Descriptor-Ring消费used-Buffers"><a href="#driver从Descriptor-Ring消费used-Buffers" class="headerlink" title="driver从Descriptor Ring消费used Buffers"></a>driver从Descriptor Ring消费used Buffers</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> *<span class="title">virtqueue_get_buf_ctx_packed</span><span class="params">(struct virtqueue *_vq,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">unsigned</span> <span class="keyword">int</span> *len,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">void</span> **ctx)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vring_virtqueue</span> *<span class="title">vq</span> = <span class="title">to_vvq</span>(_<span class="title">vq</span>);</span></span><br><span class="line">u16 last_used, id, last_used_idx;</span><br><span class="line"><span class="keyword">bool</span> used_wrap_counter;</span><br><span class="line"><span class="keyword">void</span> *ret;</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Only get used elements after they have been exposed by host. */</span></span><br><span class="line">virtio_rmb(vq-&gt;weak_barriers);</span><br><span class="line"></span><br><span class="line">last_used_idx = READ_ONCE(vq-&gt;last_used_idx);</span><br><span class="line">used_wrap_counter = packed_used_wrap_counter(last_used_idx);</span><br><span class="line">last_used = packed_last_used(last_used_idx);</span><br><span class="line">id = le16_to_cpu(vq-&gt;packed.vring.desc[last_used].id);</span><br><span class="line">*len = le32_to_cpu(vq-&gt;packed.vring.desc[last_used].len);</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="comment">/* detach_buf_packed clears data, so grab it now. */</span></span><br><span class="line">ret = vq-&gt;packed.desc_state[id].data;</span><br><span class="line">detach_buf_packed(vq, id, ctx);</span><br><span class="line"></span><br><span class="line">last_used += vq-&gt;packed.desc_state[id].num;</span><br><span class="line"><span class="keyword">if</span> (unlikely(last_used &gt;= vq-&gt;packed.vring.num)) &#123;</span><br><span class="line">last_used -= vq-&gt;packed.vring.num;</span><br><span class="line">used_wrap_counter ^= <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">last_used = (last_used | (used_wrap_counter &lt;&lt; VRING_PACKED_EVENT_F_WRAP_CTR));</span><br><span class="line">WRITE_ONCE(vq-&gt;last_used_idx, last_used);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Packed virtqueues support up to 2^15 entries each.</p><p><img src="/images/2025/05/016.png" alt></p><p>其中，<code>vq-&gt;last_used_idx</code>的第14~第0 bit为driver侧维护的last_used_idx，<code>vq-&gt;last_used_idx</code>的第15 bit为driver侧维护的last_used_idx时刻对应的used_wrap_counter。</p><h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><ol><li>Packed virtqueues支持2^15 entries;</li><li>每个packed virtqueue 有三部分构成：<ul><li>Descriptor Ring</li><li>Driver Event Suppression：后端（device）只读，用来控制后端向前端（driver）的通知（used notifications）</li><li>Device Event Suppression：前端（driver）只读，用来控制前端向后端（device）的通知（avail notifications）</li></ul></li><li>Write Flag，VIRTQ_DESC_F_WRITE<ul><li>对于avail desc这个flag用来标记其关联的buffer是只读的还是只写的；</li><li>对于used desc这个flag用来表示去关联的buffer是否有被后端（device）写入数据；</li></ul></li><li>desc中的len<ul><li>对于avail desc，len表示desc关联的buffer中被写入的数据长度；</li><li>对于uesd desc，当VIRTQ_DESC_F_WRITE被设置时，len表示后端（device）写入数据的长度，当VIRTQ_DESC_F_WRITE没有被设置时，len没有意义；</li></ul></li><li>Descriptor Chain<br>VIRTQ_DESC_F_NEXT在used desc中是没有意义的</li></ol><hr><p>参考资料:</p><ol><li><a href="http://blog.chinaunix.net/uid-28541347-id-5819237.html" target="_blank" rel="noopener">从dpdk1811看virtio1.1 的实现—packed ring</a></li><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html" target="_blank" rel="noopener">Virtual I/O Device (VIRTIO) Version 1.1</a></li><li><a href="https://archive.fosdem.org/2018/schedule/event/virtio/attachments/slides/2167/export/events/attachments/virtio/slides/2167/fosdem_virtio1_1.pdf" target="_blank" rel="noopener">What’s new in Virtio 1.1? by Jens Freimann</a></li><li><a href="https://www.youtube.com/watch?v=_UNuDBgI5VI" target="_blank" rel="noopener">What’s New in Virtio 1.1 - Jason Wang</a></li><li><a href="https://www.redhat.com/en/blog/packed-virtqueue-how-reduce-overhead-virtio" target="_blank" rel="noopener">Packed virtqueue: How to reduce overhead with virtio</a></li><li><a href="https://blog.csdn.net/faxiang1230/article/details/120210082" target="_blank" rel="noopener">virtio系列-packed virtqueue</a></li><li><a href="https://github.com/oasis-tcs/virtio-spec/issues/3" target="_blank" rel="noopener">add packed ring layout support</a></li><li><a href="https://lists.oasis-open.org/archives/virtio/201803/msg00041.html" target="_blank" rel="noopener">[PATCH v10 00/13] packed ring layout spec</a></li><li><a href="https://mp.weixin.qq.com/s/Q-ikJ3UbOvWNgDswVO0KSQ" target="_blank" rel="noopener">Virtio I/O 虚拟化（二）：Packed Virtqueue</a></li><li><a href="https://mp.weixin.qq.com/s/h_LTJiscJ17cKPQLNCCcgA" target="_blank" rel="noopener">异步模式下的 Vhost Packed Ring 设计介绍</a></li><li><a href="https://lore.kernel.org/all/20181121100330.24846-1-tiwei.bie@intel.com/" target="_blank" rel="noopener">[virtio-dev] [PATCH net-next v3 00/13] virtio: support packed ring</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将基于第一性原理，深入解析virtio packed virtqueue机制。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe verify command</title>
    <link href="http://liujunming.github.io/2025/05/18/Notes-about-NVMe-verify-command/"/>
    <id>http://liujunming.github.io/2025/05/18/Notes-about-NVMe-verify-command/</id>
    <published>2025-05-18T09:32:00.000Z</published>
    <updated>2025-05-18T10:29:41.453Z</updated>
    
    <content type="html"><![CDATA[<p>nvme verify命令就相当于大致执行了read命令，但是不会将disk内容传输到host，controller只做数据integrity校验，然后将结果告诉host。<a id="more"></a></p><p>Verify command can <strong>check the integrity of stored data and metadata</strong>.  The Verify command is roughly equivalent to a Read command that discards the data and metadata after reading. The important behavior of Verify is that <u>errors are reported </u>if the data or metadata cannot be read without expending the time and resources required to return the data and metadata to the host.</p><p>The Verify command is simple: it does everything a normal read command does, except for returning the data to the host system. If a read command would return an error, a verify command will return the same error. If a read command would be successful, a verify command will be as well. This makes it possible to do a low-level scrub of the stored data without being bottlenecked by the host interface bandwidth.</p><p><img src="/images/2025/05/014.png" alt></p><blockquote><p>The Verify command verifies integrity of stored information by reading data and metadata, if applicable, for the LBAs indicated without transferring any data or metadata to the host. A Verify operation consists of the controller actions (e.g., reading) that verify integrity of stored information during execution of a Verify command.</p></blockquote><p>总结下:在没有verify command之前，如果想做数据integrity的校验，那么就需要下发read command，此时会涉及到host interface bandwidth等资源的消耗，开销较大；而verify command可以获取到数据integrity校验结果，但是无需像read command那样消耗资源。</p><hr><p>参考资料:</p><ol><li><a href="https://nvmexpress.org/changes-in-nvme-revision-1-4/" target="_blank" rel="noopener">Changes in NVMe Revision 1.4</a></li><li><a href="https://www.anandtech.com/show/14543/nvme-14-specification-published" target="_blank" rel="noopener">NVMe 1.4 Specification Published: Further Optimizing Performance and Reliability</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;nvme verify命令就相当于大致执行了read命令，但是不会将disk内容传输到host，controller只做数据integrity校验，然后将结果告诉host。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe dataset management</title>
    <link href="http://liujunming.github.io/2025/05/18/Notes-about-NVMe-dataset-management/"/>
    <id>http://liujunming.github.io/2025/05/18/Notes-about-NVMe-dataset-management/</id>
    <published>2025-05-18T01:52:35.000Z</published>
    <updated>2025-05-18T02:22:58.342Z</updated>
    
    <content type="html"><![CDATA[<p>NVMe dataset management就是driver给controller的hints(接下来的访问模式)，为此controller可以做些优化。<a id="more"></a></p><p><img src="/images/2025/05/012.png" alt></p><p><img src="/images/2025/05/013.png" alt></p><p>上图中的potion替换为portion</p><p><img src="/images/2025/05/011.png" alt></p><blockquote><p>The Dataset Management command is used by the host to indicate attributes for ranges of logical blocks. This includes attributes like frequency that data is read or written, access size, and other information that may be used to optimize performance and reliability. This command is advisory; a compliant controller may choose to take no action based on information provided.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://files.futurememorystorage.com/proceedings/2013/20130812_PreConfD_Marks.pdf" target="_blank" rel="noopener">An NVM Express Tutorial</a></li><li>NVMe spec</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NVMe dataset management就是driver给controller的hints(接下来的访问模式)，为此controller可以做些优化。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PGO &amp;&amp; HW-PGO</title>
    <link href="http://liujunming.github.io/2025/05/17/Notes-about-HW-PGO/"/>
    <id>http://liujunming.github.io/2025/05/17/Notes-about-HW-PGO/</id>
    <published>2025-05-16T23:46:59.000Z</published>
    <updated>2025-05-15T00:27:56.854Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PGO(Profile-Guided Optimization)和HW-PGO(Hardware Profile-Guided Optimization)的相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Profile Guided Optimization (PGO) 是一种编译器优化技术，通过收集程序实际运行时的性能数据（Profile），指导编译器生成更高效的代码。其核心思想是“用实际运行行为指导优化”，相比传统的静态分析优化，PGO 能显著提升程序性能并减少代码体积。</p><h2 id="PGO-的工作原理"><a href="#PGO-的工作原理" class="headerlink" title="PGO 的工作原理"></a>PGO 的工作原理</h2><p>PGO 的实现通常分为三个阶段：</p><h3 id="插桩阶段（Instrumentation）"><a href="#插桩阶段（Instrumentation）" class="headerlink" title="插桩阶段（Instrumentation）"></a>插桩阶段（Instrumentation）</h3><p>编译器在代码中插入统计探针（Probes），生成带有数据采集功能的“插桩版本”程序。例如：</p><ul><li>统计函数调用频率</li><li>记录分支（if/else）的执行路径</li><li>跟踪循环迭代次数</li><li>监控热点代码区域（Hot Code）</li></ul><h3 id="数据收集阶段（Profiling）"><a href="#数据收集阶段（Profiling）" class="headerlink" title="数据收集阶段（Profiling）"></a>数据收集阶段（Profiling）</h3><p>运行插桩后的程序，模拟真实场景（如高负载、典型输入数据），生成运行时性能数据文件（如 <code>.profraw</code> 或 <code>.gcda</code> 文件）。</p><h3 id="优化阶段（Optimization）"><a href="#优化阶段（Optimization）" class="headerlink" title="优化阶段（Optimization）"></a>优化阶段（Optimization）</h3><p>编译器基于收集到的 Profile 数据重新编译代码，针对性优化：</p><ul><li>代码布局优化：将高频执行的代码段集中存放，提升指令缓存命中率。</li><li>分支预测优化：根据分支实际执行概率，优化条件判断顺序（如将高概率分支前置）。</li><li>函数内联（Inlining）：对高频调用的小函数进行内联，减少调用开销。</li><li>死代码消除：移除从未执行过的代码路径。</li><li>寄存器分配优化：优先为热点代码分配寄存器。</li></ul><h2 id="PGO-的优势"><a href="#PGO-的优势" class="headerlink" title="PGO 的优势"></a>PGO 的优势</h2><ol><li><p>性能提升<br>通过精准优化热点代码，典型场景下性能提升可达 10%-30%，尤其在分支密集或缓存敏感的代码中效果显著。<br>示例：<br>一个循环若实际运行中迭代次数固定，PGO 可将其展开为确定次数的指令，避免动态判断开销。</p></li><li><p>代码体积减少<br>移除未使用的代码路径，减小可执行文件大小。</p></li><li><p>优化更精准<br>静态优化可能因缺乏运行时信息而保守决策，PGO 则依赖真实数据，避免“过度优化”或“优化错误路径”。</p></li></ol><h2 id="PGO-的局限性"><a href="#PGO-的局限性" class="headerlink" title="PGO 的局限性"></a>PGO 的局限性</h2><ol><li><p>额外步骤与时间成本<br>需经历插桩、运行测试、二次编译，增加开发流程复杂度。</p></li><li><p>依赖场景代表性<br>若测试数据（Profile）不能覆盖真实场景，可能导致优化方向错误。<br>示例：<br>若测试时未触发某个分支，优化后可能错误删除该路径代码。</p></li><li><p>动态负载适应能力有限<br>对运行时行为变化剧烈的程序（如实时系统），静态 Profile 可能无法适应动态变化。</p></li></ol><h2 id="PGO-的应用场景"><a href="#PGO-的应用场景" class="headerlink" title="PGO 的应用场景"></a>PGO 的应用场景</h2><ul><li>高性能计算（HPC）：优化科学计算中的核心算法循环。</li><li>游戏引擎：提升渲染管线和物理模拟的性能。</li><li>数据库系统：加速查询执行计划的热点操作。</li><li>编译器自身优化：如 LLVM、GCC 使用 PGO 优化自身编译速度。</li></ul><h2 id="与传统优化的对比"><a href="#与传统优化的对比" class="headerlink" title="与传统优化的对比"></a>与传统优化的对比</h2><p><img src="/images/2025/05/008.png" alt></p><h2 id="硬件辅助PGO（如Intel-HW-PGO）"><a href="#硬件辅助PGO（如Intel-HW-PGO）" class="headerlink" title="硬件辅助PGO（如Intel HW-PGO）"></a>硬件辅助PGO（如Intel HW-PGO）</h2><p>传统 PGO 依赖软件插桩，而 硬件辅助 PGO 通过处理器内置的性能监控单元（PMU）直接采集硬件事件（如缓存未命中、分支预测失败），无需插桩即可生成更精细的 Profile。优势包括：</p><ul><li>更低开销：硬件级数据采集不影响程序性能。</li><li>更细粒度：可监控底层硬件行为（如指令级并行度）。</li></ul><p><img src="/images/2025/05/009.png" alt><br>这些技术允许在硬件（HW）中以更低的开销采集样本（可能一次性采集多个），并提供其他优势，例如减少采样偏移（Reduced-Skid）、精确分布（Precise Distribution）以及数据地址追踪（Data Address）。</p><p><img src="/images/2025/05/010.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>PGO 通过”用数据驱动优化”，在关键场景中显著提升程序效率，尤其适合性能敏感型应用。硬件辅助 PGO 进一步降低了数据采集成本，代表了编译器与硬件协同优化的未来趋势。</p><hr><p>参考资料:</p><ol><li>deepseek</li><li><a href="https://llvm.org/devmtg/2024-04/slides/TechnicalTalks/Xiao-EnablingHW-BasedPGO.pdf" target="_blank" rel="noopener">Enabling HW-based PGO for both Windows and Linux</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PGO(Profile-Guided Optimization)和HW-PGO(Hardware Profile-Guided Optimization)的相关notes。
    
    </summary>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="计算机系统" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes about IBGDA(InfiniBand GPUDirect Async)</title>
    <link href="http://liujunming.github.io/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/"/>
    <id>http://liujunming.github.io/2025/05/11/Notes-about-IBGDA-InfiniBand-GPUDirect-Async/</id>
    <published>2025-05-11T08:08:26.000Z</published>
    <updated>2025-05-11T12:20:09.729Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下IBGDA(InfiniBand GPUDirect Async)的相关notes。<a id="more"></a>内容主要转载自<a href="https://zhuanlan.zhihu.com/p/26082845081" target="_blank" rel="noopener">浅析DeepSeek中提到的IBGDA</a>。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><blockquote><p>Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency.</p></blockquote><p><a href="https://arxiv.org/pdf/2412.19437" target="_blank" rel="noopener">DeepSeek-V3 Technical Report</a></p><h2 id="Without-IBGDA"><a href="#Without-IBGDA" class="headerlink" title="Without IBGDA"></a>Without IBGDA</h2><p>在使用了GPU Direct RDMA的GPU中，网卡是怎么和GPU配合，实现将GPU的HBM的数据发送到远端的呢？</p><p><img src="/images/2025/05/006.png" alt></p><p>在引入InfiniBand GPUDirect Async(IBGDA)之前，是使用CPU上的代理线程来进行网络通信的。</p><p>此时流程是这样的：</p><ol><li>应用程序启动一个CUDA kernel，在GPU内存中产生数据</li><li>kernel function通过往CPU memory中的proxy buffer写入数据的方式，通知CPU要进行网络操作。我们将这个通知称为work descriptor, 它包含源地址、目标地址、数据大小及其他必要的网络信息</li><li>CPU上的proxy thread读取worker descriptor</li><li>CPU上的proxy thread发起相应的网络操作，将请求写入WQ</li><li>CPU会更新host memory中的doorbell record (DBR) buffer。（This buffer is used in the recovery path in case the NIC drops the write to its doorbell. 就是用来记录doorbell的信息，万一硬件来不及及时响应doorbell并把它丢掉，你还能从DBR buffer中恢复doorbell）</li><li>CPU通过写入NIC的 doorbell (DB)通知NIC。DB是NIC硬件中的一个寄存器</li><li>NIC从WQ中读取work descriptor</li><li>NIC使用GPUDirect RDMA直接从GPU内存搬运数据</li><li>NIC将数据传输到远程节点</li><li>NIC通过向主机内存中的CQ写入事件来指示网络操作已完成</li><li>CPU轮询CQ以检测网络操作的完成</li><li>CPU通知GPU操作已完成</li></ol><p>可以发现，这个过程竟然需要GPU, CPU, NIC三方参与。CPU就像是一个中转站，那么显然它有一些缺点：</p><ul><li>proxy thread消耗了CPU cycles</li><li>proxy thread成为瓶颈，导致在细粒度传输（小消息）时无法达到NIC的峰值吞吐。现代NIC每秒可以处理数亿个通信请求。GPU可以按照该速率生成请求，但CPU的处理速率低得多，造成了在细粒度通信时的瓶颈。</li></ul><h2 id="With-IBGDA"><a href="#With-IBGDA" class="headerlink" title="With IBGDA"></a>With IBGDA</h2><p>优化的方法显而易见，能否绕过CPU，让GPU自己来和网卡做交换呢？IBGDA给出了这样的功能。</p><p><img src="/images/2025/05/007.png" alt></p><ol><li>CPU程序启动一个CUDA kernel function，在GPU内存中生成数据</li><li>使用SM创建一个NIC work descriptor，并将其直接写入WQ。与CPU proxy thread不同，该WQ区位于GPU内存中</li><li>SM更新DBR buffer，它也位于GPU内存中</li><li>SM通过写入NIC的DB寄存器通知NIC</li><li>NIC使用GPUDirect RDMA从WQ读取工作描述符</li><li>NIC使用GPUDirect RDMA读取GPU内存中的数据</li><li>NIC将数据传输到远程节点</li><li>NIC通过使用GPUDirect RDMA向CQ缓冲区写入事件，通知GPU网络操作已完成</li></ol><p>可见，IBGDA消除了CPU在通信控制路径中的作用。在使用IBGDA时，GPU和NIC直接交换进行通信所需的信息。WQ和DBR buffer也被移到GPU内存中，以提高SM访问效率。</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/26082845081" target="_blank" rel="noopener">浅析DeepSeek中提到的IBGDA</a></li><li><a href="https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/" target="_blank" rel="noopener">Improving Network Performance of HPC Systems Using NVIDIA Magnum IO NVSHMEM and GPUDirect Async</a></li><li><a href="https://developer.nvidia.com/zh-cn/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/" target="_blank" rel="noopener">使用 NVIDIA Magnum IO NVSHMEM 和 GPUDirect Async 提高 HPC 系统的网络性能</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下IBGDA(InfiniBand GPUDirect Async)的相关notes。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
      <category term="RDMA" scheme="http://liujunming.github.io/tags/RDMA/"/>
    
  </entry>
  
</feed>
