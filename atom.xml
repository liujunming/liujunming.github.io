<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2018-12-17T13:15:13.770Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>apue 读书笔记-File I/O</title>
    <link href="http://liujunming.github.io/2018/12/17/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-File-I-O/"/>
    <id>http://liujunming.github.io/2018/12/17/apue-读书笔记-File-I-O/</id>
    <published>2018-12-17T01:15:52.000Z</published>
    <updated>2018-12-17T13:15:13.770Z</updated>
    
    <content type="html"><![CDATA[<p>The functions described in this chapter are often referred to as <em>unbuffered I/O</em>.The term <em>unbuffered</em> means that each <code>read</code> or <code>write</code> invokes a system call in the kernel.<a id="more"></a> </p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><h3 id="1-1-File-Descriptors"><a href="#1-1-File-Descriptors" class="headerlink" title="1.1 File Descriptors"></a>1.1 File Descriptors</h3><p>To the kernel, all open files are referred to by file descriptors. A file descriptor is a non-negative integer. When we open an existing file or create a new file, the kernel returns a file descriptor to the process.</p><p>By convention, UNIX System shells associate file descriptor 0 with the standard input of a process, file descriptor 1 with the standard output, and file descriptor 2 with the standard error.</p><p>Although their values are standardized by POSIX.1, the magic numbers 0, 1, and 2 should be replaced in POSIX-compliant applications with the symbolic constants <code>STDIN_FILENO</code>, <code>STDOUT_FILENO</code>, and <code>STDERR_FILENO</code> to improve readability.</p><h3 id="1-2-open-and-openat-Functions"><a href="#1-2-open-and-openat-Functions" class="headerlink" title="1.2 open and openat Functions"></a>1.2 <code>open</code> and <code>openat</code> Functions</h3><p>A file is opened or created by calling either the <code>open</code> function or the <code>openat</code> function.</p><h3 id="1-3-creat-Function"><a href="#1-3-creat-Function" class="headerlink" title="1.3 creat Function"></a>1.3 creat Function</h3><p>A new file can also be created by calling the <code>creat</code> function.</p><h3 id="1-4-close-Function"><a href="#1-4-close-Function" class="headerlink" title="1.4 close Function"></a>1.4 close Function</h3><p>An open file is closed by calling the close function.<br>When a process terminates, all of its open files are closed automatically by the kernel. Many programs take advantage of this fact and don’t explicitly close open files.</p><h3 id="1-5-lseek-Function"><a href="#1-5-lseek-Function" class="headerlink" title="1.5 lseek Function"></a>1.5 <code>lseek</code> Function</h3><p>Every open file has an associated <strong>current file offset</strong>, normally a non-negative integer that measures the number of bytes from the beginning of the file.<br>An open file’s offset can be set explicitly by calling <code>lseek</code>.</p><p><code>lseek</code> only records the current file offset within the kernel — it does not cause any I/O to take place. This offset is then used by the next read or write operation.<br>The file’s offset can be greater than the file’s current size, in which case the next write to the file will extend the file. This is referred to as creating a <em>hole</em> in a file and is allowed. Any bytes in a file that have not been written are read back as 0. A hole in a file isn’t required to have storage backing it on disk.</p><p>We use the <code>od</code> command to look at the contents of the file.</p><h3 id="1-6-read-Function"><a href="#1-6-read-Function" class="headerlink" title="1.6 read Function"></a>1.6 <code>read</code> Function</h3><p>Data is read from an open file with the <code>read</code> function.</p><h3 id="1-7-write-Function"><a href="#1-7-write-Function" class="headerlink" title="1.7 write Function"></a>1.7 <code>write</code> Function</h3><p>Data is written to an open file with the <code>write</code> function.</p><h2 id="2-I-O-Efficiency"><a href="#2-I-O-Efficiency" class="headerlink" title="2 I/O Efficiency"></a>2 I/O Efficiency</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"apue.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFSIZE 4096</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">char</span> buf[BUFFSIZE];</span><br><span class="line">    <span class="keyword">while</span> ((n = read(STDIN_FILENO, buf, BUFFSIZE)) &gt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (write(STDOUT_FILENO, buf, n) != n)</span><br><span class="line">            err_sys(<span class="string">"write error"</span>);</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">0</span>)</span><br><span class="line">            err_sys(<span class="string">"read error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Figure 3.6 shows the results for reading a 516,581,760-byte file, using 20 different buffer sizes.</p><center><img src="/images/2018/12/25.png" alt=""></center><p>This accounts for the minimum in the system time occurring at the few timing measurements starting around a BUFFSIZE of 4,096. Increasing the buffer size beyond this limit has little positive effect.</p><p>Most file systems support some kind of read-ahead to improve performance. When sequential reads are detected, the system tries to read in more data than an application requests, assuming that the application will read it shortly. The effect of read-ahead can be seen in Figure 3.6, where the elapsed time for buffer sizes as small as 32 bytes is as good as the elapsed time for larger buffer sizes.</p><h2 id="3-File-Sharing"><a href="#3-File-Sharing" class="headerlink" title="3 File Sharing"></a>3 File Sharing</h2><p>The UNIX System supports the sharing of open files among different processes.</p><ol><li>Every process has an entry in the process table. Within each process table entry is a table of open file descriptors, which we can think of as a vector, with one entry per descriptor. Associated with each file descriptor are:</li></ol><ul><li>The file descriptor flags </li><li>A pointer to a file table entry</li></ul><ol start="2"><li>The kernel maintains a file table for all open files. Each file table entry contains:</li></ol><ul><li>The file status flags for the file, such as read, write, append, sync, and nonblocking</li><li>The current file offset</li><li>A pointer to the v-node table entry for the file</li></ul><ol start="3"><li>Each open file (or device) has a v-node structure that contains information about the type of file and pointers to functions that operate on the file. For most files, the v-node also contains the i-node for the file. This information is read from disk when the file is opened, so that all the pertinent information about the file is readily available. For example, the i-node contains the owner of the file, the size of the file, pointers to where the actual data blocks for the file are located on disk, and so on.<pre><code>Linux has no v-node. Instead, a generic i-node structure is used.</code></pre></li></ol><p>Figure 3.7 shows a pictorial arrangement of these three tables for a single process that has two different files open: one file is open on standard input (file descriptor 0), and the other is open on standard output (file descriptor 1).</p><center><img src="/images/2018/12/26.png" alt=""></center><p>If two independent processes have the same file open, we could have the arrangement shown in Figure 3.8.</p><center><img src="/images/2018/12/27.png" alt=""></center><br>We assume here that the first process has the file open on descriptor 3 and that the second process has that same file open on descriptor 4. Each process that opens the file gets its own file table entry, but only a single v-node table entry is required for a given file. One reason each process gets its own file table entry is so that each process has its own current offset for the file.<br><br>## 4 File Sharing<br><br>## 4 Atomic Operations<br>### 4.1 Appending to a File<br><br>Consider a single process that wants to append to the end of a file.This works fine for a single process, but problems arise if multiple processes use this technique to append to the same file. (This scenario can arise if multiple instances of the same program are appending messages to a log file, for example.)<br><br>Assume that two independent processes, A and B, are appending to the same file. Each has opened the file but without the O_APPEND flag. This gives us the same picture as Figure 3.8. Each process has its own file table entry, but they share a single v-node table entry. Assume that process A does the lseek and that this sets the current offset for the file for process A to byte offset 1,500 (the current end of file). Then the kernel switches processes, and B continues running. Process B then does the lseek, which sets the current offset for the file for process B to byte offset 1,500 also (the current end of file). Then B calls write, which increments B’s current file offset for the file to 1,600. Because the file’s size has been extended, the kernel also updates the current file size in the v-node to 1,600. Then the kernel switches processes and A resumes. When A calls write, the data is written starting at the current file offset for A, which is byte offset 1,500. This overwrites the data that B wrote to the file.<br><br>The problem here is that our logical operation of ‘‘position to the end of file and write’’ requires two separate function calls (as we’ve shown it). The solution is to have the positioning to the current end of file and the write be an atomic operation with regard to other processes. Any operation that requires more than one function call cannot be atomic, as there is always the possibility that the kernel might temporarily suspend the process between the two function calls (as we assumed previously).<br><br>The UNIX System provides an atomic way to do this operation if we set the O_APPEND flag when a file is opened. As we described in the previous section, this causes the kernel to position the file to its current end of file before each write. We no longer have to call lseek before each write.<br><br>### 4.2 <code>pread</code> and <code>pwrite</code> Functions<br>Calling <code>pread</code> is equivalent to calling <code>lseek</code> followed by a call to <code>read</code>, with the following exceptions.<br>- There is no way to interrupt the two operations that occur when we call <code>pread</code>.<br>- The current file offset is not updated.<br><br>### 4.3 Creating a File<br><br>When <code>O_CREAT</code> and <code>O_EXCL</code> options for the open function are specified, the open will fail if the file already exists. We also said that the check for the existence of the file and the creation of the file was performed as an atomic operation. If we didn’t have this atomic operation, we might try:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">if</span> ((fd = open(path, O_WRONLY)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (errno == ENOENT) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((fd = creat(path, mode)) &lt; <span class="number">0</span>)</span><br><span class="line">            err_sys(<span class="string">"creat error"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        err_sys(<span class="string">"open error"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><br>The problem occurs if the file is created by another process between the <code>open</code> and the <code>creat</code>. If the file is created by another process between these two function calls, and if that other process writes something to the file, that data is erased when this <code>creat</code> is executed. Combining the test for existence and the creation into a single atomic operation avoids this problem.<br><br>In general, the term <em>atomic operation</em> refers to an operation that might be composed of multiple steps. If the operation is performed atomically, either all the steps are performed (on success) or none are performed (on failure). It must not be possible for only a subset of the steps to be performed.<br><br>## 5 <code>dup</code> and <code>dup2</code> Functions<br><br>An existing file descriptor is duplicated by either of the following functions:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dup</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dup2</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">int</span> fd2)</span></span>;</span><br></pre></td></tr></table></figure><br><br>The new file descriptor returned by <code>dup</code> is guaranteed to be the lowest-numbered available file descriptor.<br><br>The new file descriptor that is returned as the value of the functions shares the same file table entry as the <code>fd</code> argument. We show this in Figure 3.9.<br><center><img src="/images/2018/12/28.png" alt=""></center><p>Each descriptor has its own set of file descriptor flags.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The functions described in this chapter are often referred to as &lt;em&gt;unbuffered I/O&lt;/em&gt;.The term &lt;em&gt;unbuffered&lt;/em&gt; means that each &lt;code&gt;read&lt;/code&gt; or &lt;code&gt;write&lt;/code&gt; invokes a system call in the kernel.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Timing Measurements</title>
    <link href="http://liujunming.github.io/2018/12/15/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Timing-Measurements/"/>
    <id>http://liujunming.github.io/2018/12/15/Understanding-the-Linux-Kernel-读书笔记-Timing-Measurements/</id>
    <published>2018-12-15T00:57:28.000Z</published>
    <updated>2018-12-15T06:53:14.598Z</updated>
    
    <content type="html"><![CDATA[<p>We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel:</p><ul><li>Keeping the current time and date so they can be returned to user programs through the time(), ftime(), and gettimeofday() APIs and used by the kernel itself as timestamps for files and network packets</li><li>Maintaining timers—mechanisms that are able to notify the kernel or a user program that a certain interval of time has elapsed<a id="more"></a></li></ul><h2 id="1-Clock-and-Timer-Circuits"><a href="#1-Clock-and-Timer-Circuits" class="headerlink" title="1 Clock and Timer Circuits"></a>1 Clock and Timer Circuits</h2><p>The <em>clock circuits</em> are used both to keep track of the current time of day and to make precise time measurements. The <em>timer circuits</em> are programmed by the kernel, so that they issue interrupts at a fixed, predefined frequency.</p><h3 id="1-1-Real-Time-Clock-RTC"><a href="#1-1-Real-Time-Clock-RTC" class="headerlink" title="1.1 Real Time Clock (RTC)"></a>1.1 Real Time Clock (RTC)</h3><p>All PCs include a clock called <em>Real Time Clock</em> (RTC), which is independent of the CPU and all other chips.<br>Linux uses the RTC only to derive the time and date.</p><h3 id="1-2-Time-Stamp-Counter-TSC"><a href="#1-2-Time-Stamp-Counter-TSC" class="headerlink" title="1.2 Time Stamp Counter (TSC)"></a>1.2 Time Stamp Counter (TSC)</h3><p>Starting with the Pentium, 80×86 microprocessors sport a counter that is increased at each clock signal. The counter is accessible through the 64-bit <em>Time Stamp Counter</em> (TSC) register. When using this register, the kernel has to take into consideration the frequency of the clock signal: if, for instance, the clock ticks at 1 GHz, the Time Stamp Counter is increased once every nanosecond.<br>Linux may take advantage of this register to get much more accurate time measurements than those delivered by the Programmable Interval Timer.</p><h3 id="1-3-Programmable-Interval-Timer-PIT"><a href="#1-3-Programmable-Interval-Timer-PIT" class="headerlink" title="1.3 Programmable Interval Timer (PIT)"></a>1.3 Programmable Interval Timer (PIT)</h3><p>Besides the Real Time Clock and the Time Stamp Counter, IBM-compatible PCs include another type of time-measuring device called <em>Programmable Interval Timer</em> (PIT ). The role of a PIT is similar to the alarm clock of a microwave oven: it makes the user aware that the cooking time interval has elapsed. Instead of ringing a bell, this device issues a special interrupt called timer interrupt, which notifies the kernel that one more time interval has elapsed. Another difference from the alarm clock is that the PIT goes on issuing interrupts forever at some fixed frequency established by the kernel. </p><h3 id="1-4-CPU-Local-Timer"><a href="#1-4-CPU-Local-Timer" class="headerlink" title="1.4 CPU Local Timer"></a>1.4 CPU Local Timer</h3><p>The local APIC present in recent 80×86 microprocessors provides yet another time-measuring device: the <em>CPU local timer</em>.<br>The CPU local timer is a device similar to the Programmable Interval Timer just described that can issue one-shot or periodic interrupts. There are, however, a few differences:</p><ul><li>The APIC’s timer counter is 32bits long,while the PIT’s timer counter is 16 bits long;</li><li>The local APIC timer sends an interrupt only to its processor, while the PIT raises a global interrupt, which may be handled by any CPU in the system.</li><li>The APIC’s timer is based on the bus clock signal,the PIT, which makes use of its own clock signals, can be programmed in a more flexible way.</li></ul><h3 id="1-5-High-Precision-Event-Timer-HPET"><a href="#1-5-High-Precision-Event-Timer-HPET" class="headerlink" title="1.5 High Precision Event Timer (HPET)"></a>1.5 High Precision Event Timer (HPET)</h3><p>The HPET provides a number of hardware timers that can be exploited by the kernel.<br>The next generation of motherboards will likely support both the HPET and the 8254 PIT; in some future time, however, the HPET is expected to completely replace the PIT.</p><h3 id="1-6-ACPI-Power-Management-Timer"><a href="#1-6-ACPI-Power-Management-Timer" class="headerlink" title="1.6 ACPI Power Management Timer"></a>1.6 ACPI Power Management Timer</h3><p>The device is actually a simple counter increased at each clock tick.Its clock signal has a fixed frequency of roughly 3.58 MHz. </p><p>The ACPI Power Management Timer is preferable to the TSC if the operating system or the BIOS may dynamically lower the frequency or voltage of the CPU to save battery power. On the other hand, the high-frequency of the TSC counter is quite handy for measuring very small time intervals.</p><p>However, if an HPET device is present, it should always be preferred to the other circuits because of its richer architecture. </p><h2 id="2-The-Linux-Timekeeping-Architecture"><a href="#2-The-Linux-Timekeeping-Architecture" class="headerlink" title="2 The Linux Timekeeping Architecture"></a>2 The Linux Timekeeping Architecture</h2><p>Linux’s <em>timekeeping architecture</em> is the set of kernel data structures and functions related to the flow of time.</p><p>Linux’s timekeeping architecture depends also on the availability of the Time Stamp Counter (TSC), of the ACPI Power Management Timer, and of the High Precision Event Timer (HPET). The kernel uses two basic timekeeping functions: one to keep the current time up-to-date and another to count the number of nanoseconds that have elapsed within the current second. There are different ways to get the last value. Some methods are more precise and are available if the CPU has a Time Stamp Counter or a HPET; a less-precise method is used in the opposite case.</p><h3 id="2-1-Data-Structures-of-the-Timekeeping-Architecture"><a href="#2-1-Data-Structures-of-the-Timekeeping-Architecture" class="headerlink" title="2.1 Data Structures of the Timekeeping Architecture"></a>2.1 Data Structures of the Timekeeping Architecture</h3><h4 id="2-1-1-The-timer-object"><a href="#2-1-1-The-timer-object" class="headerlink" title="2.1.1 The timer object"></a>2.1.1 The timer object</h4><p>In order to handle the possible timer sources in a uniform way, the kernel makes use of a “timer object,” which is a descriptor of type <code>timer_opts</code>consisting of the timer name and of four standard methods.</p><p>The <code>cur_timer</code> variable stores the address of the timer object corresponding to the “best” timer source available in the system. </p><h4 id="2-1-2-The-jiffies-variable"><a href="#2-1-2-The-jiffies-variable" class="headerlink" title="2.1.2 The jiffies variable"></a>2.1.2 The jiffies variable</h4><p>The <code>jiffies</code> variable is a counter that stores the number of elapsed ticks since the system was started. It is increased by one when a timer interrupt occurs—that is, on every tick. </p><h4 id="2-1-3-The-xtime-variable"><a href="#2-1-3-The-xtime-variable" class="headerlink" title="2.1.3 The xtime variable"></a>2.1.3 The xtime variable</h4><p>The <code>xtime</code> variable stores the current time and date; it is a structure of type <code>timespec</code>.</p><h3 id="2-2-Timekeeping-Architecture-in-Uniprocessor-Systems"><a href="#2-2-Timekeeping-Architecture-in-Uniprocessor-Systems" class="headerlink" title="2.2 Timekeeping Architecture in Uniprocessor Systems"></a>2.2 Timekeeping Architecture in Uniprocessor Systems</h3><h3 id="2-3-Timekeeping-Architecture-in-Multiprocessor-Systems"><a href="#2-3-Timekeeping-Architecture-in-Multiprocessor-Systems" class="headerlink" title="2.3 Timekeeping Architecture in Multiprocessor Systems"></a>2.3 Timekeeping Architecture in Multiprocessor Systems</h3><h2 id="3-Updating-the-Time-and-Date"><a href="#3-Updating-the-Time-and-Date" class="headerlink" title="3 Updating the Time and Date"></a>3 Updating the Time and Date</h2><p>User programs get the current time and date from the xtime variable. The kernel must periodically update this variable, so that its value is always reasonably accurate.<br>The <code>update_times()</code> function, which is invoked by the global timer interrupt handler, updates the value of the <code>xtime</code> variable.</p><h2 id="4-Updating-System-Statistics"><a href="#4-Updating-System-Statistics" class="headerlink" title="4 Updating System Statistics"></a>4 Updating System Statistics</h2><p>The kernel, among the other time-related duties, must periodically collect various data used for:</p><ul><li>Checking the CPU resource limit of the running processes</li><li>Updating statistics about the local CPU workload</li><li>Computing the average system load</li><li>Profiling the kernel code</li></ul><h2 id="5-Software-Timers-and-Delay-Functions"><a href="#5-Software-Timers-and-Delay-Functions" class="headerlink" title="5 Software Timers and Delay Functions"></a>5 Software Timers and Delay Functions</h2><p>A <em>timer</em> is a software facility that allows functions to be invoked at some future moment, after a given time interval has elapsed; a <em>time-out</em> denotes a moment at which the time interval associated with a timer has elapsed.</p><p>Linux considers two types of timers called <em>dynamic timers</em> and <em>interval timers</em>. The first type is used by the kernel, while interval timers may be created by processes in User Mode.</p><p>Besides software timers, the kernel also makes use of <em>delay functions</em>, which execute a tight instruction loop until a given time interval elapses. </p><h3 id="5-1-Dynamic-Timers"><a href="#5-1-Dynamic-Timers" class="headerlink" title="5.1 Dynamic Timers"></a>5.1 Dynamic Timers</h3><h3 id="5-2-An-Application-of-Dynamic-Timers-the-nanosleep-System-Call"><a href="#5-2-An-Application-of-Dynamic-Timers-the-nanosleep-System-Call" class="headerlink" title="5.2 An Application of Dynamic Timers: the nanosleep( ) System Call"></a>5.2 An Application of Dynamic Timers: the nanosleep( ) System Call</h3><h3 id="5-3-Delay-Functions"><a href="#5-3-Delay-Functions" class="headerlink" title="5.3 Delay Functions"></a>5.3 Delay Functions</h3><p>Software timers are useless when the kernel must wait for a short time interval—let’s say, less than a few milliseconds. For instance, often a device driver has to wait for a predefined number of microseconds until the hardware completes some operation. Because a dynamic timer has a significant setup overhead and a rather large minimum wait time (1 millisecond), the device driver cannot conveniently use it.</p><h2 id="6-System-Calls-Related-to-Timing-Measurements"><a href="#6-System-Calls-Related-to-Timing-Measurements" class="headerlink" title="6 System Calls Related to Timing Measurements"></a>6 System Calls Related to Timing Measurements</h2><p>Several system calls allow User Mode processes to read and modify the time and date and to create timers. Let’s briefly review these and discuss how the kernel handles them.</p><h3 id="6-1-The-time-and-gettimeofday-System-Calls"><a href="#6-1-The-time-and-gettimeofday-System-Calls" class="headerlink" title="6.1 The time( ) and gettimeofday( ) System Calls"></a>6.1 The time( ) and gettimeofday( ) System Calls</h3><p>Processes in User Mode can get the current time and date by means of several system calls:</p><ul><li><code>time()</code><br>Returns the number of elapsed seconds since midnight at the start of January 1, 1970 (UTC).</li><li><code>gettimeofday()</code><br>Returns, in a data structure named timeval, the number of elapsed seconds since midnight of January 1, 1970 (UTC) and the number of elapsed microseconds in the last second.</li></ul><h3 id="6-2-The-adjtimex-System-Call"><a href="#6-2-The-adjtimex-System-Call" class="headerlink" title="6.2 The adjtimex( ) System Call"></a>6.2 The adjtimex( ) System Call</h3><h3 id="6-3-The-setitimer-and-alarm-System-Calls"><a href="#6-3-The-setitimer-and-alarm-System-Calls" class="headerlink" title="6.3 The setitimer( ) and alarm( ) System Calls"></a>6.3 The setitimer( ) and alarm( ) System Calls</h3><h3 id="6-4-System-Calls-for-POSIX-Timers"><a href="#6-4-System-Calls-for-POSIX-Timers" class="headerlink" title="6.4 System Calls for POSIX Timers"></a>6.4 System Calls for POSIX Timers</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keeping the current time and date so they can be returned to user programs through the time(), ftime(), and gettimeofday() APIs and used by the kernel itself as timestamps for files and network packets&lt;/li&gt;
&lt;li&gt;Maintaining timers—mechanisms that are able to notify the kernel or a user program that a certain interval of time has elapsed
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Kernel Synchronization</title>
    <link href="http://liujunming.github.io/2018/12/14/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Kernel-Synchronization/"/>
    <id>http://liujunming.github.io/2018/12/14/Understanding-the-Linux-Kernel-读书笔记-Kernel-Synchronization/</id>
    <published>2018-12-14T04:18:06.000Z</published>
    <updated>2018-12-14T10:38:57.246Z</updated>
    
    <content type="html"><![CDATA[<p>You could think of the kernel as a server that answers requests; these requests can come either from a process running on a CPU or an external device issuing an interrupt request. We make this analogy to underscore that parts of the kernel are not run serially, but in an interleaved way. Thus, they can give rise to race conditions, which must be controlled through proper synchronization techniques.<a id="more"></a> </p><h2 id="1-How-the-Kernel-Services-Requests"><a href="#1-How-the-Kernel-Services-Requests" class="headerlink" title="1 How the Kernel Services Requests"></a>1 How the Kernel Services Requests</h2><p>In the rest of this chapter, we will generally denote as “exceptions” both the system calls and the usual exceptions.</p><h3 id="1-1-Kernel-Preemption"><a href="#1-1-Kernel-Preemption" class="headerlink" title="1.1 Kernel Preemption"></a>1.1 Kernel Preemption</h3><p>内核抢占<br>In nonpreemptive kernels, the current process cannot be replaced unless it is about to switch to User Mode. Therefore, the main characteristic of a preemptive kernel is that a process running in Kernel Mode can be replaced by another process while in the middle of a kernel function.</p><p>The main motivation for making a kernel preemptive is to reduce the dispatch latency of the User Mode processes, that is, the delay between the time they become runnable and the time they actually begin running. </p><p>The kernel can be preempted only when it is executing an exception handler (in particular a system call) and the kernel preemption has not been explicitly disabled. The local CPU must have local interrupts enabled, otherwise kernel preemption is not performed.</p><p>Kernel preemption may happen either when a kernel control path (usually, an interrupt handler) is terminated, or when an exception handler reenables kernel preemption by means of <code>preempt_enable()</code>.</p><h2 id="2-Synchronization-Primitives"><a href="#2-Synchronization-Primitives" class="headerlink" title="2 Synchronization Primitives"></a>2 Synchronization Primitives</h2><center><img src="/images/2018/12/21.png" alt=""></center><h3 id="2-1-Per-CPU-Variables"><a href="#2-1-Per-CPU-Variables" class="headerlink" title="2.1 Per-CPU Variables"></a>2.1 Per-CPU Variables</h3><p>The simplest and most efficient synchronization technique consists of declaring kernel variables as <em>per-CPU variables</em>. Basically, a per-CPU variable is an array of data structures, one element per each CPU in the system. However, that the per-CPU variables can be used only in particular cases—basically, when it makes sense to logically split the data across the CPUs of the system.</p><p>Furthermore, per-CPU variables are prone to race conditions caused by kernel preemption, both in uniprocessor and multiprocessor systems. As a general rule, a kernel control path should access a per-CPU variable with kernel preemption disabled.</p><h3 id="2-2-Atomic-Operations"><a href="#2-2-Atomic-Operations" class="headerlink" title="2.2 Atomic Operations"></a>2.2 Atomic Operations</h3><p>Several assembly language instructions are of type “read-modify-write”—that is, they access a memory location twice, the first time to read the old value and the second time to write a new value.</p><p>The easiest way to prevent race conditions due to “read-modify-write” instructions is by ensuring that such operations are atomic at the chip level. Every such operation must be executed in a single instruction without being interrupted in the middle and avoiding accesses to the same memory location by other CPUs.</p><h3 id="2-3-Optimization-and-Memory-Barriers"><a href="#2-3-Optimization-and-Memory-Barriers" class="headerlink" title="2.3 Optimization and Memory Barriers"></a>2.3 Optimization and Memory Barriers</h3><p>When using optimizing compilers, you should never take for granted that instructions will be performed in the exact order in which they appear in the source code. For example, a compiler might reorder the assembly language instructions in such a way to optimize how registers are used. Moreover, modern CPUs usually execute several instructions in parallel and might reorder memory accesses. These kinds of reordering can greatly speed up the program.</p><p>When dealing with synchronization, however, reordering instructions must be avoided. Things would quickly become hairy if an instruction placed after a synchronization primitive is executed before the synchronization primitive itself. Therefore, all synchronization primitives act as optimization and memory barriers.</p><p>An <em>optimization barrier</em> primitive ensures that the assembly language instructions corresponding to C statements placed before the primitive are not mixed by the compiler with assembly language instructions corresponding to C statements placed after the primitive. In Linux the <code>barrier()</code> macro acts as an optimization barrier.</p><p>A <em>memory barrier</em> primitive ensures that the operations placed before the primitive are finished before starting the operations placed after the primitive. </p><p><img src="/images/2018/12/22.png" alt=""></p><p>Notice that in multiprocessor systems, all atomic operations described in the earlier section “Atomic Operations” act as memory barriers.</p><h3 id="2-4-Spin-Locks"><a href="#2-4-Spin-Locks" class="headerlink" title="2.4 Spin Locks"></a>2.4 Spin Locks</h3><p><em>Spin locks</em> are a special kind of lock designed to work in a multiprocessor environment. If the kernel control path finds the spin lock “open,” it acquires the lock and continues its execution. Conversely, if the kernel control path finds the lock “closed” by a kernel control path running on another CPU, it “spins” around, repeatedly executing a tight instruction loop, until the lock is released.</p><p>The instruction loop of spin locks represents a “busy wait.” The waiting kernel control path keeps running on the CPU, even if it has nothing to do besides waste time. Nevertheless, spin locks are usually convenient, because many kernel resources are locked for a fraction of a millisecond only; therefore, it would be far more time-consuming to release the CPU and reacquire it later.</p><p>As a general rule, <strong>kernel preemption is disabled in every critical region protected by spin locks</strong>. In the case of a uniprocessor system, the locks themselves are useless, and the spin lock primitives just disable or enable the kernel preemption. Please notice that kernel preemption is still enabled during the busy wait phase, thus a process waiting for a spin lock to be released could be replaced by a higher priority process.<br><img src="/images/2018/12/23.png" alt=""></p><h3 id="2-5-Read-Write-Spin-Locks"><a href="#2-5-Read-Write-Spin-Locks" class="headerlink" title="2.5 Read/Write Spin Locks"></a>2.5 Read/Write Spin Locks</h3><p><em>Read/write spin locks</em> have been introduced to increase the amount of concurrency inside the kernel. They allow several kernel control paths to simultaneously read the same data structure, as long as no kernel control path modifies it. If a kernel control path wishes to write to the structure, it must acquire the write version of the read/write lock, which grants exclusive access to the resource. Of course, allowing concurrent reads on data structures improves system performance.</p><p><strong>Getting and releasing a lock for reading</strong></p><ul><li><code>read_lock</code></li><li><code>read_unlock</code></li></ul><p><strong>Getting and releasing a lock for writing</strong></p><ul><li><code>write_lock</code></li><li><code>write_unlock</code></li></ul><h3 id="2-6-Seqlocks"><a href="#2-6-Seqlocks" class="headerlink" title="2.6 Seqlocks"></a>2.6 Seqlocks</h3><p>When using read/write spin locks, requests issued by kernel control paths to perform a <code>read_lock</code> or a <code>write_lock</code> operation have the same priority: readers must wait until the writer has finished and, similarly, a writer must wait until all readers have finished.</p><p><em>Seqlocks</em> are similar to read/write spin locks, except that they give a much higher priority to writers: in fact a writer is allowed to proceed even when readers are active. The good part of this strategy is that a writer never waits (unless another writer is active); the bad part is that a reader may sometimes be forced to read the same data several times until it gets a valid copy.</p><p>The critical regions of the readers should be short and writers should seldom acquire the seqlock</p><h3 id="2-7-Read-Copy-Update-RCU"><a href="#2-7-Read-Copy-Update-RCU" class="headerlink" title="2.7 Read-Copy Update (RCU)"></a>2.7 Read-Copy Update (RCU)</h3><p><a href="http://liujunming.top/2018/12/13/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Linux-%E7%9A%84-RCU-%E6%9C%BA%E5%88%B6/" target="_blank" rel="noopener">深入理解 Linux 的 RCU 机制</a><br><em>Read-copy update (RCU)</em> is yet another synchronization technique designed to protect data structures that are mostly accessed for reading by several CPUs. RCU is lock-free, that is, it uses no lock or counter shared by all CPUs; this is a great advantage over read/write spin locks and seqlocks, which have a high overhead due to cache line-snooping and invalidation.</p><p>How does RCU obtain the surprising result of synchronizing several CPUs without shared data structures? The key idea consists of limiting the scope of RCU as follows:</p><ol><li>Only data structures that are dynamically allocated and referenced by means of pointers can be protected by RCU.</li><li>No kernel control path can sleep inside a critical region protected by RCU.</li></ol><h3 id="2-8-Semaphores"><a href="#2-8-Semaphores" class="headerlink" title="2.8 Semaphores"></a>2.8 Semaphores</h3><p>Essentially, Semaphores implement a locking primitive that allows waiters to sleep until the desired resource becomes free.</p><p>Actually, Linux offers two kinds of semaphores:</p><ul><li>Kernel semaphores, which are used by kernel control paths</li><li>System V IPC semaphores, which are used by User Mode processes</li></ul><p>In this section, we focus on kernel semaphores.</p><p>A kernel semaphore is similar to a spin lock, in that it doesn’t allow a kernel control path to proceed unless the lock is open. However, whenever a kernel control path tries to acquire a busy resource protected by a kernel semaphore, the corresponding process is suspended. It becomes runnable again when the resource is released. Therefore, kernel semaphores can be acquired only by functions that are allowed to sleep; interrupt handlers and deferrable functions cannot use them.</p><p>The <code>init_MUTEX()</code> and <code>init_MUTEX_LOCKED()</code> functions may be used to initialize a semaphore for exclusive access.</p><p><strong>Getting and releasing semaphores</strong></p><ul><li><code>up()</code></li><li><code>down()</code></li></ul><h3 id="2-9-Read-Write-Semaphores"><a href="#2-9-Read-Write-Semaphores" class="headerlink" title="2.9 Read/Write Semaphores"></a>2.9 Read/Write Semaphores</h3><p>Read/write semaphores are similar to the read/write spin locks described earlier in the section “Read/Write Spin Locks,” except that waiting processes are suspended instead of spinning until the semaphore becomes open again.</p><p>Many kernel control paths may concurrently acquire a read/write semaphore for reading; however, every writer kernel control path must have exclusive access to the protected resource. Therefore, the semaphore can be acquired for writing only if no other kernel control path is holding it for either read or write access. Read/write semaphores improve the amount of concurrency inside the kernel and improve overall system performance.</p><p>The <code>down_read()</code> and <code>down_write()</code> functions acquire the read/write semaphore for reading and writing, respectively. Similarly, the <code>up_read()</code> and <code>up_write()</code> functions release a read/write semaphore previously acquired for reading and for writing.</p><h3 id="2-10-Completions"><a href="#2-10-Completions" class="headerlink" title="2.10 Completions"></a>2.10 Completions</h3><p>Linux 2.6 also makes use of another synchronization primitive similar to semaphores: <em>completions</em>. They have been introduced to solve a subtle race condition that occurs in multiprocessor systems when process A allocates a temporary semaphore variable, initializes it as closed MUTEX, passes its address to process B, and then invokes <code>down()</code> on it. Process A plans to destroy the semaphore as soon as it awakens. Later on, process B running on a different CPU invokes <code>up()</code> on the semaphore. However, in the current implementation <code>up()</code> and <code>down()</code> can execute concurrently on the same semaphore. Thus, process A can be woken up and destroy the temporary semaphore while process B is still executing the <code>up()</code> function. As a result, <code>up()</code>might attempt to access a data structure that no longer exists.</p><h3 id="2-11-Local-Interrupt-Disabling"><a href="#2-11-Local-Interrupt-Disabling" class="headerlink" title="2.11 Local Interrupt Disabling"></a>2.11 Local Interrupt Disabling</h3><p>Interrupt disabling is one of the key mechanisms used to ensure that a sequence of kernel statements is treated as a critical section. It allows a kernel control path to continue executing even when hardware devices issue IRQ signals, thus providing an effective way to protect data structures that are also accessed by interrupt handlers. By itself, however, local interrupt disabling does not protect against concurrent accesses to data structures by interrupt handlers running on other CPUs, so in multi-processor systems, local interrupt disabling is often coupled with spin locks.</p><p>The <code>local_irq_disable()</code> macro disables interrupts on the local CPU. The <code>local_irq_enable()</code> macro enables them.</p><h3 id="2-12-Disabling-and-Enabling-Deferrable-Functions"><a href="#2-12-Disabling-and-Enabling-Deferrable-Functions" class="headerlink" title="2.12 Disabling and Enabling Deferrable Functions"></a>2.12 Disabling and Enabling Deferrable Functions</h3><p>Deferrable functions can be executed at unpredictable times (essentially, on termination of hardware interrupt handlers). Therefore, data structures accessed by deferrable functions must be protected against race conditions.<br>The kernel sometimes needs to disable deferrable functions without disabling interrupts. Local deferrable functions can be enabled or disabled on the local CPU by acting on the softirq counter stored in the <code>preempt_count</code> field of the current’s <code>thread_info</code> descriptor.</p><h2 id="3-Synchronizing-Accesses-to-Kernel-Data-Structures"><a href="#3-Synchronizing-Accesses-to-Kernel-Data-Structures" class="headerlink" title="3 Synchronizing Accesses to Kernel Data Structures"></a>3 Synchronizing Accesses to Kernel Data Structures</h2><p>Usually, the following rule of thumb is adopted by kernel developers: <em>always keep the concurrency level as high as possible in the system</em>.</p><p>In turn, the concurrency level in the system depends on two main factors:</p><ul><li>The number of I/O devices that operate concurrently</li><li>The number of CPUs that do productive work</li></ul><p>To maximize I/O throughput, interrupts should be disabled for very short periods of time. To use CPUs efficiently, synchronization primitives based on spin locks should be avoided whenever possible.</p><h3 id="3-1-Choosing-Among-Spin-Locks-Semaphores-and-Interrupt-Disabling"><a href="#3-1-Choosing-Among-Spin-Locks-Semaphores-and-Interrupt-Disabling" class="headerlink" title="3.1 Choosing Among Spin Locks, Semaphores, and Interrupt Disabling"></a>3.1 Choosing Among Spin Locks, Semaphores, and Interrupt Disabling</h3><p>Generally speaking, choosing the synchronization primitives depends on what kinds of kernel control paths access the data structure, as shown in Table 5-8. Remember that whenever a kernel control path acquires a spin lock (as well as a read/write lock, a seqlock, or a RCU “read lock”), disables the local interrupts, or disables the local softirqs, kernel preemption is automatically disabled.</p><p><img src="/images/2018/12/24.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;You could think of the kernel as a server that answers requests; these requests can come either from a process running on a CPU or an external device issuing an interrupt request. We make this analogy to underscore that parts of the kernel are not run serially, but in an interleaved way. Thus, they can give rise to race conditions, which must be controlled through proper synchronization techniques.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/tags/Concurrency/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 Linux 的 RCU 机制</title>
    <link href="http://liujunming.github.io/2018/12/13/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Linux-%E7%9A%84-RCU-%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2018/12/13/深入理解-Linux-的-RCU-机制/</id>
    <published>2018-12-13T10:59:18.000Z</published>
    <updated>2018-12-14T02:37:43.017Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Read-copy update (RCU) is a synchronization mechanism. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, <strong>RCU supports concurrency between a single updater and multiple readers</strong>. <a id="more"></a>RCU ensures that reads are coherent by maintaining multiple versions of objects and ensuring that they are not freed up until all pre-existing read-side critical sections complete. RCU defines and uses efficient and scalable mechanisms for publishing and reading new versions of an object, and also for deferring the collection of old versions. These mechanisms distribute the work among read and update paths in such a way as to make read paths extremely fast.</p><p>RCU is made up of three fundamental mechanisms, the first being used for insertion, the second being used for deletion, and the third being used to allow readers to tolerate concurrent insertions and deletions. These mechanisms are described in the following sections, which focus on applying RCU to linked lists:</p><ol><li>Publish-Subscribe Mechanism (for insertion)</li><li>Wait For Pre-Existing RCU Readers to Complete (for deletion)</li><li>Maintain Multiple Versions of Recently Updated Objects (for readers)</li></ol><h3 id="1-1-Publish-Subscribe-Mechanism"><a href="#1-1-Publish-Subscribe-Mechanism" class="headerlink" title="1.1 Publish-Subscribe Mechanism"></a>1.1 Publish-Subscribe Mechanism</h3><p>One key attribute of RCU is the ability to safely scan data, even though that data is being modified concurrently. To provide this ability for concurrent insertion, RCU uses what can be thought of as a publish-subscribe mechanism. For example, consider an initially <code>NULL</code> global pointer <code>gp</code> that is to be modified to point to a newly allocated and initialized data structure. The following code fragment (with the addition of appropriate locking) might be used for this purpose:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> *<span class="title">gp</span> = <span class="title">NULL</span>;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line">   </span><br><span class="line">p = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">gp = p;</span><br></pre></td></tr></table></figure></p><p>Unfortunately, there is nothing forcing the compiler and CPU to execute the last four assignment statements in order. If the assignment to <code>gp</code> happens before the initialization of <code>p</code>‘s fields, then concurrent readers could see the uninitialized values. Memory barriers are required to keep things ordered, but memory barriers are notoriously difficult to use. We therefore encapsulate them into a primitive <code>rcu_assign_pointer()</code> that has publication semantics. The last four lines would then be as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">rcu_assign_pointer(gp, p);</span><br></pre></td></tr></table></figure></p><p>The <code>rcu_assign_pointer()</code> would <em>publish</em> the new structure, forcing both the compiler and the CPU to execute the assignment to <code>gp</code> after the assignments to the fields referenced by <code>p</code>.</p><p>However, it is not sufficient to only enforce ordering at the updater, as the reader must enforce proper ordering as well. Consider for example the following code fragment:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p = gp;</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Although this code fragment might well seem immune to misordering, unfortunately, the <a href="http://www.rdrop.com/users/paulmck/scalability/paper/ordering.2007.09.19a.pdf" target="_blank" rel="noopener">DEC Alpha CPU</a>  and value-speculation compiler optimizations can cause the values of <code>p-&gt;a</code>, <code>p-&gt;b</code>, and <code>p-&gt;c</code> to be fetched before the value of <code>p</code>!</p><p>Clearly, we need to prevent this sort of skullduggery on the part of both the compiler and the CPU. The <code>rcu_dereference()</code> primitive uses whatever memory-barrier instructions and compiler directives are required for this purpose:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rcu_read_lock();</span><br><span class="line">p = rcu_dereference(gp);</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br></pre></td></tr></table></figure></p><p>The <code>rcu_dereference()</code> primitive can thus be thought of as <em>subscribing</em> to a given value of the specified pointer, guaranteeing that subsequent dereference operations will see any initialization that occurred before the corresponding publish (<code>rcu_assign_pointer()</code>) operation. The <code>rcu_read_lock()</code> and <code>rcu_read_unlock()</code> calls are absolutely required: they define the extent of the RCU read-side critical section. Their purpose is explained in the next section, however, they never spin or block, nor do they prevent the <code>list_add_rcu()</code> from executing concurrently. </p><p>Although <code>rcu_assign_pointer()</code> and <code>rcu_dereference()</code> can in theory be used to construct any conceivable RCU-protected data structure, in practice it is often better to use higher-level constructs. Therefore, the <code>rcu_assign_pointer()</code> and <code>rcu_dereference()</code> primitives have been embedded in special RCU variants of Linux’s list-manipulation API. Linux has two variants of doubly linked list, the circular <code>struct list_head</code> and the linear <code>struct hlist_head</code>/<code>struct hlist_node</code> pair. The former is laid out as follows, where the green boxes represent the list header and the blue boxes represent the elements in the list.</p><center><img src="/images/2018/12/6.jpg" alt=""></center><p>Adapting the pointer-publish example for the linked list gives the following:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">list</span>;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line">LIST_HEAD(head);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line"> </span><br><span class="line">p = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_add_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;head);</span><br></pre></td></tr></table></figure></p><p>Line 15 must be protected by some synchronization mechanism (most commonly some sort of lock) to prevent multiple <code>list_add()</code> instances from executing concurrently. However, such synchronization does not prevent this <code>list_add()</code> from executing concurrently with RCU readers.</p><p>Subscribing to an RCU-protected list is straightforward:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rcu_read_lock();</span><br><span class="line">list_for_each_entry_rcu(p, head, <span class="built_in">list</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br></pre></td></tr></table></figure></p><p>The <code>list_add_rcu()</code> primitive publishes an entry into the specified list, guaranteeing that the corresponding <code>list_for_each_entry_rcu()</code> invocation will properly subscribe to this same entry.</p><p>The set of RCU publish and subscribe primitives are shown in the following table, along with additional primitives to “unpublish”, or retract:</p><center><img src="/images/2018/12/7.png" alt=""></center><p>Note that the <code>list_replace_rcu()</code>, <code>list_del_rcu()</code>, <code>hlist_replace_rcu()</code>, and <code>hlist_del_rcu()</code> APIs add a complication. When is it safe to free up the data element that was replaced or removed? In particular, how can we possibly know when all the readers have released their references to that data element?</p><p>These questions are addressed in the following section.</p><h3 id="1-2-Wait-For-Pre-Existing-RCU-Readers-to-Complete"><a href="#1-2-Wait-For-Pre-Existing-RCU-Readers-to-Complete" class="headerlink" title="1.2 Wait For Pre-Existing RCU Readers to Complete"></a>1.2 Wait For Pre-Existing RCU Readers to Complete</h3><p>In its most basic form, RCU is a way of waiting for things to finish. Of course, there are a great many other ways of waiting for things to finish, including reference counts, reader-writer locks, events, and so on. The great advantage of RCU is that it can wait for each of (say) 20,000 different things without having to explicitly track each and every one of them, and without having to worry about the performance degradation, scalability limitations, complex deadlock scenarios, and memory-leak hazards that are inherent in schemes using explicit tracking.</p><p>In RCU’s case, the things waited on are called “RCU read-side critical sections”. An RCU read-side critical section starts with an <code>rcu_read_lock()</code> primitive, and ends with a corresponding <code>rcu_read_unlock()</code> primitive. RCU read-side critical sections can be nested, and may contain pretty much any code, as long as that code does not explicitly block or sleep (although a special form of RCU called “<a href="https://lwn.net/Articles/202847/" target="_blank" rel="noopener">SRCU</a>“ does permit general sleeping in SRCU read-side critical sections). If you abide by these conventions, you can use RCU to wait for <em>any</em> desired piece of code to complete.</p><p>RCU accomplishes this feat by indirectly determining when these other things have finished, as has been described elsewhere for <a href="http://www.rdrop.com/users/paulmck/RCU/whatisRCU.html" target="_blank" rel="noopener">RCU Classic</a> and <a href="https://lwn.net/Articles/253651/" target="_blank" rel="noopener">realtime RCU</a>.</p><p>In particular, as shown in the following figure, RCU is a way of waiting for pre-existing RCU read-side critical sections to completely finish, including memory operations executed by those critical sections.</p><p><center><img src="/images/2018/12/8.png" alt=""></center><br>However, note that RCU read-side critical sections that begin after the beginning of a given grace period can and will extend beyond the end of that grace period.</p><p>The following pseudocode shows the basic form of algorithms that use RCU to wait for readers:</p><ol><li>Make a change, for example, replace an element in a linked list.</li><li>Wait for all pre-existing RCU read-side critical sections to completely finish (for example, by using the <code>synchronize_rcu()</code> primitive). The key observation here is that subsequent RCU read-side critical sections have no way to gain a reference to the newly removed element.</li><li>Clean up, for example, free the element that was replaced above.</li></ol><p>The following code fragment, adapted from those in the previous section, demonstrates this process, with field <code>a</code> being the search key:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">list</span>;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line">LIST_HEAD(head);</span><br><span class="line"> </span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line"> </span><br><span class="line">p = search(head, key);</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span>) &#123;</span><br><span class="line">    <span class="comment">/* Take appropriate action, unlock, and return. */</span></span><br><span class="line">&#125;</span><br><span class="line">q = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">*q = *p;</span><br><span class="line">q-&gt;b = <span class="number">2</span>;</span><br><span class="line">q-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_replace_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;q-&gt;<span class="built_in">list</span>);</span><br><span class="line">synchronize_rcu();</span><br><span class="line">kfree(p);</span><br></pre></td></tr></table></figure></p><p>Lines 19, 20, and 21 implement the three steps called out above. Lines 16-19 gives RCU (“read-copy update”) its name: while permitting concurrent reads, line 16 <em>copies</em> and lines 17-19 do an <em>update</em>.</p><p>The <code>synchronize_rcu()</code>must wait for all RCU read-side critical sections to complete.</p><p>RCU Classic read-side critical sections delimited by <code>rcu_read_lock()</code> and <code>rcu_read_unlock()</code> are not permitted to block or sleep.</p><p>What exactly do RCU readers see when traversing a concurrently updated list? This question is addressed in the following section.</p><h3 id="1-3-Maintain-Multiple-Versions-of-Recently-Updated-Objects"><a href="#1-3-Maintain-Multiple-Versions-of-Recently-Updated-Objects" class="headerlink" title="1.3 Maintain Multiple Versions of Recently Updated Objects"></a>1.3 Maintain Multiple Versions of Recently Updated Objects</h3><p>This section demonstrates how RCU maintains multiple versions of lists to accommodate synchronization-free readers. Two examples are presented showing how an element that might be referenced by a given reader must remain intact while that reader remains in its RCU read-side critical section. The first example demonstrates deletion of a list element, and the second example demonstrates replacement of an element.</p><h4 id="Example-1-Maintaining-Multiple-Versions-During-Deletion"><a href="#Example-1-Maintaining-Multiple-Versions-During-Deletion" class="headerlink" title="Example 1: Maintaining Multiple Versions During Deletion"></a>Example 1: Maintaining Multiple Versions During Deletion</h4><p>To start the “deletion” example, we will modify lines 11-21 in the example in the previous section as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">p = search(head, key);</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    list_del_rcu(&amp;p-&gt;<span class="built_in">list</span>);</span><br><span class="line">    synchronize_rcu();</span><br><span class="line">    kfree(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>The initial state of the list, including the pointer <code>p</code>, is as follows.</p><p><center><img src="/images/2018/12/9.jpg" alt=""></center><br>The triples in each element represent the values of fields <code>a</code>, <code>b</code>, and <code>c</code>, respectively. The red borders on each element indicate that readers might be holding references to them, and because readers do not synchronize directly with updaters, readers might run concurrently with this entire replacement process. Please note that we have omitted the backwards pointers and the link from the tail of the list to the head for clarity.</p><p>After the <code>list_del_rcu()</code> on line 3 has completed, the <code>5</code>,<code>6</code>,<code>7</code> element has been removed from the list, as shown below. Since readers do not synchronize directly with updaters, readers might be concurrently scanning this list. These concurrent readers might or might not see the newly removed element, depending on timing. However, readers that were delayed just after fetching a pointer to the newly removed element might see the old version of the list for quite some time after the removal. Therefore, we now have two versions of the list, one with element <code>5</code>,<code>6</code>,<code>7</code> and one without. The border of the <code>5</code>,<code>6</code>,<code>7</code> element is still red, indicating that readers might be referencing it.</p><p><center><img src="/images/2018/12/10.jpg" alt=""></center><br>Please note that readers are not permitted to maintain references to element ,<code>5</code>,<code>6</code>,<code>7</code> after exiting from their RCU read-side critical sections. Therefore, once the <code>synchronize_rcu()</code> on line 4 completes, so that all pre-existing readers are guaranteed to have completed, there can be no more readers referencing this element, as indicated by its black border below. We are thus back to a single version of the list.</p><p><center><img src="/images/2018/12/11.jpg" alt=""></center><br>At this point, the <code>5</code>,<code>6</code>,<code>7</code> element may safely be freed, as shown below:</p><p><center><img src="/images/2018/12/12.jpg" alt=""></center><br>At this point, we have completed the deletion of element <code>5</code>,<code>6</code>,<code>7</code>. The following section covers replacement.</p><h4 id="Example-2-Maintaining-Multiple-Versions-During-Replacement"><a href="#Example-2-Maintaining-Multiple-Versions-During-Replacement" class="headerlink" title="Example 2: Maintaining Multiple Versions During Replacement"></a>Example 2: Maintaining Multiple Versions During Replacement</h4><p>To start the replacement example, here are the last few lines of the example in the previous section:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">*q = *p;</span><br><span class="line">q-&gt;b = <span class="number">2</span>;</span><br><span class="line">q-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_replace_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;q-&gt;<span class="built_in">list</span>);</span><br><span class="line">synchronize_rcu();</span><br><span class="line">kfree(p);</span><br></pre></td></tr></table></figure></p><p>The initial state of the list, including the pointer <code>p</code>, is the same as for the deletion example:</p><p><center><img src="/images/2018/12/13.jpg" alt=""></center><br>As before, the triples in each element represent the values of fields <code>a</code>, <code>b</code>, and <code>c</code>, respectively. The red borders on each element indicate that readers might be holding references to them, and because readers do not synchronize directly with updaters, readers might run concurrently with this entire replacement process. Please note that we again omit the backwards pointers and the link from the tail of the list to the head for clarity.<br>Line 1 <code>kmalloc()</code>s a replacement element, as follows:</p><p><center><img src="/images/2018/12/14.jpg" alt=""></center><br>Line 2 copies the old element to the new one:</p><p><center><img src="/images/2018/12/15.jpg" alt=""></center><br>Line 3 updates <code>q-&gt;b</code> to the value “2”:</p><p><center><img src="/images/2018/12/16.jpg" alt=""></center><br>Line 4 updates q-&gt;c to the value “3”:</p><p><center><img src="/images/2018/12/17.jpg" alt=""></center><br>Now, line 5 does the replacement, so that the new element is finally visible to readers. At this point, as shown below, we have two versions of the list. Pre-existing readers might see the <code>5</code>,<code>6</code>,<code>7</code> element, but new readers will instead see the <code>5</code>,<code>2</code>,<code>3</code> element. But any given reader is guaranteed to see some well-defined list.</p><p><center><img src="/images/2018/12/18.jpg" alt=""></center><br>After the <code>synchronize_rcu()</code> on line 6 returns, a grace period will have elapsed, and so all reads that started before the <code>list_replace_rcu()</code> will have completed. In particular, any readers that might have been holding references to the <code>5</code>,<code>6</code>,<code>7</code> element are guaranteed to have exited their RCU read-side critical sections, and are thus prohibited from continuing to hold a reference. Therefore, there can no longer be any readers holding references to the old element, as indicated by the thin black border around the <code>5</code>,<code>6</code>,<code>7</code> element below. As far as the readers are concerned, we are back to having a single version of the list, but with the new element in place of the old.</p><p><center><img src="/images/2018/12/19.jpg" alt=""></center><br>After the <code>kfree()</code> on line 7 completes, the list will appear as follows:</p><p><center><img src="/images/2018/12/20.jpg" alt=""></center></p><h2 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2 Usage"></a>2 Usage</h2><p><a href="http://lwn.net/Articles/263130/" target="_blank" rel="noopener">What is RCU? Part 2: Usage</a><br><a href="https://lwn.net/Articles/609973/" target="_blank" rel="noopener">The RCU API tables</a><br><a href="https://github.com/jinb-park/rcu_example/blob/master/list_rcu_example.c" target="_blank" rel="noopener">list_rcu_example</a>是一个具体实例，可以仔细研究下代码。</p><hr><p>参考资料：</p><ol><li><a href="https://www.kernel.org/doc/Documentation/RCU/whatisRCU.txt" target="_blank" rel="noopener">whatisRCU</a></li><li><a href="https://www.wikiwand.com/en/Read-copy-update" target="_blank" rel="noopener">wikiwand Read-copy-update</a></li><li><a href="https://lwn.net/Articles/262464/" target="_blank" rel="noopener">What is RCU, Fundamentally?</a></li><li><a href="https://pdos.csail.mit.edu/6.828/2018/readings/rcu-decade-later.pdf" target="_blank" rel="noopener">RCU Usage In the Linux Kernel: One Decade Later</a></li><li><a href="https://github.com/jinb-park/rcu_example" target="_blank" rel="noopener">rcu_example</a></li><li><a href="https://www.cnblogs.com/qcloud1001/p/7755331.html" target="_blank" rel="noopener">深入理解 Linux 的 RCU 机制</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1 Introduction&quot;&gt;&lt;/a&gt;1 Introduction&lt;/h2&gt;&lt;p&gt;Read-copy update (RCU) is a synchronization mechanism. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, &lt;strong&gt;RCU supports concurrency between a single updater and multiple readers&lt;/strong&gt;.
    
    </summary>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/categories/Concurrency/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/tags/Concurrency/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Interrupts and Exceptions</title>
    <link href="http://liujunming.github.io/2018/12/04/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Interrupts-and-Exceptions/"/>
    <id>http://liujunming.github.io/2018/12/04/Understanding-the-Linux-Kernel-读书笔记-Interrupts-and-Exceptions/</id>
    <published>2018-12-04T09:01:41.000Z</published>
    <updated>2018-12-14T05:56:02.319Z</updated>
    
    <content type="html"><![CDATA[<p>An <em>interrupt</em> is usually defined as an event that alters the sequence of instructions executed by a processor.</p><p>Intel microprocessor manuals designate synchronous and asynchronous interrupts as <em>exceptions</em> and <em>interrupts</em>.<a id="more"></a> We’ll occasionally use the term “interrupt signal” to designate both types together (synchronous as well as asynchronous).</p><p>Interrupts are issued by interval timers and I/O devices.</p><p>Exceptions, on the other hand, are caused either by programming errors or by anomalous conditions that must be handled by the kernel.</p><h2 id="1-The-Role-of-Interrupt-Signals"><a href="#1-The-Role-of-Interrupt-Signals" class="headerlink" title="1 The Role of Interrupt Signals"></a>1 The Role of Interrupt Signals</h2><p>When an interrupt signal arrives, the CPU must stop what it’s currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the <code>eip</code> and <code>cs</code> registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter.</p><p>There is a key difference between interrupt handling and process switching: the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred. As a kernel control path, the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down).</p><h2 id="2-Interrupts-and-Exceptions"><a href="#2-Interrupts-and-Exceptions" class="headerlink" title="2 Interrupts and Exceptions"></a>2 Interrupts and Exceptions</h2><ul><li>Interrupts</li><li>Exceptions<ul><li>Processor-detected exceptions<ul><li>Faults</li><li>Traps</li><li>Aborts</li></ul></li><li>Programmed exceptions</li></ul></li></ul><p><strong>Processor-detected exceptions:</strong> These are further divided into three groups, depending on the value of the eip register that is saved on the Kernel Mode stack when the CPU control unit raises the exception.</p><p><strong>Traps:</strong>The saved value of eip is the address of the instruction that should be executed after the one that caused the trap.</p><p><strong>Aborts:</strong>A serious error occurred; the control unit is in trouble, and it may be unable to store in the eip register the precise location of the instruction causing the exception. Aborts are used to report severe errors, such as hardware failures and invalid or inconsistent values in system tables.</p><p><strong>Programmed exceptions:</strong>Occur at the request of the programmer. Programmed exceptions are handled by the control unit as traps; they are often called <code>software interrupts</code>. Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event.</p><p>Each interrupt or exception is identified by a number ranging from 0 to 255; Intel calls this 8-bit unsigned number a <em>vector</em>. The vectors of nonmaskable interrupts and exceptions are fixed, while those of maskable interrupts can be altered by programming the Interrupt Controller.</p><h3 id="2-1-IRQs-and-Interrupts"><a href="#2-1-IRQs-and-Interrupts" class="headerlink" title="2.1 IRQs and Interrupts"></a>2.1 IRQs and Interrupts</h3><p>Each hardware device controller capable of issuing interrupt requests usually has a single output line designated as the <code>Interrupt ReQuest (IRQ)</code> line.All existing IRQ lines are connected to the input pins of a hardware circuit called the <code>Programmable Interrupt Controller(PIC)</code>.</p><center><img src="/images/2018/12/1.JPG" alt=""></center><p><strong>The Advanced Programmable Interrupt Controller (APIC)</strong><br> However, if the system includes two or more CPUs, this approach is no longer valid and more sophisticated PICs are needed.<br> <center><img src="/images/2018/12/2.png" alt=""></center></p><p> Besides distributing interrupts among processors, the multi-APIC system allows CPUs to generate <code>interprocessor interrupts(IPI)</code>. </p><h3 id="2-2-Exceptions"><a href="#2-2-Exceptions" class="headerlink" title="2.2 Exceptions"></a>2.2 Exceptions</h3><p>Each exception is handled by a specific exception handler, which usually sends a Unix signal to the process that caused the exception.</p><h3 id="2-3-Interrupt-Descriptor-Table"><a href="#2-3-Interrupt-Descriptor-Table" class="headerlink" title="2.3 Interrupt Descriptor Table"></a>2.3 Interrupt Descriptor Table</h3><p>A system table called <code>Interrupt Descriptor Table (IDT)</code> associates each interrupt or exception vector with the address of the corresponding interrupt or exception handler. </p><p>The IDT may include three types of descriptors;<br> <center><img src="/images/2018/12/3.png" alt=""></center><br>Linux uses interrupt gates to handle interrupts and trap gates to handle exceptions.</p><h3 id="2-4-Hardware-Handling-of-Interrupts-and-Exceptions"><a href="#2-4-Hardware-Handling-of-Interrupts-and-Exceptions" class="headerlink" title="2.4 Hardware Handling of Interrupts and Exceptions"></a>2.4 Hardware Handling of Interrupts and Exceptions</h3><p>After the interrupt or exception is processed, the corresponding handler must relinquish control to the interrupted process.</p><h2 id="3-Nested-Execution-of-Exception-and-Interrupt-Handlers"><a href="#3-Nested-Execution-of-Exception-and-Interrupt-Handlers" class="headerlink" title="3 Nested Execution of Exception and Interrupt Handlers"></a>3 Nested Execution of Exception and Interrupt Handlers</h2><p>Every interrupt or exception gives rise to a kernel control path or separate sequence of instructions that execute in Kernel Mode on behalf of the current process.<br> <center><img src="/images/2018/12/4.png" alt=""></center><br>The price to pay for allowing nested kernel control paths is that an interrupt handler must never block, that is, 中断处理程序运行期间不能发生进程切换.</p><p>An interrupt handler may preempt both other interrupt handlers and exception handlers. Conversely, an exception handler never preempts an interrupt handler. Interrupt handlers never perform operations that can induce page faults, and thus, potentially, a process switch.</p><p>On multiprocessor systems, several kernel control paths may execute concurrently. Moreover, a kernel control path associated with an exception may start executing on a CPU and, due to a process switch, migrate to another CPU.</p><h2 id="4-Initializing-the-Interrupt-Descriptor-Table"><a href="#4-Initializing-the-Interrupt-Descriptor-Table" class="headerlink" title="4 Initializing the Interrupt Descriptor Table"></a>4 Initializing the Interrupt Descriptor Table</h2><h2 id="5-Exception-Handling"><a href="#5-Exception-Handling" class="headerlink" title="5 Exception Handling"></a>5 Exception Handling</h2><p>When one of them occurs, the kernel sends a signal to the process that caused the exception to notify it of an anomalous condition. </p><p>Exception handlers have a standard structure consisting of three steps:</p><ol><li>Save the contents of most registers in the Kernel Mode stack (this part is coded in assembly language).</li><li>Handle the exception by means of a high-level C function.</li><li>Exit from the handler by means of the <code>ret_from_exception()</code> function.</li></ol><h3 id="5-1-Saving-the-Registers-for-the-Exception-Handler"><a href="#5-1-Saving-the-Registers-for-the-Exception-Handler" class="headerlink" title="5.1 Saving the Registers for the Exception Handler"></a>5.1 Saving the Registers for the Exception Handler</h3><h3 id="5-2-Entering-and-Leaving-the-Exception-Handler"><a href="#5-2-Entering-and-Leaving-the-Exception-Handler" class="headerlink" title="5.2 Entering and Leaving the Exception Handler"></a>5.2 Entering and Leaving the Exception Handler</h3><p>Store the hardware error code and the exception vector in the process descriptor of current, and then send a suitable signal to that process:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">current-&gt;thread.error_code = error_code;</span><br><span class="line">current-&gt;thread.trap_no = <span class="built_in">vector</span>;</span><br><span class="line">force_sig(sig_number, current);</span><br></pre></td></tr></table></figure></p><p>The current process takes care of the signal right after the termination of the exception handler. The signal will be handled either in User Mode by the process’s own signal handler (if it exists) or in Kernel Mode. In the latter case, the kernel usually kills the process.</p><h2 id="6-Interrupt-Handling"><a href="#6-Interrupt-Handling" class="headerlink" title="6 Interrupt Handling"></a>6 Interrupt Handling</h2><p>Most exceptions are handled simply by sending a Unix signal to the process that caused the exception. The action to be taken is thus deferred until the process receives the signal; as a result, the kernel is able to process the exception quickly.</p><p>Interrupt handling depends on the type of interrupt. </p><ul><li>I/O interrupts</li><li>Timer interrupts</li><li>Interprocessor interrupts</li></ul><h3 id="6-1-I-O-Interrupt-Handling"><a href="#6-1-I-O-Interrupt-Handling" class="headerlink" title="6.1 I/O Interrupt Handling"></a>6.1 I/O Interrupt Handling</h3><p>In the PCI bus architecture, for instance, several devices may share the same IRQ line. This means that the interrupt vector alone does not tell the whole story. </p><ul><li>IRQ sharing</li><li>IRQ dynamic allocation</li></ul><p>The interrupt handler executes several <code>interrupt service routines (ISRs)</code>.</p><p>Not all actions to be performed when an interrupt occurs have the same urgency.Long noncritical operations should be deferred, because while an interrupt handler is running, the signals on the corresponding IRQ line are temporarily ignored. Most important, the process on behalf of which an interrupt handler is executed must always stay in the TASK_RUNNING state, or a system freeze can occur.Therefore, interrupt handlers cannot perform any blocking procedure such as an I/O disk operation. Linux divides the actions to be performed following an interrupt into three classes:</p><ul><li>Critical</li><li>Noncritical</li><li>Noncritical deferrable</li></ul><p>Regardless of the kind of circuit that caused the interrupt, all I/O interrupt handlers perform the same four basic actions:</p><ol><li>Save the IRQ value and the register’s contents on the Kernel Mode stack.</li><li>Send an acknowledgment to the PIC that is servicing the IRQ line, thus allowing it to issue further interrupts.</li><li>Execute the interrupt service routines (ISRs) associated with all the devices that share the IRQ.</li><li>Terminate by jumping to the <code>ret_from_intr()</code> address.</li></ol><center><img src="/images/2018/12/5.png" alt=""></center><h2 id="7-Softirqs-and-Tasklets"><a href="#7-Softirqs-and-Tasklets" class="headerlink" title="7 Softirqs and Tasklets"></a>7 Softirqs and Tasklets</h2><p>We mentioned earlier in the section “Interrupt Handling” that several tasks among those executed by the kernel are not critical: they can be deferred for a long period of time, if necessary.</p><p>The deferrable tasks can execute with all interrupts enabled. Taking them out of the interrupt handler helps keep kernel response time small. This is a very important property for many time-critical applications that expect their interrupt requests to be serviced in a few milliseconds.</p><p>Linux 2.6 answers such a challenge by using two kinds of non-urgent interruptible kernel functions: the so-called deferrable functions (softirqs and tasklets), and those executed by means of some work queues.</p><p>Softirqs and tasklets are strictly correlated, because tasklets are implemented on top of softirqs. As a matter of fact, the term “softirq,” which appears in the kernel source code, often denotes both kinds of deferrable functions.</p><h2 id="8-Work-Queues"><a href="#8-Work-Queues" class="headerlink" title="8 Work Queues"></a>8 Work Queues</h2><p>The <em>work queues</em> allow kernel functions to be activated (much like deferrable functions) and later executed by special kernel threads called <em>worker threads</em>.</p><p>Despite their similarities, deferrable functions and work queues are quite different. The main difference is that deferrable functions run in interrupt context while functions in work queues run in process context. Running in process context is the only way to execute functions that can block. No process switch can take place in interrupt context. A function in a work queue is executed by a kernel thread,</p><hr><p>参考资料：</p><ol><li><a href="http://home.ustc.edu.cn/~boj/courses/linux_kernel/2_int.html" target="_blank" rel="noopener">Linux源代码阅读——中断</a></li><li><a href="http://www.wowotech.net/irq_subsystem/interrupt_subsystem_architecture.html" target="_blank" rel="noopener">Linux kernel的中断子系统</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-linuxkernelint/index.html" target="_blank" rel="noopener">Linux 内核中断内幕</a></li><li><a href="https://my.oschina.net/fileoptions/blog/918164" target="_blank" rel="noopener">linux内核之中断实现原理</a></li><li><a href="https://www.tldp.org/LDP/lkmpg/2.6/html/x1256.html" target="_blank" rel="noopener">Interrupt Handlers</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;An &lt;em&gt;interrupt&lt;/em&gt; is usually defined as an event that alters the sequence of instructions executed by a processor.&lt;/p&gt;
&lt;p&gt;Intel microprocessor manuals designate synchronous and asynchronous interrupts as &lt;em&gt;exceptions&lt;/em&gt; and &lt;em&gt;interrupts&lt;/em&gt;.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Fix-Mapped Linear Addresses</title>
    <link href="http://liujunming.github.io/2018/11/28/Fix-Mapped-Linear-Addresses/"/>
    <id>http://liujunming.github.io/2018/11/28/Fix-Mapped-Linear-Addresses/</id>
    <published>2018-11-28T02:15:51.000Z</published>
    <updated>2018-11-28T03:59:22.179Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>fixmap是一段固定地址映射，kernel预留的一段虚拟地址空间，虚拟地址是在编译的时候确定。fixmap可以用来做什么？kernel启动初期，由于此时的kernel已经运行在虚拟地址上。因此我们访问具体的物理地址是不行的，必须建立虚拟地址和物理地址的映射，然后通过虚拟地址访问才可以。例如：dtb中包含bootloader传递过来的内存信息，我们需要解析dtb，但是我们得到的是dtb的物理地址。因此访问之前必须创建映射，创建映射又需要内存系统。但是由于所有的内存管理子系统还没有ready，因此我们不能使用ioremap接口创建映射，为此kernel提出fixmap的解决方案。<br><a id="more"></a></p><h2 id="fixmap空间分配"><a href="#fixmap空间分配" class="headerlink" title="fixmap空间分配"></a>fixmap空间分配</h2><p>fixmap虚拟地址空间又被平均分成两个部分permanent fixed addresses和temporary fixed addresses。permanent fixed addresses是永久映射，temporary fixed addresses是临时映射。永久映射是指在建立的映射关系在kernel阶段不会改变，仅供特定模块一直使用。临时映射就是模块使用前创建映射，使用后解除映射。</p><p>With respect to variable pointers, fix-mapped linear addresses are more efficient. In fact, dereferencing a variable pointer requires one memory access more than dereferencing an immediate constant address. Moreover, checking the value of a variable pointer before dereferencing it is a good programming practice; conversely, the check is never required for a constant linear address.</p><p>具体函数可以参考Understanding the Linux Kernel p72.</p><hr><p>参考资料：</p><ol><li><a href="https://zohead.com/archives/linux-kernel-learning-memory-addressing/" target="_blank" rel="noopener">zohead</a></li><li><a href="https://www.spinics.net/lists/newbies/msg31797.html" target="_blank" rel="noopener">What is fixmaps?</a></li><li><a href="http://www.wowotech.net/memory_management/440.html" target="_blank" rel="noopener">fixmap addresses原理</a></li><li><a href="http://students.mimuw.edu.pl/ZSO/Wyklady/04_pamiec/4_pamiec_en.html#highmem" target="_blank" rel="noopener">Mapping frames from highmem</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;fixmap是一段固定地址映射，kernel预留的一段虚拟地址空间，虚拟地址是在编译的时候确定。fixmap可以用来做什么？kernel启动初期，由于此时的kernel已经运行在虚拟地址上。因此我们访问具体的物理地址是不行的，必须建立虚拟地址和物理地址的映射，然后通过虚拟地址访问才可以。例如：dtb中包含bootloader传递过来的内存信息，我们需要解析dtb，但是我们得到的是dtb的物理地址。因此访问之前必须创建映射，创建映射又需要内存系统。但是由于所有的内存管理子系统还没有ready，因此我们不能使用ioremap接口创建映射，为此kernel提出fixmap的解决方案。&lt;br&gt;
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>内核页表和进程页表</title>
    <link href="http://liujunming.github.io/2018/11/27/%E5%86%85%E6%A0%B8%E9%A1%B5%E8%A1%A8%E5%92%8C%E8%BF%9B%E7%A8%8B%E9%A1%B5%E8%A1%A8/"/>
    <id>http://liujunming.github.io/2018/11/27/内核页表和进程页表/</id>
    <published>2018-11-27T14:00:35.000Z</published>
    <updated>2018-11-28T02:41:18.710Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自：<a href="http://blog.chinaunix.net/uid-14528823-id-4334619.html" target="_blank" rel="noopener">chinaunix</a></p><p>初学内核时，经常被“内核页表”和“进程页表”搞晕，不知道这到底是个啥东东，跟我们平时理解的页表有和关系。</p><ul><li><p>内核页表：即书上说的<strong>主内核页表</strong>，在内核中其实就是一段内存，存放在主内核页全局目录init_mm.pgd(swapper_pg_dir)中，硬件并不直接使用。</p></li><li><p>进程页表：每个进程自己的页表，放在进程自身的页目录task_struct.pgd中。</p></li></ul><a id="more"></a><p>在保护模式下，从硬件角度看，其运行的基本对象为“进程”(或线程)，而寻址则依赖于“进程页表”，在进程调度而进行上下文切换时，会进行页表的切换：即将新进程的pgd(页目录)加载到CR3寄存器中。从这个角度看，其实是完全没有用到“内核页表”的，那么“内核页表”有什么用呢？跟“进程页表”有什么关系呢？</p><p>1、内核页表中的内容为所有进程共享，每个进程都有自己的“进程页表”，“进程页表”中映射的线性地址包括两部分：</p><ul><li>用户态</li><li>内核态</li></ul><p>其中，内核态地址对应的相关页表项，对于所有进程来说都是相同的(因为内核空间对所有进程来说都是共享的)，而这部分页表内容其实就来源于“内核页表”，即每个进程的“进程页表”中内核态地址相关的页表项都是“内核页表”的一个拷贝。<br>2、“内核页表”由内核自己维护并更新，在vmalloc区发生page fault时，将“内核页表”同步到“进程页表”中。以32位系统为例，内核页表主要包含两部分：</p><ul><li>线性映射区</li><li>vmalloc区</li></ul><p>其中，线性映射区即通过<code>TASK_SIZE</code>偏移进行映射的区域，对32系统来说就是0-896M这部分区域，映射对应的虚拟地址区域为<code>TASK_SIZE~TASK_SIZE+896M</code>。这部分区域在内核初始化时就已经完成映射，并创建好相应的页表，即这部分虚拟内存区域不会发生page fault。</p><p>vmalloc区，为<code>896M~896M+128M</code>，这部分区域用于映射高端内存，有三种映射方式：vmalloc、固定、临时，这里就不详述了。。<br>以vmalloc为例(最常使用)，这部分区域对应的线性地址在内核使用vmalloc分配内存时，其实就已经分配了相应的物理内存，并做了相应的映射，建立了相应的页表项，但相关页表项仅写入了“内核页表”，并没有实时更新到“进程页表中”，内核在这里使用了“延迟更新”的策略，将“进程页表”真正更新推迟到第一次访问相关线性地址，发生page fault时，此时在<a href="https://elixir.bootlin.com/linux/v4.19.4/source/arch/x86/mm/fault.c#L1244" target="_blank" rel="noopener">page fault</a>的处理流程中进行“进程页表”的更新：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 缺页地址位于内核空间。并不代表异常发生于内核空间，有可能是用户</span></span><br><span class="line"><span class="comment"> * 态访问了内核空间的地址。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (unlikely(fault_in_kernel_space(address))) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!(error_code &amp; (PF_RSVD | PF_USER | PF_PROT))) &#123;</span><br><span class="line">        <span class="comment">//检查发生缺页的地址是否在vmalloc区，是则进行相应的处理</span></span><br><span class="line">        <span class="keyword">if</span> (vmalloc_fault(address) &gt;= <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 对于发生缺页异常的指针位于vmalloc区情况的处理，主要是将</span></span><br><span class="line"><span class="comment">  * 主内核页表向当前进程的内核页表同步。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">static</span> noinline __<span class="function">kprobes <span class="keyword">int</span> <span class="title">vmalloc_fault</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> address)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> pgd_paddr;</span><br><span class="line">    <span class="keyword">pmd_t</span> *pmd_k;</span><br><span class="line">    <span class="keyword">pte_t</span> *pte_k;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Make sure we are in vmalloc area: */</span></span><br><span class="line">    <span class="comment">/* 区域检查 */</span></span><br><span class="line">    <span class="keyword">if</span> (!(address &gt;= VMALLOC_START &amp;&amp; address &lt; VMALLOC_END))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    WARN_ON_ONCE(in_nmi());</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Synchronize this task's top level page-table</span></span><br><span class="line"><span class="comment">     * with the 'reference' page table.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * Do _not_ use "current" here. We might be inside</span></span><br><span class="line"><span class="comment">     * an interrupt in the middle of a task switch..</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">     <span class="comment">/*获取pgd(最顶级页目录)地址，直接从CR3寄存器中读取。</span></span><br><span class="line"><span class="comment">     *不要通过current获取，因为缺页异常可能在上下文切换的过程中发生，</span></span><br><span class="line"><span class="comment">     *此时如果通过current获取，则可能会出问题*/</span></span><br><span class="line">    pgd_paddr = read_cr3();</span><br><span class="line">    <span class="comment">//从主内核页表中，同步vmalloc区发生缺页异常地址对应的页表</span></span><br><span class="line">    pmd_k = vmalloc_sync_one(__va(pgd_paddr), address);</span><br><span class="line">    <span class="keyword">if</span> (!pmd_k)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">//如果同步后，相应的PTE还不存在，则说明该地址有问题了</span></span><br><span class="line">    pte_k = pte_offset_kernel(pmd_k, address);</span><br><span class="line">    <span class="keyword">if</span> (!pte_present(*pte_k))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时，问题来了，为什么需要内核页表呢？<br>详情可以参考<a href="http://bbs.chinaunix.net/thread-4190879-1-1.html" target="_blank" rel="noopener">关于内核页表初始化的问题</a>。<br><strong>目的</strong>主要是为了让cpu从real mode平稳过渡到protected mode，read mode下分页尚未开启。</p><p>更多细节请参考<a href="http://students.mimuw.edu.pl/ZSO/Wyklady/04_pamiec/4_pamiec_en.html#tablice_stron_jadra" target="_blank" rel="noopener">Kernel page tables</a></p><hr><p>参考资料：</p><ol><li><a href="http://bbs.chinaunix.net/thread-4190879-1-1.html" target="_blank" rel="noopener">关于内核页表初始化的问题</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自：&lt;a href=&quot;http://blog.chinaunix.net/uid-14528823-id-4334619.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;chinaunix&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;初学内核时，经常被“内核页表”和“进程页表”搞晕，不知道这到底是个啥东东，跟我们平时理解的页表有和关系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;内核页表：即书上说的&lt;strong&gt;主内核页表&lt;/strong&gt;，在内核中其实就是一段内存，存放在主内核页全局目录init_mm.pgd(swapper_pg_dir)中，硬件并不直接使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;进程页表：每个进程自己的页表，放在进程自身的页目录task_struct.pgd中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Memory Addressing</title>
    <link href="http://liujunming.github.io/2018/11/27/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Memory-Addressing/"/>
    <id>http://liujunming.github.io/2018/11/27/Understanding-the-Linux-Kernel-读书笔记-Memory-Addressing/</id>
    <published>2018-11-27T11:24:42.000Z</published>
    <updated>2018-11-27T12:54:18.242Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Memory-Addresses"><a href="#Memory-Addresses" class="headerlink" title="Memory Addresses"></a>Memory Addresses</h2><p><strong>Logical address</strong></p><ul><li>Included in the machine language instructions to specify the address of an operand or of an instruction. Each logical address consists of a segment and an offset that denotes the distance from the start of the segment to the actual address.<a id="more"></a></li></ul><p><strong>Linear address (also known as virtual address)</strong></p><ul><li>A single 32-bit unsigned integer that can be used to address up to 4 GB. Linear addresses are usually represented in hexadecimal notation; their values range from 0x00000000 to 0xffffffff.</li></ul><p><strong>Physical address</strong></p><ul><li>Used to address memory cells in memory chips. Physical addresses are represented as 32-bit or 36-bit unsigned integers.</li></ul><p>The Memory Management Unit (MMU) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address.<br><img src="/images/2018/11/15.png" alt=""></p><h2 id="Segmentation-in-Hardware"><a href="#Segmentation-in-Hardware" class="headerlink" title="Segmentation in Hardware"></a>Segmentation in Hardware</h2><p>Starting with the 80286 model, Intel microprocessors perform address translation in two different ways called <em>real mode</em> and <em>protected mode</em>. Real mode exists mostly to maintain processor compatibility with older models and to allow the operating system to bootstrap.<br>详情参见<a href="http://liujunming.top/2018/11/16/%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/" target="_blank" rel="noopener">分段机制解析</a></p><h2 id="Paging-in-Hardware"><a href="#Paging-in-Hardware" class="headerlink" title="Paging in Hardware"></a>Paging in Hardware</h2><p>The paging unit thinks of all RAM as partitioned into fixed-length <em>page frames</em>(sometimes referred to as <em>physical pages</em>). Each page frame contains a page — that is, the length of a page frame coincides with that of a page. A page frame is a constituent of main memory, and hence it is a storage area. It is important to distinguish a page from a page frame; the former is just a block of data, which may be stored in any page frame or on disk.</p><p>The data structures that map linear to physical addresses are called <em>page tables</em>; they are stored in main memory and must be properly initialized by the kernel before enabling the paging unit.</p><h3 id="3-1-Regular-Paging"><a href="#3-1-Regular-Paging" class="headerlink" title="3.1 Regular Paging"></a>3.1 Regular Paging</h3><p><img src="/images/2018/11/16.png" alt=""></p><h3 id="3-2-Hardware-Protection-Scheme"><a href="#3-2-Hardware-Protection-Scheme" class="headerlink" title="3.2 Hardware Protection Scheme"></a>3.2 Hardware Protection Scheme</h3><p>While 80×86 processors allow four possible privilege levels to a segment, only two privilege levels are associated with pages and Page Tables.</p><p><strong>User/Supervisor flag</strong></p><ul><li>0, can be addressed only when the CPL is less than 3 (Kernel Mode)</li><li>1, can always be addressed</li></ul><p><strong>Read/Write flag</strong></p><ul><li>0, read-only</li><li>1, can be read and written</li></ul><h3 id="3-3-The-Physical-Address-Extension-PAE-Paging-Mechanism"><a href="#3-3-The-Physical-Address-Extension-PAE-Paging-Mechanism" class="headerlink" title="3.3 The Physical Address Extension (PAE) Paging Mechanism"></a>3.3 The Physical Address Extension (PAE) Paging Mechanism</h3><h3 id="3-4-Hardware-Cache"><a href="#3-4-Hardware-Cache" class="headerlink" title="3.4 Hardware Cache"></a>3.4 Hardware Cache</h3><p>Hardware cache memories were introduced to reduce the speed mismatch between CPU and RAM. They are based on the well-known <em>locality</em> principle, which holds both for programs and data structures.</p><h3 id="3-5-Translation-Lookaside-Buffers-TLB"><a href="#3-5-Translation-Lookaside-Buffers-TLB" class="headerlink" title="3.5 Translation Lookaside Buffers (TLB)"></a>3.5 Translation Lookaside Buffers (TLB)</h3><p>Besides general-purpose hardware caches, 80x86 processors include another cache called <em>Translation Lookaside Buffers</em> (TLB) to speed up linear address translation. When a linear address is used for the first time, the corresponding physical address is computed through slow accesses to the Page Tables in RAM. The physical address is then stored in a TLB entry so that further references to the same linear address can be quickly translated.</p><p>In a multiprocessor system, each CPU has its own TLB, called the <em>local TLB</em> of the CPU. When the <em>cr3</em> control register of a CPU is modified, the hardware automatically invalidates all entries of the local TLB, because a new set of page tables is in use and the TLBs are pointing to old data.</p><h2 id="Paging-in-Linux"><a href="#Paging-in-Linux" class="headerlink" title="Paging in Linux"></a>Paging in Linux</h2><p>Linux adopts a common paging model that fits both 32-bit and 64-bit architectures. Starting with version 2.6.11, a four-level paging model has been adopted:</p><ul><li>Page Global Directory</li><li>Page Upper Directory</li><li>Page Middle Directory</li><li>Page Table</li></ul><p><img src="/images/2018/11/17.png" alt=""></p><p>Each process has its own Page Global Directory and its own set of Page Tables. When a process switch occurs, Linux saves the cr3 control register in the descriptor of the process previously in execution and then loads cr3 with the value stored in the descriptor of the process to be executed next. Thus, when the new process resumes its execution on the CPU, the paging unit refers to the correct set of Page Tables.</p><h3 id="4-1-Page-Table-Handling"><a href="#4-1-Page-Table-Handling" class="headerlink" title="4.1 Page Table Handling"></a>4.1 Page Table Handling</h3><h3 id="4-2-Physical-Memory-Layout"><a href="#4-2-Physical-Memory-Layout" class="headerlink" title="4.2 Physical Memory Layout"></a>4.2 Physical Memory Layout</h3><p>As a general rule, the Linux kernel is installed in RAM starting from the physical address 0x00100000 — i.e., from the second megabyte. The reason that kernel isn’t loaded starting with the first availalbe megabyte of RAM includes:</p><ul><li>Page frame 0 is used by BIOS to store the system hardware configuration detected during the Power-On Self-Test (POST); the BIOS of many laptops, moreover, writes data on this page frame even after the system is initialized.</li><li>Physical addresses ranging from 0x000a0000 to 0x000fffff are usually reserved to BIOS routines and to map the internal memory of ISA graphics cards.</li><li>Additional page frames within the first megabyte may be reserved by specific computer models.</li></ul><h3 id="4-3-Process-Page-Tables"><a href="#4-3-Process-Page-Tables" class="headerlink" title="4.3 Process Page Tables"></a>4.3 Process Page Tables</h3><p>The linear address space of a process is divided into two parts:</p><ul><li>Linear addresses from <code>0x00000000</code> to <code>0xbfffffff</code> can be addressed when the process runs in either User or Kernel Mode.</li><li>Linear addresses from <code>0xc0000000</code> to <code>0xffffffff</code> can be addressed only when the process runs in Kernel Mode.</li></ul><p>The <code>PAGE_OFFSET</code> macro yields the value 0xc0000000; this is the offset in the linear address space of a process where the kernel lives.</p><h3 id="4-4-Handling-the-Hardware-Cache-and-the-TLB"><a href="#4-4-Handling-the-Hardware-Cache-and-the-TLB" class="headerlink" title="4.4 Handling the Hardware Cache and the TLB"></a>4.4 Handling the Hardware Cache and the TLB</h3><hr><p>参考资料：</p><ol><li><a href="https://medium.com/hungys-blog/linux-kernel-memory-addressing-a0d304283af3" target="_blank" rel="noopener">Linux Kernel: Memory Addressing</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Memory-Addresses&quot;&gt;&lt;a href=&quot;#Memory-Addresses&quot; class=&quot;headerlink&quot; title=&quot;Memory Addresses&quot;&gt;&lt;/a&gt;Memory Addresses&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Logical address&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Included in the machine language instructions to specify the address of an operand or of an instruction. Each logical address consists of a segment and an offset that denotes the distance from the start of the segment to the actual address.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Process</title>
    <link href="http://liujunming.github.io/2018/11/26/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process/"/>
    <id>http://liujunming.github.io/2018/11/26/Understanding-the-Linux-Kernel-读书笔记-Process/</id>
    <published>2018-11-26T09:03:51.000Z</published>
    <updated>2018-11-27T09:01:16.481Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Processes-Lightweight-Processes-and-Threads"><a href="#Processes-Lightweight-Processes-and-Threads" class="headerlink" title="Processes, Lightweight Processes, and Threads"></a>Processes, Lightweight Processes, and Threads</h2><p>A process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. The purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated.<br><a id="more"></a><br>Linux uses <em>lightweight processes</em> to offer better support for multithreaded applications. A straightforward way to implement multithreaded applications is to associate a lightweight process with each thread. Each thread can be scheduled independently by the kernel so that one may sleep while another remains runnable.</p><h2 id="Process-Descriptor"><a href="#Process-Descriptor" class="headerlink" title="Process Descriptor"></a>Process Descriptor</h2><p>This is the role of the process descriptor—a <code>task_ struct</code> type structure whose fields contain all the information related to a single process.</p><h3 id="2-1-Process-State"><a href="#2-1-Process-State" class="headerlink" title="2.1 Process State"></a>2.1 Process State</h3><h3 id="2-2-Identifying-a-Process"><a href="#2-2-Identifying-a-Process" class="headerlink" title="2.2 Identifying a Process"></a>2.2 Identifying a Process</h3><p>Linux associates a different PID with each process or lightweight process in the system. However, POSIX standard states that all threads of a multithreaded application must have the same PID.<br>To comply with this standard, the identifier shared by the threads is the PID of the <em>thread group leader</em>, that is, the PID of the first lightweight process in the group; it is stored in the <code>tgid</code> field of the process descriptors.<br>The <code>getpid()</code> system call returns the value of <code>tgid</code> relative to the current process instead of the value of <code>pid</code>, so all the threads of a multithreaded application share the same identifier.</p><h4 id="2-2-1-Process-descriptors-handling"><a href="#2-2-1-Process-descriptors-handling" class="headerlink" title="2.2.1 Process descriptors handling"></a>2.2.1 Process descriptors handling</h4><p>For each process, Linux packs two different data structures in a single per-process memory area: a small data structure linked to the process descriptor, namely the <code>thread_info</code> structure, and the <strong>Kernel Mode process stack</strong>.<br><img src="/images/2018/11/12.png" alt=""></p><h4 id="2-2-2-Identifying-the-current-process"><a href="#2-2-2-Identifying-the-current-process" class="headerlink" title="2.2.2 Identifying the current process"></a>2.2.2 Identifying the current process</h4><p>The kernel can easily obtain the address of the <code>thread_info</code> structure of the process currently running on a CPU from the value of the esp register.</p><h4 id="2-2-3-The-process-list"><a href="#2-2-3-The-process-list" class="headerlink" title="2.2.3 The process list"></a>2.2.3 The process list</h4><h4 id="2-2-4-The-lists-of-TASK-RUNNING-processes"><a href="#2-2-4-The-lists-of-TASK-RUNNING-processes" class="headerlink" title="2.2.4 The lists of TASK_RUNNING processes"></a>2.2.4 The lists of TASK_RUNNING processes</h4><p>When looking for a new process to run on a CPU, the kernel has to consider only the <strong>runnable</strong> processes (that is, the processes in the <code>TASK_RUNNING</code>state).</p><p>Linux 2.6 implements the runqueue without putting all runnable processes in the same list. The aim is to allow the scheduler to select the best runnable process in constant time.</p><p>The trick is splitting the runqueue in many lists of runnable processes, one list per process priority. Each <code>task_struct</code> descriptor includes a <code>run_list</code>field which points to the corresponding runqueue.</p><p>All these are implemented by a single data structure <code>prio_array_t</code>. Noted that each CPU has its own runqueue.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">prio_array</span> &#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> nr_active; <span class="comment">/* The number of process descriptors linked into the lists */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> bitmap[BITMAP_SIZE];  <span class="comment">/* A priority bitmap: each flag is set if and only if the corre- sponding priority list is not empty */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">queue</span>[<span class="title">MAX_PRIO</span>];</span> <span class="comment">/* The 140 heads of the priority lists */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="2-3-Relationships-Among-Processes"><a href="#2-3-Relationships-Among-Processes" class="headerlink" title="2.3 Relationships Among Processes"></a>2.3 Relationships Among Processes</h3><p>The pidhash table and chained lists<br><img src="/images/2018/11/13.png" alt=""></p><p><img src="/images/2018/11/14.png" alt=""></p><h3 id="2-4-How-Processes-Are-Organized"><a href="#2-4-How-Processes-Are-Organized" class="headerlink" title="2.4 How Processes Are Organized"></a>2.4 How Processes Are Organized</h3><h4 id="2-4-1-Wait-queues"><a href="#2-4-1-Wait-queues" class="headerlink" title="2.4.1 Wait queues"></a>2.4.1 Wait queues</h4><p>A wait queue represents a set of sleeping processes, which are woken up by the kernel when some condition becomes true.</p><h4 id="2-4-2-Handling-wait-queues"><a href="#2-4-2-Handling-wait-queues" class="headerlink" title="2.4.2 Handling wait queues"></a>2.4.2 Handling wait queues</h4><h3 id="2-5-Process-Resource-Limits"><a href="#2-5-Process-Resource-Limits" class="headerlink" title="2.5 Process Resource Limits"></a>2.5 Process Resource Limits</h3><p>Each process has an associated set of resource limits, which specify the amount of system resources it can use.<br>The resource limits are stored in an array (<code>task_struct-&gt;signal</code>) of elements of type struct rlimit.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">rlimit</span> &#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span>   rlim_cur;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span>   rlim_max;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h2 id="Process-Switch"><a href="#Process-Switch" class="headerlink" title="Process Switch"></a>Process Switch</h2><h3 id="3-1-Hardware-Context"><a href="#3-1-Hardware-Context" class="headerlink" title="3.1 Hardware Context"></a>3.1 Hardware Context</h3><p>The set of data that must be loaded into the registers before the process resumes its execution on the CPU is called the <em>hardware context</em>. The hardware context is a subset of the process execution context, which includes all information needed for the process execution. In Linux, a part of the hardware context of a process is stored in the process descriptor, while the remaining part is saved in the Kernel Mode stack.</p><p>Process switching occurs only in Kernel Mode. The contents of all registers used by a process in User Mode have already been saved on the <strong>Kernel Mode</strong>stack before performing process switching.</p><h3 id="3-2-Task-State-Segment"><a href="#3-2-Task-State-Segment" class="headerlink" title="3.2 Task State Segment"></a>3.2 Task State Segment</h3><p>The 80x86 architecture includes a specific segment type called the <em>Task State Segment</em>(TSS), to store hardware contexts. While in Linux, there is only single TSS for each processor, each process descriptor includes additional field <code>struct thread_struct thread</code>, in which the kernel saves the hardware context whenever the process is being switched out. The data structutr includes fields for most of the CPU registers, except the general-purpose registers such as eax, ebx, etc., which are stored in the Kernel Mode stack.</p><h3 id="3-3-Performing-the-Process-Switch"><a href="#3-3-Performing-the-Process-Switch" class="headerlink" title="3.3 Performing the Process Switch"></a>3.3 Performing the Process Switch</h3><p>Essentially, every process switch consists of two steps:</p><ol><li>Switching the Page Global Directory to install a new address space;</li><li>Switching the Kernel Mode stack and the hardware context</li></ol><p>The <code>switch_to</code> macro is used to switch the Kernel Mode stack and the hardware context. The macro is a hardware-dependent routine.</p><h2 id="Creating-Processes"><a href="#Creating-Processes" class="headerlink" title="Creating Processes"></a>Creating Processes</h2><ul><li><em>Copy On Write</em>: Allow both the parent and the child to read the same physical pages. Whenever either one tries to write on a physical page, the kernel copies its contents into a new physical page that is assigned to the writing process.</li><li>Lightweight process: Allow both the parent and the child to share many per-process kernel data structures, such as the paging tables (and therefore the entire User Mode address space), the open file tables, and the signal dispositions.</li><li><code>vfork()</code> system call: Create a process that shares the memory address space of its parent. To prevent the parent from overwriting data needed by the child, the parent’s execution is blocked until the child exits or executes a new program.</li></ul><h3 id="4-1-The-clone-fork-and-vfork-System-Calls"><a href="#4-1-The-clone-fork-and-vfork-System-Calls" class="headerlink" title="4.1 The clone(), fork(), and vfork() System Calls"></a>4.1 The clone(), fork(), and vfork() System Calls</h3><p>Lightweight processes are created in Linux by using a function named <code>clone()</code>.clone() is actually a wrapper function defined in the C library, which sets up the stack of the new lightweight process and invokes a clone() system call hidden to the programmer.</p><p><code>fork</code> will make an exact copy of the parent’s address space and give it to the child. Therefore, the parent and child processes have separate address spaces.</p><h3 id="4-2-Kernel-Threads"><a href="#4-2-Kernel-Threads" class="headerlink" title="4.2 Kernel Threads"></a>4.2 Kernel Threads</h3><p>In Linux, kernel threads differ from regular processes in the following ways:</p><ul><li>Kernel threads run only in Kernel Mode, while regular processes run alterna- tively in Kernel Mode and in User Mode.</li><li>Because kernel threads run only in Kernel Mode, they use only linear addresses greater than PAGE_OFFSET.</li></ul><p>0号线程是scheduler，1号线程是init/systemd（所有user thread的祖先），2号线程是[kthreadd]（所有kernel thread的父进程）。</p><hr><p>参考资料：</p><ol><li><a href="https://medium.com/hungys-blog/linux-kernel-process-99629d91423c" target="_blank" rel="noopener">Linux Kernel: Process</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Processes-Lightweight-Processes-and-Threads&quot;&gt;&lt;a href=&quot;#Processes-Lightweight-Processes-and-Threads&quot; class=&quot;headerlink&quot; title=&quot;Processes, Lightweight Processes, and Threads&quot;&gt;&lt;/a&gt;Processes, Lightweight Processes, and Threads&lt;/h2&gt;&lt;p&gt;A process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. The purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated.&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>进程切换switch_to宏第三个参数分析</title>
    <link href="http://liujunming.github.io/2018/11/20/%E8%BF%9B%E7%A8%8B%E5%88%87%E6%8D%A2switch-to%E5%AE%8F%E7%AC%AC%E4%B8%89%E4%B8%AA%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90/"/>
    <id>http://liujunming.github.io/2018/11/20/进程切换switch-to宏第三个参数分析/</id>
    <published>2018-11-20T08:55:16.000Z</published>
    <updated>2018-11-26T09:00:09.614Z</updated>
    
    <content type="html"><![CDATA[<p>switch_to宏的<a href="https://elixir.bootlin.com/linux/v4.19.4/source/arch/x86/include/asm/switch_to.h#L70" target="_blank" rel="noopener">代码</a>如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> switch_to(prev, next, last)                 \</span></span><br><span class="line"><span class="keyword">do</span> &#123;                                    \</span><br><span class="line">    prepare_switch_to(next);                    \</span><br><span class="line">                                    \</span><br><span class="line">    ((last) = __switch_to_asm((prev), (next)));         \</span><br><span class="line">&#125; <span class="keyword">while</span> (<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>上下文切换不仅涉及两个进程，而是三个进程。该情形如下图所示：<br><img src="/images/2018/11/11.png" alt=""></p><p>假定3个进程A、B和C在系统上运行。在某个时间点，内核决定从进程A切换到进程B，然后从进程B切换到进程C，接下来再从进程C切换回进程A。在每个switch_to调用之前，next和prev指针位于各进程的内核栈上，prev指向当前运行的进程，而next指向将要运行的下一个进程。为执行从prev到next的切换，switch_to的前两个参数足够了。对进程A来说，prev指向进程A而next指向进程B。</p><p>在进程A被选中再次执行时，会出现一个问题，控制权返回至switch_to之后的点，如果栈准确地恢复到切换之前的状态，那么prev和next仍然指向切换之前的值，即next=B，而prev=A。在这种情况下，内核无法知道实际上在进程A之前运行的进程是C。</p><p>在新进程被选中时，底层的进程切换例程必须将此前执行的进程提供给context_switch。该宏是效果是，仿佛switch_to是带有两个参数的函数，而且返回了一个指向此前运行进程的指针。switch_to宏实际上执行的代码如下：<br><code>prev = switch_to(prev, next)</code><br>其中返回的prev值并不是用作参数的prev值，而是上一个执行的进程。在上述例子中，进程A提供给switch_to是参数是A和B，但恢复执行后得到的返回值是prev = C。内核实现该行为特性的方式依赖于底层的体系结构。</p><hr><p>参考资料：</p><ol><li>《Professional Linux Kernel Architecture》 p104</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;switch_to宏的&lt;a href=&quot;https://elixir.bootlin.com/linux/v4.19.4/source/arch/x86/include/asm/switch_to.h#L70&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;代码&lt;/a&gt;如下：&lt;br&gt;&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;define&lt;/span&gt; switch_to(prev, next, last)                 \&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;do&lt;/span&gt; &amp;#123;                                    \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    prepare_switch_to(next);                    \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                                    \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ((last) = __switch_to_asm((prev), (next)));         \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125; &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt; (&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>分段机制解析</title>
    <link href="http://liujunming.github.io/2018/11/16/%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/"/>
    <id>http://liujunming.github.io/2018/11/16/分段机制解析/</id>
    <published>2018-11-16T11:59:23.000Z</published>
    <updated>2018-11-17T07:30:23.959Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬件中的分段"><a href="#硬件中的分段" class="headerlink" title="硬件中的分段"></a>硬件中的分段</h2><h3 id="1-1-段选择符和段寄存器"><a href="#1-1-段选择符和段寄存器" class="headerlink" title="1.1 段选择符和段寄存器"></a>1.1 段选择符和段寄存器</h3><p>分段是一种朴素的内存管理机制，它将内存划分成以起始地址(Base)和长度(Limit)描述的块，这些内存块就被称为“段”。<a id="more"></a>一个逻辑地址由两部分组成：一个段选择符和段内偏移量。段选择符是一个16位长的字段，格式如下图所示：<br><img src="/images/2018/11/6.png" alt=""><br>为了快速方便地找到段选择符，处理器提供段寄存器，段寄存器的唯一目的就是存放段选择符。这些寄存器称为cs，ss，ds，es，fs和gs。</p><h3 id="1-2-段描述符"><a href="#1-2-段描述符" class="headerlink" title="1.2 段描述符"></a>1.2 段描述符</h3><p>段描述符描述某个段的基地址、长度以及各种属性。段描述符放在全局描述符表(Global Descriptor Table,GDT)或局部描述符表(Local Descriptor Table,LDT)中。</p><p>系统中至少有一个GDT可以被所有进程共享。相对的，系统中可以有一个或多个LDT,可以被某个进程私有，也可以被多个进程共享。GDT仅仅是内存中的一个数据结构，可以把它看成一个数组，每个元素由基地址(Base)和长度(Limit)描述。LDT则是一个段，它需要一个段描述符来描述它。LDT的的段描述符存放在GDT中，当系统中有多个LDT时，GDT中就必须有对应数量的段描述符。</p><p>GDT在主存中的地址和大小存放在gdtr寄存器中，当前正在被使用的LDT的地址和大小放在ldtr寄存器中。</p><p>通过段选择符索引GDT/LDT的过程如下图：<br><img src="/images/2018/11/7.png" alt=""></p><h3 id="1-3-快速访问段描述符"><a href="#1-3-快速访问段描述符" class="headerlink" title="1.3 快速访问段描述符"></a>1.3 快速访问段描述符</h3><p>段寄存器仅仅存放段选择符。为了加速逻辑地址到线性地址的转换，80x86处理器提供一种非编程的寄存器，供6个可编程的段寄存器使用。每一个非编程的寄存器含有8个字节的段描述符，由相应的段寄存器中的段选择符来指定。每当一个段选择符被装入段寄存器时，相应的段描述符就由内存装入到对应的非编程CPU寄存器。从那时起，针对那个段的逻辑地址转换就可以不访问主存中的GDT或LDT，处理器只需要直接引用存放段描述符的CPU寄存器即可。仅当段寄存器的内容改变时，才有必要访问GDT或LDT，过程如下图：<br><img src="/images/2018/11/5.png" alt=""></p><h3 id="1-4-分段单元"><a href="#1-4-分段单元" class="headerlink" title="1.4 分段单元"></a>1.4 分段单元</h3><p><img src="/images/2018/11/8.png" alt=""></p><h2 id="Linux中的分段"><a href="#Linux中的分段" class="headerlink" title="Linux中的分段"></a>Linux中的分段</h2><p>四个主要的Linux段的段描述符字段的值如下：<br><img src="/images/2018/11/9.png" alt=""></p><h3 id="2-1-Linux-GDT"><a href="#2-1-Linux-GDT" class="headerlink" title="2.1 Linux GDT"></a>2.1 Linux GDT</h3><p>在单处理器系统中只有一个GDT，而在多处理器系统中每个CPU对应一个GDT。</p><p>下图是GDT的布局示意图：<br><img src="/images/2018/11/10.png" alt=""></p><h3 id="2-2-The-Linux-LDTs"><a href="#2-2-The-Linux-LDTs" class="headerlink" title="2.2 The Linux LDTs"></a>2.2 The Linux LDTs</h3><p>大多数用户态下的Linux程序不使用局部描述符表，这样内核就定义了一个缺省的LDT供大多数进程共享。</p><hr><p>参考资料：</p><ol><li>《深入理解linux内核》</li><li>《系统虚拟化：原理与实现》</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;硬件中的分段&quot;&gt;&lt;a href=&quot;#硬件中的分段&quot; class=&quot;headerlink&quot; title=&quot;硬件中的分段&quot;&gt;&lt;/a&gt;硬件中的分段&lt;/h2&gt;&lt;h3 id=&quot;1-1-段选择符和段寄存器&quot;&gt;&lt;a href=&quot;#1-1-段选择符和段寄存器&quot; class=&quot;headerlink&quot; title=&quot;1.1 段选择符和段寄存器&quot;&gt;&lt;/a&gt;1.1 段选择符和段寄存器&lt;/h3&gt;&lt;p&gt;分段是一种朴素的内存管理机制，它将内存划分成以起始地址(Base)和长度(Limit)描述的块，这些内存块就被称为“段”。
    
    </summary>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="计算机系统" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>内核中统计mysql进程匿名页的访问热度</title>
    <link href="http://liujunming.github.io/2018/11/14/%E5%86%85%E6%A0%B8%E4%B8%AD%E7%BB%9F%E8%AE%A1mysql%E8%BF%9B%E7%A8%8B%E5%8C%BF%E5%90%8D%E9%A1%B5%E7%9A%84%E8%AE%BF%E9%97%AE%E7%83%AD%E5%BA%A6/"/>
    <id>http://liujunming.github.io/2018/11/14/内核中统计mysql进程匿名页的访问热度/</id>
    <published>2018-11-14T02:06:05.000Z</published>
    <updated>2018-12-10T06:11:00.247Z</updated>
    
    <content type="html"><![CDATA[<p>遇到一个需求，需要在内核中统计mysql进程匿名页的访问热度。<br><a id="more"></a></p><h3 id="配置系统环境"><a href="#配置系统环境" class="headerlink" title="配置系统环境"></a>配置系统环境</h3><p>将透明大页关闭:<br><code>echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</code></p><h3 id="寻找mysql的task-struct"><a href="#寻找mysql的task-struct" class="headerlink" title="寻找mysql的task_struct"></a>寻找mysql的task_struct</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> *<span class="title">task</span> = <span class="title">NULL</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//0:not found 1:found</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findMysql</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> *<span class="title">proc</span>;</span></span><br><span class="line">    for_each_process(proc)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">strcmp</span>(proc-&gt;comm, <span class="string">"mysqld"</span>))&#123;</span><br><span class="line">            task = proc;</span><br><span class="line">            ret = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="统计匿名页热度"><a href="#统计匿名页热度" class="headerlink" title="统计匿名页热度"></a>统计匿名页热度</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> tem;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">page</span> *<span class="title">page</span> = <span class="title">NULL</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">page_cgroup</span> *<span class="title">pc</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vm_area_struct</span> *<span class="title">vma</span>;</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> addr = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(vma = task-&gt;mm-&gt;mmap; vma != <span class="literal">NULL</span>; vma = vma-&gt;vm_next)&#123;</span><br><span class="line">    <span class="keyword">for</span>(addr = vma-&gt;vm_start; addr &lt; vma-&gt;vm_end; addr += PAGE_SIZE)&#123;</span><br><span class="line">        page = follow_page(vma, addr, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (page &amp;&amp; PageAnon(page))&#123;</span><br><span class="line">            pc = lookup_page_cgroup(page);</span><br><span class="line">            tem = page_referenced(page, <span class="number">0</span>, pc, &amp;vm_flags) ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">            <span class="built_in">sprintf</span>(str_buf, <span class="string">"22222222:%ul:%d\n"</span>, page_to_pfn(page), tem);</span><br><span class="line">            write_to_file((<span class="keyword">void</span> *)str_buf, <span class="built_in">strlen</span>(str_buf));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遇到一个需求，需要在内核中统计mysql进程匿名页的访问热度。&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>内核中遍历所有的page</title>
    <link href="http://liujunming.github.io/2018/11/09/%E5%86%85%E6%A0%B8%E4%B8%AD%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E7%9A%84page/"/>
    <id>http://liujunming.github.io/2018/11/09/内核中遍历所有的page/</id>
    <published>2018-11-09T01:25:44.000Z</published>
    <updated>2018-11-09T01:35:12.330Z</updated>
    
    <content type="html"><![CDATA[<p>遇到一个需求，需要在内核模块中统计page cache页面的数量。<br><a id="more"></a><br>此刻，需要遍历所有的page，下面是相关代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> scan_pfn=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(;scan_pfn &lt; max_pfn; scan_pfn++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(pfn_valid(scan_pfn))&#123;</span><br><span class="line">        page = pfn_to_page(scan_pfn);</span><br><span class="line">        <span class="keyword">if</span>(page)&#123;</span><br><span class="line">            ......</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>从上述代码可以看出，通过<code>max_pfn</code>即可获取最大的page frame号。</p><p>判断page是否为page cache可使用下面的函数:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">is_file_page</span><span class="params">(struct page* pg)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> ret = <span class="literal">false</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">address_space</span> *<span class="title">mapping</span> = <span class="title">NULL</span>;</span></span><br><span class="line">    <span class="keyword">if</span>(pg &amp;&amp; !PageAnon(pg) &amp;&amp; !PageSwapCache(pg)) &#123;</span><br><span class="line">        mapping = page_file_mapping(pg);</span><br><span class="line">        <span class="keyword">if</span> (mapping) &#123;</span><br><span class="line">            BUG_ON(!mapping-&gt;host);</span><br><span class="line">        &#125;</span><br><span class="line">        ret = mapping;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遇到一个需求，需要在内核模块中统计page cache页面的数量。&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Linux内核中使用文件</title>
    <link href="http://liujunming.github.io/2018/11/08/Linux%E5%86%85%E6%A0%B8%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6/"/>
    <id>http://liujunming.github.io/2018/11/08/Linux内核中使用文件/</id>
    <published>2018-11-08T12:28:18.000Z</published>
    <updated>2018-11-08T13:00:12.749Z</updated>
    
    <content type="html"><![CDATA[<p>在Linux内核中，printk函数可以打印相关信息，并在/var/log/syslog中查看到对应的输出。但是，当需要在内核中统计一些信息的时候，由于数据较多，此刻，在syslog中会发生大量的信息丢失，因而，我们需要在内核中使用文件来存储统计信息。<br><a id="more"></a><br>由于需要在内存模块中使用文件，这就引发了一个问题：文件系统可能还未加载到内核中，在内存模块中无法使用文件。解决方法：在内存管理模块中，应该不断循环判断，直到文件系统加载到内核中，此刻，就可以使用文件来存储统计信息。</p><p><a href="https://github.com/liujunming/Tools/tree/master/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%9E%E9%AA%8C" target="_blank" rel="noopener">虚拟机实验</a>中ksm(页面访问频率统计)是具体使用的例子，在这里只解析使用文件的代码片段。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MY_FILE <span class="meta-string">"/home/test/stat.txt"</span> <span class="comment">//定义存放统计信息的文件</span></span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file</span> *<span class="title">log_file</span> = <span class="title">NULL</span>;</span> <span class="comment">//定义对应的file结构体</span></span><br><span class="line"><span class="keyword">char</span> str_buf[<span class="number">64</span>];  <span class="comment">//暂存数据缓冲区</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">write_to_file</span><span class="params">(<span class="keyword">void</span> *buffer, <span class="keyword">size_t</span> length)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (IS_ERR(log_file))&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">mm_segment_t</span> old_fs;</span><br><span class="line">    old_fs = get_fs();</span><br><span class="line">    set_fs(KERNEL_DS);</span><br><span class="line">    vfs_write(log_file, (<span class="keyword">char</span> *)buffer, length, &amp;log_file-&gt;f_pos);</span><br><span class="line">    set_fs(old_fs);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>处理逻辑:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (log_file == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        log_file = filp_open(MY_FILE, O_RDWR | O_APPEND | O_CREAT, <span class="number">0777</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (IS_ERR(log_file))&#123;</span><br><span class="line">        log_file = filp_open(MY_FILE, O_RDWR | O_APPEND | O_CREAT, <span class="number">0777</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(log_file != <span class="literal">NULL</span> &amp;&amp; !IS_ERR(log_file))&#123;</span><br><span class="line">        <span class="built_in">sprintf</span>(str_buf, <span class="string">"22222222:%ul:%d\n"</span>, page_to_pfn(page), tem); <span class="comment">//sprintf与printf用法相同</span></span><br><span class="line">        write_to_file((<span class="keyword">void</span> *)str_buf, <span class="built_in">strlen</span>(str_buf));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这样，就可以在内存模块使用文件了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Linux内核中，printk函数可以打印相关信息，并在/var/log/syslog中查看到对应的输出。但是，当需要在内核中统计一些信息的时候，由于数据较多，此刻，在syslog中会发生大量的信息丢失，因而，我们需要在内核中使用文件来存储统计信息。&lt;br&gt;
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -Consistency and Consensus</title>
    <link href="http://liujunming.github.io/2018/10/07/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Consistency-and-Consensus/"/>
    <id>http://liujunming.github.io/2018/10/07/Designing-Data-Intensive-Applications-读书笔记-Consistency-and-Consensus/</id>
    <published>2018-10-07T09:08:17.000Z</published>
    <updated>2018-11-06T04:58:45.491Z</updated>
    
    <content type="html"><![CDATA[<p><em>consensus</em>: that is, getting all of the nodes to agree on something. </p><p>If two nodes both believe that they are the leader, that situation is called split brain, and it often leads to data loss. Correct implementations of consensus help avoid such problems.<br><a id="more"></a></p><h2 id="Consistency-Guarantees"><a href="#Consistency-Guarantees" class="headerlink" title="Consistency Guarantees"></a>Consistency Guarantees</h2><p>Systems with stronger guarantees may have worse performance or be less fault-tolerant than systems with weaker guarantees.</p><p>Transaction isolation is primarily about avoiding race conditions due to concurrently executing transactions, whereas distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults.</p><h2 id="Linearizability"><a href="#Linearizability" class="headerlink" title="Linearizability"></a>Linearizability</h2><p>中文翻译：线性一致性(强一致性)</p><p>Linearizability，the basic idea is to make a system appear as if there were only one copy of the data, and all operations on it are atomic.</p><p><img src="/images/2018/10/1.png" alt=""></p><h3 id="What-Makes-a-System-Linearizable"><a href="#What-Makes-a-System-Linearizable" class="headerlink" title="What Makes a System Linearizable?"></a>What Makes a System Linearizable?</h3><p><img src="/images/2018/10/2.png" alt=""></p><p>To make the system linearizable, we need to add another constraint.<br><img src="/images/2018/10/3.png" alt=""></p><p><img src="/images/2018/10/4.png" alt=""></p><p><strong>Linearizability Versus Serializability</strong></p><ul><li><em>Serializability</em></li></ul><p>Serializability is an isolation property of transactions, where every transaction may read and write multiple objects (rows, documents, records).It guarantees that transactions behave the same as if they had executed in some serial order (each transaction running to completion before the next transaction starts).</p><ul><li><em>Linearizability</em></li></ul><p>Linearizability is a recency guarantee on reads and writes of a register (an individual object).  It doesn’t group operations together into transactions, so it does not prevent problems such as write skew, unless you take additional measures.</p><h3 id="Relying-on-Linearizability"><a href="#Relying-on-Linearizability" class="headerlink" title="Relying on Linearizability"></a>Relying on Linearizability</h3><h4 id="Locking-and-leader-election"><a href="#Locking-and-leader-election" class="headerlink" title="Locking and leader election"></a>Locking and leader election</h4><h4 id="Constraints-and-uniqueness-guarantees"><a href="#Constraints-and-uniqueness-guarantees" class="headerlink" title="Constraints and uniqueness guarantees"></a>Constraints and uniqueness guarantees</h4><p>Uniqueness constraints are common in databases.</p><p>Similar issues arise if you want to ensure that a bank account balance never goes neg‐ ative, or that you don’t sell more items than you have in stock in the warehouse, or that two people don’t concurrently book the same seat on a flight or in a theater. These constraints all require there to be a single up-to-date value (the account balance, the stock level, the seat occupancy) that all nodes agree on.</p><h4 id="Cross-channel-timing-dependencies"><a href="#Cross-channel-timing-dependencies" class="headerlink" title="Cross-channel timing dependencies"></a>Cross-channel timing dependencies</h4><p><img src="/images/2018/10/5.png" alt=""></p><p> The message queue (steps 3 and 4 in Figure 9-5) might be faster than the internal replication inside the storage service.</p><p> This problem arises because there are two different communication channels between the web server and the resizer: the file storage and the message queue.</p><h3 id="Implementing-Linearizable-Systems"><a href="#Implementing-Linearizable-Systems" class="headerlink" title="Implementing Linearizable Systems"></a>Implementing Linearizable Systems</h3><p>Linearizability essentially means “behave as though there is only a single copy of the data, and all operations on it are atomic”.</p><ul><li>Single-leader replication (potentially linearizable)</li><li>Consensus algorithms (linearizable)</li><li>Multi-leader replication (not linearizable)</li><li>Leaderless replication (probably not linearizable)</li></ul><h4 id="Linearizability-and-quorums"><a href="#Linearizability-and-quorums" class="headerlink" title="Linearizability and quorums"></a>Linearizability and quorums</h4><p>Intuitively, it seems as though strict quorum reads and writes should be linearizable in a Dynamo-style model. However, when we have variable network delays, it is possible to have race conditions, as demonstrated in Figure 9-6.</p><p><img src="/images/2018/10/6.png" alt=""></p><p>It is safest to assume that a leaderless system with Dynamo-style replication does not provide linearizability.</p><h3 id="The-Cost-of-Linearizability"><a href="#The-Cost-of-Linearizability" class="headerlink" title="The Cost of Linearizability"></a>The Cost of Linearizability</h3><p><img src="/images/2018/10/7.png" alt=""></p><h4 id="The-CAP-theorem"><a href="#The-CAP-theorem" class="headerlink" title="The CAP theorem"></a>The CAP theorem</h4><p>The CAP theorem as formally defined is of very narrow scope: it only considers one consistency model (namely linearizability) and one kind of fault(network partitions). It doesn’t say anything about network delays, dead nodes, or other trade-offs. Thus, although CAP has been historically influential, it has little practical value for designing systems.</p><h4 id="Linearizability-and-network-delays"><a href="#Linearizability-and-network-delays" class="headerlink" title="Linearizability and network delays"></a>Linearizability and network delays</h4><h2 id="Ordering-Guarantees"><a href="#Ordering-Guarantees" class="headerlink" title="Ordering Guarantees"></a>Ordering Guarantees</h2><h3 id="Ordering-and-Causality"><a href="#Ordering-and-Causality" class="headerlink" title="Ordering and Causality"></a>Ordering and Causality</h3><p>Causality imposes an ordering on events: cause comes before effect.If a system obeys the ordering imposed by causality, we say that it is <em>causally consistent</em>. </p><h4 id="The-causal-order-is-not-a-total-order"><a href="#The-causal-order-is-not-a-total-order" class="headerlink" title="The causal order is not a total order"></a>The causal order is not a total order</h4><h4 id="Linearizability-is-stronger-than-causal-consistency"><a href="#Linearizability-is-stronger-than-causal-consistency" class="headerlink" title="Linearizability is stronger than causal consistency"></a>Linearizability is stronger than causal consistency</h4><p>Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.</p><h4 id="Capturing-causal-dependencies"><a href="#Capturing-causal-dependencies" class="headerlink" title="Capturing causal dependencies"></a>Capturing causal dependencies</h4><p>In order to maintain causality, you need to know which operation happened before which other operation. </p><h3 id="Sequence-Number-Ordering"><a href="#Sequence-Number-Ordering" class="headerlink" title="Sequence Number Ordering"></a>Sequence Number Ordering</h3><p>We can use sequence numbers or timestamps to order events.</p><h4 id="Lamport-timestamps"><a href="#Lamport-timestamps" class="headerlink" title="Lamport timestamps"></a>Lamport timestamps</h4><p><img src="/images/2018/10/8.png" alt=""></p><h4 id="Timestamp-ordering-is-not-sufficient"><a href="#Timestamp-ordering-is-not-sufficient" class="headerlink" title="Timestamp ordering is not sufficient"></a>Timestamp ordering is not sufficient</h4><p>It is not sufficient when a node has just received a request from a user to create a username, and needs to decide <em>right now</em> whether the request should succeed or fail. </p><p>In order to be sure that no other node is in the process of concurrently creating an account with the same username and a lower timestamp, you would have to check with every other node to see what it is doing.</p><p>The problem here is that the total order of operations only emerges after you have collected all of the operations.</p><p>In order to implement something like a uniqueness constraint for usernames, it’s not sufficient to have a total ordering of operations—you also need to know when that order is finalized. </p><p>This idea of knowing when your total order is finalized is captured in the topic of total order broadcast.</p><h3 id="Total-Order-Broadcast"><a href="#Total-Order-Broadcast" class="headerlink" title="Total Order Broadcast"></a>Total Order Broadcast</h3><p>Total order broadcast is usually described as a protocol for exchanging messages between nodes. Informally, it requires that two safety properties always be satisfied:</p><ul><li><em>Reliable delivery</em></li><li><em>Totally ordered delivery</em></li></ul><h4 id="Using-total-order-broadcast"><a href="#Using-total-order-broadcast" class="headerlink" title="Using total order broadcast"></a>Using total order broadcast</h4><p>Consensus services such as ZooKeeper and etcd actually implement total order broadcast. </p><p>An important aspect of total order broadcast is that the order is fixed at the time the messages are delivered.</p><h4 id="Implementing-linearizable-storage-using-total-order-broadcast"><a href="#Implementing-linearizable-storage-using-total-order-broadcast" class="headerlink" title="Implementing linearizable storage using total order broadcast"></a>Implementing linearizable storage using total order broadcast</h4><p>Total order broadcast is asynchronous: messages are guaranteed to be delivered reliably in a fixed order, but there is no guarantee about when a message will be delivered. By contrast, linearizability is a recency guarantee: a read is guaranteed to see the latest value written.</p><h2 id="Distributed-Transactions-and-Consensus"><a href="#Distributed-Transactions-and-Consensus" class="headerlink" title="Distributed Transactions and Consensus"></a>Distributed Transactions and Consensus</h2><ul><li><em>Leader election</em></li><li><em>Atomic commit</em></li></ul><h3 id="Atomic-Commit-and-Two-Phase-Commit-2PC"><a href="#Atomic-Commit-and-Two-Phase-Commit-2PC" class="headerlink" title="Atomic Commit and Two-Phase Commit (2PC)"></a>Atomic Commit and Two-Phase Commit (2PC)</h3><h4 id="From-single-node-to-distributed-atomic-commit"><a href="#From-single-node-to-distributed-atomic-commit" class="headerlink" title="From single-node to distributed atomic commit"></a>From single-node to distributed atomic commit</h4><h4 id="Introduction-to-two-phase-commit"><a href="#Introduction-to-two-phase-commit" class="headerlink" title="Introduction to two-phase commit"></a>Introduction to two-phase commit</h4><p>Two-phase commit is an algorithm for achieving atomic transaction commit across multiple nodes—i.e., to ensure that either all nodes commit or all nodes abort.</p><p>The commit/abort process in 2PC is split into two phases (hence the name).</p><p><img src="/images/2018/10/9.png" alt=""></p><h4 id="Coordinator-failure"><a href="#Coordinator-failure" class="headerlink" title="Coordinator failure"></a>Coordinator failure</h4><p>If any of the prepare requests fail or time out, the coordinator aborts the transaction; if any of the commit or abort requests fail, the coordinator retries them indefinitely.</p><p><img src="/images/2018/10/10.png" alt=""></p><p>The only way 2PC can complete is by waiting for the coordinator to recover.</p><h4 id="Three-phase-commit"><a href="#Three-phase-commit" class="headerlink" title="Three-phase commit"></a>Three-phase commit</h4><p>Two-phase commit is called a <em>blocking</em> atomic commit protocol due to the fact that 2PC can become stuck waiting for the coordinator to recover. </p><p>three-phase commit (3PC)  assumes a network with bounded delay and nodes with bounded response times; in most practical systems with unbounded network delay and process pauses , it cannot guarantee atomicity.</p><h3 id="Distributed-Transactions-in-Practice"><a href="#Distributed-Transactions-in-Practice" class="headerlink" title="Distributed Transactions in Practice"></a>Distributed Transactions in Practice</h3><p><em>Database-internal distributed transactions</em><br><em>Heterogeneous distributed transactions</em></p><h4 id="Exactly-once-message-processing"><a href="#Exactly-once-message-processing" class="headerlink" title="Exactly-once message processing"></a>Exactly-once message processing</h4><h4 id="XA-transactions"><a href="#XA-transactions" class="headerlink" title="XA transactions"></a>XA transactions</h4><p>XA is not a network protocol—it is merely a C API for interfacing with a transaction coordinator.</p><h4 id="Holding-locks-while-in-doubt"><a href="#Holding-locks-while-in-doubt" class="headerlink" title="Holding locks while in doubt"></a>Holding locks while in doubt</h4><h4 id="Recovering-from-coordinator-failure"><a href="#Recovering-from-coordinator-failure" class="headerlink" title="Recovering from coordinator failure"></a>Recovering from coordinator failure</h4><h4 id="Limitations-of-distributed-transactions"><a href="#Limitations-of-distributed-transactions" class="headerlink" title="Limitations of distributed transactions"></a>Limitations of distributed transactions</h4><h3 id="Fault-Tolerant-Consensus"><a href="#Fault-Tolerant-Consensus" class="headerlink" title="Fault-Tolerant Consensus"></a>Fault-Tolerant Consensus</h3><p>Informally, consensus means getting several nodes to agree on something.</p><p>The consensus problem is normally formalized as follows: one or more nodes may <em>propose</em> values, and the consensus algorithm <em>decides</em> on one of those values.</p><p>In this formalism, a consensus algorithm must satisfy the following properties:</p><ul><li>Uniform agreement</li><li>Integrity</li><li>Validity</li><li>Termination</li></ul><h4 id="Consensus-algorithms-and-total-order-broadcast"><a href="#Consensus-algorithms-and-total-order-broadcast" class="headerlink" title="Consensus algorithms and total order broadcast"></a>Consensus algorithms and total order broadcast</h4><p>The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft, and Zab.</p><p>They decide on a <em>sequence</em> of values, which makes them <em>total order broadcast</em> algorithms</p><h4 id="Single-leader-replication-and-consensus"><a href="#Single-leader-replication-and-consensus" class="headerlink" title="Single-leader replication and consensus"></a>Single-leader replication and consensus</h4><h4 id="Epoch-numbering-and-quorums"><a href="#Epoch-numbering-and-quorums" class="headerlink" title="Epoch numbering and quorums"></a>Epoch numbering and quorums</h4><p>All of the consensus protocols discussed so far internally use a leader in some form or another, but they don’t guarantee that the leader is unique. Instead, they can make a weaker guarantee: the protocols define an <em>epoch number</em> and guarantee that within each epoch, the leader is unique.</p><p>We have two rounds of voting: once to choose a leader, and a second time to vote on a leader’s proposal. </p><p>This voting process looks superficially similar to two-phase commit. The biggest differences are that in 2PC the coordinator is not elected, and that fault-tolerant consensus algorithms only require votes from a majority of nodes, whereas 2PC requires a “yes” vote from every participant. Moreover, consensus algorithms define a recovery process by which nodes can get into a consistent state after a new leader is elected, ensuring that the safety properties are always met. These differences are key to the correctness and fault tolerance of a consensus algorithm.</p><h4 id="Limitations-of-consensus"><a href="#Limitations-of-consensus" class="headerlink" title="Limitations of consensus"></a>Limitations of consensus</h4><p>The process by which nodes vote on proposals before they are decided is a kind of synchronous replication.<br>Consensus systems always require a strict majority to operate.<br>Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can’t just add or remove nodes in the cluster.<br>Consensus systems generally rely on timeouts to detect failed nodes.<br>Sometimes, consensus algorithms are particularly sensitive to network problems.</p><h3 id="Membership-and-Coordination-Services"><a href="#Membership-and-Coordination-Services" class="headerlink" title="Membership and Coordination Services"></a>Membership and Coordination Services</h3><p>ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory.That small amount of data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm. </p><p>ZooKeeper is modeled after Google’s Chubby lock service , implementing not only total order broadcast (and hence consensus), but also an interesting set of other features that turn out to be particularly useful when building distributed systems:</p><ul><li>Linearizable atomic operations</li><li>Total ordering of operations</li><li>Failure detection</li><li>Change notifications</li></ul><h4 id="Allocating-work-to-nodes"><a href="#Allocating-work-to-nodes" class="headerlink" title="Allocating work to nodes"></a>Allocating work to nodes</h4><p>Normally, the kind of data managed by ZooKeeper is quite slow-changing.</p><h4 id="Service-discovery"><a href="#Service-discovery" class="headerlink" title="Service discovery"></a>Service discovery</h4><h4 id="Membership-services"><a href="#Membership-services" class="headerlink" title="Membership services"></a>Membership services</h4><p>A membership service determines which nodes are currently active and live members of a cluster. </p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>It turns out that a wide range of problems are actually reducible to consensus and are equivalent to each other.</p><ul><li>Linearizable compare-and-set registers</li><li>Atomic transaction commit</li><li>Total order broadcast</li><li>Locks and leases</li><li>Membership/coordination service</li><li>Uniqueness constraint</li></ul><p>Not every system necessarily requires consensus: for example, leaderless and multi-leader replication systems typically do not use global consensus.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;consensus&lt;/em&gt;: that is, getting all of the nodes to agree on something. &lt;/p&gt;
&lt;p&gt;If two nodes both believe that they are the leader, that situation is called split brain, and it often leads to data loss. Correct implementations of consensus help avoid such problems.&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -The Trouble with Distributed Systems</title>
    <link href="http://liujunming.github.io/2018/09/25/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-The-Trouble-with-Distributed-Systems/"/>
    <id>http://liujunming.github.io/2018/09/25/Designing-Data-Intensive-Applications-读书笔记-The-Trouble-with-Distributed-Systems/</id>
    <published>2018-09-25T06:02:17.000Z</published>
    <updated>2018-11-06T08:18:30.576Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Unreliable-Networks"><a href="#Unreliable-Networks" class="headerlink" title="Unreliable Networks"></a>Unreliable Networks</h2><p><img src="/images/2018/9/41.png" alt=""><br>The usual way of handling this issue is a <em>timeout</em>: after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don’t know whether the remote node got your request or not.<br><a id="more"></a></p><h3 id="Network-Faults-in-Practice"><a href="#Network-Faults-in-Practice" class="headerlink" title="Network Faults in Practice"></a>Network Faults in Practice</h3><h3 id="Detecting-Faults"><a href="#Detecting-Faults" class="headerlink" title="Detecting Faults"></a>Detecting Faults</h3><h3 id="Timeouts-and-Unbounded-Delays"><a href="#Timeouts-and-Unbounded-Delays" class="headerlink" title="Timeouts and Unbounded Delays"></a>Timeouts and Unbounded Delays</h3><p>如果超时是检测故障的唯一可靠方法，那么超时应该等待多久？不幸的是没有简单的答案。</p><h3 id="Synchronous-Versus-Asynchronous-Networks"><a href="#Synchronous-Versus-Asynchronous-Networks" class="headerlink" title="Synchronous Versus Asynchronous Networks"></a>Synchronous Versus Asynchronous Networks</h3><p>It is possible to give hard real-time response guarantees and bounded delays in networks, but doing so is very expensive and results in lower utilization of hardware resources. Most non-safety-critical systems choose cheap and unreliable over expensive and reliable.</p><h2 id="Unreliable-Clocks"><a href="#Unreliable-Clocks" class="headerlink" title="Unreliable Clocks"></a>Unreliable Clocks</h2><p>可以在一定程度上同步时钟：最常用的机制是网络时间协议（NTP），它允许根据一组服务器报告的时间来调整计算机时钟。服务器则从更精确的时间源（如GPS接收机）获取时间。</p><h3 id="Monotonic-Versus-Time-of-Day-Clocks"><a href="#Monotonic-Versus-Time-of-Day-Clocks" class="headerlink" title="Monotonic Versus Time-of-Day Clocks"></a>Monotonic Versus Time-of-Day Clocks</h3><p>时钟可以及时跳回。<br>单调钟适用于测量持续时间（时间间隔），例如超时或服务的响应时间。</p><p>​在分布式系统中，使用单调钟测量经过时间（比如超时）通常很好，因为它不假定不同节点的时钟之间存在任何同步，并且对测量的轻微不准确性不敏感。</p><h3 id="Clock-Synchronization-and-Accuracy"><a href="#Clock-Synchronization-and-Accuracy" class="headerlink" title="Clock Synchronization and Accuracy"></a>Clock Synchronization and Accuracy</h3><h3 id="Relying-on-Synchronized-Clocks"><a href="#Relying-on-Synchronized-Clocks" class="headerlink" title="Relying on Synchronized Clocks"></a>Relying on Synchronized Clocks</h3><h3 id="Process-Pauses"><a href="#Process-Pauses" class="headerlink" title="Process Pauses"></a>Process Pauses</h3><p> Say you have a database with a single leader per partition. Only the leader is allowed to accept writes. How does a node know that it is still leader (that it hasn’t been declared dead by the others), and that it may safely accept writes?</p><p> One option is for the leader to obtain a <em>lease</em> from the other nodes. Only one node can hold the lease at any one time—thus, when a node obtains a lease, it knows that it is the leader for some amount of time, until the lease expires. In order to remain leader, the node must periodically renew the lease before it expires. If the node fails, it stops renewing the lease, so another node can take over when it expires.</p><p> You can imagine the request-handling loop looking something like this:</p><p><img src="/images/2018/9/42.png" alt=""></p><p>Firstly, it’s relying on synchronized clocks.However, what if there is an unexpected pause in the execution of the program? For example, imagine the thread stops for 15 seconds around the line <code>lease.isValid()</code> before finally continuing. In that case, it’s likely that the lease will have expired by the time the request is processed, and another node has already taken over as leader. However, there is nothing to tell this thread that it was paused for so long, so this code won’t notice that the lease has expired until the next iteration of the loop—by which time it may have already done something unsafe by processing the request.</p><h2 id="Knowledge-Truth-and-Lies"><a href="#Knowledge-Truth-and-Lies" class="headerlink" title="Knowledge, Truth, and Lies"></a>Knowledge, Truth, and Lies</h2><p>So far in this chapter we have explored the ways in which distributed systems are different from programs running on a single computer: there is no shared memory, only message passing via an unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks, and processing pauses.</p><h3 id="The-Truth-Is-Defined-by-the-Majority"><a href="#The-Truth-Is-Defined-by-the-Majority" class="headerlink" title="The Truth Is Defined by the Majority"></a>The Truth Is Defined by the Majority</h3><p>A node cannot necessarily trust its own judgment of a situation.A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes.</p><p>That includes decisions about declaring nodes dead. If a quorum of nodes declares another node dead, then it must be considered dead, even if that node still very much feels alive. The individual node must abide by the quorum decision and step down.</p><h4 id="The-leader-and-the-lock"><a href="#The-leader-and-the-lock" class="headerlink" title="The leader and the lock"></a>The leader and the lock</h4><p><img src="/images/2018/9/43.png" alt=""><br>If the client holding the lease is paused for too long, its lease expires. Another client can obtain a lease for the same file, and start writing to the file. When the paused client comes back, it believes (incorrectly) that it still has a valid lease and proceeds to also write to the file. As a result, the clients’ writes clash and corrupt the file.</p><h4 id="Fencing-tokens"><a href="#Fencing-tokens" class="headerlink" title="Fencing tokens"></a>Fencing tokens</h4><p>Fencing tokens(防护令牌)</p><p>We need to ensure that a node that is under a false belief of being “the chosen one” cannot disrupt the rest of the system.</p><p><img src="/images/2018/9/44.png" alt=""></p><h3 id="Byzantine-Faults"><a href="#Byzantine-Faults" class="headerlink" title="Byzantine Faults"></a>Byzantine Faults</h3><p>Fencing tokens can detect and block a node that is <em>inadvertently</em> acting in error.If the node deliberately wanted to subvert the system’s guarantees, it could easily do so by sending messages with a fake fencing token.</p><p>Distributed systems problems become much harder if there is a risk that nodes may “lie” (send arbitrary faulty or corrupted responses)—for example, if a node may claim to have received a particular message when in fact it didn’t. Such behavior is known as a <em>Byzantine fault</em>, and the problem of reaching consensus in this untrusting environment is known as the <em>Byzantine Generals Problem</em>.</p><p>A system is <em>Byzantine fault-tolerant</em> if it continues to operate correctly even if some of the nodes are malfunctioning and not obeying the protocol, or if malicious attackers are interfering with the network.</p><p>Byzantine是错综复杂的意思。</p><h3 id="System-Model-and-Reality"><a href="#System-Model-and-Reality" class="headerlink" title="System Model and Reality"></a>System Model and Reality</h3><p>With regard to timing assumptions, three system models are in common use:</p><ul><li><em>Synchronous model</em></li><li><em>Partially synchronous model</em></li><li><em>Asynchronous model</em></li></ul><p>Moreover, besides timing issues, we have to consider node failures. The three most common system models for nodes are:</p><ul><li><em>Crash-stop faults</em></li><li><em>Crash-recovery faults</em></li><li><em>Byzantine (arbitrary) faults</em></li></ul><p>For modeling real systems, the partially synchronous model with crash-recovery faults is generally the most useful model.</p><h4 id="Correctness-of-an-algorithm"><a href="#Correctness-of-an-algorithm" class="headerlink" title="Correctness of an algorithm"></a>Correctness of an algorithm</h4><h4 id="Safety-and-liveness"><a href="#Safety-and-liveness" class="headerlink" title="Safety and liveness"></a>Safety and liveness</h4><p>Liveness properties often include the word “eventually” in their definition.</p><p>Safety is often informally defined as <em>nothing bad happens</em>, and liveness as <em>something good eventually happens</em>.</p><p>An advantage of distinguishing between safety and liveness properties is that it helps us deal with difficult system models. </p><h4 id="Mapping-system-models-to-the-real-world"><a href="#Mapping-system-models-to-the-real-world" class="headerlink" title="Mapping system models to the real world"></a>Mapping system models to the real world</h4><p>Safety and liveness properties and system models are very useful for reasoning about the correctness of a distributed algorithm.</p><p>Proving an algorithm correct does not mean its <em>implementation</em> on a real system will necessarily always behave correctly.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Unreliable-Networks&quot;&gt;&lt;a href=&quot;#Unreliable-Networks&quot; class=&quot;headerlink&quot; title=&quot;Unreliable Networks&quot;&gt;&lt;/a&gt;Unreliable Networks&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/2018/9/41.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;The usual way of handling this issue is a &lt;em&gt;timeout&lt;/em&gt;: after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don’t know whether the remote node got your request or not.&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -Transactions</title>
    <link href="http://liujunming.github.io/2018/09/23/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Transactions/"/>
    <id>http://liujunming.github.io/2018/09/23/Designing-Data-Intensive-Applications-读书笔记-Transactions/</id>
    <published>2018-09-23T05:03:31.000Z</published>
    <updated>2018-11-06T08:19:49.434Z</updated>
    
    <content type="html"><![CDATA[<p>本章主要是在单机数据库的上下文中，探讨了各种概念与想法。<br><a id="more"></a></p><h2 id="The-Slippery-Concept-of-a-Transaction"><a href="#The-Slippery-Concept-of-a-Transaction" class="headerlink" title="The Slippery Concept of a Transaction"></a>The Slippery Concept of a Transaction</h2><h3 id="The-Meaning-of-ACID"><a href="#The-Meaning-of-ACID" class="headerlink" title="The Meaning of ACID"></a>The Meaning of ACID</h3><p>Systems that do not meet the ACID criteria are sometimes called BASE, which stands for Basically Available, Soft state, and Eventual consistency. </p><h4 id="Atomicity"><a href="#Atomicity" class="headerlink" title="Atomicity"></a>Atomicity</h4><p>ACID atomicity describes what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed.</p><h4 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h4><p>The word consistency is terribly overloaded:</p><ul><li>replica consistency and the issue of eventual consistency that arises in asynchronously replicated systems.</li><li>Consistent hashing is an approach to partitioning that some systems use for rebalancing.</li><li>In the CAP theorem , the word consistency is used to mean linearizability.</li></ul><p>The idea of ACID consistency is that you have certain statements about your data (invariants) that must always be true.</p><h4 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h4><p>Most databases are accessed by several clients at the same time. That is no problem if they are reading and writing different parts of the database, but if they are accessing the same database records, you can run into concurrency problems (race conditions).</p><p>ACID意义上的隔离性意味着，同时执行的事务是相互隔离的：它们不能相互冒犯。</p><h4 id="Durability"><a href="#Durability" class="headerlink" title="Durability"></a>Durability</h4><p>持久性 是一个承诺，即一旦事务成功完成，即使发生硬件故障或数据库崩溃，写入的任何数据也不会丢失。</p><h3 id="Single-Object-and-Multi-Object-Operations"><a href="#Single-Object-and-Multi-Object-Operations" class="headerlink" title="Single-Object and Multi-Object Operations"></a>Single-Object and Multi-Object Operations</h3><p>图7-2展示了一个来自邮件应用的例子。执行以下查询来显示用户未读邮件数量：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> emails <span class="keyword">WHERE</span> recipient_id = <span class="number">2</span> <span class="keyword">AND</span> unread_flag = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>但如果邮件太多，你可能会觉得这个查询太慢，并决定用单独的字段存储未读邮件的数量。现在每当一个新消息写入时，也必须增长未读计数器，每当一个消息被标记为已读时，也必须减少未读计数器。</p><p>在图7-2中，用户2 遇到异常情况：邮件列表里显示有未读消息，但计数器显示为零未读消息，因为计数器增长还没有发生。隔离性可以避免这个问题：通过确保用户2要么同时看到新邮件和增长后的计数器，要么都看不到，反正不会看到执行到一半的中间结果。</p><p><img src="/images/2018/9/28.png" alt=""></p><p>图7-3说明了对原子性的需求：如果在事务过程中发生错误，邮箱和未读计数器的内容可能会失去同步。在原子事务中，如果对计数器的更新失败，事务将被中止，并且插入的电子邮件将被回滚。</p><p><img src="/images/2018/9/29.png" alt=""></p><p>A transaction is usually understood as a mechanism for grouping multiple operations on multiple objects into one unit of execution.</p><h2 id="Weak-Isolation-Levels"><a href="#Weak-Isolation-Levels" class="headerlink" title="Weak Isolation Levels"></a>Weak Isolation Levels</h2><p>如果两个事务不触及相同的数据，它们可以安全地并行（parallel） 运行，因为两者都不依赖于另一个。当一个事务读取由另一个事务同时修改的数据时，或者当两个事务试图同时修改相同的数据时，并发问题（竞争条件）才会出现。</p><h3 id="Read-Committed"><a href="#Read-Committed" class="headerlink" title="Read Committed"></a>Read Committed</h3><p>The most basic level of transaction isolation is read committed.It makes two guarantees:</p><ol><li>When reading from the database, you will only see data that has been committed (no dirty reads).</li><li>When writing to the database, you will only overwrite data that has been committed (no dirty writes).</li></ol><h4 id="No-dirty-reads"><a href="#No-dirty-reads" class="headerlink" title="No dirty reads"></a>No dirty reads</h4><p>Imagine a transaction has written some data to the database, but the transaction has not yet committed or aborted. Can another transaction see that uncommitted data? If yes, that is called a <em>dirty read</em>.</p><p><img src="/images/2018/9/31.png" alt=""></p><h4 id="No-dirty-writes"><a href="#No-dirty-writes" class="headerlink" title="No dirty writes"></a>No dirty writes</h4><p>What happens if two transactions concurrently try to update the same object in a database? We don’t know in which order the writes will happen, but we normally assume that the later write overwrites the earlier write.</p><p>However, what happens if the earlier write is part of a transaction that has not yet committed, so the later write overwrites an uncommitted value? This is called a <em>dirty write</em>. Transactions running at the read committed isolation level must prevent dirty writes, usually by delaying the second write until the first write’s transaction has committed or aborted.</p><p><img src="/images/2018/9/32.png" alt=""></p><h4 id="Implementing-read-committed"><a href="#Implementing-read-committed" class="headerlink" title="Implementing read committed"></a>Implementing read committed</h4><p>Most commonly, databases prevent dirty writes by using row-level locks.</p><p>Most databases prevent dirty reads using the approach illustrated in Figure 7-4: for every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. </p><h3 id="Snapshot-Isolation"><a href="#Snapshot-Isolation" class="headerlink" title="Snapshot Isolation"></a>Snapshot Isolation</h3><p>在PostgreSQL and MySQL中，Snapshot Isolation即为Repeatable Read。</p><p>图7-6说明了read committed可能发生的问题。<br><img src="/images/2018/9/33.png" alt=""></p><p>这种异常被称为不可重复读（nonrepeatable read）或读取偏差（read skew）。</p><p><em>Snapshot isolation</em>能解决read skew问题。The idea is that each transaction reads from a <em>consistent snapshot</em> of the database—that is, the transaction sees all the data that was committed in the database at the start of the transaction. Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.</p><h4 id="Implementing-snapshot-isolation"><a href="#Implementing-snapshot-isolation" class="headerlink" title="Implementing snapshot isolation"></a>Implementing snapshot isolation</h4><p>A key principle of snapshot isolation is <em>readers never block writers, and writers never block readers</em>.</p><p>The database must potentially keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as <em>multi-version concurrency control</em>(MVCC).</p><p>图7-7说明了如何在PostgreSQL中实现基于MVCC的快照隔离。当一个事务开始时，它被赋予一个唯一的事务ID。每当事务向数据库写入任何内容时，它所写入的数据都会被标记上写入者的事务ID。</p><p><img src="/images/2018/9/34.png" alt=""></p><h4 id="Visibility-rules-for-observing-a-consistent-snapshot"><a href="#Visibility-rules-for-observing-a-consistent-snapshot" class="headerlink" title="Visibility rules for observing a consistent snapshot"></a>Visibility rules for observing a consistent snapshot</h4><p>When a transaction reads from the database, transaction IDs are used to decide which objects it can see and which are invisible. By carefully defining visibility rules,the database can present a consistent snapshot of the database to the application. </p><h4 id="Indexes-and-snapshot-isolation"><a href="#Indexes-and-snapshot-isolation" class="headerlink" title="Indexes and snapshot isolation"></a>Indexes and snapshot isolation</h4><p><img src="/images/2018/9/30.png" alt=""></p><h3 id="Preventing-Lost-Updates"><a href="#Preventing-Lost-Updates" class="headerlink" title="Preventing Lost Updates"></a>Preventing Lost Updates</h3><p>到目前为止已经讨论的read committed和snapshot isolation级别，主要保证了只读事务在并发写入时可以看到什么。却忽略了两个事务并发写入的问题——我们只讨论了脏写。</p><p>并发的写入事务之间还有其他几种有趣的冲突。其中最着名的是丢失更新（lost update） 问题，如下图所示，以两个并发计数器增量为例。</p><p><img src="/images/2018/9/35.png" alt=""></p><p>The lost update problem can occur if an application reads some value from the database, modifies it, and writes back the modified value (a <em>read-modify-write cycle</em>). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification.</p><p>Because this is such a common problem, a variety of solutions have been developed.</p><h4 id="Atomic-write-operations"><a href="#Atomic-write-operations" class="headerlink" title="Atomic write operations"></a>Atomic write operations</h4><p>Many databases provide atomic update operations, which remove the need to implement read-modify-write cycles in application code.</p><h4 id="Explicit-locking"><a href="#Explicit-locking" class="headerlink" title="Explicit locking"></a>Explicit locking</h4><p>Another option for preventing lost updates, if the database’s built-in atomic operations don’t provide the necessary functionality, is for the application to explicitly lock objects that are going to be updated.</p><p><img src="/images/2018/9/36.png" alt=""></p><h4 id="Automatically-detecting-lost-updates"><a href="#Automatically-detecting-lost-updates" class="headerlink" title="Automatically detecting lost updates"></a>Automatically detecting lost updates</h4><p>Atomic operations and locks are ways of preventing lost updates by forcing the read-modify-write cycles to happen sequentially. An alternative is to allow them to execute in parallel and, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.</p><h4 id="Compare-and-set-CAS"><a href="#Compare-and-set-CAS" class="headerlink" title="Compare-and-set(CAS)"></a>Compare-and-set(CAS)</h4><p>In databases that don’t provide transactions, you sometimes find an <strong>atomic</strong> compare-and-set operation. The purpose of this operation is to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it. If the current value does not match what you previously read, the update has no effect, and the read-modify-write cycle must be retried.</p><p>For example, to prevent two users concurrently updating the same wiki page, you might try something like this, expecting the update to occur only if the content of the page hasn’t changed since the user started editing it:</p><p><img src="/images/2018/9/37.png" alt=""></p><h4 id="Conflict-resolution-and-replication"><a href="#Conflict-resolution-and-replication" class="headerlink" title="Conflict resolution and replication"></a>Conflict resolution and replication</h4><p>Locks and compare-and-set operations assume that there is a single up-to-date copy of the data. However, databases with multi-leader or leaderless replication usually allow several writes to happen concurrently and replicate them asynchronously, so they cannot guarantee that there is a single up-to-date copy of the data. Thus, techniques based on locks or CAS do not apply in this context.</p><h3 id="Write-Skew-and-Phantoms"><a href="#Write-Skew-and-Phantoms" class="headerlink" title="Write Skew and Phantoms"></a>Write Skew and Phantoms</h3><p>phantoms在本文中的含义是幻读。</p><p>想象一下这个例子：你正在为医院写一个医生轮班管理程序。医院通常会同时要求几位医生值班，但底线是至少有一位医生在值班。医生可以放弃他们的班次（例如，如果他们自己生病了），只要至少有一个同事在这一班中继续工作。<br>现在想象一下，Alice和Bob是两位值班医生。两人都感到不适，所以他们都决定请假。不幸的是，他们恰好在同一时间点击按钮下班。图7-8说明了接下来的事情。</p><p><img src="/images/2018/9/38.png" alt=""></p><p>在两个事务中，应用首先检查是否有两个或以上的医生正在值班；如果是的话，它就假定一名医生可以安全地休班。由于数据库使用Snapshot Isolation，两次检查都返回 2 ，所以两个事务都进入下一个阶段。Alice更新自己的记录休班了，而Bob也做了一样的事情。两个事务都成功提交了，现在没有医生值班了。违反了至少有一名医生在值班的要求。</p><h4 id="Characterizing-write-skew"><a href="#Characterizing-write-skew" class="headerlink" title="Characterizing write skew"></a>Characterizing write skew</h4><p>这种异常称为 <em>write skew</em>.</p><p> Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</p><h4 id="More-examples-of-write-skew"><a href="#More-examples-of-write-skew" class="headerlink" title="More examples of write skew"></a>More examples of write skew</h4><h4 id="Phantoms-causing-write-skew"><a href="#Phantoms-causing-write-skew" class="headerlink" title="Phantoms causing write skew"></a>Phantoms causing write skew</h4><p>This effect, where a write in one transaction changes the result of a search query in another transaction, is called a <em>phantom</em>.</p><h2 id="Serializability"><a href="#Serializability" class="headerlink" title="Serializability"></a>Serializability</h2><p>目前大多数提供可序列化的数据库都使用了三种技术。</p><h3 id="Actual-Serial-Execution"><a href="#Actual-Serial-Execution" class="headerlink" title="Actual Serial Execution"></a>Actual Serial Execution</h3><p>If you can make each transaction very fast to execute, and the transaction throughput is low enough to process on a single CPU core, this is a simple and effective option.</p><h3 id="Two-Phase-Locking-2PL"><a href="#Two-Phase-Locking-2PL" class="headerlink" title="Two-Phase Locking (2PL)"></a>Two-Phase Locking (2PL)</h3><h4 id="Implementation-of-two-phase-locking"><a href="#Implementation-of-two-phase-locking" class="headerlink" title="Implementation of two-phase locking"></a>Implementation of two-phase locking</h4><p>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two-phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.</p><h3 id="Serializable-Snapshot-Isolation-SSI"><a href="#Serializable-Snapshot-Isolation-SSI" class="headerlink" title="Serializable Snapshot Isolation (SSI)"></a>Serializable Snapshot Isolation (SSI)</h3><h4 id="Pessimistic-versus-optimistic-concurrency-control"><a href="#Pessimistic-versus-optimistic-concurrency-control" class="headerlink" title="Pessimistic versus optimistic concurrency control"></a>Pessimistic versus optimistic concurrency control</h4><p>Two-phase locking is a so-called pessimistic concurrency control mechanism.<br>Serializable snapshot isolation is an optimistic concurrency control technique. </p><p>SSI is based on snapshot isolation—that is, all reads within a transaction are made from a consistent snapshot of the database. This is the main difference compared to earlier optimistic concurrency control techniques. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort.</p><p>In order to provide serializable isolation, the database must detect situations in which a transaction may have acted on an outdated premise and abort the transaction in that case.</p><p>How does the database know if a query result might have changed? There are two cases to consider:</p><ul><li>Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)</li><li>Detecting writes that affect prior reads (the write occurs after the read)</li></ul><h4 id="Detecting-stale-MVCC-reads"><a href="#Detecting-stale-MVCC-reads" class="headerlink" title="Detecting stale MVCC reads"></a>Detecting stale MVCC reads</h4><p><img src="/images/2018/9/39.png" alt=""></p><h4 id="Detecting-writes-that-affect-prior-reads"><a href="#Detecting-writes-that-affect-prior-reads" class="headerlink" title="Detecting writes that affect prior reads"></a>Detecting writes that affect prior reads</h4><p><img src="/images/2018/9/40.png" alt=""></p><hr><p>参考资料：</p><ol><li><a href="https://github.com/Vonng/ddia/blob/master/ch7.md" target="_blank" rel="noopener">Vonng ddia</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本章主要是在单机数据库的上下文中，探讨了各种概念与想法。&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -Partitioning</title>
    <link href="http://liujunming.github.io/2018/09/20/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Partitioning/"/>
    <id>http://liujunming.github.io/2018/09/20/Designing-Data-Intensive-Applications-读书笔记-Partitioning/</id>
    <published>2018-09-20T08:23:50.000Z</published>
    <updated>2018-11-06T08:20:17.203Z</updated>
    
    <content type="html"><![CDATA[<p>partition在本文中的翻译为分区。<br>数据量非常大的时候，在单台机器上存储和处理不再可行，则分区十分必要。分区的目标是在多台机器上均匀分布数据和查询负载，避免出现热点（负载不成比例的节点）。这需要选择适合于您的数据的分区方案，并在将节点添加到集群或从集群删除时进行再分区。分区主要是为了可扩展性(scalability)。<br><a id="more"></a></p><h2 id="Partitioning-and-Replication"><a href="#Partitioning-and-Replication" class="headerlink" title="Partitioning and Replication"></a>Partitioning and Replication</h2><p>分区通常与复制结合使用，使得每个分区的副本存储在多个节点上。 这意味着，即使每条记录属于一个分区，它仍然可以存储在多个不同的节点上以获得容错能力。</p><p><img src="/images/2018/9/20.png" alt=""></p><h2 id="Partitioning-of-Key-Value-Data"><a href="#Partitioning-of-Key-Value-Data" class="headerlink" title="Partitioning of Key-Value Data"></a>Partitioning of Key-Value Data</h2><p>如果分区是不公平的，一些分区比其他分区有更多的数据或查询，我们称之为偏斜（skew）。数据偏斜的存在使分区效率下降很多。在极端的情况下，所有的负载可能压在一个分区上，其余节点空闲的，那么瓶颈落在这一个繁忙的节点上。不均衡导致的高负载的分区被称为热点（hot spot）。</p><p>避免热点最简单的方法是将记录随机分配给节点。这将在所有节点上平均分配数据，但是它有一个很大的缺点：当你试图读取一个特定的值时，你无法知道它在哪个节点上，所以你必须并行地查询所有的节点。</p><h3 id="Partitioning-by-Key-Range"><a href="#Partitioning-by-Key-Range" class="headerlink" title="Partitioning by Key Range"></a>Partitioning by Key Range</h3><p>一种分区的方法是为每个分区指定一块连续的键范围（从最小值到最大值），如果知道范围之间的边界，则可以轻松确定哪个分区包含某个值。</p><p><img src="/images/2018/9/21.png" alt=""></p><p>在每个分区中，我们可以按照一定的顺序保存键。优点是进行范围扫描非常简单，缺点是某些特定的访问模式会导致热点。</p><h3 id="Partitioning-by-Hash-of-Key"><a href="#Partitioning-by-Hash-of-Key" class="headerlink" title="Partitioning by Hash of Key"></a>Partitioning by Hash of Key</h3><p>一个好的散列函数可以将将偏斜的数据均匀分布。假设你有一个32位散列函数,无论何时给定一个新的字符串输入，它将返回一个0到2^{32}-1之间的”随机”数。即使输入的字符串非常相似，它们的散列也会均匀分布在这个数字范围内。</p><p>一旦你有一个合适的键散列函数，你可以为每个分区分配一个散列范围（而不是键的范围），每个通过哈希散列落在分区范围内的键将被存储在该分区中。如下图所示。</p><p><img src="/images/2018/9/22.png" alt=""></p><p>不幸的是，通过使用Key散列进行分区，我们失去了键范围分区的一个很好的属性：高效执行范围查询的能力。曾经相邻的Key现在分散在所有分区中，所以它们之间的顺序就丢失了。 </p><p>Cassandra采取了折衷的策略。Cassandra中的表可以使用由多个列组成的复合主键来声明。键中只有第一列会作为散列的依据，而其他列则被用作Casssandra的SSTables中排序数据的连接索引。尽管查询无法在复合主键的第一列中按范围扫表，但如果第一列已经指定了固定值，则可以对该键的其他列执行有效的范围扫描。</p><h3 id="Skewed-Workloads-and-Relieving-Hot-Spots"><a href="#Skewed-Workloads-and-Relieving-Hot-Spots" class="headerlink" title="Skewed Workloads and Relieving Hot Spots"></a>Skewed Workloads and Relieving Hot Spots</h3><p>在极端情况下，所有的读写操作都是针对同一个键的，所有的请求都会被路由到同一个分区。</p><p>​如今，大多数数据系统无法自动补偿这种高度偏斜的负载，因此应用程序有责任减少偏斜。例如，如果一个主键被认为是非常火爆的，一个简单的方法是在主键的开始或结尾添加一个随机数。只要一个两位数的十进制随机数就可以将主键分散为100钟不同的主键,从而存储在不同的分区中。</p><h2 id="Partitioning-and-Secondary-Indexes"><a href="#Partitioning-and-Secondary-Indexes" class="headerlink" title="Partitioning and Secondary Indexes"></a>Partitioning and Secondary Indexes</h2><h3 id="Partitioning-Secondary-Indexes-by-Document"><a href="#Partitioning-Secondary-Indexes-by-Document" class="headerlink" title="Partitioning Secondary Indexes by Document"></a>Partitioning Secondary Indexes by Document</h3><p>按文档分区（本地索引），其中二级索引存储在与主键和值相同的分区中。这意味着只有一个分区需要在写入时更新，但是读取二级索引需要在所有分区之间进行scatter/gather。</p><p><img src="/images/2018/9/23.png" alt=""></p><h3 id="Partitioning-Secondary-Indexes-by-Term"><a href="#Partitioning-Secondary-Indexes-by-Term" class="headerlink" title="Partitioning Secondary Indexes by Term"></a>Partitioning Secondary Indexes by Term</h3><p>按关键词分区（全局索引），其中二级索引存在不同的分区的。当文档写入时，需要更新多个分区中的二级索引；但是可以从单个分区中进行读取</p><p><img src="/images/2018/9/24.png" alt=""></p><h2 id="Rebalancing-Partitions"><a href="#Rebalancing-Partitions" class="headerlink" title="Rebalancing Partitions"></a>Rebalancing Partitions</h2><p>随着时间的推移，数据库会有各种变化。如机器出现故障，其他机器需要接管故障机器。<br>这些更改需要将数据和请求从一个节点移动到另一个节点。 将load从集群中的一个节点向另一个节点移动的过程称为再平衡（reblancing）。</p><h3 id="Strategies-for-Rebalancing"><a href="#Strategies-for-Rebalancing" class="headerlink" title="Strategies for Rebalancing"></a>Strategies for Rebalancing</h3><p>有几种不同的分区分配方法,让我们依次简要讨论一下。</p><h4 id="反面教材：hash-mod-N"><a href="#反面教材：hash-mod-N" class="headerlink" title="反面教材：hash mod N"></a>反面教材：hash mod N</h4><p>模N方法的问题是，如果节点数量N发生变化，大多数Key将需要从一个节点移动到另一个节点。如此频繁的移动使得重新平衡的代价过于昂贵。</p><p>我们需要一种只移动必需数据的方法。</p><h4 id="固定数量的分区"><a href="#固定数量的分区" class="headerlink" title="固定数量的分区"></a>固定数量的分区</h4><p>创建比节点更多的分区，并为每个节点分配多个分区。例如，运行在10个节点的集群上的数据库可能会从一开始就被拆分为1,000个分区，因此大约有100个分区被分配给每个节点。</p><p>现在，如果一个节点被添加到集群中，新节点可以从当前每个节点中窃取一些分区，直到分区再次公平分配。这个过程下图所示。</p><p><img src="/images/2018/9/25.png" alt=""></p><p>如果数据集的总大小难以预估（例如，如果它开始很小，但随着时间的推移可能会变得更大），选择正确的分区数是困难的。由于每个分区包含了总数据量固定比率的数据，因此每个分区的大小与集群中的数据总量成比例增长。如果分区非常大，再平衡和从节点故障恢复变得昂贵。但是，如果分区太小，则会产生太多的开销。当分区大小“恰到好处”的时候才能获得很好的性能，如果分区数量固定，但数据量变动很大，则难以达到最佳性能。</p><h4 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h4><p>对于使用键范围分区的数据库，具有固定边界的固定数量的分区将非常不便，手动重新配置分区边界将非常繁琐。</p><p>出于这个原因，按键的范围进行分区的数据库（如HBase）会动态创建分区。当分区增长到超过配置的大小时（在HBase上，默认值是10GB），会被分成两个分区，每个分区约占一半的数据。与之相反，如果大量数据被删除并且分区缩小到某个阈值以下，则可以将其与相邻分区合并。</p><p>动态分区的一个优点是分区数量适应总数据量。如果只有少量的数据，少量的分区就足够了，所以开销很小;如果有大量的数据，每个分区的大小被限制在一个可配置的最大值。</p><p>动态分区不仅适用于数据的范围分区，而且也适用于hash分区。</p><h4 id="Partitioning-proportionally-to-nodes"><a href="#Partitioning-proportionally-to-nodes" class="headerlink" title="Partitioning proportionally to nodes"></a>Partitioning proportionally to nodes</h4><p>每个节点具有固定数量的分区，在这种情况下，每个分区的大小与数据集大小成比例地增长，而节点数量保持不变，但是当增加节点数时，分区将再次变小。由于较大的数据量通常需要较大数量的节点进行存储，因此这种方法也使每个分区的大小较为稳定。</p><h2 id="Request-Routing"><a href="#Request-Routing" class="headerlink" title="Request Routing"></a>Request Routing</h2><p>现在我们已经将数据集分割到多个机器上运行的多个节点上。但是仍然存在一个悬而未决的问题：当客户想要发出请求时，如何知道要连接哪个节点？随着分区重新平衡，分区对节点的分配也发生变化。为了回答这个问题，需要有人知晓这些变化：如果我想读或写键“foo”，需要连接哪个IP地址和端口号？</p><p>​这个问题可以概括为 服务发现(service discovery) 。</p><p>概括来说，这个问题有几种不同的方案（如下图所示）：<br><img src="/images/2018/9/26.png" alt=""></p><ol><li>允许客户联系任何节点（例如，通过循环策略的负载均衡（Round-Robin Load Balancer））。如果该节点恰巧拥有请求的分区，则它可以直接处理该请求;否则，它将请求转发到适当的节点，接收回复并传递给客户端。</li><li>首先将所有来自客户端的请求发送到路由层，它决定了应该处理请求的节点，并相应地转发。此路由层本身不处理任何请求，它仅负责分区的负载均衡。</li><li>要求客户端知道分区和节点的分配。在这种情况下，客户端可以直接连接到适当的节点，而不需要任何中介。</li></ol><p>以上所有情况中的关键问题是：作出路由决策的组件（可能是节点之一，还是路由层或客户端）如何了解分区-节点之间的分配关系变化？</p><p>许多分布式数据系统都依赖于一个独立的协调服务，比如用ZooKeeper来跟踪集群元数据，如下图所示。 </p><p><img src="/images/2018/9/27.png" alt=""></p><p>每个节点在ZooKeeper中注册自己，ZooKeeper维护分区到节点的可靠映射。 其他参与者（如路由层或分区感知客户端）可以在ZooKeeper中订阅此信息。 只要分区分配发生的改变，或者集群中添加或删除了一个节点，ZooKeeper就会通知路由层使路由信息保持最新状态。</p><p>Cassandra和Riak采取不同的方法：他们在节点之间使用流言协议（gossip protocol） 来传播群集状态的变化。请求可以发送到任意节点，该节点会转发到包含所请求的分区的适当节点（图Figure 6-7中的方法1）。这个模型在数据库节点中增加了更多的复杂性，但是避免了对像ZooKeeper这样的外部协调服务的依赖。</p><hr><p>参考资料：</p><ol><li><a href="https://github.com/Vonng/ddia/blob/master/ch6.md" target="_blank" rel="noopener">Vonng ddia翻译</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;partition在本文中的翻译为分区。&lt;br&gt;数据量非常大的时候，在单台机器上存储和处理不再可行，则分区十分必要。分区的目标是在多台机器上均匀分布数据和查询负载，避免出现热点（负载不成比例的节点）。这需要选择适合于您的数据的分区方案，并在将节点添加到集群或从集群删除时进行再分区。分区主要是为了可扩展性(scalability)。&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统中clock相关知识点</title>
    <link href="http://liujunming.github.io/2018/09/19/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%ADclock%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>http://liujunming.github.io/2018/09/19/分布式系统中clock相关知识点/</id>
    <published>2018-09-19T06:43:42.000Z</published>
    <updated>2018-11-06T08:20:50.065Z</updated>
    
    <content type="html"><![CDATA[<p>因为网上资料较多，本文主要是总结一下相关资料。<br><a id="more"></a></p><h2 id="Time-clocks-and-the-ordering-of-events-in-a-distributed-system"><a href="#Time-clocks-and-the-ordering-of-events-in-a-distributed-system" class="headerlink" title="Time, clocks, and the ordering of events in a distributed system"></a>Time, clocks, and the ordering of events in a distributed system</h2><ul><li>论文： Time, clocks, and the ordering of events in a distributed system</li><li>看paper时可以结合着这篇<a href="https://www.cnblogs.com/hzmark/p/- Time_Clocks_Ordering.html" target="_blank" rel="noopener">译文</a></li><li><a href="https://zhuanlan.zhihu.com/p/34057588" target="_blank" rel="noopener">论文笔记</a>总结了该论文</li></ul><h2 id="clock同步"><a href="#clock同步" class="headerlink" title="clock同步"></a>clock同步</h2><ul><li><a href="https://www.cs.rutgers.edu/~pxk/417/notes/content/05-clock-synchronization-slides.pdf" target="_blank" rel="noopener">Clock synchronization</a></li></ul><p>Real-Time Clock (RTC)</p><ul><li><a href="https://www.cs.rutgers.edu/~pxk/417/notes/ptp.html" target="_blank" rel="noopener">Precision Time Protocol - notes</a></li><li><a href="https://www.cs.rutgers.edu/~pxk/417/notes/content/06-logical-clocks-slides.pdf" target="_blank" rel="noopener">Logical clocks</a></li><li><a href="https://www.cs.rutgers.edu/~pxk/417/notes/clocks/index.html" target="_blank" rel="noopener">Vector clocks - notes</a></li></ul><h2 id="Vector-Clock-Version-Clock"><a href="#Vector-Clock-Version-Clock" class="headerlink" title="Vector Clock/Version Clock"></a>Vector Clock/Version Clock</h2><ul><li><a href="http://www.cnblogs.com/foxmailed/p/4985848.html" target="_blank" rel="noopener">吴镝 Vector Clock/Version Clock</a></li></ul><p>Version Clock即为Version Vectors。<br>Vector Clock最初是为了给分布式系统的事件定序发明的，本质上是一种捕获causality的手段，只是他们捕获的是事件的关系。而Version Clock是捕获同一个数据的不同版本之间的causality.</p><ul><li><a href="https://haslab.wordpress.com/2011/07/08/version-vectors-are-not-vector-clocks/" target="_blank" rel="noopener">Version Vectors are not Vector Clocks</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为网上资料较多，本文主要是总结一下相关资料。&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Designing Data-Intensive Applications 读书笔记 -Replication</title>
    <link href="http://liujunming.github.io/2018/09/18/Designing-Data-Intensive-Applications-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Replication/"/>
    <id>http://liujunming.github.io/2018/09/18/Designing-Data-Intensive-Applications-读书笔记-Replication/</id>
    <published>2018-09-18T05:23:36.000Z</published>
    <updated>2018-11-06T08:19:17.360Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Leader-和-Follower"><a href="#Leader-和-Follower" class="headerlink" title="Leader 和 Follower"></a>Leader 和 Follower</h2><ul><li>用户端写入的时候，必须先经过Leader处理</li><li>其它节点是Follower，Leader写入完毕后会通知他们复制数据，保证一致性</li><li>客户端读的时候，可以随便读，但写的时候只能向Leader写</li></ul><p><img src="/images/2018/9/10.png" alt=""><br><a id="more"></a></p><h3 id="同步复制与异步复制"><a href="#同步复制与异步复制" class="headerlink" title="同步复制与异步复制"></a>同步复制与异步复制</h3><p><img src="/images/2018/9/11.png" alt=""><br>上图中Follower1是同步复制，Follower2是异步复制</p><p><strong>同步复制：</strong></p><p>写入请求时，Leader 会一直等到所有 Follower 都确认已经写入后（期间不处理任何写请求），才向客户端返回成功</p><p>优点：保证强一致性</p><p>缺点：如果任何 Follower 挂掉，都会写失败，这在大型系统中是不现实的</p><p>所以在实际的数据库中，使用的都是半同步（semi-synchronous），即一个 Follower 是同步的，其它都是异步；如果同步的那个 Follower 挂了，那么设置一个新的 Follower 为同步模式</p><p><strong>异步复制：</strong></p><p>写入请求时，Leader 自己写入成功后就返回，不等待 Follower</p><p>优点：可以立刻响应写入请求，即使所有 Follower 都挂掉了</p><p>缺点：可能会导致不一致(Leader和Follower中的状态不一样)</p><h3 id="增加新的-Follower"><a href="#增加新的-Follower" class="headerlink" title="增加新的 Follower"></a>增加新的 Follower</h3><p>即如何在集群不断写入数据的同时，加入新的 Follower，让它的数据跟上大部队</p><ol><li>给 Leader 某个时刻的数据做一个快照</li><li>把快照复制到新的 Follower 上</li><li>新的 Follower 连接上 Leader，告诉它从哪个时刻开始同步数据</li><li>直到新 Follower 的数据跟上了 Leader 的步伐（caught up），开始进入工作</li></ol><h3 id="处理节点宕机"><a href="#处理节点宕机" class="headerlink" title="处理节点宕机"></a>处理节点宕机</h3><h4 id="Follower-宕机"><a href="#Follower-宕机" class="headerlink" title="Follower 宕机"></a>Follower 宕机</h4><p>从宕机前的日志开始和 Leader 同步即可，直到Follower 的数据跟上了 Leader 的步伐，开始进入工作</p><h4 id="Leader-宕机"><a href="#Leader-宕机" class="headerlink" title="Leader 宕机"></a>Leader 宕机</h4><p>one of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader, and the other followers need to start consuming data changes from the new leader. This process is called <code>failover</code>.</p><p>failover的步骤如下：</p><ol><li>检测 Leader 宕机</li><li>选出新的 Leader</li><li>把系统配置改为新的 Leader</li></ol><h3 id="Implementation-of-Replication-Logs"><a href="#Implementation-of-Replication-Logs" class="headerlink" title="Implementation of Replication Logs"></a>Implementation of Replication Logs</h3><h4 id="Statement-based-replication"><a href="#Statement-based-replication" class="headerlink" title="Statement-based replication"></a>Statement-based replication</h4><p>基于语句的复制，比如在 SQL 中复制 INSERT、UPDATE、DELETE 语句到 Follower。</p><p>存在一些问题：</p><ul><li>NOW()、RANDOM()这样的函数，没法基于语句复制，因为每次运行的结果都不一样</li><li>如果语句依赖自增数，或者跟数据库中现有的数据强相关，那么必须保证语句执行顺序跟 Leader 完全一致，在并发处理多个事务时这一点很难保证</li><li>语句有副作用时，可能会导致不一致的出现</li></ul><h4 id="Write-ahead-log-WAL-shipping"><a href="#Write-ahead-log-WAL-shipping" class="headerlink" title="Write-ahead log (WAL) shipping"></a>Write-ahead log (WAL) shipping</h4><p>本书的第三章讨论了日志结构的储存引擎的实现（SSTable、LSM-Tree 和 B-Tree），如果是这种储存引擎，我们可以把它的每一次写日志都复制到 Follower 上，这样可以保证一致性。</p><p>PostgreSQL 和 Oracle 就是这样实现的，缺陷在于，这种复制方式非常底层，每一条 WAL 包含的信息实际上是“向哪一个硬盘 block 写哪些 bytes”，这就导致 WAL 和储存引擎强相关，也就是必须保证 Leader 和 Follower 的储存引擎底层完全一致，导致集群很难进行版本升级。</p><h4 id="Logical-row-based-log-replication"><a href="#Logical-row-based-log-replication" class="headerlink" title="Logical (row-based) log replication"></a>Logical (row-based) log replication</h4><p>把日志抽象为与底层引擎无关，采用 change data capture，每次有数据更改的时候都记下改了什么，例如记录每次写入的值和行号，MySQL 的 binlog 就是这样实现的。</p><h2 id="复制滞后产生的问题"><a href="#复制滞后产生的问题" class="headerlink" title="复制滞后产生的问题"></a>复制滞后产生的问题</h2><p>对于单 Leader，多 Follower的架构来说，一般是只能向 Leader 写，但可以向任何 Follower 读，这样可以大大增加读的性能。</p><p>但由于写操作需要向 Follower 复制，这里就会产生滞后问题，写完后立刻读，有可能会从 Follower 中读到旧的值（因为此时 Leader 可能还没有同步变化到 Follower 上）。</p><p>当然这种不一致的状态是转临时逝的（如果停止向数据库中写入数据并等待一段时间，从库最终会赶上并与主库保持一致），不会永久存在，也就是所谓的 “最终一致性”。</p><p>因为滞后时间太长引入的不一致性，可不仅是一个理论问题，更是应用设计中会遇到的真实问题。本节将重点介绍三个由复制滞后所带来的问题，并简述解决这些问题的一些方法。</p><h3 id="Reading-Your-Own-Writes"><a href="#Reading-Your-Own-Writes" class="headerlink" title="Reading Your Own Writes"></a>Reading Your Own Writes</h3><p>许多应用让用户提交一些数据，然后查看他们提交的内容。但对于异步复制，问题就来了。如下图所示：如果用户在写入后马上就查看数据，则新数据可能尚未到达副本。对用户而言，看起来好像是刚提交的数据丢失了。<br><img src="/images/2018/9/12.png" alt=""></p><p>在这种情况下，我们需要读写一致性（read-after-write consistency）。这是一个保证，如果用户重新加载页面，他们总会看到他们自己提交的任何更新。它不会对其他用户的写入做出承诺：其他用户的更新可能稍等才会看到。它保证用户自己的输入已被正确保存。</p><p>具体可以有以下策略：</p><ul><li>如果读的字段可能已经发生了变化，那么向 Leader 读取（因为 Leader 的数据一定是最新的）；</li><li>如果读的字段距离上一次变更时间很短，那么向 Leader 读；</li><li>客户端在读请求的时候带上自己最近一次写操作的时间戳，处理这个读请求的服务器看到这个时间戳，就可以知道自己本地的数据是否过时了</li></ul><h3 id="单调读（Monotonic-Reads）"><a href="#单调读（Monotonic-Reads）" class="headerlink" title="单调读（Monotonic Reads）"></a>单调读（Monotonic Reads）</h3><p>客户端进行多次读操作时，这些读操作可能会分配到不同的 Follower 上，所以可能会发生第一次读到了数据，然后第二次读的时候数据又消失了的问题，如下图 User 2345，第一次在 Follower1 上读到了评论，第二次在 Follower2 上没有读到评论：</p><p><img src="/images/2018/9/13.png" alt=""></p><p>所以，客户端读到了新的数据，那么就不能让它读到旧数据。最简单的解决方法就是，把每个客户端的读请求都分配到固定的 Follower 上。</p><h3 id="Consistent-Prefix-Reads"><a href="#Consistent-Prefix-Reads" class="headerlink" title="Consistent Prefix Reads"></a>Consistent Prefix Reads</h3><p>由于服务器之间复制数据可能产生的滞后，数据的时序可能会产生问题。</p><p>比如下图，Mr. Poons 先说了一句话，然后 Mrs. Cake 回复了他，然而对于第三方观察者而言，他们的对话时序可能是混乱的：</p><p><img src="/images/2018/9/14.png" alt=""></p><p>防止这种异常，需要另一种类型的保证：一致前缀读（consistent prefix reads）。 这个保证了：如果一系列写入按某个顺序发生，那么任何人读取这些写入时，也会看见它们以同样的顺序出现。</p><p>​这是分区（partitioned）数据库中的一个特殊问题。如果数据库总是以相同的顺序应用写入，则读取总是会看到一致的前缀，所以这种异常不会发生。但是在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序：当用户从数据库中读取数据时，可能会看到数据库的某些部分处于较旧的状态，而某些处于较新的状态。</p><p>​一种解决方案是，确保任何因果相关的写入都写入相同的分区。对于某些无法高效完成这种操作的应用，还有一些显式跟踪因果依赖关系的算法。</p><h2 id="Multi-Leader-Replication"><a href="#Multi-Leader-Replication" class="headerlink" title="Multi-Leader Replication"></a>Multi-Leader Replication</h2><p>单个 Leader 的缺点在于，如果任何因素导致无法连接 Leader，那么你就无法向数据库写入任何数据了，这会让整个系统非常脆弱，所以我们在一些情境下需要多 Leader 的架构。</p><h3 id="Use-Cases-for-Multi-Leader-Replication"><a href="#Use-Cases-for-Multi-Leader-Replication" class="headerlink" title="Use Cases for Multi-Leader Replication"></a>Use Cases for Multi-Leader Replication</h3><p>下面是一些多 Leader 架构的示例</p><h4 id="多个数据中心"><a href="#多个数据中心" class="headerlink" title="多个数据中心"></a>多个数据中心</h4><p><img src="/images/2018/9/15.png" alt=""></p><p>像上图这种情况，你可以有多个 Leader 分布在不同地方的数据中心，每个数据中心都是一个独立的集群，它们的 Leader 之间会相互同步数据。</p><h4 id="可以离线的客户端"><a href="#可以离线的客户端" class="headerlink" title="可以离线的客户端"></a>可以离线的客户端</h4><p>我们可以把一个支持离线运行的客户端，和服务器端，视为两个“数据中心”，比如一些日历应用，会在本地维护一份数据，直到有网络时，才会和服务器进行数据同步，这就是一个异步的多 Leader 架构。</p><p>CouchDB 就是为此设计的。</p><h4 id="多人协作编辑"><a href="#多人协作编辑" class="headerlink" title="多人协作编辑"></a>多人协作编辑</h4><p>像 Etherpad、Google Docs 这样的应用，允许多人同时编辑同一份文档，每个人都是一个 “Leader”，相互之间同步数据，但这显然会遇到冲突的问题。</p><h3 id="解决写冲突"><a href="#解决写冲突" class="headerlink" title="解决写冲突"></a>解决写冲突</h3><p>多 Leader 之间同步数据，最大的问题就是如何解决写冲突。比如下图中，两个用户都修改了文档的标题，发请求给服务器，都返回了成功，但直到 Leader 之间进行同步时才发现之前的数据有冲突。</p><p><img src="/images/2018/9/16.png" alt=""></p><h4 id="同步冲突检测"><a href="#同步冲突检测" class="headerlink" title="同步冲突检测"></a>同步冲突检测</h4><p>单 Leader 不会发生冲突，因为每次写入都是一个原子化的事务。</p><p>多 Leader 如果采用同步的方式检测冲突，也不会发生冲突。即每次写入时，都向其它的 Leader 检查有没有冲突，如果都没有冲突，那么写入成功。但这样性能极差，也丢掉了多 Leader 架构的好处，还不如用单个 Leader。</p><h4 id="避免冲突"><a href="#避免冲突" class="headerlink" title="避免冲突"></a>避免冲突</h4><p>多 Leader 架构避免冲突最简单的方式就是，让可能产生冲突的请求，都走向同一个 Leader。比如对于同一项资料的修改，都路由到固定的某个 Leader 上。</p><p>这样做的缺陷在于，集群是不断变化的，很难做到长期固定，Leader 的变化就会让这个策略失效。</p><h4 id="收敛至一致的状态"><a href="#收敛至一致的状态" class="headerlink" title="收敛至一致的状态"></a>收敛至一致的状态</h4><p>实现冲突合并解决有多种途径：</p><ul><li>给每个写入一个唯一的ID（例如，一个时间戳，一个长的随机数，一个UUID或者一个键和值的哈希），挑选最高ID的写入作为胜利者，并丢弃其他写入。如果使用时间戳，这种技术被称为最后写入胜利（LWW, last write wins）。虽然这种方法很流行，但是很容易造成数据丢失。</li><li>为每个副本分配一个唯一的ID，ID编号更高的写入具有更高的优先级。这种方法也意味着数据丢失。</li><li>以某种方式将这些值合并在一起 - 例如，按字母顺序排序，然后连接它们（在图5-7中，合并的标题可能类似于“B/C”）。</li><li>在保留所有信息的显式数据结构中记录冲突，并编写解决冲突的应用程序代码（也许通过提示用户的方式）。<br>​ </li></ul><h4 id="自定义冲突解决逻辑"><a href="#自定义冲突解决逻辑" class="headerlink" title="自定义冲突解决逻辑"></a>自定义冲突解决逻辑</h4><p>作为解决冲突最合适的方法可能取决于应用程序，大多数多主复制工具允许使用应用程序代码编写冲突解决逻辑。该代码可以在写入或读取时执行。</p><blockquote><p><strong>自动冲突解决</strong><br>冲突解决规则可能很快变得复杂，并且自定义代码可能容易出错。<br>已经有一些有趣的研究来自动解决由于数据修改引起的冲突。有几个研究值得一提：</p><ul><li>无冲突复制数据类型（Conflict-free replicated datatypes）（CRDT）是可以由多个用户同时编辑的集合，映射，有序列表，计数器等的一系列数据结构，它们以合理的方式自动解决冲突。</li><li>可合并的持久数据结构（Mergeable persistent data structures）显式跟踪历史记录，类似于Git版本控制系统。</li><li>可执行的转换（operational transformation）是Etherpad和Google Docs等合作编辑应用背后的冲突解决算法。</li></ul><p>这些算法在数据库中的实现还很年轻，但很可能将来它们将被集成到更多的复制数据系统中。自动冲突解决方案可以使应用程序处理多领导者数据同步更为简单。</p></blockquote><h3 id="多-Leader-的拓扑结构"><a href="#多-Leader-的拓扑结构" class="headerlink" title="多 Leader 的拓扑结构"></a>多 Leader 的拓扑结构</h3><p>多 Leader 可以有很多种拓扑结构，环形、星形、全连接形。<br><img src="/images/2018/9/17.png" alt=""><br>MySQL 使用的是环形连接。全连接形是最符合直觉的，每个 Leader 都和其它所有 Leader 相互交换数据。<br>另一方面，全连接形拓扑也可能有问题。特别是，一些网络链接可能比其他网络链接更快（例如，由于网络拥塞），结果是一些复制消息可能“超过”其他复制消息，如下图示。</p><p><img src="/images/2018/9/18.png" alt=""></p><p>这是一个因果关系的问题：更新取决于先前的插入，所以我们需要确保所有节点先处理插入，然后再处理更新。</p><p>要正确排序这些事件，可以使用一种称为<strong>version vectors</strong>的技术。</p><h2 id="无-Leader-复制"><a href="#无-Leader-复制" class="headerlink" title="无 Leader 复制"></a>无 Leader 复制</h2><p>无 Leader 复制完全不需要 Leader 的存在，这种架构中，客户端可以向多个节点发起读写请求。</p><h3 id="当有节点挂掉时，如何写入数据库"><a href="#当有节点挂掉时，如何写入数据库" class="headerlink" title="当有节点挂掉时，如何写入数据库"></a>当有节点挂掉时，如何写入数据库</h3><p>只要保证多个节点写入成功，那么客户端就可以认为写入成功。<br><img src="/images/2018/9/19.png" alt=""></p><h4 id="Read-repair-and-anti-entropy"><a href="#Read-repair-and-anti-entropy" class="headerlink" title="Read repair and anti-entropy"></a>Read repair and anti-entropy</h4><p>在读取的时候，可能会存在不一致（因为有部分节点写入失败），这时可以发现不一致并且修复它。或者所有节点都定期检查是否自己的数据跟别人有不一致的地方。</p><h4 id="Quorums-for-reading-and-writing"><a href="#Quorums-for-reading-and-writing" class="headerlink" title="Quorums for reading and writing"></a>Quorums for reading and writing</h4><p>如果有n个副本，每个写入必须由w节点确认才能被认为是成功的，并且我们必须至少为每个读查询r个节点。 只要<code>w + r&gt; n</code>，我们期望在读取时获得最新的值，因为r个读取中至少有一个节点是最新的。</p><h3 id="Quorums-机制的局限性"><a href="#Quorums-机制的局限性" class="headerlink" title="Quorums 机制的局限性"></a>Quorums 机制的局限性</h3><h3 id="Sloppy-Quorums-and-Hinted-Handoff"><a href="#Sloppy-Quorums-and-Hinted-Handoff" class="headerlink" title="Sloppy Quorums and Hinted Handoff"></a>Sloppy Quorums and Hinted Handoff</h3><p><em>sloppy quorum</em>:写和读仍然需要w和r成功的响应，但是那些可能包括不在指定的n个“主”节点中的值。比方说，如果你把自己锁在房子外面，你可能会敲开邻居的门，问你是否可以暂时停留在沙发上。</p><p>​ 一旦网络中断得到解决，代表另一个节点临时接受的一个节点的任何写入都被发送到适当的“本地”节点，这就是hinted handoff。 （一旦你再次找到你的房子的钥匙，你的邻居礼貌地要求你离开沙发回家。）</p><h3 id="Detecting-Concurrent-Writes"><a href="#Detecting-Concurrent-Writes" class="headerlink" title="Detecting Concurrent Writes"></a>Detecting Concurrent Writes</h3><h4 id="Last-write-wins-discarding-concurrent-writes"><a href="#Last-write-wins-discarding-concurrent-writes" class="headerlink" title="Last write wins (discarding concurrent writes)"></a>Last write wins (discarding concurrent writes)</h4><h4 id="The-“happens-before”-relationship-and-concurrency"><a href="#The-“happens-before”-relationship-and-concurrency" class="headerlink" title="The “happens-before” relationship and concurrency"></a>The “happens-before” relationship and concurrency</h4><h4 id="Capturing-the-happens-before-relationship"><a href="#Capturing-the-happens-before-relationship" class="headerlink" title="Capturing the happens-before relationship"></a>Capturing the happens-before relationship</h4><h4 id="Merging-concurrently-written-values"><a href="#Merging-concurrently-written-values" class="headerlink" title="Merging concurrently written values"></a>Merging concurrently written values</h4><h4 id="Version-vectors"><a href="#Version-vectors" class="headerlink" title="Version vectors"></a>Version vectors</h4><hr><p>参考资料：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/36282816" target="_blank" rel="noopener">知乎 Starkwang</a></li><li><a href="https://github.com/Vonng/ddia/blob/master/ch5.md" target="_blank" rel="noopener">Vonng/ddia</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Leader-和-Follower&quot;&gt;&lt;a href=&quot;#Leader-和-Follower&quot; class=&quot;headerlink&quot; title=&quot;Leader 和 Follower&quot;&gt;&lt;/a&gt;Leader 和 Follower&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;用户端写入的时候，必须先经过Leader处理&lt;/li&gt;
&lt;li&gt;其它节点是Follower，Leader写入完毕后会通知他们复制数据，保证一致性&lt;/li&gt;
&lt;li&gt;客户端读的时候，可以随便读，但写的时候只能向Leader写&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/images/2018/9/10.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://liujunming.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="分布式系统" scheme="http://liujunming.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
