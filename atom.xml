<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2022-12-03T11:38:55.483Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes about Intel turbo boost</title>
    <link href="http://liujunming.github.io/2022/12/03/Notes-about-Intel-turbo-boost/"/>
    <id>http://liujunming.github.io/2022/12/03/Notes-about-Intel-turbo-boost/</id>
    <published>2022-12-03T11:08:59.000Z</published>
    <updated>2022-12-03T11:38:55.483Z</updated>
    
    <content type="html"><![CDATA[<p>Intel turbo boost的中文翻译为”睿频加速”，一般情况下turbo(睿频)指的就是Intel turbo boost。本文将记录turbo相关notes。<a id="more"></a> </p><p><img src="/images/2022/12/01.jpg" alt></p><p>CPUs don’t always need to run at their maximum frequency. Some programs are more dependent on memory to run smoothly, while others are CPU-intensive. Intel Turbo Boost Technology is an energy-efficient solution to this imbalance: it lets the CPU run at its base clock speed when handling light workloads, then jump to a higher clock speed for heavy workloads.</p><p>Running at a lower clock rate (the number of cycles executed by the processor every second) allows the processor to use less power, which can reduce heat and positively impact battery life in laptops. But when more speed is needed, Intel Turbo Boost Technology dynamically increases the clock rate to compensate.</p><p>Intel Turbo Boost Technology can potentially increase CPU speeds up to the Max Turbo Frequency while staying within safe temperature and power limits. This can increase performance in both single-threaded and multithreaded applications (programs that utilize several processor cores).</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/support/articles/000030893/processors.html" target="_blank" rel="noopener">What Is Intel® Turbo Boost Technology and How Does It Work?</a></li><li><a href="https://www.intel.com/content/www/us/en/gaming/resources/turbo-boost.htm" target="_blank" rel="noopener">What Is Intel® Turbo Boost Technology?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Intel turbo boost的中文翻译为”睿频加速”，一般情况下turbo(睿频)指的就是Intel turbo boost。本文将记录turbo相关notes。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>How to use IO poll in Linux NVMe driver</title>
    <link href="http://liujunming.github.io/2022/11/29/How-to-use-IO-poll-in-Linux-NVMe-driver/"/>
    <id>http://liujunming.github.io/2022/11/29/How-to-use-IO-poll-in-Linux-NVMe-driver/</id>
    <published>2022-11-29T15:09:37.000Z</published>
    <updated>2022-11-29T15:35:56.810Z</updated>
    
    <content type="html"><![CDATA[<p>使用NVMe driver的<code>poll_queues</code>参数即可开启IO queue的poll。<a id="more"></a> </p><h3 id="内核源码"><a href="#内核源码" class="headerlink" title="内核源码"></a>内核源码</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//https://elixir.bootlin.com/linux/v6.0/source/drivers/nvme/host/pci.c</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> poll_queues;</span><br><span class="line">module_param_cb(poll_queues, &amp;io_queue_count_ops, &amp;poll_queues, <span class="number">0644</span>);</span><br><span class="line">MODULE_PARM_DESC(poll_queues, <span class="string">"Number of queues to use for polled IO."</span>);</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">nvme_setup_io_queues</span><span class="params">(struct nvme_dev *dev)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line">dev-&gt;nr_poll_queues = poll_queues;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>当只有一个IO queue时，设置<code>poll_queues</code>为1后，发现依然有中断。其实这是符合预取的。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">nvme_setup_irqs</span><span class="params">(struct nvme_dev *dev, <span class="keyword">unsigned</span> <span class="keyword">int</span> nr_io_queues)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">pci_dev</span> *<span class="title">pdev</span> = <span class="title">to_pci_dev</span>(<span class="title">dev</span>-&gt;<span class="title">dev</span>);</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">irq_affinity</span> <span class="title">affd</span> = &#123;</span></span><br><span class="line">.pre_vectors= <span class="number">1</span>,</span><br><span class="line">.calc_sets= nvme_calc_irq_sets,</span><br><span class="line">.priv= dev,</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> irq_queues, poll_queues;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Poll queues don't need interrupts, but we need at least one I/O queue</span></span><br><span class="line"><span class="comment"> * left over for non-polled I/O.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">poll_queues = min(dev-&gt;nr_poll_queues, nr_io_queues - <span class="number">1</span>);</span><br><span class="line">dev-&gt;io_queues[HCTX_TYPE_POLL] = poll_queues;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Initialize for the single interrupt case, will be updated in</span></span><br><span class="line"><span class="comment"> * nvme_calc_irq_sets().</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">dev-&gt;io_queues[HCTX_TYPE_DEFAULT] = <span class="number">1</span>;</span><br><span class="line">dev-&gt;io_queues[HCTX_TYPE_READ] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * We need interrupts for the admin queue and each non-polled I/O queue,</span></span><br><span class="line"><span class="comment"> * but some Apple controllers require all queues to use the first</span></span><br><span class="line"><span class="comment"> * vector.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">irq_queues = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> (!(dev-&gt;ctrl.quirks &amp; NVME_QUIRK_SINGLE_VECTOR))</span><br><span class="line">irq_queues += (nr_io_queues - poll_queues);</span><br><span class="line"><span class="keyword">return</span> pci_alloc_irq_vectors_affinity(pdev, <span class="number">1</span>, irq_queues,</span><br><span class="line">      PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &amp;affd);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>由上述代码可知，driver至少留一个IO queue使用interrupt而非poll。</p><p>因此，当只有一个IO queue时，即使driver参数设置了<code>poll_queues</code>为1，其实是不生效的(<code>nvme_setup_irqs</code>中的<code>poll_queues</code>变量为0)，这个唯一的IO queue使用的依然是interrupt而非poll。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用NVMe driver的&lt;code&gt;poll_queues&lt;/code&gt;参数即可开启IO queue的poll。
    
    </summary>
    
      <category term="存储" scheme="http://liujunming.github.io/categories/%E5%AD%98%E5%82%A8/"/>
    
    
      <category term="存储" scheme="http://liujunming.github.io/tags/%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Notes about sysctl</title>
    <link href="http://liujunming.github.io/2022/11/27/Notes-about-sysctl/"/>
    <id>http://liujunming.github.io/2022/11/27/Notes-about-sysctl/</id>
    <published>2022-11-27T07:28:45.000Z</published>
    <updated>2022-11-27T11:15:38.702Z</updated>
    
    <content type="html"><![CDATA[<p><code>sysctl</code>的用法可参考<code>man sysctl</code>。<a id="more"></a><br>本文主要转载自<a href="https://cloud.tencent.com/developer/article/1657639" target="_blank" rel="noopener">Linux 下的 Sysctl 命令</a>。</p><p>作为一个 Linux 系统管理员，有时候你需要修改默认的内核行为。例如，你可能想要启用 SysRq 或者增加 Kernel 能够接受的连接数量。 内核参数可以在构建内核的时候，在系统启动时，或者在运行时进行设置。</p><p>本文讲解如何使用<code>sysct</code>l命令在运行时进行查看并且修改内核参数。</p><h3 id="1-使用sysctl查看-Kernel-参数"><a href="#1-使用sysctl查看-Kernel-参数" class="headerlink" title="1. 使用sysctl查看 Kernel 参数"></a>1. 使用sysctl查看 Kernel 参数</h3><p>想要查看所有的当前内核参数，运行<code>sysctl</code> 命令加上<code>-a</code>选项：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a</span><br></pre></td></tr></table></figure></p><p>这将会输出一个很大的列表，看起来像下面这样，每行包含一个参数和对应的值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">abi.vsyscall32 = 1</span><br><span class="line">debug.exception-trace = 1</span><br><span class="line">debug.kprobes-optimization = 1</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>所有用户可以查看当前的内核参数；仅仅 root 用户可以修改它们的值。</p><p>通过将参数名传递给<code>sysctl</code>,你可以检查单个参数的取值。例如，想要检查当前的 swappiness 取值，你可以输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl vm.swappiness</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vm.swappiness = 60</span><br></pre></td></tr></table></figure></p><p>Swappiness 是一个内核属性，它定义了系统多长时间会使用swap space。</p><p>这个<code>sysctl</code>命令将会从<code>/proc/sys</code>目录下读取信息。 <code>/proc/sys</code>是一个虚拟目录，它包含文件对象，可以被用来查看或者设置当前的内核参数。</p><p>你也可以通过显示合适的文件，来查看参数值。唯一的不同就是文件如何被展示。例如，<code>sysctl vm.swappiness</code>和<code>cat /proc/sys/vm/swappiness</code>都将给出同样的输出。当使用<code>sysctl</code>时，目录中的斜杠将会被点所替代，并且<code>proc.sys</code>部分被去掉了。</p><h3 id="2-使用sysctl来修改内核参数"><a href="#2-使用sysctl来修改内核参数" class="headerlink" title="2. 使用sysctl来修改内核参数"></a>2. 使用sysctl来修改内核参数</h3><p>想要在系统运行时设置一个内核参数，按照下面的格式运行<code>sysctl</code>命令加上参数名和取值：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w parameter=value</span><br></pre></td></tr></table></figure><p>如果这个取值包含空格或者特殊符号，使用双引号包裹取值。你还可以在同一个命令中传递多个<code>parameter=value</code> 键值对。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在生产系统中修改内核设置必须非常小心，这可能会使得内核不稳当，并且你需要重启系统。</span><br></pre></td></tr></table></figure><p>例如，想要允许 IPV4 包转发，你需要运行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.ip_forward=<span class="number">1</span></span><br></pre></td></tr></table></figure><p>这个修改立即生效，但是它不是持久化的。在系统重启后，默认值会被重新加载。</p><p>想要永久修改一个参数，你需要修改设置到文件<code>/etc/sysctl.conf</code> ：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.ip_forward=<span class="number">1</span> &gt;&gt; <span class="regexp">/etc/</span>sysctl.conf</span><br></pre></td></tr></table></figure><p>另外修改参数的方式就是使用<code>echo</code>命令将设置写入到<code>/proc/sys</code>目录下的文件中。例如，不使用上面的命令，你还可以用：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="number">1</span> &gt; <span class="regexp">/proc/</span>sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure><p>这个<code>-p</code>选项允许你从一个配置文件中加载设置：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -p /etc/sysctl.d/file_name.conf</span><br></pre></td></tr></table></figure><p>如果没有给出文件，那么 <code>sysctl</code> 从 <code>/etc/sysctl.conf</code>文件中读取。</p><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p><code>sysctl</code> 命令允许你查看并且修改 Linux 内核参数。</p><hr><p>参考资料:</p><ol><li><a href="https://linux.die.net/man/8/sysctl" target="_blank" rel="noopener">man sysctl</a></li><li><a href="https://cloud.tencent.com/developer/article/1657639" target="_blank" rel="noopener">Linux 下的 Sysctl 命令</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;sysctl&lt;/code&gt;的用法可参考&lt;code&gt;man sysctl&lt;/code&gt;。
    
    </summary>
    
      <category term="工具" scheme="http://liujunming.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Linux Hungtask机制</title>
    <link href="http://liujunming.github.io/2022/11/27/Linux-Hungtask%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2022/11/27/Linux-Hungtask机制/</id>
    <published>2022-11-27T03:36:29.000Z</published>
    <updated>2022-11-27T06:33:40.495Z</updated>
    
    <content type="html"><![CDATA[<p>本文将总结Linux的Hungtask机制。<a id="more"></a><br>本文参考的内核源码版本为<a href="https://elixir.bootlin.com/linux/v4.0/source" target="_blank" rel="noopener">v4.0</a>。</p><h3 id="1-现象"><a href="#1-现象" class="headerlink" title="1. 现象"></a>1. 现象</h3><p>内核日志会看到如下信息:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO: task filebench:7143 blocked for more than 120 seconds.</span><br><span class="line">21794 Oct 24 13:21:33 localhost kernel: &quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.</span><br></pre></td></tr></table></figure></p><h3 id="2-背景知识"><a href="#2-背景知识" class="headerlink" title="2. 背景知识"></a>2. 背景知识</h3><p>长期以来，处于D状态(<code>TASK_UNINTERRUPTIBLE</code>状态)的进程都是让人比较烦恼的问题，处于D状态的进程不能接收信号，kill不掉。在一些场景下，常见的进程长期处于D状态，用户对此无能为力，也不知道原因，只能重启恢复。<br>其实进程长期处于D状态肯定是不正常的，内核中设计D状态的目的是为了让进程等待IO完成，正常情况下IO应该会顺利完成，然后唤醒相应的D状态进程，即使在异常情况下(比如磁盘离或损坏、磁阵链路断开等)，IO处理也是有超时机制的，原理上不会存在永久处于D状态的进程。但是因为内核代码流程中可能存在一些bug，或者用户内核模块中的相关机制不合理，可能导致进程长期处于D状态，无法唤醒，类似于死锁状态。<br>针对这种情况，内核中提供了hung task机制用于检测系统中是否存在处于D状态超过120s(时长可以设置)的进程，如果存在，则打印相关警告和进程堆栈。如果配置了<code>hung_task_panic</code>，则直接发起panic，结合kdump可以搜集到vmcore。从内核的角度看，如果有进程处于D状态的时间超过了120s，那肯定已经出现异常了，以此机制来收集相关的异常信息，用于分析定位问题。</p><h3 id="3-基本原理"><a href="#3-基本原理" class="headerlink" title="3. 基本原理"></a>3. 基本原理</h3><p>创建一个内核线程(khungtaskd)，定期(120s)唤醒后，遍历系统中的所有进程，检查是否存在处于D状态超过120s(时长可以设置)的进程，如果存在，则打印相关警告和进程堆栈。如果配置了hung_task_panic（proc或内核启动参数），则直接发起panic。</p><h3 id="4-源码解析"><a href="#4-源码解析" class="headerlink" title="4. 源码解析"></a>4. 源码解析</h3><p><a href="https://elixir.bootlin.com/linux/v4.0/source/kernel/hung_task.c" target="_blank" rel="noopener">kernel/hung_task.c</a><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hung_task_init</span><br><span class="line">└── watchdog</span><br><span class="line">    └── check_hung_uninterruptible_tasks</span><br><span class="line">        └── check_hung_task</span><br></pre></td></tr></table></figure></p><h4 id="4-1-初始化"><a href="#4-1-初始化" class="headerlink" title="4.1 初始化"></a>4.1 初始化</h4><p>初始化一个内核线程来检测系统中是否存在D状态超过120s的进程。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __<span class="function">init <span class="title">hung_task_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/*注册panic通知链，在panic时执行相关操作。*/</span></span><br><span class="line">atomic_notifier_chain_register(&amp;panic_notifier_list, &amp;panic_block);</span><br><span class="line"><span class="comment">/*创建内核线程khungtaskd，执行函数为watchdog*/</span></span><br><span class="line">watchdog_task = kthread_run(watchdog, <span class="literal">NULL</span>, <span class="string">"khungtaskd"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-2-watchdog"><a href="#4-2-watchdog" class="headerlink" title="4.2 watchdog"></a>4.2 watchdog</h4><p>khungtaskd内核线程的处理函数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * kthread which checks for tasks stuck in D state</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">watchdog</span><span class="params">(<span class="keyword">void</span> *dummy)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/*设置当前khungtaskd内核线程的nice为0，即普通优先级，为了不影响业务运行*/</span></span><br><span class="line">set_user_nice(current, <span class="number">0</span>);</span><br><span class="line"><span class="comment">/*死循环进行检测*/</span></span><br><span class="line"><span class="keyword">for</span> ( ; ; ) &#123;</span><br><span class="line"><span class="comment">/*进程处于D状态的时间上限可通过sysctl/proc控制，默认为120s*/</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> timeout = sysctl_hung_task_timeout_secs;</span><br><span class="line"><span class="comment">/*检测线程(khungtaskd)sleep 120s(默认)后，再次唤醒。*/</span></span><br><span class="line"><span class="keyword">while</span> (schedule_timeout_interruptible(timeout_jiffies(timeout)))</span><br><span class="line">timeout = sysctl_hung_task_timeout_secs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (atomic_xchg(&amp;reset_hung_task, <span class="number">0</span>))</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line"><span class="comment">/*醒来后执行实际的检测操作*/</span></span><br><span class="line">check_hung_uninterruptible_tasks(timeout);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-3-check-hung-uninterruptible-tasks"><a href="#4-3-check-hung-uninterruptible-tasks" class="headerlink" title="4.3 check_hung_uninterruptible_tasks"></a>4.3 check_hung_uninterruptible_tasks</h4><p>遍历系统中的所有进程，检测是否有处于D状态超过120s的进程，如果有则打印警告或panic。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Check whether a TASK_UNINTERRUPTIBLE does not get woken up for</span></span><br><span class="line"><span class="comment"> * a really long time (120 seconds). If that happens, print out</span></span><br><span class="line"><span class="comment"> * a warning.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">check_hung_uninterruptible_tasks</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> timeout)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/*hung task检测是检查的最大进程数，默认为最大的进程号*/</span></span><br><span class="line"><span class="keyword">int</span> max_count = sysctl_hung_task_check_count;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 每次遍历进程数的上限，默认为1024，这样做的目的是为了:</span></span><br><span class="line"><span class="comment"> * 1、防止rcu_read_lock的占用时间太长。</span></span><br><span class="line"><span class="comment"> * 2、hung task的watchdog占用CPU时间太长。如果没开内核抢占，则如果内核线程不主动调度的话，是不能发生进程切换的</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 如果系统中的进程数比较多，那么就可能检测不到部分D状态进程了?不会，因为这里只是会调度一次，调度回来后，会继续遍历后面的进程</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> batch_count = HUNG_TASK_BATCHING;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> *<span class="title">g</span>, *<span class="title">t</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * If the system crashed already then all bets are off,</span></span><br><span class="line"><span class="comment"> * do not report extra hung tasks:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">/*如果系统已经处于crash状态了，就不再报hung task了。*/</span></span><br><span class="line"><span class="keyword">if</span> (test_taint(TAINT_DIE) || did_panic)</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">rcu_read_lock();</span><br><span class="line"><span class="comment">/*遍历系统中的所有进程*/</span></span><br><span class="line">do_each_thread(g, t) &#123;</span><br><span class="line"><span class="keyword">if</span> (!max_count--)</span><br><span class="line"><span class="keyword">goto</span> unlock;</span><br><span class="line"><span class="comment">/*如果每次检测的进程数量超过1024了，则需要发起调度，结束rcu优雅周期*/</span></span><br><span class="line"><span class="keyword">if</span> (!--batch_count) &#123;</span><br><span class="line">batch_count = HUNG_TASK_BATCHING;</span><br><span class="line"><span class="comment">/*释放rcu，并主动调度，调度回来后检查相应进程是否还在，如果不在了，则退出遍历，否则继续*/</span></span><br><span class="line"><span class="keyword">if</span> (!rcu_lock_break(g, t))</span><br><span class="line"><span class="keyword">goto</span> unlock;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* use "==" to skip the TASK_KILLABLE tasks waiting on NFS */</span></span><br><span class="line"><span class="comment">/*检测进程状态是否为D*/</span></span><br><span class="line"><span class="keyword">if</span> (t-&gt;state == TASK_UNINTERRUPTIBLE)</span><br><span class="line"><span class="comment">/*检测进程处于D状态的时间是否超过120s。*/</span></span><br><span class="line">check_hung_task(t, timeout);</span><br><span class="line">&#125; while_each_thread(g, t);</span><br><span class="line"> unlock:</span><br><span class="line">rcu_read_unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-4-check-hung-task"><a href="#4-4-check-hung-task" class="headerlink" title="4.4 check_hung_task"></a>4.4 check_hung_task</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">check_hung_task</span><span class="params">(struct task_struct *t, <span class="keyword">unsigned</span> <span class="keyword">long</span> timeout)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/*进程上下文切换计数，以此来判断该进程是否发生过调度*/</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> switch_count = t-&gt;nvcsw + t-&gt;nivcsw;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Ensure the task is not frozen.</span></span><br><span class="line"><span class="comment"> * Also, skip vfork and any other user process that freezer should skip.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (unlikely(t-&gt;flags &amp; (PF_FROZEN | PF_FREEZER_SKIP)))</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * When a freshly created task is scheduled once, changes its state to</span></span><br><span class="line"><span class="comment"> * TASK_UNINTERRUPTIBLE without having ever been switched out once, it</span></span><br><span class="line"><span class="comment"> * musn't be checked.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (unlikely(!switch_count))</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 如果当前switch_count等于last_switch_count，则说明在khungtaskd进程被唤醒期间，该进程没有发生过调度。</span></span><br><span class="line"><span class="comment"> * 也就是说，该进程一直处于D状态，因为last_switch_count只在这里更新，其它地方不会。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (switch_count != t-&gt;last_switch_count) </span><br><span class="line"><span class="comment">/* 更新last_switch_count计数，只在这里更新，该计数专用于hung task的检测。*/</span></span><br><span class="line">t-&gt;last_switch_count = switch_count;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">trace_sched_process_hang(t);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * hung task错误打印次数限制，防止dos攻击。默认为10次，由于是全局变量，</span></span><br><span class="line"><span class="comment"> * 表示系统运行期间最多打印10次，超过后就不打印了。该参数应该可以</span></span><br><span class="line"><span class="comment"> * 通过sysctl修改</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (!sysctl_hung_task_warnings)</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (sysctl_hung_task_warnings &gt; <span class="number">0</span>)</span><br><span class="line">sysctl_hung_task_warnings--;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Ok, the task did not get scheduled for more than 2 minutes,</span></span><br><span class="line"><span class="comment"> * complain:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">/*如下就是我们平常常见的hung task打印了*/</span></span><br><span class="line">pr_err(<span class="string">"INFO: task %s:%d blocked for more than %ld seconds.\n"</span>,</span><br><span class="line">t-&gt;comm, t-&gt;pid, timeout);</span><br><span class="line">pr_err(<span class="string">"      %s %s %.*s\n"</span>,</span><br><span class="line">print_tainted(), init_utsname()-&gt;release,</span><br><span class="line">(<span class="keyword">int</span>)<span class="built_in">strcspn</span>(init_utsname()-&gt;version, <span class="string">" "</span>),</span><br><span class="line">init_utsname()-&gt;version);</span><br><span class="line">pr_err(<span class="string">"\"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs\""</span></span><br><span class="line"><span class="string">" disables this message.\n"</span>);</span><br><span class="line"><span class="comment">/*打印堆栈*/</span></span><br><span class="line">sched_show_task(t);</span><br><span class="line"><span class="comment">/*如果开启了debug_lock，则打印锁的占用情况*/</span></span><br><span class="line">debug_show_held_locks(t);</span><br><span class="line"></span><br><span class="line">touch_nmi_watchdog();</span><br><span class="line"><span class="comment">/*检测是否配置了/proc/sys/kernel/hung_task_panic，如果配置则直接触发panic*/</span></span><br><span class="line"><span class="keyword">if</span> (sysctl_hung_task_panic) &#123;</span><br><span class="line"><span class="comment">/*打印所有CPU的堆栈*/</span></span><br><span class="line">trigger_all_cpu_backtrace();</span><br><span class="line"><span class="comment">/*触发panic，如果配置了kdump就有用了*/</span></span><br><span class="line">panic(<span class="string">"hung_task: blocked tasks"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-Hungtask定位思路"><a href="#5-Hungtask定位思路" class="headerlink" title="5. Hungtask定位思路"></a>5. Hungtask定位思路</h3><p><img src="/images/2022/11/14.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s/Jpex9c0_GBZsxB4J21ojxA" target="_blank" rel="noopener">Hungtask原理及分析</a></li><li><a href="https://zhuanlan.zhihu.com/p/463433198" target="_blank" rel="noopener">内核Hungtask原理和定位思路总结</a></li><li><a href="https://blog.csdn.net/weixin_33921089/article/details/86390346" target="_blank" rel="noopener">hung task机制</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将总结Linux的Hungtask机制。
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Notes about linux进程D状态</title>
    <link href="http://liujunming.github.io/2022/11/26/Notes-about-%E8%BF%9B%E7%A8%8BD%E7%8A%B6%E6%80%81/"/>
    <id>http://liujunming.github.io/2022/11/26/Notes-about-进程D状态/</id>
    <published>2022-11-26T08:50:19.000Z</published>
    <updated>2022-11-27T03:37:43.130Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录进程D状态相关笔记。</p><a id="more"></a> <h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>man ps中的描述:<br><img src="/images/2022/11/11.jpg" alt></p><p>Linux kernel中的宏定义: <a href="https://elixir.bootlin.com/linux/v6.0/source/include/linux/sched.h#L86" target="_blank" rel="noopener">TASK_UNINTERRUPTIBLE</a></p><h3 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h3><p><img src="/images/2022/11/12.jpg" alt><br><img src="/images/2022/11/13.jpg" alt></p><h3 id="资料推荐"><a href="#资料推荐" class="headerlink" title="资料推荐"></a>资料推荐</h3><p>强烈推荐<a href="/pdf/炫技！bug 排查大曝光，涉及Linux 内核的那种.pdf">炫技！bug 排查大曝光，涉及Linux 内核的那种</a>一文，会收获颇丰!</p><hr><p>参考资料:</p><ol><li><a href="https://www.man7.org/linux/man-pages/man1/ps.1.html" target="_blank" rel="noopener">man ps</a></li><li><a href="https://www.cnblogs.com/embedded-linux/p/7043569.html" target="_blank" rel="noopener">linux进程D状态</a></li><li><a href="https://mp.weixin.qq.com/s/5OOqJRhBRhdih7Pb0f5e5A" target="_blank" rel="noopener">炫技！bug 排查大曝光，涉及Linux 内核的那种</a></li><li><a href="https://www.zouhl.com/posts/linux%E4%B8%8B%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90-d%E7%8A%B6%E6%80%81%E5%92%8Cz%E7%8A%B6%E6%80%81/" target="_blank" rel="noopener">Linux下常见的系统问题分析 D状态和Z状态</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录进程D状态相关笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>利用bpftrace打印内核函数调用栈</title>
    <link href="http://liujunming.github.io/2022/11/20/%E5%88%A9%E7%94%A8bpftrace%E6%89%93%E5%8D%B0%E5%86%85%E6%A0%B8%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E6%A0%88/"/>
    <id>http://liujunming.github.io/2022/11/20/利用bpftrace打印内核函数调用栈/</id>
    <published>2022-11-20T08:30:23.000Z</published>
    <updated>2022-11-20T08:39:58.395Z</updated>
    
    <content type="html"><![CDATA[<p>本文将以<code>vp_notify</code>函数为例，介绍下如何利用bpftrace打印内核函数调用栈。<a id="more"></a> </p><ul><li><p>确定目标内核函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bpftrace -l &apos;kprobe:*&apos; | grep vp_notify</span><br><span class="line">kprobe:vp_notify</span><br></pre></td></tr></table></figure></li><li><p><code>kstack</code>: Stack Traces, Kernel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bpftrace -e &apos;kprobe:vp_notify &#123; @[kstack] = count(); &#125;&apos;</span><br><span class="line">Attaching 1 probe...</span><br><span class="line">^C</span><br></pre></td></tr></table></figure></li></ul><p><a href="https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#7-kstack-stack-traces-kernel" target="_blank" rel="noopener">https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#7-kstack-stack-traces-kernel</a></p><p><a href="https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#2-count-count" target="_blank" rel="noopener">count()</a>的含义</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将以&lt;code&gt;vp_notify&lt;/code&gt;函数为例，介绍下如何利用bpftrace打印内核函数调用栈。
    
    </summary>
    
      <category term="工具" scheme="http://liujunming.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="debug" scheme="http://liujunming.github.io/tags/debug/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel TME and MKTME technology</title>
    <link href="http://liujunming.github.io/2022/11/19/Notes-about-TME-and-MKTME/"/>
    <id>http://liujunming.github.io/2022/11/19/Notes-about-TME-and-MKTME/</id>
    <published>2022-11-19T07:52:04.000Z</published>
    <updated>2022-11-19T11:08:50.049Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录Intel的TME(Total Memory Encryption)和MKTME(Multi-Key Total Memory Encryption)技术。<a id="more"></a> </p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>Total Memory Encryption (TME) is a  x86 instruction set extension proposed by Intel for a full physical memory encryption for DRAM and NVRAM with a single ephemeral key. TME can be further extended with the Multi-Key Total Memory Encryption (MKTME) extension which builds on TME and adds support multiple encryption keys.</p><h3 id="TME"><a href="#TME" class="headerlink" title="TME"></a>TME</h3><p>普通RAM里面储存的数据，在掉电之后，一般都以为是彻底消失了。但在一些复杂的离线攻击下，这些数据仍然是能被恢复出来并导致泄密。TME可以对抗这种攻击。</p><p>Total Memory Encryption (TME) – as name would imply is a capability to encrypt entirety of physical memory of a system. This capability is typically enabled in very early stages of boot process with small change to BIOS and once configured and locked will encrypt all the data on external memory buses of an SOC using NIST standard AES-XTS algorithm with 128-bit keys. The encryption key used for TME uses hardware random number generator implemented in Intel SOC and the keys are not accessible by software or using external interfaces to Intel SOC. TME capability is intended to provide protections of AES-XTS to external memory buses and DIMMs. The architecture is flexible and will support additional memory protections schemes in future. This capability when enabled is intended to support (unmodified) existing system and application software. Overall performance impact of this capability is likely to be relatively small and is highly dependent on workload.</p><p>Inside the chip itself (e.g., registers and caches) the data remains in plain text. This is done in order to maintain compatibility with all existing software and I/O models. An AES-XTS encryption engine is physically located directly on the data paths to external memory buses ensuring all data entering and leaving the chip is encrypted. Note that there is one exception for a specially defined <a href="/2022/11/19/Notes-about-TME-and-MKTME/#Exclusion-range">exclusion range</a>.</p><p><img src="/images/2022/11/07.png" alt></p><p><img src="/images/2022/11/08.png" alt></p><p>TME的最大缺点是只能使用一把平台密钥来加密内存，不支持在系统里划分出多个基于加密密钥构建的加密内存domain；但MKTME就支持使用多把密钥，进而实现per进程/容器/VM粒度的加密内存domain。</p><h3 id="MKTME"><a href="#MKTME" class="headerlink" title="MKTME"></a>MKTME</h3><p>Multi-Key Total Memory Encryption (MKTME) builds on TME and adds support for multiple encryption keys. The SOC implementation will support a fixed number of encryption keys, and software can configure SOC to use a subset of available keys. Software manages the use of keys and can use each of the available key for encrypting any page of the memory. Thus, MKTME allows page granular encryption of memory. By default MKTME uses TME encryption key unless explicitly specified by software. In addition to supporting CPU generated ephemeral key (not accessible by software or using external interfaces to SOC), MKTME also supports software provided keys. Software provided keys are particularly useful when used with non-volatile memory or when combined with attestation mechanisms and/or used with key provisioning services. In virtualization scenario, we anticipate VMM or hypervisor to manage use of keys to transparently support legacy operating systems without any changes. An OS may be enabled to take additional advantage of MKTME capability both in native or virtualized environment. When properly enabled, MKTME is available to each guest OS in virtualized environment, and guest OS can take advantage of MKTME in same was as native OS.</p><p>MKTME是在TME架构的基础上，实现了<strong>以页为粒度、支持使用多把密钥对内存进行加密的功能，同时还允许由软件设置AES-XTS加解密引擎所使用的密钥</strong>。</p><p>下图是将MKTME用在虚拟化场景中的一个示例图：<br><img src="/images/2022/11/09.png" alt><br>在这个示例中：</p><ul><li>Hypervisor使用KeyID 0 (即TME定义的平台密钥)来访问自己的加密内存</li><li>VM1和VM2都可以使用KeyID 0来访问自己的加密内存</li><li>VM1使用KeyID 1来访问自己的私有加密内存</li><li>VM2使用KeyID 2来访问自己的私有加密内存</li><li>VM1和VM2可以使用KeyID 3来访问两个VM共享的加密内存</li></ul><p>KeyID字段被包含在PTE中，且位于物理地址字段的高位，就像是物理地址字段的一部分（即通过减少一部分物理地址宽度来实现），这个特性叫做<strong>物理地址位超卖</strong>（oversubscribing）。该特性使物理地址具有了别名，即具有相同物理地址的页可以有不同的KeyID。<br><img src="/images/2022/11/10.png" alt></p><p>KeyID信息是不会出现在处理器外部的（比如内存总线上）。物理地址位超卖不会影响cache和TLB的行为，因为KeyID仅被当做成物理地址的一部分来处理；但物理地址位超卖会影响大多数的页表类型：Host普通IA页表、EPT和IOMMU页表。</p><ul><li><p>IA paging<br>MKTME会影响Host侧的IA paging（含每一级页表），即在物理地址字段的高位中包含KeyID字段；CR3寄存器也受此影响，也包含了KeyID。</p></li><li><p>EPT paging<br>MKTME会影响EPT paging（含每一级页表），因为EPT用于将GPA映射到HPA，而HPA必须要包含KeyID。</p></li><li><p>IOMMU paging<br>MKTME会影响IOMMU paging（含每一级页表），因为EPT用于将GPA映射到HPA(虚拟化场景下)，而HPA必须要包含KeyID。</p></li><li><p>其他物理地址<br>其他的物理地址结构（如VMCS指针、物理地址位图等）也都需要包含KeyID。</p></li></ul><p>虽然例子中Hypervisor使用的是KeyID 0，但Hypervisor具有特权，可以使用任意KeyID访问自己的加密内存，也能管理和设置每个VM所能使用的KeyID。</p><p>MKTME支持的密钥数量总是固定的，而具体数量由特定的处理器实现来决定。软件可以通过配置只使用其中的部分密钥，这组密钥被称为可用密钥。软件负责管理可用密钥，并可以使用可用密钥对任意一个内存页进行加密。</p><p>在软件不进行任何显式配置的情况下，MKTME引擎默认使用TME的平台密钥进行内存加密。<strong>MKTME也允许使用软件提供的密钥或处理器RNG生成的密钥</strong>。 在虚拟化场景中，Hypervisor负责管理每个VM所使用的密钥，并透明地对Guest OS实施加密保护（在这个场景中，可以将MKTME视为TME虚拟化技术）。</p><p>总而言之，MKTME希望在系统层面能够创建<strong>多个独立的加密内存domain</strong>。 这对用户来说也更加安全。</p><h3 id="安全威胁模型分析"><a href="#安全威胁模型分析" class="headerlink" title="安全威胁模型分析"></a>安全威胁模型分析</h3><p><strong>TME和MKTME的安全性依赖于特权软件（OS和Hypervisor），这点与传统虚拟化技术的安全边界完全一致。</strong> 假设在攻击者拥有特权的情况下，攻击者能将所有物理页的加密模式都改为非加密模式。事实上只要攻击者拥有特权，就已经能够访问任意内存了，只不过需要使用正确的KeyID来访问per进程/容器/VM实例的加密内存，比如在访问VM实例内的数据前需要在EPT PTE中找出正确的KeyID，然后建立一个使用该KeyID的PTE映射来访问该物理页。</p><p>此外，TME和MKTME没有对数据提供完整性保护，因此软件使用错误的KeyID访问加密内存、直接篡改加密内存中的内容都是可行的。</p><p>由于软件和处理器接口无法访问到TME平台密钥以及MKTME中由处理器硬件自生成的密钥，因此密钥本身是存储安全的；但由软件提供的MKTME密钥可能会因调用者考虑不周而遭到泄露，这个难题需要软件设计者自己来解决。</p><p>由于cache中的数据是明文的，因此TME和MKTME无法抵御像L1TF这种利用处理器speculative execution侧信道漏洞的攻击方式来间接probe cache中的明文数据的这种攻击方式。</p><p>综上所述，<strong>由于特权软件仍有足够的权限来降低TME和MKTME的安全性，因此TME/MKTME技术目前还不属于机密计算的范畴，即无法做到哪怕在被攻破的OS/VMM环境里也能够保护租户机密数据的强度。</strong> TME和MKTME防范的攻击路径是从恶意VM实例到Hypervisor。更具体来说，只要攻击者无法跨域安全域（指从guest ring0到host ring0）且在软件采用了正确配置的情况下，TME和MKTME就能够抵御恶意VM实例对Host或其他VM实例的数据泄露攻击；但前提是租户必须信任CSP和Intel CPU。</p><p>目前Intel TDX(使用到了MKTME技术)做到了在被攻破的OS/VMM环境里也能够保护租户机密数据的强度，前提是租户必须信任Intel CPU。</p><h3 id="Exclusion-range"><a href="#Exclusion-range" class="headerlink" title="Exclusion range"></a>Exclusion range</h3><p>A single exclusion range is supported (for both TME/MKTME for <code>KeyID 0</code> only) for special use cases such as BIOS memory ranges that are not generally available to the operating system. Once the physical addresses are set, no memory encryption is applied to this range.</p><hr><p>参考资料:</p><ol><li><a href="https://developer.aliyun.com/article/767096" target="_blank" rel="noopener">Intel TME和MKTME技术解析</a></li><li><a href="https://en.wikichip.org/wiki/x86/tme" target="_blank" rel="noopener">wikichip:Total Memory Encryption</a></li><li><a href="https://www.linux-kvm.org/images/d/d7/Mktme_kvm_forum_2018.pdf" target="_blank" rel="noopener">Protect Data of Virtual Machines with MKTME on KVM</a></li><li><a href="https://firmwaresecurity.com/2017/12/14/intel-total-memory-encryption/" target="_blank" rel="noopener">Intel Total Memory Encryption (TME) and Multi-Key Total Memory Encryption (MKTME)</a></li><li><a href="https://lwn.net/Articles/787852/" target="_blank" rel="noopener">Intel MKTME enabling</a></li><li><a href="https://zhuanlan.zhihu.com/p/429055957" target="_blank" rel="noopener">对抗内存物理读取攻击的利器：Intel TME和AMD SME</a></li><li><a href="https://www.tomshardware.com/news/intel-mktme-amd-memory-encryption,39467.html" target="_blank" rel="noopener">Intel Follows AMD’s Lead on Full Memory Encryption</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录Intel的TME(Total Memory Encryption)和MKTME(Multi-Key Total Memory Encryption)技术。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about KVM coalesced MMIO/PIO</title>
    <link href="http://liujunming.github.io/2022/11/19/Notes-about-KVM-coalesced-MMIO-PIO/"/>
    <id>http://liujunming.github.io/2022/11/19/Notes-about-KVM-coalesced-MMIO-PIO/</id>
    <published>2022-11-19T03:06:45.000Z</published>
    <updated>2022-11-19T07:13:35.487Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下KVM coalesced MMIO/PIO相关notes。<a id="more"></a> </p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><a href="https://elixir.bootlin.com/linux/v6.0/source/virt/kvm" target="_blank" rel="noopener">https://elixir.bootlin.com/linux/v6.0/source/virt/kvm</a><br><img src="/images/2022/11/06.jpg" alt><br>在KVM源码中经常看到coalesced MMIO，本文将一探究竟。当然本文只侧重于high level层面，不涉及源码中的细节。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote><p>When kernel has to send MMIO writes to userspace, it stores them in memory until it has to pass the hand to userspace for another reason. This avoids to have too many context switches on operations that can wait.</p></blockquote><blockquote><p>Coalesced I/O is used if one or more write accesses to a hardware register can be deferred until a read or a write to another hardware register on the same device.  This last access will cause a vmexit and userspace will process accesses from the ring buffer before emulating it. That will avoid exiting to userspace on repeated writes.</p></blockquote><ul><li><p>Without KVM coalesced MMIO/PIO</p><ul><li>可deferred的MMIO/PIO write导致Non-root mode VM Exit -&gt; KVM -&gt; QEMU(处理可deferred的MMIO/PIO write) -&gt; KVM -&gt; 返回到Non-root mode</li></ul></li><li><p>With KVM coalesced MMIO/PIO</p><ul><li>可deferred的MMIO/PIO write导致Non-root mode VM Exit -&gt; KVM(将可deferred的MMIO/PIO write记录到ring buffer) -&gt;  返回到Non-root mode</li><li>不可deferred的MMIO/PIO导致Non-root mode VM Exit -&gt; KVM -&gt; QEMU(先处理完ring buffer中记录的可deferred的MMIO/PIO write，再处理这次不可deferred的MMIO/PIO) -&gt; KVM -&gt; 返回到Non-root mode</li></ul></li></ul><p>从上述的对比可知，KVM coalesced MMIO/PIO可以在KVM中记录可deferred的MMIO/PIO write，不用退出的qemu来处理，可以将这些请求dealy到下一次不可deferred的MMIO/PIO来处理。That will avoid exiting to userspace on repeated writes.</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">4.116 KVM_(UN)REGISTER_COALESCED_MMIO</span><br><span class="line"></span><br><span class="line">Capability: KVM_CAP_COALESCED_MMIO (for coalesced mmio)</span><br><span class="line">    KVM_CAP_COALESCED_PIO (for coalesced pio)</span><br><span class="line">Architectures: all</span><br><span class="line">Type: vm ioctl</span><br><span class="line">Parameters: struct kvm_coalesced_mmio_zone</span><br><span class="line">Returns: 0 on success, &lt; 0 on error</span><br><span class="line"></span><br><span class="line">Coalesced I/O is a performance optimization that defers hardware</span><br><span class="line">register write emulation so that userspace exits are avoided.  It is</span><br><span class="line">typically used to reduce the overhead of emulating frequently accessed</span><br><span class="line">hardware registers.</span><br><span class="line"></span><br><span class="line">When a hardware register is configured for coalesced I/O, write accesses</span><br><span class="line">do not exit to userspace and their value is recorded in a ring buffer</span><br><span class="line">that is shared between kernel and userspace.</span><br><span class="line"></span><br><span class="line">Coalesced I/O is used if one or more write accesses to a hardware</span><br><span class="line">register can be deferred until a read or a write to another hardware</span><br><span class="line">register on the same device.  This last access will cause a vmexit and</span><br><span class="line">userspace will process accesses from the ring buffer before emulating</span><br><span class="line">it. That will avoid exiting to userspace on repeated writes.</span><br><span class="line"></span><br><span class="line">Coalesced pio is based on coalesced mmio. There is little difference</span><br><span class="line">between coalesced mmio and pio except that coalesced pio records accesses</span><br><span class="line">to I/O ports.</span><br></pre></td></tr></table></figure><h3 id="QEMU-example"><a href="#QEMU-example" class="headerlink" title="QEMU example"></a>QEMU example</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://gitlab.com/qemu-project/qemu/-/blob/stable-6.0/hw/net/e1000.c#L1631-L1647</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span></span><br><span class="line">e1000_mmio_setup(E1000State *d)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">uint32_t</span> excluded_regs[] = &#123;</span><br><span class="line">        E1000_MDIC, E1000_ICR, E1000_ICS, E1000_IMS,</span><br><span class="line">        E1000_IMC, E1000_TCTL, E1000_TDT, PNPMMIO_SIZE</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    memory_region_init_io(&amp;d-&gt;mmio, OBJECT(d), &amp;e1000_mmio_ops, d,</span><br><span class="line">                          <span class="string">"e1000-mmio"</span>, PNPMMIO_SIZE);</span><br><span class="line">    memory_region_add_coalescing(&amp;d-&gt;mmio, <span class="number">0</span>, excluded_regs[<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; excluded_regs[i] != PNPMMIO_SIZE; i++)</span><br><span class="line">        memory_region_add_coalescing(&amp;d-&gt;mmio, excluded_regs[i] + <span class="number">4</span>,</span><br><span class="line">                                     excluded_regs[i+<span class="number">1</span>] - excluded_regs[i] - <span class="number">4</span>);</span><br><span class="line">    memory_region_init_io(&amp;d-&gt;io, OBJECT(d), &amp;e1000_io_ops, d, <span class="string">"e1000-io"</span>, IOPORT_SIZE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>本case的<code>memory_region_add_coalescing</code>注册了coalesced MMIO region。对于细节，需要结合e1000的spec与qemu、kvm相关代码了，在此不再描述。</p><hr><p>参考资料:</p><ol><li><a href="https://lore.kernel.org/kvm/1212156357946-git-send-email-Laurent.Vivier@bull.net/" target="_blank" rel="noopener">kvm: Batch writes to MMIO</a></li><li><a href="https://www.kernel.org/doc/Documentation/virtual/kvm/api.txt" target="_blank" rel="noopener">Documentation/virtual/kvm/api.txt</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下KVM coalesced MMIO/PIO相关notes。
    
    </summary>
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/categories/QEMU-KVM/"/>
    
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/tags/QEMU-KVM/"/>
    
  </entry>
  
  <entry>
    <title>Comparing VIRTIO, NVMe, and io_uring queue designs</title>
    <link href="http://liujunming.github.io/2022/11/13/Comparing-VIRTIO-NVMe-and-io-uring-queue-designs/"/>
    <id>http://liujunming.github.io/2022/11/13/Comparing-VIRTIO-NVMe-and-io-uring-queue-designs/</id>
    <published>2022-11-13T13:11:13.000Z</published>
    <updated>2022-11-13T13:19:55.638Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://blog.vmsplice.net/2022/06/comparing-virtio-nvme-and-iouring-queue.html" target="_blank" rel="noopener">http://blog.vmsplice.net/2022/06/comparing-virtio-nvme-and-iouring-queue.html</a><br>深度好文，强烈推荐！<a id="more"></a> </p><h3 id="Ring-buffer-basics"><a href="#Ring-buffer-basics" class="headerlink" title="Ring buffer basics"></a>Ring buffer basics</h3><p>A ring buffer is a circular array where new elements are produced on one side and consumed on the other side. Often terms such as head and tail are used to <strong>describe the array indices at which the next element is accessed</strong>. When the end of the array is reached, one moves back to the start of the array. The empty and full conditions are special states that must be checked to avoid underflow and overflow.</p><p>VIRTIO, NVMe, and io_uring all use single producer, single consumer shared memory ring buffers. This allows a CPU and an I/O device or two CPUs to communicate across a region of memory to which both sides have access.</p><p><a href="https://mp.weixin.qq.com/s/pjuHWagzhONS3eOmF6xkXQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/pjuHWagzhONS3eOmF6xkXQ</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://blog.vmsplice.net/2022/06/comparing-virtio-nvme-and-iouring-queue.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://blog.vmsplice.net/2022/06/comparing-virtio-nvme-and-iouring-queue.html&lt;/a&gt;&lt;br&gt;深度好文，强烈推荐！
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Notes about KVM dedicated vCPUs hint</title>
    <link href="http://liujunming.github.io/2022/11/13/Notes-about-KVM-dedicated-vCPUs-hint/"/>
    <id>http://liujunming.github.io/2022/11/13/Notes-about-KVM-dedicated-vCPUs-hint/</id>
    <published>2022-11-13T02:44:24.000Z</published>
    <updated>2022-11-13T03:08:40.720Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下KVM中的dedicated vCPUs hint KVM_HINTS_DEDICATED。<a id="more"></a> </p><p>This feature introduces dedicated vCPUs(vCPU pinning, and there is no vCPU over-commitment) hint KVM_HINTS_DEDICATED, it has two users now:</p><ol><li><p>Waiman Long mentioned that:<br> Generally speaking, unfair lock performs well for VMs with a small number of vCPUs. Native qspinlock may perform better than pvqspinlock if there is vCPU pinning and there is no vCPU over-commitment.</p></li><li><p>vCPUs are very unlikely to get preempted when they are the only task running on a CPU. PV TLB flush is slower than the native flush in that case.</p></li></ol><p><img src="/images/2022/11/05.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://static.sched.com/hosted_files/kvmforum2019/e3/Boosting%20Dedicated%20Instances%20by%20KVM%20Tax%20Cut.pdf" target="_blank" rel="noopener">Boosting Dedicated InstanceviaKVMTaxCut</a></li><li><a href="https://lore.kernel.org/kvm/1518483942-14741-1-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">KVM: Introduce dedicated vCPUs hint KVM_HINTS_DEDICATED</a></li><li><a href="https://lore.kernel.org/qemu-devel/1518083060-5881-1-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">target-i386: adds PV_DEDICATED hint CPUID feature bit</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下KVM中的dedicated vCPUs hint KVM_HINTS_DEDICATED。
    
    </summary>
    
      <category term="KVM" scheme="http://liujunming.github.io/categories/KVM/"/>
    
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about io_uring</title>
    <link href="http://liujunming.github.io/2022/11/12/Notes-about-io-uring/"/>
    <id>http://liujunming.github.io/2022/11/12/Notes-about-io-uring/</id>
    <published>2022-11-12T05:13:37.000Z</published>
    <updated>2022-11-13T01:24:51.868Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录io_uring相关笔记。<a id="more"></a> </p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>The native Linux AIO framework suffers from various limitations, which io_uring aims to overcome:</p><ul><li>It does not support buffered I/O, only direct I/O is supported.</li><li>It has non-deterministic behavior which may block under various circumstances.</li><li>It has a sub-optimal API, which requires at least two system calls per I/O, one to submit a request, and one to wait for its completion.<ul><li>Each submission needs to copy 64 + 8 bytes of data, and each completion needs to copy 32 bytes.</li></ul></li></ul><h3 id="Communication-channel"><a href="#Communication-channel" class="headerlink" title="Communication channel"></a>Communication channel</h3><p><img src="/images/2022/11/01.jpg" alt></p><p>An io_uring instance has two rings, a submission queue (SQ) and a completion queue (CQ), shared between the kernel and the application. The queues are single producer, single consumer, and power of two in size.</p><p>The queues provide a lock-less access interface, coordinated with memory barriers.</p><p>The application creates one or more SQ entries (SQE), and then updates the SQ tail. The kernel consumes the SQEs , and updates the SQ head.</p><p>The kernel creates CQ entries (CQE) for one or more completed requests, and updates the CQ tail. The application consumes the CQEs and updates the CQ head.</p><p>Completion events may arrive in any order but they are always associated with specific SQEs.</p><h3 id="System-call"><a href="#System-call" class="headerlink" title="System call"></a>System call</h3><p><img src="/images/2022/11/02.jpg" alt></p><h3 id="默认流程"><a href="#默认流程" class="headerlink" title="默认流程"></a>默认流程</h3><p>默认情形下，提交任务的流程，以及获取结果的方式:</p><ol><li>把sqe放入sqring</li><li>调用<code>io_uring_enter</code>通知内核</li><li><strong>可以轮询cqring等待结果</strong>或者通过带<code>IORING_ENTER_GETEVENTS</code>和<code>min_complete</code>参数的<code>io_uring_enter</code>阻塞等待指定数目的任务完成，再去cqring中检查结果</li></ol><h3 id="Submission-Queue-Polling"><a href="#Submission-Queue-Polling" class="headerlink" title="Submission Queue Polling"></a>Submission Queue Polling</h3><p>如果在调用<code>io_uring_setup</code> 时设置了 <code>IORING_SETUP_SQPOLL</code> 的 flag，内核会额外启动一个内核线程，我们称作 SQ 线程。这个内核线程可以运行在某个指定的 core 上（通过 <code>sq_thread_cpu</code> 配置）。这个内核线程会不停的 Poll SQ，除非在一段时间内没有 Poll 到任何请求（通过 <code>sq_thread_idle</code> 配置），才会被挂起。</p><p><img src="/images/2022/11/04.jpeg" alt></p><p>当程序在用户态设置完 SQE，并通过修改 SQ 的 tail 完成一次插入时，如果此时 SQ 线程处于唤醒状态，那么可以立刻捕获到这次提交，这样就避免了用户程序调用<code>io_uring_enter</code>这个系统调用。如果 SQ 线程处于休眠状态，则需要通过调用<code>io_uring_enter</code>，并使用<code>IORING_SQ_NEED_WAKEUP</code> 参数，来唤醒 SQ 线程。用户态可以通过 sqring 的 flags 变量获取 SQ 线程的状态。</p><h3 id="io-polling"><a href="#io-polling" class="headerlink" title="io polling"></a>io polling</h3><p>在默认情况下，当设备处理完IO请求后，设备会发送中断通知内核往cqring添加cqe，并更新cqring的tail指针。用户态程序会轮询cqring获取新的cqe。</p><p>但是对于IO low latency或者high IOPS的场景，使用中断并不合适，应该使用polling(refers to performing IO without relying on hardware interrupts to signal a completion event)。此时因为没有中断通知，内核就不会往 cqring中填充cqe，因此用户态程序就不能去轮询cqring了。此时，用户态程序必须调用<code>io_uring_enter</code> with <code>IORING_ENTER_GETEVENTS</code> set and <code>min_complete</code> set to 0来下发轮询任务给内核，内核会轮询检查是否有结果产生，如果有，则将结果放入cqring。 </p><p>Tips:搞清楚内核什么时候更新cqring，分为如下两种case:</p><ol><li><p>硬件中断通知内核</p></li><li><p>用户态程序调用<code>io_uring_enter</code>来下发polling任务给内核</p></li></ol><h3 id="liburing"><a href="#liburing" class="headerlink" title="liburing"></a>liburing</h3><p>为了简化使用io_uring， liburing 库应用而生。用户无需了解诸多 io_uring 细节便可以使用起来，如无需关心 memory barrier，以及 ring buffer 的管理等。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/images/2022/11/03.jpg" alt></p><p>io_uring 主要通过用户态与内核态共享内存的途径，来摒弃使用系统调用来提交 I/O 操作和获取 I/O 操作的结果，从而避免了上下文切换的情况。另外，由于用户态进程与内核态线程通过共享内存的方式通信，从而避免了内存拷贝的过程，提升了 I/O 操作的性能。</p><p>所以，io_uring 主要通过两个优化点来提升 I/O 操作的性能：</p><ul><li>摒弃使用系统调用来提交 I/O 操作和获取 I/O 操作结果</li><li>减少用户态与内核态之间的内存拷贝</li></ul><hr><p>参考资料:</p><ol><li><a href="https://blogs.oracle.com/linux/post/an-introduction-to-the-io-uring-asynchronous-io-framework" target="_blank" rel="noopener">An Introduction to the io_uring Asynchronous I/O Framework</a></li><li><a href="https://unixism.net/loti/what_is_io_uring.html" target="_blank" rel="noopener">What is io_uring?</a></li><li><a href="https://kernel.dk/io_uring.pdf" target="_blank" rel="noopener">Efficient IO with io_uring</a></li><li><a href="https://zhuanlan.zhihu.com/p/62682475" target="_blank" rel="noopener">AIO 的新归宿：io_uring</a></li><li><a href="https://kernel.dk/axboe-kr2022.pdf" target="_blank" rel="noopener">What’s new with io_uring</a></li><li><a href="https://mp.weixin.qq.com/s/4hXwPhCOJFjUjMJqzjKWgg" target="_blank" rel="noopener">io_uring 新异步 IO 机制，性能提升超 150%，堪比 SPDK</a></li><li><a href="https://zhuanlan.zhihu.com/p/361955546" target="_blank" rel="noopener">浅析开源项目之io_uring</a></li><li><a href="https://mp.weixin.qq.com/s/fzvkGpvxFXYEWMCWcfTb-w" target="_blank" rel="noopener">下一代异步 IO io_uring 技术解密</a></li><li><a href="Submission Queue Polling[¶](https://unixism.net/loti/tutorial/sq_poll.html#submission-queue-polling">Submission Queue Polling</a>)</li><li><a href="https://www.jianshu.com/p/32a3c72da1c1" target="_blank" rel="noopener">io_uring</a></li><li><a href="https://mp.weixin.qq.com/s/1wZpFhwJR-LNkQm-QzFxRQ" target="_blank" rel="noopener">Linux I/O 神器之 io_uring</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录io_uring相关笔记。
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes about virtual IPI fastpath and virtual TSC-Deadline timer fastpath</title>
    <link href="http://liujunming.github.io/2022/10/30/Notes-about-virtual-IPI-fastpath/"/>
    <id>http://liujunming.github.io/2022/10/30/Notes-about-virtual-IPI-fastpath/</id>
    <published>2022-10-29T17:20:02.000Z</published>
    <updated>2022-10-30T01:39:38.435Z</updated>
    
    <content type="html"><![CDATA[<p>本文参考的内核版本为<a href="https://elixir.bootlin.com/linux/v6.0/source" target="_blank" rel="noopener">v6.0</a>。<a id="more"></a></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><img src="/images/2022/10/18.jpg" alt></p><h3 id="virtual-IPI-fastpath"><a href="#virtual-IPI-fastpath" class="headerlink" title="virtual IPI fastpath"></a>virtual IPI fastpath</h3><p><img src="/images/2022/10/19.jpg" alt></p><p><img src="/images/2022/10/20.jpg" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vmx_vcpu_run</span><br><span class="line">└── vmx_exit_handlers_fastpath</span><br><span class="line">    └── handle_fastpath_set_msr_irqoff</span><br><span class="line">        └── handle_fastpath_set_x2apic_icr_irqoff</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * The fast path for frequent and performance sensitive wrmsr emulation,</span></span><br><span class="line"><span class="comment"> * i.e. the sending of IPI, sending IPI early in the VM-Exit flow reduces</span></span><br><span class="line"><span class="comment"> * the latency of virtual IPI by avoiding the expensive bits of transitioning</span></span><br><span class="line"><span class="comment"> * from guest to host, e.g. reacquiring KVM's SRCU lock. In contrast to the</span></span><br><span class="line"><span class="comment"> * other cases which must be called after interrupts are enabled on the host.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">handle_fastpath_set_x2apic_icr_irqoff</span><span class="params">(struct kvm_vcpu *vcpu, u64 data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu-&gt;arch.apic))</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (((data &amp; APIC_SHORT_MASK) == APIC_DEST_NOSHORT) &amp;&amp;</span><br><span class="line">    ((data &amp; APIC_DEST_MASK) == APIC_DEST_PHYSICAL) &amp;&amp;</span><br><span class="line">    ((data &amp; APIC_MODE_MASK) == APIC_DM_FIXED) &amp;&amp;</span><br><span class="line">    ((u32)(data &gt;&gt; <span class="number">32</span>) != X2APIC_BROADCAST))</span><br><span class="line"><span class="keyword">return</span> kvm_x2apic_icr_write(vcpu-&gt;arch.apic, data);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="virtual-TSC-Deadline-timer-fastpath"><a href="#virtual-TSC-Deadline-timer-fastpath" class="headerlink" title="virtual TSC-Deadline timer fastpath"></a>virtual TSC-Deadline timer fastpath</h3><p><img src="/images/2022/10/21.jpg" alt></p><p><img src="/images/2022/10/22.jpg" alt></p><p><a href="https://lore.kernel.org/kvm/1587709364-19090-5-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">KVM: X86: TSCDEADLINE MSR emulation fastpath</a><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vmx_vcpu_run</span><br><span class="line">└── vmx_exit_handlers_fastpath</span><br><span class="line">    └── handle_fastpath_set_msr_irqoff</span><br><span class="line">        └── handle_fastpath_set_tscdeadline</span><br><span class="line">            └── kvm_set_lapic_tscdeadline_msr</span><br></pre></td></tr></table></figure></p><p><img src="/images/2022/10/23.jpg" alt></p><p><a href="https://lore.kernel.org/kvm/1587709364-19090-6-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">KVM: VMX: Handle preemption timer fastpath</a><br>该patch优化的是<a href="/2022/10/22/lapic-timer-virtualization/">使用preemption timer模拟lapic timer的case</a>。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmx_vcpu_run</span><br><span class="line">└── vmx_exit_handlers_fastpath</span><br><span class="line">    └── handle_fastpath_preemption_timer</span><br></pre></td></tr></table></figure></p><p><img src="/images/2022/10/24.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://static.sched.com/hosted_files/kvmforum2020/6e/KVM%20Latency%20and%20Scalability%20Performance%20Tuning.pdf" target="_blank" rel="noopener">KVM Latency and Scalability Performance Tuning</a></li><li><a href="https://lore.kernel.org/kvm/1574306232-872-1-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">KVM: VMX: FIXED+PHYSICAL mode single target IPI fastpath</a></li><li><a href="https://lore.kernel.org/kvm/1587709364-19090-1-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">KVM: VMX: Tscdeadline timer emulation fastpath</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文参考的内核版本为&lt;a href=&quot;https://elixir.bootlin.com/linux/v6.0/source&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;v6.0&lt;/a&gt;。
    
    </summary>
    
      <category term="KVM" scheme="http://liujunming.github.io/categories/KVM/"/>
    
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>KVM halt-polling机制分析</title>
    <link href="http://liujunming.github.io/2022/10/29/Notes-about-kvm-halt-polling/"/>
    <id>http://liujunming.github.io/2022/10/29/Notes-about-kvm-halt-polling/</id>
    <published>2022-10-29T07:35:53.000Z</published>
    <updated>2022-10-29T08:51:57.874Z</updated>
    
    <content type="html"><![CDATA[<p>   本文转载自:<a href="https://www.cnblogs.com/163yun/p/10114699.html" target="_blank" rel="noopener">KVM halt-polling机制分析</a>。<a id="more"></a> </p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在实际业务中，guest执行HLT指令是导致虚拟化overhead的一个重要原因。如[1]。</p><p>KVM halt polling特性就是为了解决这一个问题被引入的，它在Linux 4.3-rc1被合入主干内核，其基本原理是当guest idle发生vm-exit时，host 继续polling一段时间，用于减少guest的业务时延。进一步讲，在vcpu进入idle之后，guest内核默认处理是执行HLT指令，就会发生vm-exit，host kernel并不马上让出物理核给调度器，而是poll一段时间，若guest在这段时间内被唤醒，便可以马上调度回该vcpu线程继续运行。</p><p>polling机制带来时延上的降低，至少是一个线程调度周期，通常是几微妙，但最终的性能提升是跟guest内业务模型相关的。如果在host kernel polling期间，没有唤醒事件发生或是运行队列里面其他任务变成runnable状态，那么调度器就会被唤醒去干其他任务的事。因此，halt polling机制对于那些在很短时间间隔就会被唤醒一次的业务特别有效。</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li>该机制有可能导致物理CPU实际空闲的情况下占用率表现为100%。因为如果guest上业务模型是隔一段时间被唤醒一次来处理很少量的流量，并且这个时间间隔比kvm halt_poll_ns短，那么host将poll整个虚拟机的block时间，cpu占用率也会冲上100%。</li><li>halt polling是电源能耗和业务时延的一个权衡。为了减少进入guest的时延，idle cpu时间转换为host kernel时间。</li><li>该机制只有在CPU上没有其他running任务的情况得以应用，不然polling动作被立马终止，唤醒调度器，调度其他进程。</li></ol><h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><p>业界针对虚拟机idle这个课题有比较多的研究，因为它带来了比较大的overhead。主要可以归结为以下几种：</p><ol><li><p>idle=poll，即把虚拟机idle时一直polling，空转，不退出。这样不利于物理CPU超线程的发挥。</p></li><li><p>阿里提出guest里面提供halt polling机制，即在VM退出前先等会儿，这样可以减少VM退出次数。 优点：性能较kvm halt polling机制好；缺点：需要修改guest内核；状态：社区未接收 <a href="https://lore.kernel.org/kvm/1510567565-5118-1-git-send-email-quan.xu0@gmail.com/" target="_blank" rel="noopener">x86/idle: add halt poll support</a> 值得注意的是， 类似idea的工作<a href="/2022/10/28/Notes-about-Guest-halt-polling/">guest halt polling</a>社区已接受</p></li><li><p>腾讯考虑guest HLT指令不退出。优点：性能较阿里好；缺点：只适用于vcpu独占物理核场景；状态：社区已接受。<a href="https://lore.kernel.org/kvm/1517813878-22248-1-git-send-email-wanpengli@tencent.com/" target="_blank" rel="noopener">KVM: X86: Add per-VM no-HLT-exiting capability</a></p></li></ol><hr><p>参考资料:</p><ol><li><a href="https://www.linux-kvm.org/images/2/27/Kvm-forum-2013-idle-latency.pdf" target="_blank" rel="noopener">KVM vs. Message Passing Throughput</a></li><li><a href="http://events17.linuxfoundation.org/sites/events/files/slides/Message%20Passing%20Workloads%20in%20KVM%20%28SLIDES%29.pdf" target="_blank" rel="noopener">Message Passing Workloads in KVM</a></li><li><a href="http://events17.linuxfoundation.org/sites/events/files/slides/KVM%20performance%20tuning%20on%20Alibaba%20Cloud.pdf" target="_blank" rel="noopener">KVM performance tuning</a></li><li><a href="https://www.kernel.org/doc/Documentation/virtual/kvm/halt-polling.txt" target="_blank" rel="noopener">The KVM halt polling system</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;   本文转载自:&lt;a href=&quot;https://www.cnblogs.com/163yun/p/10114699.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;KVM halt-polling机制分析&lt;/a&gt;。
    
    </summary>
    
      <category term="KVM" scheme="http://liujunming.github.io/categories/KVM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about guest halt polling</title>
    <link href="http://liujunming.github.io/2022/10/28/Notes-about-Guest-halt-polling/"/>
    <id>http://liujunming.github.io/2022/10/28/Notes-about-Guest-halt-polling/</id>
    <published>2022-10-28T13:20:37.000Z</published>
    <updated>2022-10-29T08:49:07.967Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about guest halt polling feature.<a id="more"></a> </p><p>前提:    需要对intel的<a href="/2020/05/01/Introduction-to-halt-pause-monitor-mwait-instruction/#hlt">hlt</a>指令有一定的了解。 </p><p>The cpuidle_haltpoll driver, with the haltpoll governor, allows the guest vcpus to poll for a specified amount of time before halting.</p><p>This provides the following benefits to host side polling:</p><ol><li>The POLL flag is set while polling is performed, which allows a remote vCPU to avoid sending an IPI (and the associated cost of handling the IPI) when performing a wakeup.</li><li>The VM-exit cost can be avoided.</li></ol><p>The downside of guest side polling is that polling is performed even with other runnable tasks in the host.</p><p>The basic logic as follows: A global value, <code>guest_halt_poll_ns</code>, is configured by the user, indicating the maximum amount of time polling is allowed. This value is fixed.</p><p>Each vcpu has an adjustable <code>guest_halt_poll_ns</code> (“per-cpu <code>guest_halt_poll_ns</code>”), which is adjusted by the algorithm in response to events.</p><p>The module parameters can be set from the debugfs files in:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/sys/module/haltpoll/parameters/</span><br></pre></td></tr></table></figure></p><hr><p>参考资料:</p><ol><li><a href="https://www.kernel.org/doc/html/latest/virt/guest-halt-polling.html" target="_blank" rel="noopener">Guest halt polling</a></li><li><a href="https://lore.kernel.org/kvm/20190613224532.949768676@redhat.com/" target="_blank" rel="noopener">cpuidle haltpoll driver and governor</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about guest halt polling feature.
    
    </summary>
    
      <category term="KVM" scheme="http://liujunming.github.io/categories/KVM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Intel Data Streaming Accelerator(DSA)</title>
    <link href="http://liujunming.github.io/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/"/>
    <id>http://liujunming.github.io/2022/10/23/Notes-about-Intel-Data-Streaming-Accelerator-DSA/</id>
    <published>2022-10-23T12:34:39.000Z</published>
    <updated>2022-11-19T11:08:37.984Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel Data Streaming Accelerator(DSA)相关notes。想要了解细节的话，还是需要去查询spec。<a id="more"></a> </p><p>Intel DSA is a high-performance data copy and transformation accelerator that will be integrated in future Intel® processors, targeted for optimizing streaming data movement and transformation operations common with applications for high-performance storage, networking, persistent memory, and various data processing applications.</p><p>The goal is to provide higher overall system performance for data mover and transformation operations, while freeing up CPU cycles for higher level functions. Intel DSA enables high performance data mover capability to/from volatile memory, persistent memory, memory-mapped I/O, and through a Non-Transparent Bridge (NTB) device to/from remote volatile and persistent memory on another node in a cluster. Enumeration and configuration is done with a PCI Express compatible programming interface to the Operating System (OS) and can be controlled through a device driver.</p><p>Besides the basic data mover operations, Intel DSA supports a set of transformation operations on memory. For example:</p><ul><li>Generate and test CRC checksum, or Data Integrity Field (DIF) to support storage and networking applications.</li><li>Memory Compare and delta generate/merge to support VM migration, VM Fast check-pointing and software managed memory deduplication usages.</li></ul><p>比如用DSA做memcpy的话，与CPU相比，优势在哪里呢？</p><ol><li>可以释放CPU资源，将memcpy的功能offload到DSA上</li><li>DSA支持并行批量化处理，当memcpy大量数据的话，DSA可以并行处理，因此可以提高效。如果是少量的memcpy的话，用DSA的效率就不如CPU的了。</li></ol><p><img src="/images/2022/10/17.jpg" alt></p><p>Figure 3-1 illustrates the high-level blocks within the device at a conceptual level. The I/O fabric interface is used for receiving downstream work requests from clients and for upstream read, write, and address translation operations.</p><p>Each device contains the following basic components:</p><ul><li>Work Queues (WQ) - On device storage to queue descriptors to the device. Requests are added to a WQ by using new instructions to write to the memory mapped “portal” associated with each WQ.</li><li>Groups - Abstract container that can include one or more engines and work queues.</li><li>Engines - Pulls work submitted to the WQs and process them.</li></ul><p>Two types of WQs are supported:</p><ul><li>Dedicated WQ (DWQ) - A single client owns this exclusively and can submit work to it.</li><li>Shared WQ (SWQ) - Multiple clients can submit work to the SWQ.</li></ul><p>A client using DWQ submits work descriptors using the <em>MOVDIR64B</em> instruction. This is a posted write, so the client must track the number of descriptors submitted to ensure that it does not exceed the configured work queue length as any additional descriptors would be dropped.</p><p>Clients using shared work queues submit work descriptors using either <em>ENQCMDS</em> (from supervisor mode) or <em>ENQCMD</em> (from user mode). These instructions indicate via the <strong>EFLAGS.ZF</strong> bit whether the request was accepted.</p><hr><p>参考资料:</p><ol><li><a href="https://01.org/blogs/2019/introducing-intel-data-streaming-accelerator" target="_blank" rel="noopener">INTRODUCING THE INTEL® DATA STREAMING ACCELERATOR </a></li><li><a href="https://software.intel.com/en-us/download/intel-data-streaming-accelerator-preliminary-architecture-specification" target="_blank" rel="noopener">DSA spec</a></li><li><a href="https://www.cnblogs.com/shaohef/p/12820952.html" target="_blank" rel="noopener">intel DSA spec 解读</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel Data Streaming Accelerator(DSA)相关notes。想要了解细节的话，还是需要去查询spec。
    
    </summary>
    
      <category term="Intel" scheme="http://liujunming.github.io/categories/Intel/"/>
    
    
      <category term="Intel" scheme="http://liujunming.github.io/tags/Intel/"/>
    
  </entry>
  
  <entry>
    <title>Notes about vt-x &quot;acknowledge interrupt on exit&quot; feature</title>
    <link href="http://liujunming.github.io/2022/10/23/Notes-about-vtx-acknowledge-interrupt-on-exit-feature/"/>
    <id>http://liujunming.github.io/2022/10/23/Notes-about-vtx-acknowledge-interrupt-on-exit-feature/</id>
    <published>2022-10-23T01:28:25.000Z</published>
    <updated>2022-10-23T12:59:47.683Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍VT-x中的”acknowledge interrupt on exit” feature。 <a id="more"></a> </p><h3 id="definition"><a href="#definition" class="headerlink" title="definition"></a>definition</h3><p>sdm中的相关描述:</p><p><img src="/images/2022/10/14.jpg" alt></p><p><img src="/images/2022/10/15.jpg" alt></p><h3 id="description"><a href="#description" class="headerlink" title="description"></a>description</h3><blockquote><p>The “acknowledge interrupt on exit” VM-exit control in the controlling VMCS controls processor behavior for external interrupt acknowledgement. If the control is 1, the processor acknowledges the interrupt controller to acquire the interrupt vector upon VM exit, and stores the vector in the VM-exit interruption-information field. If the control is 0, the external interrupt is not acknowledged during VM exit. Since RFLAGS.IF is automatically cleared on VM exits due to external interrupts, VMM re-enabling of interrupts(setting RFLAGS.IF = 1) initiates the external interrupt acknowledgement and vectoring of the external interrupt through the monitor/host IDT.</p></blockquote><p><img src="/images/2022/10/16.jpg" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>当“acknowledge interrupt on exit” VM-exit control位为0：</li></ul><p>当vCPU在non-root mode时，external interrupt会导致VM Exit，此时VM Exit interruption information field is marked as invalid。在root mode下，lapic的IRR对应的bit位会被置上。在root mode下，硬件会完成interrupt evaluation和interrupt recognition，当hypervisor设置RFLAGS.IF后，就会发生interrupt delivery，处理器就调用IDT对应的中断处理函数。</p><ul><li>当“acknowledge interrupt on exit” VM-exit control位为1：</li></ul><p>当vCPU在non-root mode时，external interrupt会导致VM Exit，此时VM Exit interruption information field is marked as valid，并且会记录external interrupt的vector号，此时lapic的IRR对应的bit位并没有被置上，但lapic ISR对应的bit位会被置上(logical processor acknowledges the interrupt controller)。在root mode下，由于lapic的IRR对应的bit位并没有被置上，此时就不会走interrupt evaluation和interrupt recognition这条路径了，也就不会发生interrupt delivery了，处理器不会通过IDT调用中断处理函数。此时需要hypervisor手动调用IDT的中断处理函数。</p><h3 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h3><p>本文参考的内核版本为<a href="https://elixir.bootlin.com/linux/v5.0/source/" target="_blank" rel="noopener">v5.0</a>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="title">int</span> <span class="params">(*kvm_vmx_exit_handlers[])</span><span class="params">(struct kvm_vcpu *vcpu)</span> </span>= &#123;</span><br><span class="line">...</span><br><span class="line">[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> __<span class="function">always_inline <span class="keyword">int</span> <span class="title">handle_external_interrupt</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">++vcpu-&gt;stat.irq_exits;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由代码可知，external interrupt的handler只是增加统计信息而已，并没有处理外部中断。</p><p>最终是vmx_handle_external_intr进行了外部中断的真正处理。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vcpu_run</span><br><span class="line">└── vcpu_enter_guest</span><br><span class="line">    └── vmx_handle_external_intr[kvm_x86_ops-&gt;handle_external_intr]</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">vmx_handle_external_intr</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 如果中断的类型是外部中断 */</span></span><br><span class="line"><span class="keyword">if</span> ((exit_intr_info &amp; (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK))</span><br><span class="line">== (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) &#123;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> <span class="built_in">vector</span>;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> entry;</span><br><span class="line">gate_desc *desc;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vcpu_vmx</span> *<span class="title">vmx</span> = <span class="title">to_vmx</span>(<span class="title">vcpu</span>);</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_X86_64</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> tmp;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 取得外部中断的vector值，</span></span><br><span class="line"><span class="comment"> * (这是因为处理器的"acknowledge interrupt on exit"特性会自动ACK，就自动拿到了vector，</span></span><br><span class="line"><span class="comment"> *  但是这个特性使能之后，处理器不会再通过IDT调用中断处理函数，而是使用vmx handler </span></span><br><span class="line"><span class="comment"> *  vmx handler在这里构造中断栈帧，然后根据vector的值到IDT中找到真正的处理函数完成中断的处理</span></span><br><span class="line"><span class="comment"> *  注意，这里会将中断栈帧中的IF置位，这样中断处理完成的时候，就会自动的开启中断了)   </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span> =  exit_intr_info &amp; INTR_INFO_VECTOR_MASK;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 找到中断描述符，并得到门入口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">desc = (gate_desc *)vmx-&gt;host_idt_base + <span class="built_in">vector</span>;</span><br><span class="line">entry = gate_offset(desc);</span><br><span class="line"><span class="function"><span class="keyword">asm</span> <span class="title">volatile</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">#ifdef CONFIG_X86_64</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"mov %%"</span> _ASM_SP <span class="string">", %[sp]\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"and $0xfffffffffffffff0, %%"</span> _ASM_SP <span class="string">"\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"push $%c[ss]\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"push %[sp]\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params">#endif</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"pushf\n\t"</span></span></span></span><br><span class="line">__ASM_SIZE(push) " $%c[cs]\n\t"</span><br><span class="line">CALL_NOSPEC <span class="comment">/*调用真正的中断处理函数*/</span></span><br><span class="line">:</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_X86_64</span></span><br><span class="line">[sp]<span class="string">"=&amp;r"</span>(tmp),</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">ASM_CALL_CONSTRAINT</span><br><span class="line">:</span><br><span class="line">THUNK_TARGET(entry),</span><br><span class="line">[ss]<span class="string">"i"</span>(__KERNEL_DS),</span><br><span class="line">[cs]<span class="string">"i"</span>(__KERNEL_CS)</span><br><span class="line">);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://lore.kernel.org/kvm/1359549372-4764-1-git-send-email-yang.z.zhang@intel.com/" target="_blank" rel="noopener">KVM: VMX: enable acknowledge interupt on vmexit</a></li><li><a href="https://blog.csdn.net/leoufung/article/details/52502192" target="_blank" rel="noopener">关于KVM中处理外部中断的处理代码</a></li><li><a href="https://blog.csdn.net/jemmy858585/article/details/5854437" target="_blank" rel="noopener">kvm对外部中断的处理</a></li><li><a href="/pdf/Interrupt_and_interrupt_virtualization.pdf">Interrupt and Interrupt Virtualization</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍VT-x中的”acknowledge interrupt on exit” feature。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
      <category term="虚拟化 " scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>lapic timer virtualization</title>
    <link href="http://liujunming.github.io/2022/10/22/lapic-timer-virtualization/"/>
    <id>http://liujunming.github.io/2022/10/22/lapic-timer-virtualization/</id>
    <published>2022-10-22T08:55:24.000Z</published>
    <updated>2022-10-22T11:03:18.142Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍kvm中lapic timer的虚拟化。为了方便，只介绍<a href="/2022/09/05/The-relationship-between-LAPIC-timer-and-TSC/">TSC-deadline模式的lapic timer</a>。<a id="more"></a><br>本文参考的内核版本为<a href="https://elixir.bootlin.com/linux/v5.18/source" target="_blank" rel="noopener">v5.18</a>。</p><blockquote><p>host有自己的lapic timer，硬件实现，guest也有自己的lapic timer，kvm模拟。一个pcup上要运行很多个vcpu，每个vcpu都有自己的lapic timer，kvm要模拟很多个lapic timer，kvm用软件定时器hrtimer来模拟lapic timer，guest写tscdeadline msr，kvm把这个tsc值转换成一个软件定时器的值，启动软件定时器，硬件定时器<strong>驱动</strong>软件定时器，软件定时器超时后，假如硬件timer中断正好把vcpu exiting出来，那么设置timer interrupt pending，重新enter时把timer中断注入，如果vcpu运行在其它pcpu上，需要把vcpu kick出来，所以最好保持timer绑定的物理cpu和vcpu所运行的物理cpu始终一致，如果vcpu运行的物理cpu变化了，migrate timer到新的物理cpu，这样中断来了vcpu自动exit，不再需要kick一次。</p></blockquote><p>hrtimer由physical lapic timer来驱动。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kvm_set_lapic_tscdeadline_msr</span><br><span class="line">└── start_apic_timer</span><br><span class="line">    └── __start_apic_timer</span><br><span class="line">        └── restart_apic_timer</span><br><span class="line">            └── <span class="keyword">if</span> (!start_hv_timer(apic)) start_sw_timer(apic);</span><br></pre></td></tr></table></figure><p>这里的hv_timer就是<a href="/2022/04/01/Introduction-to-VT-x-Preemption-Timer/">preemption timer</a>，sw_timer是软件hrtimer。有preemption timer就用hv_timer，没有就用sw_timer。hv_timer的问题就是可能时间没到，vcpu由于其它原因exit出来，那么就需要kvm_lapic_switch_to_sw_timer，再次enter时kvm_lapic_switch_to_hv_timer。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_hv_timer</span><br><span class="line">├── vmx_set_hv_timer[kvm_x86_set_hv_timer]</span><br><span class="line">├── ktimer-&gt;hv_timer_in_use = <span class="literal">true</span>;</span><br><span class="line">└── hrtimer_cancel(&amp;ktimer-&gt;timer);</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start_sw_timer</span><br><span class="line">└── start_sw_tscdeadline</span><br><span class="line">    └── hrtimer_start</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/390345238" target="_blank" rel="noopener">kvm timer虚拟化</a></li><li><a href="https://www.codeleading.com/article/91825014645/" target="_blank" rel="noopener">APIC Timer模拟</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍kvm中lapic timer的虚拟化。为了方便，只介绍&lt;a href=&quot;/2022/09/05/The-relationship-between-LAPIC-timer-and-TSC/&quot;&gt;TSC-deadline模式的lapic timer&lt;/a&gt;。
    
    </summary>
    
      <category term="Time" scheme="http://liujunming.github.io/categories/Time/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>每周分享第33期</title>
    <link href="http://liujunming.github.io/2022/10/22/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB%E7%AC%AC33%E6%9C%9F/"/>
    <id>http://liujunming.github.io/2022/10/22/每周分享第33期/</id>
    <published>2022-10-22T08:01:51.000Z</published>
    <updated>2022-10-22T09:58:14.333Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ASCII-Tree-树形目录图表生成器"><a href="#ASCII-Tree-树形目录图表生成器" class="headerlink" title="ASCII Tree 树形目录图表生成器"></a>ASCII Tree 树形目录图表生成器</h3><p>链接: <a href="https://tree.nathanfriend.io/" target="_blank" rel="noopener">https://tree.nathanfriend.io/</a><br>源码: <a href="https://github.com/nfriend/tree-online" target="_blank" rel="noopener">https://github.com/nfriend/tree-online</a></p><p><img src="/images/2022/10/13.jpg" alt></p><p>以tree的形式输出函数调用图。<br><a id="more"></a> </p><h3 id="intel、amd、arm和power-spec"><a href="#intel、amd、arm和power-spec" class="headerlink" title="intel、amd、arm和power spec"></a>intel、amd、arm和power spec</h3><p><a href="https://kib.kiev.ua/x86docs/" target="_blank" rel="noopener">https://kib.kiev.ua/x86docs/</a></p><h3 id="Linus-Torvalds：Rust-将被合并到-Linux-6-1-主线"><a href="#Linus-Torvalds：Rust-将被合并到-Linux-6-1-主线" class="headerlink" title="Linus Torvalds：Rust 将被合并到 Linux 6.1 主线"></a>Linus Torvalds：Rust 将被合并到 Linux 6.1 主线</h3><p><a href="https://mp.weixin.qq.com/s/yv2qceJZsZyiTXdnnMMAPA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/yv2qceJZsZyiTXdnnMMAPA</a></p><h3 id="soft-lockup和hard-lockup的检测原理"><a href="#soft-lockup和hard-lockup的检测原理" class="headerlink" title="soft lockup和hard lockup的检测原理"></a>soft lockup和hard lockup的检测原理</h3><p><a href="https://mp.weixin.qq.com/s/OwdLj3EdpWCW-lUYnexl1A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/OwdLj3EdpWCW-lUYnexl1A</a></p><p><a href="/pdf/soft lockup和hard lockup的检测原理.pdf">archive pdf</a></p><h3 id="王选院士：我一生中的八个重要抉择"><a href="#王选院士：我一生中的八个重要抉择" class="headerlink" title="王选院士：我一生中的八个重要抉择"></a>王选院士：我一生中的八个重要抉择</h3><p><a href="https://mp.weixin.qq.com/s/mzGku8RZ8yc4z3SecflZXw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/mzGku8RZ8yc4z3SecflZXw</a></p><h3 id="“GPU池化”术语发布"><a href="#“GPU池化”术语发布" class="headerlink" title="“GPU池化”术语发布"></a>“GPU池化”术语发布</h3><p><a href="https://mp.weixin.qq.com/s/6RRhhosfMYk2k72kXnxfNA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/6RRhhosfMYk2k72kXnxfNA</a></p><h3 id="Infrastructure-Programmer-Development-Kit-IPDK"><a href="#Infrastructure-Programmer-Development-Kit-IPDK" class="headerlink" title="Infrastructure Programmer Development Kit (IPDK)"></a>Infrastructure Programmer Development Kit (IPDK)</h3><p><a href="https://ipdk.io/" target="_blank" rel="noopener">https://ipdk.io/</a><br>Infrastructure Programmer Development Kit (IPDK) is an open source, vendor agnostic framework of drivers and APIs for infrastructure offload and management that runs on a CPU, IPU, DPU or switch.</p><h3 id="p50-p90-p99-pct-50-pct-90-pct-99-指什么？"><a href="#p50-p90-p99-pct-50-pct-90-pct-99-指什么？" class="headerlink" title="p50, p90, p99 (pct 50, pct 90, pct 99)指什么？"></a>p50, p90, p99 (pct 50, pct 90, pct 99)指什么？</h3><p><a href="https://blog.csdn.net/Solo95/article/details/119110134" target="_blank" rel="noopener">https://blog.csdn.net/Solo95/article/details/119110134</a></p><h3 id="CPU-IPU：揭秘英特尔数据中心芯片布局"><a href="#CPU-IPU：揭秘英特尔数据中心芯片布局" class="headerlink" title="CPU+IPU：揭秘英特尔数据中心芯片布局"></a>CPU+IPU：揭秘英特尔数据中心芯片布局</h3><p><a href="https://mp.weixin.qq.com/s/a1eU0LWjyjTM-UvMjHF22Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/a1eU0LWjyjTM-UvMjHF22Q</a></p><h3 id="使用DSA加速NVMe-TCP-PDU-Digest的CRC32C计算"><a href="#使用DSA加速NVMe-TCP-PDU-Digest的CRC32C计算" class="headerlink" title="使用DSA加速NVMe/TCP PDU Digest的CRC32C计算"></a>使用DSA加速NVMe/TCP PDU Digest的CRC32C计算</h3><p><a href="https://mp.weixin.qq.com/s/2nYlD7MuxVbVNdDTrgSOCw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/2nYlD7MuxVbVNdDTrgSOCw</a><br>Idea:使用DSA卸载SPDK NVMf over TCP的CRC32C计算</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;ASCII-Tree-树形目录图表生成器&quot;&gt;&lt;a href=&quot;#ASCII-Tree-树形目录图表生成器&quot; class=&quot;headerlink&quot; title=&quot;ASCII Tree 树形目录图表生成器&quot;&gt;&lt;/a&gt;ASCII Tree 树形目录图表生成器&lt;/h3&gt;&lt;p&gt;链接: &lt;a href=&quot;https://tree.nathanfriend.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://tree.nathanfriend.io/&lt;/a&gt;&lt;br&gt;源码: &lt;a href=&quot;https://github.com/nfriend/tree-online&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/nfriend/tree-online&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2022/10/13.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;以tree的形式输出函数调用图。&lt;br&gt;
    
    </summary>
    
      <category term="经验" scheme="http://liujunming.github.io/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="经验" scheme="http://liujunming.github.io/tags/%E7%BB%8F%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>posted interrupt的一些思考</title>
    <link href="http://liujunming.github.io/2022/10/16/posted-interrupt%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"/>
    <id>http://liujunming.github.io/2022/10/16/posted-interrupt的一些思考/</id>
    <published>2022-10-16T10:36:36.000Z</published>
    <updated>2022-10-22T11:21:47.886Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录一些对posted interrupt的思考。<a id="more"></a> </p><p>本文参考的内核版为<a href="https://elixir.bootlin.com/linux/v5.18/source" target="_blank" rel="noopener">v5.18</a></p><h3 id="1-irqfd亦可使用posted-interrupt"><a href="#1-irqfd亦可使用posted-interrupt" class="headerlink" title="1. irqfd亦可使用posted interrupt"></a>1. irqfd亦可使用posted interrupt</h3><p><a href="/2021/10/27/Dive-into-irqfd-KVM-side-mechanism/">irqfd</a>其实也可以使用VT-x posted interrupt来避免interrupt acceptance的一次VM Exit。</p><h4 id="1-1-kvm-arch-set-irq-inatomic"><a href="#1-1-kvm-arch-set-irq-inatomic" class="headerlink" title="1.1 kvm_arch_set_irq_inatomic"></a>1.1 kvm_arch_set_irq_inatomic</h4><blockquote><p>QEMU写了<code>irqfd</code>后，KVM内核模块中的irqfd poll就收到一个<code>POLL_IN</code>事件，然后将MSIx中断自动投递给对应的LAPIC。 大致流程是：<code>POLL_IN</code> -&gt; <code>kvm_arch_set_irq_inatomic</code> -&gt; <code>kvm_set_msi_irq</code>, <code>kvm_irq_delivery_to_apic_fast</code></p></blockquote><p><a href="https://elixir.bootlin.com/linux/v5.18/source/arch/x86/kvm/irq_comm.c#L157" target="_blank" rel="noopener">kvm_arch_set_irq_inatomic</a>最终会调用<a href="https://elixir.bootlin.com/linux/v5.18/source/arch/x86/kvm/lapic.c#L1015" target="_blank" rel="noopener">kvm_irq_delivery_to_apic_fast</a>来给guest注入interrupt。</p><h4 id="1-2-kvm-irq-delivery-to-apic-fast"><a href="#1-2-kvm-irq-delivery-to-apic-fast" class="headerlink" title="1.2 kvm_irq_delivery_to_apic_fast"></a>1.2 kvm_irq_delivery_to_apic_fast</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kvm_irq_delivery_to_apic_fast</span><br><span class="line">└── kvm_apic_set_irq</span><br><span class="line">    └── __apic_accept_irq</span><br><span class="line">        └── vmx_deliver_interrupt</span><br><span class="line">            └── vmx_deliver_posted_interrupt</span><br><span class="line">                └── kvm_vcpu_trigger_posted_interrupt</span><br></pre></td></tr></table></figure><h3 id="2-apic-accept-irq"><a href="#2-apic-accept-irq" class="headerlink" title="2. __apic_accept_irq"></a>2. __apic_accept_irq</h3><p><a href="https://elixir.bootlin.com/linux/v5.18/source/arch/x86/kvm/lapic.c#L1098" target="_blank" rel="noopener">__apic_accept_irq</a>其实就会使用VT-x的posted interrupt完成中断的注入。</p><p><code>KVM_SIGNAL_MSI</code>、<code>KVM_IRQ_LINE</code>等ioctl其实会在KVM中调用<code>__apic_accept_irq</code>函数，因此，最终会使用到posted interrupt来完成虚拟中断的注入。</p><p>根据我的理解(待实验验证)，在VNC中的鼠标键盘操作，其实本质上是给虚拟机注入中断，对于这种中断，也是可以使用posted interrupt的。</p><h3 id="3-在虚拟化下，lapic-timer可以用VT-x-posted-interrupt呢？"><a href="#3-在虚拟化下，lapic-timer可以用VT-x-posted-interrupt呢？" class="headerlink" title="3. 在虚拟化下，lapic timer可以用VT-x posted interrupt呢？"></a>3. 在虚拟化下，lapic timer可以用VT-x posted interrupt呢？</h3><p>在虚拟化场景下，假设vCPU与pCPU一一绑定，那么，lapic timer可以使用VT-x posted interrupt吗？<br>在KVM架构下，答案是否定的，分析如下:<br><img src="/images/2022/10/12.jpg" alt><br>当前是设置了External-interrupt exiting这个位的。当vCPU在non-root mode，此时物理的lapic timer的中断来了，那么就会导致VM Exit，此时使用VT-x posted interrupt已经没有意义了。</p><p><a href="/2022/09/11/LAPIC-Implement-Exitless-Timer/">Injection Exitless LAPIC Timer</a>的Idea是offload lapic timer to housekeeping cpus，然后由housekeeping cpu利用VT-x posted interrupt为vCPU注入中断！</p><p>ps:如果<a href="/2022/10/22/lapic-timer-virtualization/">vCPU的lapic timer</a>由preemption timer进行模拟的话，定时器到期后vCPU会陷出，此时也没有必要用posted interrupt了。</p><h3 id="4-WNV的发送in-VT-d"><a href="#4-WNV的发送in-VT-d" class="headerlink" title="4. WNV的发送in VT-d"></a>4. WNV的发送in VT-d</h3><p><a href="https://elixir.bootlin.com/linux/v5.18/source/arch/x86/kvm/vmx/posted_intr.c#L163" target="_blank" rel="noopener">new.nv = POSTED_INTR_WAKEUP_VECTOR</a></p><p>当vCPU处于ready-to-run或者halted状态时，物理中断来了，此时IOMMU会发送WNV来<a href="https://elixir.bootlin.com/linux/v5.18/source/arch/x86/kvm/vmx/posted_intr.c#L212" target="_blank" rel="noopener">唤醒vCPU</a>。<br>值得注意的是: WNV是IOMMU发送的，而非软件。</p><hr><p>参考资料:</p><ol><li><a href="https://kernelgo.org/virtio-overview.html" target="_blank" rel="noopener">Virtio Spec Overview</a></li><li><a href="https://biscuitos.github.io/blog/Broiler-vInterrupt/" target="_blank" rel="noopener">Broiler Interrupt Virtualization Technology</a></li><li><a href="https://martins3.github.io/qemu/interrupt.html" target="_blank" rel="noopener">QEMU 如何模拟中断</a></li><li><a href="https://www.binss.me/blog/qemu-note-of-interrupt/" target="_blank" rel="noopener">QEMU学习笔记——中断</a></li><li><a href="https://blog.csdn.net/weixin_43780260/article/details/110224589" target="_blank" rel="noopener">QEMU 如何处理PCI设备的中断（二）</a></li><li><a href="https://blog.csdn.net/qihoo_tech/article/details/117137150" target="_blank" rel="noopener">kvm post interrupt</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录一些对posted interrupt的思考。
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>bytedance trace-irqoff tool</title>
    <link href="http://liujunming.github.io/2022/10/15/bytedance-trace-irqoff-tool/"/>
    <id>http://liujunming.github.io/2022/10/15/bytedance-trace-irqoff-tool/</id>
    <published>2022-10-15T11:44:40.000Z</published>
    <updated>2022-10-16T10:56:59.862Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/ByteDanceTech/article/details/105632131" target="_blank" rel="noopener">Trace-irqoff</a><br><a href="https://github.com/bytedance/trace-irqoff/tree/master" target="_blank" rel="noopener">Open Source Repo</a><br><a id="more"></a> </p><p><img src="/images/2022/10/10.jpg" alt></p><p><img src="/images/2022/10/11.jpg" alt></p><p>个人总结: </p><ul><li>对于hardirq的关闭检测，是通过定期的hrtimer来判断的，hrtimer执行的上下文就是hardirq，利用相邻两次hrtimer的时间间隔来评估hardirq的关闭时间</li><li>对于softirq，是利用普通的定时器timer(执行的上下文就是softirq)两次执行的时间间隔来采样两次相邻softirq之间的时间间隔，因此检测的不仅仅是softirq的关闭时间。当第二次的timer执行在ksoftirqd进程时，打印出的堆栈就没有意义了，此时，利用hrtimer(在hardirq上下文中)来记录softirq多长时间没有得到执行</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ByteDanceTech/article/details/105632131&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Trace-irqoff&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/bytedance/trace-irqoff/tree/master&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Open Source Repo&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="debug" scheme="http://liujunming.github.io/categories/debug/"/>
    
    
      <category term="debug" scheme="http://liujunming.github.io/tags/debug/"/>
    
  </entry>
  
</feed>
