<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2024-04-05T13:56:00.935Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Intel new feature:monitorless MWAIT</title>
    <link href="http://liujunming.github.io/2024/04/05/Intel-new-feature-monitorless-MWAIT/"/>
    <id>http://liujunming.github.io/2024/04/05/Intel-new-feature-monitorless-MWAIT/</id>
    <published>2024-04-05T03:37:30.000Z</published>
    <updated>2024-04-05T13:56:00.935Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Intel的新feature:<a href="/pdf/monitorless MWAIT.pdf">monitorless MWAIT</a>。<a id="more"></a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Prior this feature, execution of the MWAIT instruction causes a logical processor to suspend execution and enter an implementation-dependent optimized state only if the MONITOR instruction was executed previously, specifying an address range to monitor, and there have been no stores to that address range since MONITOR executed. The logical processor leaves the optimized state and resumes execution when there is a write to the monitored address range.</p><p>This existing functionality <strong>supports software that seeks to suspend execution until an event associated with a write to the monitored address range</strong>. For example, that range may contain the head pointer of a work queue that is written when there is work for the suspended logical processor.</p><p>It is possible that software may wish to suspend execution with no requirement to resume execution in response to a memory write. Such software is not well served by the existing MWAIT instruction since it must incur the overhead of monitoring some (irrelevant) address range and may resume execution earlier than intended following a memory write.<br>软件可能希望暂停执行，而不要求在内存写入后恢复执行。现有的 MWAIT 指令并不能很好地满足这类软件的需求，因为它必须承担监控某些（不相关的）地址范围的开销，而且在内存写入后，可能会比预定时间提前恢复执行。</p><p>Monitorless MWAIT enhances the MWAIT instruction by allowing suspension of execution without monitoring an address range.<br>Monitorless MWAIT允许在不监控地址范围的情况下暂停执行，从而增强了 MWAIT 指令。</p><p>The feature is defined with an enumeration independent of that of existing MONITOR/MWAIT. That allows a VMM to virtualize monitorless MWAIT without having to virtualize the address-range monitoring of the existing feature.<br>该特性是用一个独立于现有MONITOR/MWAIT的枚举定义的。这使得 VMM 可以虚拟化monitorless MWAIT，而无需虚拟化现有功能的地址范围监控。</p><h3 id="Q-amp-amp-A"><a href="#Q-amp-amp-A" class="headerlink" title="Q &amp;&amp; A"></a>Q &amp;&amp; A</h3><p>Q: 使用了monitorless MWAIT后，logical processor什么时候可以resume execution呢？<br>A: spec中其实已经给出了答案:<br><img src="/images/2024/04/002.jpg" alt></p><ul><li>an NMI or SMI</li><li>a debug exception</li><li>a machine check exception</li><li>the BINIT# signal</li><li>the INIT# signal</li><li>the RESET# signal</li><li>an external interrupt if ECX[0] = 1</li></ul><p>Q: monitorless MWAIT与hlt的区别在哪里？<br>A: 与hlt相比，monitorless MWAIT resume execution的delay更低(因为monitorless MWAIT的节能程度不如hlt)，同时monitorless MWAIT还可以精细地指定C-state及Sub C-state。<br><img src="/images/2024/04/003.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="/2020/05/01/Introduction-to-halt-pause-monitor-mwait-instruction/">Introduction to hlt/pause/monitor/mwait instruction</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Intel的新feature:&lt;a href=&quot;/pdf/monitorless MWAIT.pdf&quot;&gt;monitorless MWAIT&lt;/a&gt;。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Intel new feature:APIC-timer virtualization</title>
    <link href="http://liujunming.github.io/2024/04/04/Intel-new-feature-APIC-timer-virtualization/"/>
    <id>http://liujunming.github.io/2024/04/04/Intel-new-feature-APIC-timer-virtualization/</id>
    <published>2024-04-04T10:40:40.000Z</published>
    <updated>2024-04-04T12:52:44.724Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录下<a href="https://cdrdv2.intel.com/v1/dl/getContent/671368" target="_blank" rel="noopener">Intel® Architecture Instruction Set Extensions Programming Reference</a>中的APIC-timer virtualization技术。笔者特意裁剪了本文相关的描述: <a href="/pdf/APIC-timer virtualization.pdf">APIC-timer virtualization</a><a id="more"></a></p><h2 id="1-overview"><a href="#1-overview" class="headerlink" title="1. overview"></a>1. overview</h2><p>The new feature virtualizes the TSC-deadline mode of the APIC timer. When this mode is active, software can program the APIC timer with a deadline written to the IA32_TSC_DEADLINE MSR. A timer interrupt becomes pending when the logical processor’s timestamp counter (TSC) is greater or equal to the deadline.</p><p>APIC-timer virtualization operates in conjunction with the existing virtual-interrupt delivery feature. With that feature, a virtual-machine monitor (VMM) establishes a virtual-APIC page in memory for each virtual logical processor (vCPU). A logical processor uses this page to virtualize certain aspects of APIC operation for the vCPU.</p><p>The feature is based on new guest-timer hardware that introduces two new architectural features: <strong>guest-timer events</strong> and a <strong>guest deadline</strong>. With APIC-timer virtualization, guest writes to the IA32_TSC_DEADLINE MSR do not interact with the APIC (or its timer) but instead establish a guest deadline to arm the guest-timer hardware. When a logical processor’s TSC is greater than or equal to the guest deadline(shadow context), a guest-timer event becomes pending. (笔者注:硬件)Processing of a guest-timer event updates the virtual-APIC page to record the fact that a new virtual interrupt is pending.</p><h2 id="2-guest-timer-hardware"><a href="#2-guest-timer-hardware" class="headerlink" title="2. guest-timer hardware"></a>2. guest-timer hardware</h2><p>A logical processor supports APIC-timer virtualization using new guest-timer hardware. Software controls this hardware using an unsigned 64-bit value called the <strong>guest deadline</strong>. (There is a separate guest deadline for each logical processor.) If the guest deadline is non-zero, a guest-timer event will be pending when the timestamp counter (TSC) reaches or exceeds the guest deadline.</p><h2 id="3-changes-to-vmx-non-root-operation"><a href="#3-changes-to-vmx-non-root-operation" class="headerlink" title="3. changes to vmx non-root operation"></a>3. changes to vmx non-root operation</h2><p>The 1-setting of the “APIC-timer virtualization” VM-execution control changes how a logical processor responds to accesses to the IA32_TSC_DEADLINE MSR.</p><h3 id="3-1-Accesses-to-the-IA32-TSC-DEADLINE-MSR"><a href="#3-1-Accesses-to-the-IA32-TSC-DEADLINE-MSR" class="headerlink" title="3.1 Accesses to the IA32_TSC_DEADLINE MSR"></a>3.1 Accesses to the IA32_TSC_DEADLINE MSR</h3><p>If the “APIC-timer virtualization” VM-execution control is 1, the operation of reads and writes to the<br>IA32_TSC_DEADLINE MSR (MSR 6E0H) is modified:</p><ul><li>Any read from the IA32_TSC_DEADLINE MSR (e.g., by RDMSR) that does not cause a fault or a VM exit returns the value of the guest deadline shadow (from the VMCS).</li><li>Any write to the IA32_TSC_DEADLINE MSR (e.g., by WRMSR) that does not cause a fault or a VM exit is treated as follows:<ul><li>The source operand is written to the guest deadline shadow (updating the VMCS).</li><li>If the source operand is zero, the guest deadline (<strong>the value that controls when hardware generates a guest time event</strong>) is cleared to 0.</li><li>If the source operand is not zero, the guest deadline is computed as follows. The source operand is interpreted as a virtual deadline. The processor converts that value to the actual guest deadline based on the current configuration of TSC offsetting and TSC scaling.</li></ul></li></ul><p>Note that when the “APIC-timer virtualization” VM-execution control is 1, such writes do not change the value of the IA32_TSC_DEADLINE MSR nor do they interact with the APIC timer in any way.</p><h3 id="3-2-Processing-of-Guest-Timer-Events"><a href="#3-2-Processing-of-Guest-Timer-Events" class="headerlink" title="3.2 Processing of Guest-Timer Events"></a>3.2 Processing of Guest-Timer Events</h3><p>Processing of a guest-timer event updates the virtual-APIC page to cause a virtual timer interrupt to become pending. Specifically, the logical processor performs the following steps:</p><ul><li>V := virtual timer vector;</li><li>VIRR[V] := 1;// update virtual IRR field on virtual-APIC page</li><li>RVI := max{RVI, V};// update guest interrupt status field in VMCS</li><li>evaluate pending virtual interrupts;// a virtual interrupt may be delivered immediately after this processing </li><li>Guest deadline := 0;</li><li>Guest deadline shadow := 0;</li></ul><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><ul><li>guest 在non-root mode写IA32_TSC_DEADLINE MSR时，无需发生VM Exit，硬件会更新guest deadline shadow(virtual context)，同时也会更新guest deadline(shadow context)；</li><li>硬件会利用guest deadline(<strong>the value that controls when hardware generates a guest time event</strong>)与physical TSC相比，当physical timestap大于等于guest deadline时，就会给non-root mode的vCPU注入timer中断</li><li>给vCPU注入的timer中断vector由Virtual timer vector决定</li></ul><p><img src="/images/2024/04/001.jpg" alt></p><p>利用<a href="/2021/03/20/虚拟化学习心得-three-context/">three context思想</a>以及理解上述三个VMCS fields的作用，即可对APIC-timer virtualization有深入的理解。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录下&lt;a href=&quot;https://cdrdv2.intel.com/v1/dl/getContent/671368&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Intel® Architecture Instruction Set Extensions Programming Reference&lt;/a&gt;中的APIC-timer virtualization技术。笔者特意裁剪了本文相关的描述: &lt;a href=&quot;/pdf/APIC-timer virtualization.pdf&quot;&gt;APIC-timer virtualization&lt;/a&gt;
    
    </summary>
    
      <category term="Time" scheme="http://liujunming.github.io/categories/Time/"/>
    
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
  </entry>
  
  <entry>
    <title>Notes about vhost-user-nvme</title>
    <link href="http://liujunming.github.io/2024/03/31/Notes-about-vhost-user-nvme/"/>
    <id>http://liujunming.github.io/2024/03/31/Notes-about-vhost-user-nvme/</id>
    <published>2024-03-31T04:21:45.000Z</published>
    <updated>2024-03-31T05:34:46.391Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍下vhost-user-nvme(SPDK Vhost-NVMe)技术。</p><p>The SPDK Vhost-NVMe target combines NVMe 1.3 new feature as well as vhost-user technology to accelerate NVMe IOs inside Guest VM. It uses NVMe as the paravirtualization protocol between Guest and SPDK Vhost-NVMe target. Also, no special paravirtualization driver is required inside Guest.<a id="more"></a></p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p><img src="/images/2024/03/018.jpg" alt></p><p><img src="/images/2024/03/019.jpg" alt></p><p><img src="/images/2024/03/020.jpg" alt></p><p><img src="/images/2024/03/021.jpg" alt></p><h2 id="SPDK-vhost-solution"><a href="#SPDK-vhost-solution" class="headerlink" title="SPDK vhost solution"></a>SPDK vhost solution</h2><p>Combine virtio and NVMe to inform a uniform SPDK vhost solution(结合virtio和NVMe形成统一的SPDK vhost解决方案)<br><img src="/images/2024/03/022.jpg" alt></p><p><img src="/images/2024/03/023.jpg" alt></p><p><img src="/images/2024/03/016.jpg" alt></p><p><img src="/images/2024/03/017.jpg" alt></p><p><img src="/images/2024/03/024.jpg" alt></p><p><img src="/images/2024/03/014.jpg" alt></p><p><img src="/images/2024/03/025.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/11/Accelerating-NVMe-I_Os-in-Virtual-Machine-via-SPDK-vhost_-Solution-Ziye-Yang-_-Changpeng-Liu-Intel.pdf" target="_blank" rel="noopener">Accelerating NVMe I/Os in Virtual Machine via SPDK vhost* Solution</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2018/75/KVM_Forum_26_Oct_2018_Vhost-NVMe.pdf" target="_blank" rel="noopener">slides: SPDK vhost Target: A Practical Solution to Accelerate Storage I/Os</a></li><li><a href="https://www.youtube.com/watch?v=paTvtJ6JdAc" target="_blank" rel="noopener">video: SPDK vhost Target: A Practical Solution to Accelerate Storage I/Os</a></li><li><a href="https://patchwork.kernel.org/project/qemu-devel/patch/1516003315-17878-2-git-send-email-changpeng.liu@intel.com/" target="_blank" rel="noopener">block/NVMe: introduce a new vhost NVMe host device to QEMU</a></li><li><a href="https://www.youtube.com/watch?v=y2vXN10AveM" target="_blank" rel="noopener">Vhost-NVMe: A New Virtualization Solution to Accelerate Guest NVMe IOs</a></li><li><a href="https://ieeexplore.ieee.org/document/8567374" target="_blank" rel="noopener">SPDK Vhost-NVMe: Accelerating I/Os in Virtual Machines on NVMe SSDs via User Space Vhost Target</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍下vhost-user-nvme(SPDK Vhost-NVMe)技术。&lt;/p&gt;
&lt;p&gt;The SPDK Vhost-NVMe target combines NVMe 1.3 new feature as well as vhost-user technology to accelerate NVMe IOs inside Guest VM. It uses NVMe as the paravirtualization protocol between Guest and SPDK Vhost-NVMe target. Also, no special paravirtualization driver is required inside Guest.
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe Shadow doorbell buffer</title>
    <link href="http://liujunming.github.io/2024/03/30/Notes-about-NVMe-Shadow-doorbell-buffer/"/>
    <id>http://liujunming.github.io/2024/03/30/Notes-about-NVMe-Shadow-doorbell-buffer/</id>
    <published>2024-03-30T08:20:37.000Z</published>
    <updated>2024-03-30T10:56:43.201Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍下NVMe中的Shadow doorbell buffer机制。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2024/03/014.jpg" alt><br>The guest driver only writes to the MMIO register when EventIdx has been reached.  This eliminates some MMIO writes.</p><h2 id="Identify"><a href="#Identify" class="headerlink" title="Identify"></a>Identify</h2><p><img src="/images/2024/03/015.jpg" alt></p><p>可以参考<a href="https://github.com/qemu/qemu/commit/3f7fe8de3d4" target="_blank" rel="noopener">hw/nvme: Implement shadow doorbell buffer support</a>中的<code>NVME_OACS_DBBUF = 1 &lt;&lt; 8</code>。</p><h2 id="Doorbell-Buffer-Config-command"><a href="#Doorbell-Buffer-Config-command" class="headerlink" title="Doorbell Buffer Config command"></a>Doorbell Buffer Config command</h2><p><img src="/images/2024/03/012.jpg" alt></p><blockquote><p>doorbell values are written by the nvme driver (guest OS) and the event index is written by the virtual device (host OS).</p></blockquote><p>The Doorbell Buffer Config admin command is implemented for the guest to enable shadow doobell buffer. When this feature is enabled, each SQ/CQ is associated with two buffers, i.e., Shadow Doorbell buffer and EventIdx buffer. According to the Spec, each queue’s doorbell register is only updated when the Shadow Doorbell buffer value changes from being less than or equal to the value of the corresponding EventIdx buffer entry to being greater than that value. Therefore, the number of MMIO’s on the doorbell registers is greatly reduced.</p><h2 id="Updating-Controller-Doorbell-Registers-using-a-Shadow-Doorbell-Buffer"><a href="#Updating-Controller-Doorbell-Registers-using-a-Shadow-Doorbell-Buffer" class="headerlink" title="Updating Controller Doorbell Registers using a Shadow Doorbell Buffer"></a>Updating Controller Doorbell Registers using a Shadow Doorbell Buffer</h2><p><img src="/images/2024/03/013.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://nvmexpress.org/wp-content/uploads/NVM_Express_Revision_1.3.pdf" target="_blank" rel="noopener">https://nvmexpress.org/wp-content/uploads/NVM_Express_Revision_1.3.pdf</a></li><li><a href="https://patchwork.kernel.org/project/qemu-devel/patch/1516003315-17878-2-git-send-email-changpeng.liu@intel.com/#21421425" target="_blank" rel="noopener">block/NVMe: introduce a new vhost NVMe host device to QEMU</a></li><li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=f9f38e33389" target="_blank" rel="noopener">nvme: improve performance for virtual NVMe devices</a></li><li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/11/Accelerating-NVMe-I_Os-in-Virtual-Machine-via-SPDK-vhost_-Solution-Ziye-Yang-_-Changpeng-Liu-Intel.pdf" target="_blank" rel="noopener">Accelerating NVMe I/Os in Virtual Machine via SPDK vhost* Solution</a></li><li><a href="https://lore.kernel.org/all/20220616123408.3306055-1-fanjinhao21s@ict.ac.cn/" target="_blank" rel="noopener">hw/nvme: Add shadow doorbell buffer support</a></li><li><a href="https://github.com/qemu/qemu/commit/3f7fe8de3d4" target="_blank" rel="noopener">hw/nvme: Implement shadow doorbell buffer support</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍下NVMe中的Shadow doorbell buffer机制。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
  </entry>
  
  <entry>
    <title>(译)QEMU Internals: vhost architecture</title>
    <link href="http://liujunming.github.io/2024/03/24/QEMU-Internals-vhost-architecture/"/>
    <id>http://liujunming.github.io/2024/03/24/QEMU-Internals-vhost-architecture/</id>
    <published>2024-03-24T10:27:39.000Z</published>
    <updated>2024-03-24T10:34:07.875Z</updated>
    
    <content type="html"><![CDATA[<p>原文链接: <a href="https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html" target="_blank" rel="noopener">https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html</a></p><p>This post explains how vhost provides in-kernel virtio devices for KVM. I have been hacking on vhost-scsi and have answered questions about ioeventfd, irqfd, and vhost recently, so I thought this would be a useful QEMU Internals post.<br>这篇文章介绍了 vhost 如何为 KVM 提供内核 virtio 设备。我最近一直在研究 vhost-scsi，并回答了有关 ioeventfd、irqfd 和 vhost 的问题，因此我认为这将是一篇有用的 QEMU Internals 帖子。<a id="more"></a></p><h2 id="Vhost-overview"><a href="#Vhost-overview" class="headerlink" title="Vhost overview"></a>Vhost overview</h2><p>The vhost drivers in Linux provide in-kernel virtio device emulation. Normally the QEMU userspace process emulates I/O accesses from the guest. Vhost puts virtio emulation code into the kernel, taking QEMU userspace out of the picture. This allows device emulation code to directly call into kernel subsystems instead of performing system calls from userspace.<br>Linux 中的 vhost 驱动程序提供内核 virtio 设备模拟。通常，QEMU 用户空间进程会模拟guest的 I/O 访问。Vhost 将 virtio 仿真代码放入内核，将 QEMU 用户空间排除在外。这允许设备仿真代码直接调用内核子系统，而不是从用户空间执行系统调用。</p><p>The vhost-net driver emulates the virtio-net network card in the host kernel. Vhost-net is the oldest vhost device and the only one which is available in mainline Linux. Experimental vhost-blk and vhost-scsi devices have also been developed.<br>vhost-net 驱动程序在主机内核中模拟 virtio-net 网卡。vhost-net 是最古老的 vhost 设备，也是主线 Linux 中唯一可用的设备。此外，还开发了试验性的 vhost-blk 和 vhost-scsi 设备。</p><p>In Linux 3.0 the vhost code lives in drivers/vhost/. Common code that is used by all devices is in drivers/vhost/vhost.c. This includes the virtio vring access functions which all virtio devices need in order to communicate with the guest. The vhost-net code lives in drivers/vhost/net.c.<br>在 Linux 3.0 中，vhost 代码位于 drivers/vhost/。所有设备都要使用的通用代码位于 drivers/vhost/vhost.c 中。其中包括 virtio vring 访问函数，所有 virtio 设备都需要这些函数才能与guest通信。vhost-net 代码位于 drivers/vhost/net.c 中。</p><h2 id="The-vhost-driver-model"><a href="#The-vhost-driver-model" class="headerlink" title="The vhost driver model"></a>The vhost driver model</h2><p>The vhost-net driver creates a /dev/vhost-net character device on the host. This character device serves as the interface for configuring the vhost-net instance.<br>vhost-net 驱动程序会在主机上创建一个 /dev/vhost-net 字符设备。该字符设备是配置 vhost-net 实例的接口。</p><p>When QEMU is launched with -netdev tap,vhost=on it opens /dev/vhost-net and initializes the vhost-net instance with several ioctl(2) calls. These are necessary to associate the QEMU process with the vhost-net instance, prepare for virtio feature negotiation, and pass the guest physical memory mapping to the vhost-net driver.<br>当使用 -netdev tap,vhost=on 启动 QEMU 时，它会打开 /dev/vhost-net 并通过几个 ioctl(2) 调用初始化 vhost-net 实例。这些调用对于将 QEMU 进程与 vhost-net 实例关联、准备 virtio 功能协商以及将guest物理内存映射传递给 vhost-net 驱动程序都是必要的。</p><p>During initialization the vhost driver creates a kernel thread called vhost-$pid, where $pid is the QEMU process pid. This thread is called the “vhost worker thread”. The job of the worker thread is to handle I/O events and perform the device emulation.<br>在初始化过程中，vhost 驱动程序会创建一个名为 vhost-$pid 的内核线程，其中 $pid 是 QEMU 进程的 pid。该线程被称为 “vhost 工作线程”。工作线程的任务是处理 I/O 事件和执行设备仿真。</p><p><img src="/images/2024/03/010.png" alt></p><h2 id="In-kernel-virtio-emulation"><a href="#In-kernel-virtio-emulation" class="headerlink" title="In-kernel virtio emulation"></a>In-kernel virtio emulation</h2><p>Vhost does not emulate a complete virtio PCI adapter. Instead it restricts itself to virtqueue operations only. QEMU is still used to perform virtio feature negotiation and live migration, for example. This means a vhost driver is not a self-contained virtio device implementation, it depends on userspace to handle the control plane while the data plane is done in-kernel.<br>Vhost 不会模拟完整的 virtio PCI 适配器。相反，它仅限于进行 virtqueue 操作。例如，QEMU 仍用于执行 virtio 功能协商和热迁移。这意味着 vhost 驱动程序不是独立的 virtio 设备实现，它依赖用户空间来处理控制面，而数据面则在内核中完成。</p><p>The vhost worker thread waits for virtqueue kicks and then handles buffers that have been placed on the virtqueue. In vhost-net this means taking packets from the tx virtqueue and transmitting them over the tap file descriptor.<br>vhost 工作线程会等待 virtqueue kicks，然后处理放在 virtqueue 上的缓冲区。在 vhost-net 中，这意味着从 tx virtqueue 获取数据包并通过 tap 文件描述符传输。</p><p>File descriptor polling is also done by the vhost worker thread. In vhost-net the worker thread wakes up when packets come in over the tap file descriptor and it places them into the rx virtqueue so the guest can receive them.<br>文件描述符轮询也由 vhost 工作线程完成。在 vhost-net 中，当数据包通过 tap 文件描述符进入时，工作线程就会被唤醒，并将数据包放入 rx virtqueue，这样guest就能接收到这些数据包。</p><h2 id="Vhost-as-a-userspace-interface"><a href="#Vhost-as-a-userspace-interface" class="headerlink" title="Vhost as a userspace interface"></a>Vhost as a userspace interface</h2><p>One surprising aspect of the vhost architecture is that it is not tied to KVM in any way. Vhost is a userspace interface and has no dependency on the KVM kernel module. This means other userspace code, like libpcap, could in theory use vhost devices if they find them convenient high-performance I/O interfaces.<br>vhost 架构令人惊讶的一点是，它与 KVM 没有任何关联。Vhost 是一个用户空间接口，不依赖于 KVM 内核模块。这意味着其他用户空间代码（如 libpcap）如果发现 vhost 设备是方便的高性能 I/O 接口，理论上也可以使用 vhost 设备。</p><p>When a guest kicks the host because it has placed buffers onto a virtqueue, there needs to be a way to signal the vhost worker thread that there is work to do. Since vhost does not depend on the KVM kernel module they cannot communicate directly. Instead vhost instances are set up with an eventfd file descriptor which the vhost worker thread watches for activity. The KVM kernel module has a feature known as ioeventfd for taking an eventfd and hooking it up to a particular guest I/O exit. QEMU userspace registers an ioeventfd for the VIRTIO_PCI_QUEUE_NOTIFY hardware register access which kicks the virtqueue. This is how the vhost worker thread gets notified by the KVM kernel module when the guest kicks the virtqueue.<br>当guest因为在 virtqueue 上放置了buffers而kick主机时，需要有一种方法来向 vhost 工作线程发出有工作要做的信号。由于 vhost 并不依赖于 KVM 内核模块，因此它们无法直接通信。相反，vhost 实例会设置一个 eventfd 文件描述符，由 vhost 工作线程监视其活动。KVM 内核模块有一个名为 ioeventfd 的功能，用于获取 eventfd 并将其连接到特定的guest I/O VM exit。QEMU 用户空间会为 VIRTIO_PCI_QUEUE_NOTIFY 硬件寄存器访问注册一个 ioeventfd，从而kick virtqueue。这样，当 guest kick virtqueue 时，vhost 工作线程就会收到 KVM 内核模块的通知。</p><p>On the return trip from the vhost worker thread to interrupting the guest a similar approach is used. Vhost takes a “call” file descriptor which it will write to in order to kick the guest. The KVM kernel module has a feature called irqfd which allows an eventfd to trigger guest interrupts. QEMU userspace registers an irqfd for the virtio PCI device interrupt and hands it to the vhost instance. This is how the vhost worker thread can interrupt the guest.<br>在从 vhost 工作线程返回到中断guest的过程中，也使用了类似的方法。Vhost 会获取一个 “call “文件描述符，并写入该文件描述符以通知guest。KVM 内核模块有一个名为 irqfd 的功能，允许 eventfd 触发guest中断。QEMU 用户空间为 virtio PCI 设备中断注册了一个 irqfd，并将其交给 vhost 实例。这就是 vhost 工作线程中断guest的方式。</p><p>In the end the vhost instance only knows about the guest memory mapping, a kick eventfd, and a call eventfd.<br>最终，vhost 实例只知道 guest 内存映射、ioeventfd 和 irqfd。</p><h2 id="Where-to-find-out-more"><a href="#Where-to-find-out-more" class="headerlink" title="Where to find out more"></a>Where to find out more</h2><p>Here are the main points to begin exploring the code:<br>以下是开始探索代码的要点：</p><ul><li>drivers/vhost/vhost.c - common vhost driver code</li><li>drivers/vhost/net.c - vhost-net driver</li><li>virt/kvm/eventfd.c - ioeventfd and irqfd</li></ul><p>The QEMU userspace code shows how to initialize the vhost instance:<br>QEMU 用户空间代码显示了如何初始化 vhost 实例:</p><ul><li>hw/vhost.c - common vhost initialization code</li><li>hw/vhost_net.c - vhost-net initialization</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文链接: &lt;a href=&quot;https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.vmsplice.net/2011/09/qemu-internals-vhost-architecture.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This post explains how vhost provides in-kernel virtio devices for KVM. I have been hacking on vhost-scsi and have answered questions about ioeventfd, irqfd, and vhost recently, so I thought this would be a useful QEMU Internals post.&lt;br&gt;这篇文章介绍了 vhost 如何为 KVM 提供内核 virtio 设备。我最近一直在研究 vhost-scsi，并回答了有关 ioeventfd、irqfd 和 vhost 的问题，因此我认为这将是一篇有用的 QEMU Internals 帖子。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about vhost-pci</title>
    <link href="http://liujunming.github.io/2024/03/09/Notes-about-vhost-pci/"/>
    <id>http://liujunming.github.io/2024/03/09/Notes-about-vhost-pci/</id>
    <published>2024-03-09T12:27:55.000Z</published>
    <updated>2024-03-09T12:40:38.797Z</updated>
    
    <content type="html"><![CDATA[<p>vhost-pci的详细介绍可以参考<a href="https://www.linux-kvm.org/images/5/55/02x07A-Wei_Wang-Design_of-Vhost-pci.pdf" target="_blank" rel="noopener">Design of Vhost-pci slides</a>与<a href="https://www.youtube.com/watch?v=xITj0qsaSJQ" target="_blank" rel="noopener">video</a>，本文主要mark下相关notes。<a id="more"></a></p><h2 id="Usage-and-Motivation"><a href="#Usage-and-Motivation" class="headerlink" title="Usage and Motivation"></a>Usage and Motivation</h2><p><img src="/images/2024/03/007.jpg" alt></p><p><img src="/images/2024/03/008.jpg" alt></p><p><img src="/images/2024/03/009.jpg" alt></p><p>如上图所示，通过network packet的数据流向对比，可以显示vhost-pci的作用:<strong>high performance inter-VM communication schemes</strong>，vhost-pci机制可以让network packet直接从一个vm传输到另外一个vm中，无需经过vSwitch的中转。</p><h2 id="Vhost-pci-Design-Details"><a href="#Vhost-pci-Design-Details" class="headerlink" title="Vhost-pci Design Details"></a>Vhost-pci Design Details</h2><p><img src="/images/2024/03/001.jpg" alt></p><p><img src="/images/2024/03/005.jpg" alt></p><p><img src="/images/2024/03/006.jpg" alt></p><h2 id="vhost-pci-vs-virtio-vhost-user"><a href="#vhost-pci-vs-virtio-vhost-user" class="headerlink" title="vhost-pci vs virtio-vhost-user"></a>vhost-pci vs virtio-vhost-user</h2><p><img src="/images/2024/03/002.jpg" alt><br>详细内容可以参考<a href="https://lore.kernel.org/qemu-devel/20180110161438.GA28096@stefanha-x1.localdomain/" target="_blank" rel="noopener">vhost-pci and virtio-vhost-user</a>。</p><hr><p>参考资料:</p><ol><li><a href="https://www.linux-kvm.org/images/5/55/02x07A-Wei_Wang-Design_of-Vhost-pci.pdf" target="_blank" rel="noopener">Design of Vhost-pci slides</a></li><li><a href="https://www.youtube.com/watch?v=xITj0qsaSJQ" target="_blank" rel="noopener">Design of Vhost-pci video</a></li><li><a href="https://archive.fosdem.org/2018/schedule/event/virtio/attachments/slides/2167/export/events/attachments/virtio/slides/2167/fosdem_virtio1_1.pdf" target="_blank" rel="noopener">What’s new in Virtio 1.1?</a></li><li><a href="https://static.sched.com/hosted_files/dpdkuserspace22/93/DPDK22_virtualization_of_DPDK_applications_using_virtio_vhost_user.pdf" target="_blank" rel="noopener">Virtualization of DPDK applications using virtio-vhost-user</a></li><li><a href="https://lore.kernel.org/qemu-devel/1494578148-102868-1-git-send-email-wei.w.wang@intel.com/" target="_blank" rel="noopener">Vhost-pci for inter-VM communication</a></li><li><a href="https://lore.kernel.org/qemu-devel/20180110161438.GA28096@stefanha-x1.localdomain/" target="_blank" rel="noopener">vhost-pci and virtio-vhost-user</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;vhost-pci的详细介绍可以参考&lt;a href=&quot;https://www.linux-kvm.org/images/5/55/02x07A-Wei_Wang-Design_of-Vhost-pci.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Design of Vhost-pci slides&lt;/a&gt;与&lt;a href=&quot;https://www.youtube.com/watch?v=xITj0qsaSJQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;video&lt;/a&gt;，本文主要mark下相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about VirtioVhostUser</title>
    <link href="http://liujunming.github.io/2024/03/03/Notes-about-VirtioVhostUser/"/>
    <id>http://liujunming.github.io/2024/03/03/Notes-about-VirtioVhostUser/</id>
    <published>2024-03-03T12:11:04.000Z</published>
    <updated>2024-03-09T10:07:30.162Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA</a><a id="more"></a></p><h2 id="1-What"><a href="#1-What" class="headerlink" title="1. What"></a>1. What</h2><p>The virtio-vhost-user device lets guests act as vhost device backends so that virtual network switches and storage appliance VMs can provide virtio devices to other guests.</p><p>virtio-vhost-user 设备可让客户机充当 vhost 设备后端，这样虚拟网络交换机和存储设备虚拟机就能为其他客户机提供 virtio 设备。</p><p>virtio-vhost-user was inspired by vhost-pci by Wei Wang and Zhiyong Yang.</p><p>virtio-vhost-user 的灵感来源于Wei Wang和Zhiyong Yang的 vhost-pci。</p><h2 id="2-Use-cases"><a href="#2-Use-cases" class="headerlink" title="2. Use cases"></a>2. Use cases</h2><h3 id="2-1-Appliances-for-cloud-environments"><a href="#2-1-Appliances-for-cloud-environments" class="headerlink" title="2.1 Appliances for cloud environments"></a>2.1 Appliances for cloud environments</h3><p><strong>2.1 用于云环境的设备</strong></p><p>In cloud environments everything is a guest. It is not possible for users to run vhost-user processes on the host. This precludes high-performance vhost-user appliances from running in cloud environments.</p><p>在云环境中，一切都是客户机。用户不可能在主机上运行 vhost-user进程。这使得高性能 vhost-user 设备无法在云环境中运行。</p><p>virtio-vhost-user allows vhost-user appliances to be shipped as virtual machine images. They can provide I/O services directly to other guests instead of going through an extra layer of device emulation like a host network switch:</p><p>virtio-vhost-user 允许 vhost-user 设备作为虚拟机镜像发布。它们可以直接向其他客户机提供 I/O 服务，而无需通过额外的设备仿真层（如主机网络交换机）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    Traditional Appliance VMs       virtio-vhost-user Appliance VMs</span><br><span class="line">+-------------+   +-------------+  +-------------+   +-------------+</span><br><span class="line">|     VM1     |   |     VM2     |  |     VM1     |   |     VM2     |</span><br><span class="line">|  Appliance  |   |   Consumer  |  |  Appliance  |   |   Consumer  |</span><br><span class="line">|      ^      |   |      ^      |  |      &lt;------+---+------&gt;      |</span><br><span class="line">+------|------+---+------|------+  +-------------+---+-------------+</span><br><span class="line">|      +-----------------+      |  |                               |</span><br><span class="line">|             Host              |  |             Host              |</span><br><span class="line">+-------------------------------+  +-------------------------------+</span><br></pre></td></tr></table></figure></p><h3 id="2-2-Exitless-VM-to-VM-communication"><a href="#2-2-Exitless-VM-to-VM-communication" class="headerlink" title="2.2 Exitless VM-to-VM communication"></a>2.2 Exitless VM-to-VM communication</h3><p>Once the vhost-user session has been established all vring activity can be performed by poll mode drivers in shared memory. This eliminates vmexits in the data path so that the highest possible VM-to-VM communication performance can be achieved.</p><p>一旦 vhost-user 会话建立，所有 vring 活动都可由共享内存中的轮询模式驱动程序执行。这样就消除了数据路径中的 vmexits，从而实现尽可能高的VM-to-VM 通信性能。</p><p>Even when interrupts are necessary, virtio-vhost-user can use lightweight vmexits thanks to ioeventfd instead of exiting to host userspace. This ensures that VM-to-VM communication bypasses device emulation in QEMU.</p><p>即使需要中断(笔者注: virtio前端驱动kick需要发生VM Exit)，virtio-vhost-user 也可以通过 ioeventfd 使用轻量级 vmexits，而不是退出到主机用户空间。这可确保VM-to-VM 通信绕过 QEMU 中的设备仿真。</p><h2 id="3-How-it-works"><a href="#3-How-it-works" class="headerlink" title="3. How it works"></a>3. How it works</h2><p>Virtio devices were originally emulated inside the QEMU host userspace process. Later on, vhost allowed a subset of a virtio device, called the vhost device backend, to be implement inside the host kernel. vhost-user then allowed vhost device backends to reside in host userspace processes instead.</p><p>Virtio 设备最初是在 QEMU 主机用户空间进程内仿真的。后来，vhost 允许一部分virtio 设备（称为 vhost 设备后端）在主机内核中实现。vhost-user 允许 vhost 设备后端驻留在主机用户空间进程中。</p><p>virtio-vhost-user takes this one step further by moving the vhost device backend into a guest. It works by tunneling the vhost-user protocol over a new virtio device type called virtio-vhost-user.</p><p>virtio-vhost-user 在此基础上更进一步，将 vhost 设备后端移至客户机中。它的工作原理是在名为 virtio-vhost-user 的新 virtio 设备类型上传输 vhost-user 协议。</p><p>The following diagram shows how two guests communicate:</p><p>下图显示了两个客户机的通信方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+-------------+                     +-------------+</span><br><span class="line">|     VM1     |                     |     VM2     |</span><br><span class="line">|             |                     |             |</span><br><span class="line">|    vhost    |    shared memory    |             |</span><br><span class="line">|   device    | +-----------------&gt; |             |</span><br><span class="line">|   backend   |                     |             |</span><br><span class="line">|             |                     | virtio-net  |</span><br><span class="line">+-------------+                     +-------------+</span><br><span class="line">|             |                     |             |</span><br><span class="line">|  virtio-    |  vhost-user socket  |             |</span><br><span class="line">| vhost-user  | &lt;-----------------&gt; | vhost-user  |</span><br><span class="line">|    QEMU     |                     |    QEMU     |</span><br><span class="line">+-------------+                     +-------------+</span><br></pre></td></tr></table></figure></p><p>VM2 sees a regular virtio-net device. VM2’s QEMU uses the existing vhost-user feature as if it were talking to a host userspace vhost-user backend.</p><p>VM2 看到的是普通的 virtio-net 设备。VM2 的 QEMU 使用现有的 vhost-user 功能，就像与主机用户空间 vhost-user 后端对话一样。</p><p>VM1’s QEMU tunnels the vhost-user protocol messages from VM1’s QEMU to the new virtio-vhost-user device so that guest software in VM1 can act as the vhost-user backend.</p><p>VM1 的 QEMU 将 vhost-user 协议信息从 VM1 的 QEMU 隧道(笔者注:可以类比于网络中的隧道技术，a method for transporting data across a network using protocols that are not supported by that network)传输到新的 virtio-vhost-user 设备，这样 VM1 中的客户机软件就可以充当 vhost-user 后端。</p><p>It is possible to reuse existing vhost-user backend software with virtio-vhost-user since they use the same vhost-user protocol messages. A driver is required for the virtio-vhost-user PCI device that carries the message instead of the usual vhost-user UNIX domain socket. The driver can be implemented in a guest userspace process using Linux vfio-pci but guest kernel driver implementation would also be also possible.</p><p>由于 virtio-vhost-user 使用相同的 vhost-user 协议信息，因此可以重新使用现有的 vhost-user 后端软件。virtio-vhost-user PCI 设备需要一个驱动程序来传输信息，而不是通常的 vhost-user UNIX 域套接字。该驱动程序可在客户机用户空间进程中使用 Linux vfio-pci 实现，也可在客户机内核驱动程序中实现。</p><p>The vhost device backend vrings are accessed through shared memory and do not require vhost-user message exchanges in the data path. No vmexits are taken when poll mode drivers are used. Even when interrupts are used, QEMU is not involved in the data path because ioeventfd lightweight vmexits are taken.</p><p>vhost 设备后端 vrings 通过共享内存访问，不需要在数据路径中进行 vhost 用户信息交换。使用轮询模式驱动程序时，不会出现 vmexits。即使使用中断(笔者注: virtio前端驱动kick需要发生VM Exit)，QEMU 也不会参与数据路径，因为会使用 ioeventfd 轻量级 vmexits。</p><p>All vhost device types work with virtio-vhost-user, including net, scsi, and blk.</p><p>所有 vhost 设备类型都能与 virtio-vhost-user 一起使用，包括 net、scsi 和 blk。</p><h2 id="4-DPDK使用案例"><a href="#4-DPDK使用案例" class="headerlink" title="4. DPDK使用案例"></a>4. DPDK使用案例</h2><p>下面截取了DPDK中VirtioVhostUser的使用案例:</p><p><img src="/images/2024/03/003.png" alt></p><p><img src="/images/2024/03/004.png" alt></p><p>slides中的Memory region I/O in device，笔者的理解就是VVU(VirtioVhostUser) device的MMIO寄存器。</p><hr><p>参考资料:</p><ol><li><a href="https://wiki.qemu.org/Features/VirtioVhostUser" target="_blank" rel="noopener">https://wiki.qemu.org/Features/VirtioVhostUser</a></li><li><a href="https://archive.fosdem.org/2018/schedule/event/virtio/attachments/slides/2167/export/events/attachments/virtio/slides/2167/fosdem_virtio1_1.pdf" target="_blank" rel="noopener">https://archive.fosdem.org/2018/schedule/event/virtio/attachments/slides/2167/export/events/attachments/virtio/slides/2167/fosdem_virtio1_1.pdf</a></li><li><a href="https://static.sched.com/hosted_files/dpdkuserspace22/93/DPDK22_virtualization_of_DPDK_applications_using_virtio_vhost_user.pdf" target="_blank" rel="noopener">https://static.sched.com/hosted_files/dpdkuserspace22/93/DPDK22_virtualization_of_DPDK_applications_using_virtio_vhost_user.pdf</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/hm5FVrcEsp19hlQny6euSA&lt;/a&gt;
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Share Virtual Address</title>
    <link href="http://liujunming.github.io/2024/02/25/Notes-about-Share-Virtual-Address/"/>
    <id>http://liujunming.github.io/2024/02/25/Notes-about-Share-Virtual-Address/</id>
    <published>2024-02-25T12:49:48.000Z</published>
    <updated>2024-02-25T13:15:05.974Z</updated>
    
    <content type="html"><![CDATA[<p>之前记录过<a href="/2022/03/30/Introduction-to-Shared-Virtual-Memory/">Introduction to Shared Virtual Memory</a>，但主要偏向于Intel的SVA方案。本文主要是mark下多个架构下的SVA方案。<a id="more"></a></p><p><img src="/images/2024/02/001.jpg" alt></p><p><img src="/images/2024/02/002.jpg" alt></p><p>Shared Virtual Addressing (SVA) is the ability to share process address spaces with devices. It is called “SVM” (Shared Virtual Memory) by OpenCL and some IOMMU architectures, but since that abbreviation is already used for AMD virtualisation in Linux (Secure Virtual Machine), we prefer the less ambiguous “SVA”.<br><a href="https://lwn.net/Articles/747230/" target="_blank" rel="noopener">Shared Virtual Addressing for the IOMMU</a></p><p><img src="/images/2024/02/003.jpg" alt></p><p><img src="/images/2024/02/004.jpg" alt></p><p><img src="/images/2024/02/005.jpg" alt></p><p>cc:Cache Coherent</p><p><img src="/images/2024/02/006.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://soft.cs.tsinghua.edu.cn/os2atc2018/ppt/osd5.pdf" target="_blank" rel="noopener">SVA：基于异构系统的内存管理技术</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Shared-Virtual-Addressing_Yisheng-Xie-_-Bob-Liu.pdf" target="_blank" rel="noopener">Share Virtual Address</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前记录过&lt;a href=&quot;/2022/03/30/Introduction-to-Shared-Virtual-Memory/&quot;&gt;Introduction to Shared Virtual Memory&lt;/a&gt;，但主要偏向于Intel的SVA方案。本文主要是mark下多个架构下的SVA方案。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>深入理解VMDq(Virtual Machine Device Queue)</title>
    <link href="http://liujunming.github.io/2024/01/27/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3VMDq-Virtual-Machine-Device-Queue/"/>
    <id>http://liujunming.github.io/2024/01/27/深入理解VMDq-Virtual-Machine-Device-Queue/</id>
    <published>2024-01-27T12:31:01.000Z</published>
    <updated>2024-01-28T09:06:50.969Z</updated>
    
    <content type="html"><![CDATA[<p>本文将深入探究VMDq(Virtual Machine Device Queue)相关内容。<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>强烈建议观看视频<a href="https://www.youtube.com/watch?v=QvKXbpV6WXk" target="_blank" rel="noopener">Intel VMDq Explanation</a>，非常清晰地介绍了VMDq，下面主要是视频中的概要总结，将以Without VMDq与With VMDq来做对比。</p><h3 id="Without-VMDq"><a href="#Without-VMDq" class="headerlink" title="Without VMDq"></a>Without VMDq</h3><p>A single core(you have one core that’s actually in charge of handling every packet before it determines which other core interrupt for the action, copy the data to the target vm) cannot keep up with 10 Gbps of data. Most packets coming in require two interrupts, one for the core assigned to handle Ethernet interrupts, followed by an interrupt of the core processing the VM where the packet is targetd for.</p><h3 id="With-VMDq"><a href="#With-VMDq" class="headerlink" title="With VMDq"></a>With VMDq</h3><p>Receive Path</p><ul><li>Data packets for different VMs get sorted at the Ethernet silicon based on MAC address/VLAN tags</li><li>Sorted data packets get parsed to the respective VMs</li><li>Data packets being received by respective VMs</li></ul><p>Reduces overhead and increases throughput by sorting packets with the Intel Ethernet Controller and spreading the workload amongst multiple CPU cores.</p><h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><p>这节主要是mark下<a href="https://www.intel.sg/content/dam/www/public/us/en/documents/white-papers/vmdq-technology-paper.pdf" target="_blank" rel="noopener">Intel® VMDq Technology white paper</a>中的关键notes。</p><p><img src="/images/2024/01/009.jpg" alt></p><p><img src="/images/2024/01/010.jpg" alt></p><h2 id="VMDq-vs-SR-IOV"><a href="#VMDq-vs-SR-IOV" class="headerlink" title="VMDq vs SR-IOV"></a>VMDq vs SR-IOV</h2><ul><li><p>VMDq<br>VMM在服务器的物理网卡中为每个虚机分配一个独立的队列，这样虚机出来的流量可以直接经过软件交换机发送到指定队列上，软件交换机无需进行排序和路由操作。<br>但是，VMM和虚拟交换机仍然需要将网络流量在VMDq和虚机之间进行复制。</p></li><li><p>SR-IOV<br>对于SR-IOV来说，则更加彻底，它通过创建不同虚拟功能（VF）的方式，呈现给虚拟机的就是独立的网卡，因此，虚拟机直接跟网卡通信，不需要经过软件交换机，VF和VM之间通过DMA进行高速数据传输，SR-IOV的性能是最好的。</p></li></ul><p><img src="/images/2024/01/011.jpeg" alt></p><p>Unlike SR-IOV, which exposes a complete device interface to the virtual machine guest, VMDq only provides network queues to the virtual machine guest.</p><p>VMDq只是一个过渡性的技术，当前已经被SR-IOV所替代。</p><hr><p>参考资料:</p><ol><li><a href="https://www.youtube.com/watch?v=QvKXbpV6WXk" target="_blank" rel="noopener">Intel VMDq Explanation</a></li><li><a href="https://www.intel.sg/content/dam/www/public/us/en/documents/white-papers/vmdq-technology-paper.pdf" target="_blank" rel="noopener">Intel® VMDq Technology white paper</a></li><li><a href="https://blog.csdn.net/yeasy/article/details/39178335" target="_blank" rel="noopener">网卡虚拟化技术：VMDq和SR-IOV</a></li><li>CompSC: live migration with pass-through devices</li><li><a href="https://forum.huawei.com/enterprise/en/what-is-the-difference-between-sr-iov-and-vmdq/thread/667229636603559936-667213860102352896" target="_blank" rel="noopener">What is the difference between SR-IOV and VMDQ?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将深入探究VMDq(Virtual Machine Device Queue)相关内容。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Live Migration with SR-IOV Pass-through on KVM Forum 2015</title>
    <link href="http://liujunming.github.io/2024/01/27/Live-Migration-with-SR-IOV-Pass-through-on-KVM-Forum-2015/"/>
    <id>http://liujunming.github.io/2024/01/27/Live-Migration-with-SR-IOV-Pass-through-on-KVM-Forum-2015/</id>
    <published>2024-01-27T12:00:36.000Z</published>
    <updated>2024-01-27T12:22:05.235Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about Live migration with SR-IOV pass-through by Weidong Han on KVM Forum 2015.<a id="more"></a></p><ul><li><a href="https://www.linux-kvm.org/images/9/9a/03x07-Juniper-Weidong_Han-LiveMigrationWithSR-IOVPass-through.pdf" target="_blank" rel="noopener">slides</a></li><li><a href="https://www.youtube.com/watch?v=vnwEnzVp9Zo" target="_blank" rel="noopener">video</a></li></ul><p><img src="/images/2024/01/005.jpg" alt></p><p>iproute2 is a collection of userspace utilities for controlling and monitoring various aspects of networking in the Linux kernel, including routing, network interfaces, tunnels, traffic control, and network-related device drivers.</p><p><img src="/images/2024/01/008.jpg" alt></p><p><img src="/images/2024/01/006.jpg" alt></p><p><img src="/images/2024/01/007.jpg" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.wikiwand.com/en/Iproute2" target="_blank" rel="noopener">https://www.wikiwand.com/en/Iproute2</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about Live migration with SR-IOV pass-through by Weidong Han on KVM Forum 2015.
    
    </summary>
    
      <category term="live migration" scheme="http://liujunming.github.io/categories/live-migration/"/>
    
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
  </entry>
  
  <entry>
    <title>notes:Live migration with pass-through device for Linux VM</title>
    <link href="http://liujunming.github.io/2024/01/21/notes-Live-migration-with-pass-through-device-for-Linux-VM/"/>
    <id>http://liujunming.github.io/2024/01/21/notes-Live-migration-with-pass-through-device-for-Linux-VM/</id>
    <published>2024-01-21T04:29:09.000Z</published>
    <updated>2024-01-21T04:40:21.839Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/liujunming/paper_reading_notes/issues/54" target="_blank" rel="noopener">Live migration with pass-through device for Linux VM</a><a id="more"></a></p><p><img src="/images/2024/01/004.jpg" alt></p><p>通过Linux bounding机制实现，将直通网络设备和PV设备通过bounding机制绑定为一张网卡，做热迁移时切换到PV设备。</p><p><strong>Bonded Interface</strong>用于将多个网络接口聚合成一个逻辑上的”bonded”接口。可用于故障备份或负载均衡等场景。</p><p><img src="/images/2024/01/003.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://calinyara.github.io/technology/2019/08/22/vnet_interface.html" target="_blank" rel="noopener">虚拟网络设备简介</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/liujunming/paper_reading_notes/issues/54&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Live migration with pass-through device for Linux VM&lt;/a&gt;
    
    </summary>
    
      <category term="live migration" scheme="http://liujunming.github.io/categories/live-migration/"/>
    
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
  </entry>
  
  <entry>
    <title>TC Filter Actions</title>
    <link href="http://liujunming.github.io/2024/01/06/TC-Filter-Actions/"/>
    <id>http://liujunming.github.io/2024/01/06/TC-Filter-Actions/</id>
    <published>2024-01-06T13:37:56.000Z</published>
    <updated>2024-01-06T14:37:12.286Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下TC Filter Actions机制的相关notes。<a id="more"></a></p><p>TC框架实现中加入了Filter Actions机制。filter实际作用就是classifier，当数据包匹配到特定的filter之后，可以执行该filter所挂载的actions对数据包进行处理。</p><p>如图:<br><img src="/images/2024/01/002.png" alt></p><p>The tc filter framework provides the infrastructure to another extensible set of tools as well, namely tc actions. As the name suggests, they allow to do things with packets (or associated data). (The list of) Actions are part of a given filter. If it matches, each action it contains is executed in order before returning the classification result.</p><p>在lwn中看到了<a href="https://lwn.net/Articles/879034/" target="_blank" rel="noopener">offload tc action to net device</a>的工作，有机会再细看。</p><hr><p>参考资料:</p><ol><li><a href="https://just4coding.com/2022/08/05/tc/" target="_blank" rel="noopener">Linux流量控制(Traffic Control)介绍</a></li><li><a href="http://linux-ip.net/gl/tc-filters/tc-filters-node2.html" target="_blank" rel="noopener">Filter Actions</a></li><li><a href="https://people.netfilter.org/pablo/netdev0.1/papers/Linux-Traffic-Control-Classifier-Action-Subsystem-Architecture.pdf" target="_blank" rel="noopener">Linux Traffic Control Classifier-Action Subsystem Architecture</a></li><li><a href="https://lwn.net/Articles/879034/" target="_blank" rel="noopener">allow user to offload tc action to net device</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下TC Filter Actions机制的相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ifb(Intermediate Functional Block) in TC</title>
    <link href="http://liujunming.github.io/2024/01/06/Notes-about-ifb-Intermediate-Functional-Block-in-TC/"/>
    <id>http://liujunming.github.io/2024/01/06/Notes-about-ifb-Intermediate-Functional-Block-in-TC/</id>
    <published>2024-01-06T07:23:49.000Z</published>
    <updated>2024-01-06T13:33:25.783Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux TC的ifb(Intermediate Functional Block)相关notes。<br><a id="more"></a></p><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* drivers/net/ifb.c:</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    The purpose of this driver is to provide a device that allows</span></span><br><span class="line"><span class="comment">    for sharing of resources:</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    1) qdiscs/policies that are per device as opposed to system wide.</span></span><br><span class="line"><span class="comment">    ifb allows for a device which can be redirected to thus providing</span></span><br><span class="line"><span class="comment">    an impression of sharing.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    2) Allows for queueing incoming traffic for shaping instead of</span></span><br><span class="line"><span class="comment">    dropping.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    The original concept is based on what is known as the IMQ</span></span><br><span class="line"><span class="comment">    driver initially written by Martin Devera, later rewritten</span></span><br><span class="line"><span class="comment">    by Patrick McHardy and then maintained by Andre Correa.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    You need the tc action  mirror or redirect to feed this device</span></span><br><span class="line"><span class="comment">    packets.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    Authors:    Jamal Hadi Salim (2005)</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><p>从内核的注释中可知，ifb的motivation是为了解决如下两个问题：</p><ol><li>Qdisc的多网卡共享</li><li>对输入方向的流量做队列调度</li></ol><h3 id="Qdisc的多网卡共享"><a href="#Qdisc的多网卡共享" class="headerlink" title="Qdisc的多网卡共享"></a>Qdisc的多网卡共享</h3><p>在多个网卡之间共享一个根Qdisc是ifb实现的一个初衷。如果你有10块网卡，想在这10块网卡上实现相同的流控策略，你需要配置10遍吗？将相同的东西抽出来，实现一个ifb虚拟网卡，然后将这10块网卡的流量全部重定向到这个ifb虚拟网卡上，此时只需要在这个虚拟网卡上配置一个Qdisc就可以了。</p><h3 id="对输入方向的流量做队列调度"><a href="#对输入方向的流量做队列调度" class="headerlink" title="对输入方向的流量做队列调度"></a>对输入方向的流量做队列调度</h3><p>Linux中的QoS分为入口(Ingress)部分和出口(Egress)部分，入口部分主要用于进行入口流量限速(policing)，出口部分主要用于队列调度(queuing scheduling)。</p><p>大多数排队规则(qdisc)都是用于输出方向的，输入方向只有一个排队规则，即ingress qdisc。ingress qdisc本身的功能很有限，但可用于重定向incoming packets。通过Ingress qdisc把输入方向的数据包重定向到虚拟设备ifb，而ifb的输出方向可以配置多种qdisc，就可以达到对输入方向的流量做队列调度的目的。</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>IFB is an alternative to tc filters for handling ingress traffic, by redirecting it to a virtual interface and treat is as egress traffic there.You need one ifb interface per physical interface, to redirect ingress traffic from eth0 to ifb0, eth1 to ifb1 and so on.</p><p>When inserting the ifb module, tell it the number of virtual interfaces you need. The default is 2:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe ifb numifbs=1</span><br></pre></td></tr></table></figure></p><p>Now, enable all ifb interfaces:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip link set dev ifb0 up # repeat for ifb1, ifb2, ...</span><br></pre></td></tr></table></figure></p><p>And redirect ingress traffic from the physical interfaces to corresponding ifb interface. For eth0 -&gt; ifb0:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc qdisc add dev eth0 handle ffff: ingress</span><br><span class="line">tc filter add dev eth0 parent ffff: protocol ip u32 match u32 0 0 action mirred egress redirect dev ifb0</span><br></pre></td></tr></table></figure></p><p>Again, repeat for eth1 -&gt; ifb1, eth2 -&gt; ifb2 and so on, until all the interfaces you want to shape are covered.</p><p>Now, you can apply all the rules you want. Egress rules for eth0 go as usual in eth0. Let’s limit bandwidth, for example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tc qdisc add dev eth0 root handle 1: htb default 10</span><br><span class="line">tc class add dev eth0 parent 1: classid 1:1 htb rate 1mbit</span><br><span class="line">tc class add dev eth0 parent 1:1 classid 1:10 htb rate 1mbit</span><br></pre></td></tr></table></figure></p><p>Needless to say, repeat for eth1, eth2, …</p><p>Ingress rules for eth0, now go as egress rules on ifb0 (whatever goes into ifb0 must come out, and only eth0 ingress traffic goes into ifb0). Again, a bandwidth limit example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tc qdisc add dev ifb0 root handle 1: htb default 10</span><br><span class="line">tc class add dev ifb0 parent 1: classid 1:1 htb rate 1mbit</span><br><span class="line">tc class add dev ifb0 parent 1:1 classid 1:10 htb rate 1mbit</span><br></pre></td></tr></table></figure></p><p>The advantage of this approach is that egress rules are much more flexible than ingress filters. Ingress filters only allow you to drop packets, not introduce wait times, for example. By handling ingress traffic as egress you can setup queue disciplines, with traffic classes and, if need be, filters. You get access to the whole tc tree, not only simple filters.</p><hr><p>参考资料:</p><ol><li><a href="https://man7.org/linux/man-pages/man8/tc-mirred.8.html" target="_blank" rel="noopener">man tc-mirred</a></li><li><a href="https://blog.csdn.net/dog250/article/details/40680765" target="_blank" rel="noopener">Linux TC的ifb原理以及ingress流控</a></li><li><a href="http://linux-ip.net/gl/tc-filters/tc-filters-node3.html" target="_blank" rel="noopener">Intermediate Functional Block</a></li><li><a href="https://blog.csdn.net/eydwyz/article/details/53392227" target="_blank" rel="noopener">输入方向的流量控制 –ifb</a></li><li><a href="https://serverfault.com/questions/350023/tc-ingress-policing-and-ifb-mirroring" target="_blank" rel="noopener">Tc: ingress policing and ifb mirroring</a></li><li><a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#ifb" target="_blank" rel="noopener">Introduction to Linux interfaces for virtual networking</a></li><li><a href="https://wiki.linuxfoundation.org/networking/ifb" target="_blank" rel="noopener">Linux Foundation wiki on IFB</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux TC的ifb(Intermediate Functional Block)相关notes。&lt;br&gt;
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Basic knowledge of linux Traffic Control(TC)</title>
    <link href="http://liujunming.github.io/2024/01/06/Basic-knowledge-of-linux-Traffic-Control-TC/"/>
    <id>http://liujunming.github.io/2024/01/06/Basic-knowledge-of-linux-Traffic-Control-TC/</id>
    <published>2024-01-06T02:37:26.000Z</published>
    <updated>2024-01-06T07:15:09.115Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux Traffic Control(TC)机制的相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>流量控制Traffic Control简称TC，表示网络设备接收和发送数据包的排队机制。比如，数据包的接收速率、发送速率、多个数据包的发送顺序等。</p><p>Linux实现了流量控制子系统，它包括两部分：</p><ul><li>内核部分的traffic control框架</li><li>用户态的规则配置工具：iproute2软件包中的tc程序</li></ul><p>它们有些类似于内核态的netfilter框架和用户态的iptables程序。</p><p>Traffic Control的作用包括以下几种:</p><ul><li>调整(Shaping): 通过推迟数据包发送来控制发送速率，只用于网络出方向(egress)</li><li>时序(Scheduling)：调度不同类型数据包发送顺序，比如在交互流量和批量下载类型数据包之间进行发送顺序的调整。只用于网络出方向(egress)</li><li>监督(Policing): 根据到达速率决策接收还是丢弃数据包，用于网络入方向(ingress)</li><li>丢弃(Dropping): 根据带宽丢弃数据包，可以用于出入两个方向</li></ul><h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><h3 id="qdisc"><a href="#qdisc" class="headerlink" title="qdisc"></a>qdisc</h3><p>Simply put, a qdisc is a scheduler. Every output interface needs a scheduler of some kind, and the default scheduler is a FIFO. Other qdiscs available under Linux will rearrange the packets entering the scheduler’s queue in accordance with that scheduler’s rules.</p><p>The qdisc is the major building block on which all of Linux traffic control is built, and is also called a <strong>queuing discipline</strong>.</p><p>The classful qdiscs can contain classes, and provide a handle to which to attach filters.</p><p>The classless qdiscs can contain no classes, nor is it possible to attach filter to a classless qdisc.</p><p>要实现对数据包接收和发送的这些控制行为，需要使用队列结构来临时保存数据包。在Linux实现中，把这种包括数据结构和算法实现的控制机制抽象为结构队列规程:Queuing discipline，简称为qdisc。qdisc对外暴露两个回调接口enqueue和dequeue分别用于数据包入队和数据包出队，而具体的排队算法实现则在qdisc内部隐藏。</p><p>A qdisc has two operations:</p><ul><li>enqueue requests so that a packet can be queued up for later transmission</li><li>dequeue requests so that one of the queued-up packets can be chosen for immediate transmission</li></ul><h3 id="class"><a href="#class" class="headerlink" title="class"></a>class</h3><p>Classes only exist inside a classful qdisc (e.g., HTB and CBQ). Classes are immensely flexible and can always contain either multiple children classes or a single child qdisc. </p><p>Any class can also have an arbitrary number of filters attached to it, which allows the selection of a child class or the use of a filter to reclassify or drop traffic entering a particular class.</p><p>A leaf class is a terminal class in a qdisc. It contains a qdisc (default FIFO) and will never contain a child class. Any class which contains a child class is an inner class (or root class) and not a leaf class.</p><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>A filter is used by a classful qdisc to determine in which class a packet will be enqueued.</p><h2 id="Full-picture"><a href="#Full-picture" class="headerlink" title="Full picture"></a>Full picture</h2><p>基于qdisc, class和filter三种元素可以构建出非常复杂的树形qdisc结构，极大扩展流量控制的能力。</p><p>对于树形结构的qdisc, 当数据包流至最顶层qdisc时，会层层向下递归进行调用。如，父对象(qdisc/class)的enqueue回调接口被调用时，其上所挂载的所有filter依次被调用，直到一个filter匹配成功。然后将数据包入队到filter所指向的class，具体实现则是调用class所配置的Qdisc的enqueue函数。没有成功匹配filter的数据包分类到默认的class中。</p><p>如图:<br><img src="/images/2024/01/001.png" alt></p><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><h3 id="handle"><a href="#handle" class="headerlink" title="handle"></a>handle</h3><p>Every class and classful qdisc requires a unique identifier within the traffic control structure. This unique identifier is known as a handle and has two constituent members, a major number and a minor number. These numbers can be assigned arbitrarily by the user in accordance with the following rules.</p><p><strong>The numbering of handles for classes and qdiscs</strong></p><ul><li><p>major<br>This parameter is completely free of meaning to the kernel. The user may use an arbitrary numbering scheme, however all objects in the traffic control structure with the same parent must share a major handle number. Conventional numbering schemes start at 1 for objects attached directly to the root qdisc.</p></li><li><p>minor<br>This parameter unambiguously identifies the object as a qdisc if minor is 0. Any other value identifies the object as a class. All classes sharing a parent must have unique minor numbers.</p></li></ul><p>The special handle ffff:0 is reserved for the ingress qdisc.</p><p>The handle is used as the target in <strong>classid</strong> and <strong>flowid</strong> phrases of tc filter statements. These handles are external identifiers for the objects, usable by userland applications. The kernel maintains internal identifiers for each object.</p><h3 id="man"><a href="#man" class="headerlink" title="man"></a>man</h3><p>可以查询<a href="https://man7.org/linux/man-pages/man8/tc.8.html" target="_blank" rel="noopener">man tc</a>、<a href="https://man7.org/linux/man-pages/man8/tc-u32.8.html" target="_blank" rel="noopener">man tc-u32</a>、<a href="https://man7.org/linux/man-pages/man8/tc-htb.8.html" target="_blank" rel="noopener">man tc-htb</a>等man手册。</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>As a simple example, in order to limit bandwidth of individual IP addresses stored in <code>CLIENT_IP</code> shell variable, with limitations like the following:</p><ul><li>device name = eth0</li><li>total bandwidth available/allowed for the device = 1000kbps up to 1500kbps</li><li>default bandwidth (for clients that do not fall into our filters) = 1kbps up to 2kbps</li><li>bandwidth of <code>CLIENT_IP</code> = 100kbps</li><li>Maximum bandwidth of <code>CLIENT_IP</code> (if there is more bandwidth available) = 200kbps</li></ul><p>Commands below would suffice:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tc qdisc add dev eth0 root handle 1: htb default 10</span><br><span class="line">tc class add dev eth0 parent 1: classid 1:1 htb rate 1000kbps ceil 1500kbps </span><br><span class="line">tc class add dev eth0 parent 1:1 classid 1:10 htb rate 1kbps ceil 2kbps</span><br><span class="line">tc class add dev eth0 parent 1:1 classid 1:11 htb rate 100kbps ceil 200kbps</span><br><span class="line">tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src $&#123;CLIENT_IP&#125; flowid 1:11</span><br></pre></td></tr></table></figure></p><hr><p>参考资料:</p><ol><li><a href="https://man7.org/linux/man-pages/man8/tc.8.html" target="_blank" rel="noopener">man tc</a></li><li><a href="https://serverfault.com/questions/174010/limit-network-bandwith-for-an-ip" target="_blank" rel="noopener">limit network bandwith for an ip</a></li><li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/linux-traffic-control_configuring-and-managing-networking" target="_blank" rel="noopener">Linux traffic control</a></li><li><a href="https://just4coding.com/2022/08/05/tc/" target="_blank" rel="noopener">Linux流量控制(Traffic Control)介绍</a></li><li><a href="https://zhuanlan.zhihu.com/p/627042688" target="_blank" rel="noopener">利用 Linux tc (traffic control) 进行egress, ingress的网络流量管控</a></li><li><a href="https://tldp.org/HOWTO/Traffic-Control-HOWTO/" target="_blank" rel="noopener">Traffic Control HOWTO</a></li><li><a href="https://github.com/tonydeng/sdn-handbook/blob/master/linux/tc.md" target="_blank" rel="noopener">Linux的流量控制文档</a></li><li><a href="http://linux-ip.net/gl/tc-filters/tc-filters.html" target="_blank" rel="noopener">QoS in Linux with TC and Filters</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux Traffic Control(TC)机制的相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about ingress and egress  in network</title>
    <link href="http://liujunming.github.io/2023/12/30/Notes-about-ingress-and-egress-in-network/"/>
    <id>http://liujunming.github.io/2023/12/30/Notes-about-ingress-and-egress-in-network/</id>
    <published>2023-12-30T10:07:02.000Z</published>
    <updated>2023-12-30T10:09:07.056Z</updated>
    
    <content type="html"><![CDATA[<p>The answer from quora:</p><p>Network ingress and egress are terms used in networking to describe the direction of network traffic. In general, ingress refers to network traffic that enters a network or a device, while egress refers to network traffic that exits a network or a device. <a id="more"></a></p><p>For example, when you browse a website, the data packets that are sent from the website’s server to your browser are considered ingress traffic for your device and egress traffic for the server. Conversely, the data packets that are sent from your browser to the website’s server are considered egress traffic for your device and ingress traffic for the server.</p><p><a href="https://www.quora.com/What-are-network-ingress-and-egress" target="_blank" rel="noopener">https://www.quora.com/What-are-network-ingress-and-egress</a></p><p>The answer from chatgpt:</p><p><strong>Network ingress and egress refer to the movement of data into and out of a network</strong>. Ingress: This refers to the <strong>incoming</strong> data traffic that enters a network from an external source, such as the internet or another network. Egress: This refers to the outgoing data traffic that leaves a network to an external destination, such as the internet or another network. In networking, understanding and managing network ingress and egress is important for maintaining the security, performance, and efficiency of a network.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The answer from quora:&lt;/p&gt;
&lt;p&gt;Network ingress and egress are terms used in networking to describe the direction of network traffic. In general, ingress refers to network traffic that enters a network or a device, while egress refers to network traffic that exits a network or a device.
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Standardizing Live Migration with NVM Express</title>
    <link href="http://liujunming.github.io/2023/12/03/Notes-about-Standardizing-Live-Migration-with-NVM-Express/"/>
    <id>http://liujunming.github.io/2023/12/03/Notes-about-Standardizing-Live-Migration-with-NVM-Express/</id>
    <published>2023-12-03T12:06:55.000Z</published>
    <updated>2023-12-03T14:03:01.465Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要是mark下<a href="https://drive.google.com/file/d/1obwZNWd89MPfrZWSsj24ub0N7JHJZ6FU/view?usp=drive_link" target="_blank" rel="noopener">Standardizing Live Migration with NVM Express®</a>相关notes，相关细节可以参考原文。<a id="more"></a></p><p><img src="/images/2023/12/005.jpg" alt></p><p>slides中有如下描述：</p><blockquote><p>Host may use a new mechanism to throttle commands processing by migrating controller to slow down changes</p></blockquote><p>其对应的是:</p><blockquote><p>Support limit the BW and IOPS of a controller to allow slowing down of command processing on a migrating controller</p></blockquote><p>这是QoS的相关实现，考虑写磁盘多的workload，不限速的话，最后一轮的脏LBAs可能会很多，downtime就会有些大了。</p><p>原文考虑了本地盘与非本地盘的NVMe Live Migration。</p><p>对于本地盘的情况，需要记录脏的LBAs，在热迁移每轮迭代中，会传输脏的LBAs(类似于热迁移的脏页传输)。</p><p>对于非本地盘的情况，其实就无效考虑脏的LBAs了。</p><p>对于IPU/DPU的NVMe Live Migration，详情可以参考<a href="https://mp.weixin.qq.com/s/GnN06H864XuXU41-jFH4jA" target="_blank" rel="noopener">NVMe VFIO Live Migration for IPU/DPU Devices</a>。</p><p><img src="/images/2023/12/003.jpg" alt></p><p><img src="/images/2023/12/004.jpg" alt><br>值得注意的是，如果host上的IOMMU支持DMA脏页记录的话，就无需NVMe Device自己去记录DMA脏页了。</p><hr><p>参考资料:</p><ol><li><a href="https://nvmexpress.org/wp-content/uploads/FMS-2023-Host-Controlled-Live-Migration.pdf" target="_blank" rel="noopener">FMS 2023 Host Controlled Live Migration</a></li><li><a href="https://www.bilibili.com/video/BV19N4y1S74F/" target="_blank" rel="noopener">Standardizing Live Migration with NVM Express®</a></li><li><a href="https://www.opencompute.org/events/past-events/2023-ocp-global-summit" target="_blank" rel="noopener">2023 OCP Global Summit</a></li><li><a href="https://mp.weixin.qq.com/s/GnN06H864XuXU41-jFH4jA" target="_blank" rel="noopener">NVMe VFIO Live Migration for IPU/DPU Devices</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要是mark下&lt;a href=&quot;https://drive.google.com/file/d/1obwZNWd89MPfrZWSsj24ub0N7JHJZ6FU/view?usp=drive_link&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Standardizing Live Migration with NVM Express®&lt;/a&gt;相关notes，相关细节可以参考原文。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="live migration" scheme="http://liujunming.github.io/tags/live-migration/"/>
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
      <category term="存储" scheme="http://liujunming.github.io/tags/%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVMe Namespaces</title>
    <link href="http://liujunming.github.io/2023/12/02/Notes-about-NVMe-Namespaces/"/>
    <id>http://liujunming.github.io/2023/12/02/Notes-about-NVMe-Namespaces/</id>
    <published>2023-12-02T12:20:04.000Z</published>
    <updated>2024-01-06T07:24:22.343Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下NVMe Namespaces相关notes。<a id="more"></a></p><ul><li>What is the meaning of nvme0n1 nvme0n2 nvme1n1 nvme1n2 in Linux?<ul><li>nvmeXnY: X means controller, Y means namespace</li></ul></li><li>NVMe controller<ul><li>A PCI Express function that implements the NVM Express interface</li></ul></li><li>NVMe namespace<ul><li>A namespace is a collection of logical block addresses (LBA) accessible to host software. A namespace ID (NSID) is an identifier used by a controller to provide access to a namespace. A namespace is not the physical isolation of blocks, rather the isolation of logical blocks addressable by the host software</li></ul></li><li>A NVM Express controller may support multiple namespaces that are referenced using a namespace ID</li></ul><p><img src="/images/2023/12/001.jpg" alt></p><p>无需理解LUNs。</p><p><img src="/images/2023/12/002.jpg" alt><br>无需理解vSAN。</p><hr><p>参考资料:</p><ol><li><a href="https://www.snia.org/sites/default/files/SDCEMEA/2020/4%20-%20Or%20Lapid%20Micron%20-%20Understanding%20NVMe%20namespaces%20-%20Final.pdf" target="_blank" rel="noopener">NVMe™ Namespaces:Micron Storage Solutions Engineering</a></li><li><a href="https://nvmexpress.org/resource/nvme-namespaces/" target="_blank" rel="noopener">NVMe Namespaces</a></li><li><a href="https://narasimhan-v.github.io/2020/06/12/Managing-NVMe-Namespaces.html" target="_blank" rel="noopener">Managing Nvme Namespaces</a></li><li><a href="https://unix.stackexchange.com/questions/520231/what-are-nvme-namespaces-how-do-they-work" target="_blank" rel="noopener">What are nvme namespaces? How do they work?</a></li><li><a href="https://www.flashmemorysummit.com/English/Collaterals/Proceedings/2013/20130812_PreConfD_Marks.pdf" target="_blank" rel="noopener">An NVM Express Tutorial</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下NVMe Namespaces相关notes。
    
    </summary>
    
      <category term="NVMe" scheme="http://liujunming.github.io/categories/NVMe/"/>
    
    
      <category term="NVMe" scheme="http://liujunming.github.io/tags/NVMe/"/>
    
      <category term="存储" scheme="http://liujunming.github.io/tags/%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Notes about virtio-net configuration changes</title>
    <link href="http://liujunming.github.io/2023/10/28/Notes-about-virtio-net-configuration-changes/"/>
    <id>http://liujunming.github.io/2023/10/28/Notes-about-virtio-net-configuration-changes/</id>
    <published>2023-10-28T07:15:59.000Z</published>
    <updated>2023-10-28T08:26:01.702Z</updated>
    
    <content type="html"><![CDATA[<p>参考<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html#x1-2230004" target="_blank" rel="noopener">virtio 1.2 spec</a>，Linux kernel version <a href="https://elixir.bootlin.com/linux/v6.0/source" target="_blank" rel="noopener">v6.0</a>。<a id="more"></a></p><ul><li><em>speed</em> contains the device speed, in units of 1 MBit per second, 0 to 0x7fffffff, or 0xffffffff for unknown speed.</li><li><em>duplex</em> has the values of 0x01 for full duplex, 0x00 for half duplex and 0xff for unknown duplex state.</li></ul><p>Both <em>speed</em> and <em>duplex</em> can change, thus the driver is expected to re-read these values after receiving a configuration change notification.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">virtio_net_config</span> &#123;</span> </span><br><span class="line">        u8 mac[<span class="number">6</span>]; </span><br><span class="line">        le16 status; </span><br><span class="line">        le16 max_virtqueue_pairs; </span><br><span class="line">        le16 mtu; </span><br><span class="line">        le32 speed; </span><br><span class="line">        u8 duplex; </span><br><span class="line">        u8 rss_max_key_size; </span><br><span class="line">        le16 rss_max_indirection_table_length; </span><br><span class="line">        le32 supported_hash_types; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Linux kernel source code:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">virtnet_config_changed_work</span><span class="params">(struct work_struct *work)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">virtnet_info</span> *<span class="title">vi</span> =</span></span><br><span class="line"><span class="class">        <span class="title">container_of</span>(<span class="title">work</span>, <span class="title">struct</span> <span class="title">virtnet_info</span>, <span class="title">config_work</span>);</span></span><br><span class="line">    u16 v;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (virtio_cread_feature(vi-&gt;vdev, VIRTIO_NET_F_STATUS,</span><br><span class="line">                 struct virtio_net_config, status, &amp;v) &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (v &amp; VIRTIO_NET_S_ANNOUNCE) &#123;</span><br><span class="line">        netdev_notify_peers(vi-&gt;dev);</span><br><span class="line">        virtnet_ack_link_announce(vi);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Ignore unknown (future) status bits */</span></span><br><span class="line">    v &amp;= VIRTIO_NET_S_LINK_UP;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (vi-&gt;status == v)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    vi-&gt;status = v;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (vi-&gt;status &amp; VIRTIO_NET_S_LINK_UP) &#123;</span><br><span class="line">        virtnet_update_settings(vi);</span><br><span class="line">        netif_carrier_on(vi-&gt;dev);</span><br><span class="line">        netif_tx_wake_all_queues(vi-&gt;dev);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        netif_carrier_off(vi-&gt;dev);</span><br><span class="line">        netif_tx_stop_all_queues(vi-&gt;dev);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">virtnet_update_settings</span><span class="params">(struct virtnet_info *vi)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    u32 speed;</span><br><span class="line">    u8 duplex;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!virtio_has_feature(vi-&gt;vdev, VIRTIO_NET_F_SPEED_DUPLEX))</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    virtio_cread_le(vi-&gt;vdev, struct virtio_net_config, speed, &amp;speed);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ethtool_validate_speed(speed))</span><br><span class="line">        vi-&gt;speed = speed;</span><br><span class="line"></span><br><span class="line">    virtio_cread_le(vi-&gt;vdev, struct virtio_net_config, duplex, &amp;duplex);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ethtool_validate_duplex(duplex))</span><br><span class="line">        vi-&gt;duplex = duplex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考&lt;a href=&quot;https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html#x1-2230004&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;virtio 1.2 spec&lt;/a&gt;，Linux kernel version &lt;a href=&quot;https://elixir.bootlin.com/linux/v6.0/source&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;v6.0&lt;/a&gt;。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about XDP</title>
    <link href="http://liujunming.github.io/2023/10/22/Notes-about-XDP/"/>
    <id>http://liujunming.github.io/2023/10/22/Notes-about-XDP/</id>
    <published>2023-10-22T06:14:58.000Z</published>
    <updated>2023-10-22T11:12:06.844Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下eXpress Data Path (XDP)相关notes。<a id="more"></a></p><h3 id="What"><a href="#What" class="headerlink" title="What"></a>What</h3><p>XDP其实是位于网卡驱动程序里的一个快速处理数据包的HOOK点，为什么快？因为数据包处理位置非常底层，避开了很多内核skb处理开销。</p><p>XDP暴露了一个可以加载BPF程序的网络钩子。在这个钩子中，程序能够对传入的数据包进行任意修改和快速决策，避免了内核内部处理带来的额外开销。这使得XDP在性能速度方面成为最佳钩子，例如缓解DDoS攻击等。 </p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p><img src="/images/2023/11/016.jpg" alt></p><p><img src="/images/2023/11/015.jpg" alt></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><img src="/images/2023/11/014.jpg" alt></p><p><img src="/images/2023/11/017.jpg" alt></p><h3 id="Introduction-to-eBPF-and-XDP"><a href="#Introduction-to-eBPF-and-XDP" class="headerlink" title="Introduction to eBPF and XDP"></a>Introduction to eBPF and XDP</h3><ul><li><a href="https://www.bilibili.com/video/BV1qq4y1r7uB/" target="_blank" rel="noopener">B站视频</a></li><li><a href="https://www.slideshare.net/lcplcp1/introduction-to-ebpf-and-xdp" target="_blank" rel="noopener">slides</a></li></ul><p>建议阅读上述资料，会对XDP有不错的认识。</p><p><img src="/images/2023/11/005.jpg" alt></p><p>以DDoS为例:</p><p><img src="/images/2023/11/006.jpg" alt></p><p><img src="/images/2023/11/007.jpg" alt></p><p><img src="/images/2023/11/008.jpg" alt></p><p><img src="/images/2023/11/009.jpg" alt></p><p>The XDP program is executed at the earliest possible moment after a packet is received from the hardware, before the kernel allocates its per-packet <code>sk_buff</code> data structure.</p><p><img src="/images/2023/11/010.jpg" alt></p><p><img src="/images/2023/11/011.jpg" alt></p><p>代码层的理解:<br><img src="/images/2023/11/001.jpg" alt></p><p><img src="/images/2023/11/002.jpg" alt></p><p><img src="/images/2023/11/003.jpg" alt></p><p><img src="/images/2023/11/004.jpg" alt></p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/images/2023/11/012.jpg" alt></p><h3 id="Execution-flow-of-a-typical-XDP-program"><a href="#Execution-flow-of-a-typical-XDP-program" class="headerlink" title="Execution flow of a typical XDP program"></a>Execution flow of a typical XDP program</h3><p><img src="/images/2023/11/013.jpg" alt></p><p>详情参考<a href="https://github.com/tohojo/xdp-paper/blob/master/xdp-the-express-data-path.pdf" target="_blank" rel="noopener">xdp paper</a>3.1 The XDP Driver Hook。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>建议阅读<a href="http://arthurchiao.art/blog/xdp-paper-acm-2018-zh/" target="_blank" rel="noopener">[译] [论文] XDP (eXpress Data Path)：在操作系统内核中实现快速、可编程包处理（ACM，2018）</a>。</p><p>某种意义上来说，XDP 可以认为是一种 offload 方式：</p><ol><li>性能敏感的处理逻辑下放到网卡驱动中，以提升性能；</li><li>其他的处理逻辑仍然走内核网络栈；</li><li>如果没有用到内核 helper 函数，那整个 XDP 程序都可以 offload 到网卡（目前 Netronome smart-NICs已经支持）。</li></ol><hr><p>参考资料:</p><ol><li><a href="https://github.com/tohojo/xdp-paper/blob/master/xdp-the-express-data-path.pdf" target="_blank" rel="noopener">xdp paper</a></li><li><a href="https://github.com/tohojo/xdp-paper/blob/master/xdp-presentation.pdf" target="_blank" rel="noopener">xdp slides</a></li><li><a href="http://arthurchiao.art/blog/xdp-paper-acm-2018-zh/" target="_blank" rel="noopener">[译] [论文] XDP (eXpress Data Path)：在操作系统内核中实现快速、可编程包处理（ACM，2018）</a></li><li><a href="https://mp.weixin.qq.com/s/BqXhOlRisvNXETRj-TehUQ" target="_blank" rel="noopener">初识XDP</a></li><li><a href="https://mp.weixin.qq.com/s/qlgdIAGGv7yQXFGlGA5I8Q" target="_blank" rel="noopener">实现一个基于XDP_eBPF的学习型网桥</a></li><li><a href="https://www.youtube.com/watch?v=arq5XzodNmY" target="_blank" rel="noopener">Introduction to eBPF and XDP</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下eXpress Data Path (XDP)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>PCI Express Max Read Request, Max Payload Size and why you care</title>
    <link href="http://liujunming.github.io/2023/10/06/PCI-Express-Max-Read-Request-Max-Payload-Size-and-why-you-care/"/>
    <id>http://liujunming.github.io/2023/10/06/PCI-Express-Max-Read-Request-Max-Payload-Size-and-why-you-care/</id>
    <published>2023-10-05T22:16:38.000Z</published>
    <updated>2023-10-06T08:26:40.127Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自:<a href="https://codywu2010.wordpress.com/2015/11/26/pci-express-max-read-request-max-payload-size-and-why-you-care/" target="_blank" rel="noopener">PCI Express Max Read Request, Max Payload Size and why you care</a><a id="more"></a></p><p>Modern high performance server is nearly all based on PCIE architecture and technologies derived from it such as Direct Media Interface (DMI) or Quick Path Interconnect (QPI).</p><p>For example below is a sample block diagram for a dual processor system:</p><p><img src="/images/2023/10/020.png" alt></p><p>A PCI Express system consists of many components, most important of which to us are:</p><ul><li>CPU</li><li>Root Complex (Root Port)</li><li>PCIE Switch</li><li>End Point</li></ul><p>Root Complex acts as the agent which helps with:</p><ul><li>Receive CPU request to initiate Memory/IO read/write towards end point</li><li>Receive End Point read/write request and either pass it to another end point or access system memory on their behalf</li></ul><p>The End point is usually of most interest to us because that’s where we put our high performance device.</p><p>It is GPU in the sample block diagram while in real time it can be a high speed Ethernet card or data collecting/processing card, or an infiniband card talking to some storage device in a large data center.</p><p>Below is a refined block diagram that amplify the interconnection of those components:</p><p><img src="/images/2023/10/021.png" alt></p><p>Based on this topology let’s talk about a typical scenario where Remote Direct Memory Access (RDMA) is used to allow a end point PCIE device to write directly to a pre-allocated system memory whenever data arrives, which offload to the maximum any involvements of CPU.</p><p>So the device will initiate a write request with data and send it along hoping root complex will help it get the data into system memory.</p><p>PCIE, different from traditional PCI or PCI-X, bases its communication traffic on the concepts of packets flying over point-to-point serial link, which is sometimes why people mention PCIE as a sort of tiny network topology.</p><p>So the RDMA device, acting as requester, sends its request package bearing the data along the link towards root complex.</p><p>The packet will arrive at intermediary PCIE switch and forward to root complex and root complex will diligently move data in the payload to system memory through its private memory controller.</p><p>Of course we would expect some overhead besides pure data payload and here goes the packet structure of PICE gen3:</p><p><img src="/images/2023/10/022.png" alt></p><p>So obviously given those additional “tax” you have to pay you would hope that you can put as large a payload as you can which would hopefully increase the effective utilization ratio.</p><p>However it does not always work and here comes to our discussion about <strong>“max payload size”</strong>.</p><p>Each device has a <strong>“max payload size supported”</strong> in its dev cap config register part indicating its capability and a “max payload size” in its dev control register part which will be programmed with actual <strong>“max playload set”</strong> it can use.</p><p>Below shows the related registers extracted from pcie base spec:</p><p><img src="/images/2023/10/018.jpg" alt></p><p><img src="/images/2023/10/019.jpg" alt></p><p>So how do we decide on what value to set within the range not above max payload supported?</p><p>The idea is it has to be equal to the minimum max payload supported along the route.</p><p>So for our data write request it would have to consider end point’s max payload supported as well as pcie switch (which is abstracted as pcie device while we do enumeration) and root complex’s root port (which is also abstracted as a device).</p><p>PCIE base spec actually described it this way without giving detailed implementation:</p><p><img src="/images/2023/10/023.png" alt></p><p>Now let’s take a look at how linux does it.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">pcie_write_mps</span><span class="params">(struct pci_dev *dev, <span class="keyword">int</span> mps)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> rc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pcie_bus_config == PCIE_BUS_PERFORMANCE) &#123;</span><br><span class="line">        mps = <span class="number">128</span> &lt;&lt; dev-&gt;pcie_mpss;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (pci_pcie_type(dev) != PCI_EXP_TYPE_ROOT_PORT &amp;&amp;</span><br><span class="line">            dev-&gt;bus-&gt;self)</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * For "Performance", the assumption is made that</span></span><br><span class="line"><span class="comment">             * downstream communication will never be larger than</span></span><br><span class="line"><span class="comment">             * the MRRS.  So, the MPS only needs to be configured</span></span><br><span class="line"><span class="comment">             * for the upstream communication.  This being the case,</span></span><br><span class="line"><span class="comment">             * walk from the top down and set the MPS of the child</span></span><br><span class="line"><span class="comment">             * to that of the parent bus.</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * Configure the device MPS with the smaller of the</span></span><br><span class="line"><span class="comment">             * device MPSS or the bridge MPS (which is assumed to be</span></span><br><span class="line"><span class="comment">             * properly configured at this point to the largest</span></span><br><span class="line"><span class="comment">             * allowable MPS based on its parent bus).</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            mps = min(mps, pcie_get_mps(dev-&gt;bus-&gt;self));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    rc = pcie_set_mps(dev, mps);</span><br><span class="line">    <span class="keyword">if</span> (rc)</span><br><span class="line">        pci_err(dev, <span class="string">"Failed attempting to set the MPS\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So linux follows the same idea and take the minimum of upstream device capability and downstream pci device.</p><p>The only exception is for root port which is supposed to be the top of PCI hierarchy so we can simply set by its max supported.</p><p><code>pcie_set_mps</code> does real setting of the config register and it can be seen that it is taking the min.</p><p>Now we have finished talking about <strong>max payload size</strong>, let’s turn our attention to <strong>max read request size</strong>.</p><p>It does not apply to memory write request but it applies to memory read request by that you cannot request more than that size in a single memory request.</p><p>We can imagine a slightly different use case where some application prepares a block of data to be processed by the end point device and then we notifying the device of the memory address of size and ask the device to take over.</p><p>The device will have to initiate a series of memory read request to fetch the data and process in place on the card and put the result int some preset location.</p><p>So even though packet payload can go at max to 4096 bytes the device will have to work in trickle like way if we program its max read request to be a very small value.</p><p>Here is the explanation from PCIE base spec on max read request:</p><p><img src="/images/2023/10/024.png" alt></p><p><img src="/images/2023/10/025.png" alt></p><p>So again let’s say how linux programs <strong>max read request size</strong>:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">pcie_write_mrrs</span><span class="params">(struct pci_dev *dev)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> rc, mrrs;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * In the "safe" case, do not configure the MRRS.  There appear to be</span></span><br><span class="line"><span class="comment">     * issues with setting MRRS to 0 on a number of devices.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">if</span> (pcie_bus_config != PCIE_BUS_PERFORMANCE)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * For max performance, the MRRS must be set to the largest supported</span></span><br><span class="line"><span class="comment">     * value.  However, it cannot be configured larger than the MPS the</span></span><br><span class="line"><span class="comment">     * device or the bus can support.  This should already be properly</span></span><br><span class="line"><span class="comment">     * configured by a prior call to pcie_write_mps().</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    mrrs = pcie_get_mps(dev);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * MRRS is a R/W register.  Invalid values can be written, but a</span></span><br><span class="line"><span class="comment">     * subsequent read will verify if the value is acceptable or not.</span></span><br><span class="line"><span class="comment">     * If the MRRS value provided is not acceptable (e.g., too large),</span></span><br><span class="line"><span class="comment">     * shrink the value until it is acceptable to the HW.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">while</span> (mrrs != pcie_get_readrq(dev) &amp;&amp; mrrs &gt;= <span class="number">128</span>) &#123;</span><br><span class="line">        rc = pcie_set_readrq(dev, mrrs);</span><br><span class="line">        <span class="keyword">if</span> (!rc)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">        pci_warn(dev, <span class="string">"Failed attempting to set the MRRS\n"</span>);</span><br><span class="line">        mrrs /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mrrs &lt; <span class="number">128</span>)</span><br><span class="line">        pci_err(dev, <span class="string">"MRRS was unable to be configured with a safe value.  If problems are experienced, try running with pci=pcie_bus_safe\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>pcie_set_readrq</code> does the real setting and surprisingly it uses <strong>max payload size</strong> as the ceiling even though it has not relationship with that.</p><p>We can well send a large read request but when data is returned from root complex it will be split into many small packets each with payload size less or equal to max payload size.</p><p>So above code is mainly executed in PCI bus enumeration phase.</p><p>And if we grep with this function name <code>pcie_set_readrq</code> we can see other device drivers provide overrides probably to increase the read request efficiency.</p><p>So how big an impact the two settings has on your specific device?</p><p>It’s hard to tell though you can easily find on the internet discussions talking about it.</p><p>Here is a good one <a href="http://www.xilinx.com/support/documentation/white_papers/wp350.pdf" target="_blank" rel="noopener">Understanding Performance of PCI Express Systems</a>.</p><p>And here is another good one <a href="https://billauer.co.il/blog/2011/05/pcie-pci-express-linux-max-payload-size-configuration-capabilities-tlp-lspci/" target="_blank" rel="noopener">PCI Express Max Payload size and its impact on Bandwidth</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自:&lt;a href=&quot;https://codywu2010.wordpress.com/2015/11/26/pci-express-max-read-request-max-payload-size-and-why-you-care/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PCI Express Max Read Request, Max Payload Size and why you care&lt;/a&gt;
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
</feed>
