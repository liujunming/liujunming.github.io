<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2022-04-02T02:06:19.459Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Introduction to GPUDirect RDMA</title>
    <link href="http://liujunming.github.io/2022/04/02/Introduction-to-GPUDirect-RDMA/"/>
    <id>http://liujunming.github.io/2022/04/02/Introduction-to-GPUDirect-RDMA/</id>
    <published>2022-04-02T00:16:16.000Z</published>
    <updated>2022-04-02T02:06:19.459Z</updated>
    
    <content type="html"><![CDATA[<p>阅读了<a href="https://developer.aliyun.com/article/603617" target="_blank" rel="noopener">浅析GPU通信技术（下）-GPUDirect RDMA</a> 一文，收获颇丰，故转载到博客中。<a id="more"></a></p><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>当前深度学习模型越来越复杂，计算数据量暴增，对于大规模深度学习训练任务，单机已经无法满足计算要求，多机多卡的分布式训练成为了必要的需求，这个时候多机间的通信成为了分布式训练性能的重要指标。</p><p>本篇文章我们就来谈谈GPUDirect RDMA技术，这是用于加速多机间GPU通信的技术。</p><h2 id="2-RDMA介绍"><a href="#2-RDMA介绍" class="headerlink" title="2. RDMA介绍"></a>2. RDMA介绍</h2><p>我们先来看看RDMA技术是什么？RDMA即Remote DMA，是Remote Direct Memory Access的英文缩写。</p><h3 id="2-1-DMA原理"><a href="#2-1-DMA原理" class="headerlink" title="2.1 DMA原理"></a>2.1 DMA原理</h3><p>在介绍RDMA之前，我们先来复习下DMA技术。</p><p>我们知道DMA（直接内存访问）技术是Offload CPU负载的一项重要技术。DMA的引入，使得原来设备内存与系统内存的数据交换必须要CPU参与，变为交给DMA控制来进行数据传输。</p><p>直接内存访问(DMA)方式，是一种完全由硬件执行I/O交换的工作方式。在这种方式中， DMA控制器从CPU完全接管对总线的控制，数据交换不经过CPU，而直接在内存和IO设备之间进行。DMA工作时，由DMA 控制器向内存发出地址和控制信号，进行地址修改，对传送字的个数计数，并且以中断方式向CPU 报告传送操作的结束。</p><p>使用DMA方式的目的是减少大批量数据传输时CPU 的开销。采用专用DMA控制器(DMAC) 生成访存地址并控制访存过程。优点有操作均由硬件电路实现，传输速度快；CPU 基本不干预，仅在初始化和结束时参与，CPU与外设并行工作，效率高。</p><h3 id="2-2-RMDA原理"><a href="#2-2-RMDA原理" class="headerlink" title="2.2 RMDA原理"></a>2.2 RMDA原理</h3><p>RDMA则是在计算机之间网络数据传输时Offload CPU负载的高吞吐、低延时通信技术。<br><img src="/images/2022/04/07.jpg" alt></p><p>如上图所示，传统的TCP/IP协议，应用程序需要要经过多层复杂的协议栈解析，才能获取到网卡中的数据包，而使用RDMA协议，应用程序可以直接旁路内核获取到网卡中的数据包。</p><p>RDMA可以简单理解为利用相关的硬件和网络技术，服务器1的网卡可以直接读写服务器2的内存，最终达到高带宽、低延迟和低资源利用率的效果。如下图所示，应用程序不需要参与数据传输过程，只需要指定内存读写地址，开启传输并等待传输完成即可。<br><img src="/images/2022/04/08.jpg" alt></p><p>在实现上，RDMA实际上是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过在网卡上将RDMA协议固化于硬件，以及支持零复制网络技术和内核内存旁路技术这两种途径来达到其高性能的远程直接数据存取的目标。</p><ol><li>零复制：零复制网络技术使网卡可以直接与应用内存相互传输数据，从而消除了在应用内存与内核之间复制数据的需要。因此，传输延迟会显著减小。</li><li>内核旁路：内核协议栈旁路技术使应用程序无需执行内核内存调用就可向网卡发送命令。在不需要任何内核内存参与的条件下，RDMA请求从用户空间发送到本地网卡并通过网络发送给远程网卡，这就减少了在处理网络传输流时内核内存空间与用户空间之间环境切换的次数。</li></ol><p>在具体的远程内存读写中，RDMA操作用于读写操作的远程虚拟内存地址包含在RDMA消息中传送，远程应用程序要做的只是在其本地网卡中注册相应的内存缓冲区。远程节点的CPU除在连接建立、注册调用等之外，在整个RDMA数据传输过程中并不提供服务，因此没有带来任何负载。</p><h3 id="2-3-RDMA实现"><a href="#2-3-RDMA实现" class="headerlink" title="2.3 RDMA实现"></a>2.3 RDMA实现</h3><p>如下图RMDA软件栈所示，目前RDMA的实现方式主要分为InfiniBand和Ethernet两种传输网络。而在以太网上，又可以根据与以太网融合的协议栈的差异分为iWARP和RoCE（包括RoCEv1和RoCEv2）。</p><p><img src="/images/2022/04/09.jpg" alt></p><p>其中，InfiniBand是最早实现RDMA的网络协议，被广泛应用到高性能计算中。但是InfiniBand和传统TCP/IP网络的差别非常大，需要专用的硬件设备，承担昂贵的价格。相比之下RoCE和iWARP的硬件成本则要低的多。</p><h2 id="3-GPUDirect-RDMA介绍"><a href="#3-GPUDirect-RDMA介绍" class="headerlink" title="3. GPUDirect RDMA介绍"></a>3. GPUDirect RDMA介绍</h2><h3 id="3-1-原理"><a href="#3-1-原理" class="headerlink" title="3.1 原理"></a>3.1 原理</h3><p>有了前文RDMA的介绍，从下图我们可以很容易明白，所谓GPUDirect RDMA，就是计算机1的GPU可以直接访问计算机2的GPU内存。而在没有这项技术之前，GPU需要先将数据从GPU内存搬移到系统内存，然后再利用RDMA传输到计算机2，计算机2的GPU还要做一次数据从系统内存到GPU内存的搬移动作。GPUDirect RDMA技术进一步减少了GPU通信的数据复制次数，进一步降低了通信延迟。<br><img src="/images/2022/04/10.jpg" alt></p><h3 id="3-2-使用"><a href="#3-2-使用" class="headerlink" title="3.2 使用"></a>3.2 使用</h3><p>需要注意的是，要想使用GPUDirect RDMA，需要保证GPU卡和RDMA网卡在同一个ROOT COMPLEX下，如下图所示：<br><img src="/images/2022/04/11.jpg" alt></p><h3 id="3-3-性能"><a href="#3-3-性能" class="headerlink" title="3.3 性能"></a>3.3 性能</h3><p>Mellanox网卡已经提供了GPUDirect RDMA的支持（既支持InfiniBand传输，也支持RoCE传输）。</p><p>下图分别是使用OSU micro-benchmarks在Mellanox的InfiniBand网卡上测试的延时和带宽数据，可以看到使用GPUDirect RDMA技术后延时大大降低，带宽则大幅提升：<br><img src="/images/2022/04/12.jpg" alt><br><img src="/images/2022/04/13.jpg" alt></p><p>下图是一个实际的高性能计算应用的性能数据（使用HOOMD做粒子动态仿真），可以看到随着节点增多，使用GPUDirect RDMA技术的集群的性能有明显提升，最多可以提升至2倍：<br><img src="/images/2022/04/14.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读了&lt;a href=&quot;https://developer.aliyun.com/article/603617&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;浅析GPU通信技术（下）-GPUDirect RDMA&lt;/a&gt; 一文，收获颇丰，故转载到博客中。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to VT-x Preemption Timer</title>
    <link href="http://liujunming.github.io/2022/04/01/Introduction-to-VT-x-Preemption-Timer/"/>
    <id>http://liujunming.github.io/2022/04/01/Introduction-to-VT-x-Preemption-Timer/</id>
    <published>2022-04-01T09:48:14.000Z</published>
    <updated>2022-04-01T13:08:01.255Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自:<a href="https://blog.csdn.net/xelatex_kvm/article/details/17761415" target="_blank" rel="noopener">Intel VT技术中的Preemption Timer</a>。</p><h3 id="1-什么是Preemption-Timer"><a href="#1-什么是Preemption-Timer" class="headerlink" title="1. 什么是Preemption Timer"></a>1. 什么是Preemption Timer</h3><p>Preemption Timer是一种可以周期性使VCPU触发VMExit的一种机制。即设置了Preemption Timer之后，可以使VCPU在指定的TSC cycle(注意文章最后的rate)之后产生一次VMExit。<a id="more"></a></p><p>使用Preemption Timer时需要注意下面两个问题：</p><ol><li>在旧版本的Intel CPU中Preemption Timer是不精确的。在Intel的设计中，Preemption Timer应该是严格和TSC保持一致，但是在Haswell之前的处理器并不能严格保持一致。</li><li>Preemption Timer只有在VCPU进入到non-root mode时才会开始工作，在VCPU进入VMM时或者VCPU被调度出CPU时，其值都不会变化。</li></ol><h3 id="2-如何使用Preemption-Timer"><a href="#2-如何使用Preemption-Timer" class="headerlink" title="2. 如何使用Preemption Timer"></a>2. 如何使用Preemption Timer</h3><p>Preemption Timer在VMCS中有三个域需要设置：</p><ol><li><strong>Pin-Based VM-Execution Controls</strong>,Bit 6,”Activate VMX preemption timer”: 该位如果设置为1，则打开Preemption Timer；如果为0，则下面两个域的设置均无效。</li><li><strong>VM-Exit Controls</strong>,Bit 22,”Save VMX-preemption timer value”:This control determines whether the value of the VMX-preemption timer is saved on VM exit.</li><li><strong>VMX-preemption timer value</strong>:This field contains the value that the VMX-preemption timer will use following the next VM entry with that setting. 如果设置了”Save VMX-preemption timer value”，那么在VM exit时会更新该域为新的值。</li></ol><p>和Preemption Timer相关的内容去SDM中全文搜索”Preemption Timer”。</p><p>在使用时，需要首先设置” Activate VMX preemption  timer”和 “VMX-preemption timer value”，如果需要VM exit时保存VMX-preemption timer value的话，需要设置 “Save VMX-preemption  timer  value”，这样在VCPU因为其他原因VMExit的时候不会重置VMX-preemption timer value。</p><p>注意：在由Preemption Timer Time-out产生的VMExit中，是需要重置VMX-preemption timer value的。</p><p><img src="/images/2022/04/06.PNG" alt><br>注意下这个rate。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自:&lt;a href=&quot;https://blog.csdn.net/xelatex_kvm/article/details/17761415&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Intel VT技术中的Preemption Timer&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;1-什么是Preemption-Timer&quot;&gt;&lt;a href=&quot;#1-什么是Preemption-Timer&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是Preemption Timer&quot;&gt;&lt;/a&gt;1. 什么是Preemption Timer&lt;/h3&gt;&lt;p&gt;Preemption Timer是一种可以周期性使VCPU触发VMExit的一种机制。即设置了Preemption Timer之后，可以使VCPU在指定的TSC cycle(注意文章最后的rate)之后产生一次VMExit。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NVLink and NVSwitch</title>
    <link href="http://liujunming.github.io/2022/04/01/Notes-about-NVLink-and-NVSwitch/"/>
    <id>http://liujunming.github.io/2022/04/01/Notes-about-NVLink-and-NVSwitch/</id>
    <published>2022-04-01T01:37:23.000Z</published>
    <updated>2022-04-01T04:19:40.637Z</updated>
    
    <content type="html"><![CDATA[<p>Notes aout NVLink and NVSwitch。<a id="more"></a></p><h3 id="1-NVLink"><a href="#1-NVLink" class="headerlink" title="1. NVLink"></a>1. NVLink</h3><p><strong>NVLink</strong> is a wire-based serial multi-lane near-range communications link developed by Nvidia. Unlike PCI Express, a device can consist of <em>multiple NVLinks</em>, and devices use <a href="https://en.wikipedia.org/wiki/Mesh_networking" target="_blank" rel="noopener">mesh networking</a> to communicate instead of a central hub.</p><p><img src="/images/2022/04/01.png" alt></p><p><img src="/images/2022/04/02.png" alt></p><p><img src="/images/2022/04/03.PNG" alt></p><h3 id="2-NVSwitch"><a href="#2-NVSwitch" class="headerlink" title="2. NVSwitch"></a>2. NVSwitch</h3><p><img src="/images/2022/04/05.jpg" alt><br>In the above figure, GPU to GPU memory transfers via NVLink are at most two hops away – a memory request may have to be routed through the NVLink controllers on two GPUs. For example, GPU 0 may need data in GPU 5’s memory, it needs two hops (such as:GPU 0 -&gt; GPU 1 -&gt;GPU5). Each NVLink controller has a memory access latency, so each memory access latency multiplies via the number of hops is the total latency.</p><p>NVSwitch存在的作用是避免GPU和GPU之间的通信会存在多跳。</p><p><img src="/images/2022/04/04.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.cnblogs.com/kongchung/p/12945019.html" target="_blank" rel="noopener">https://www.cnblogs.com/kongchung/p/12945019.html</a></li><li><a href="https://www.nvidia.com/en-us/data-center/nvlink/" target="_blank" rel="noopener">https://www.nvidia.com/en-us/data-center/nvlink/</a></li><li><a href="https://en.wikichip.org/wiki/nvidia/nvlink" target="_blank" rel="noopener">https://en.wikichip.org/wiki/nvidia/nvlink</a></li><li><a href="https://en.wikichip.org/wiki/nvidia/nvswitch" target="_blank" rel="noopener">https://en.wikichip.org/wiki/nvidia/nvswitch</a></li><li><a href="https://en.wikipedia.org/wiki/NVLink" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/NVLink</a></li><li><a href="https://www.nextplatform.com/2018/04/13/building-bigger-faster-gpu-clusters-using-nvswitches/" target="_blank" rel="noopener">https://www.nextplatform.com/2018/04/13/building-bigger-faster-gpu-clusters-using-nvswitches/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes aout NVLink and NVSwitch。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
      <category term="GPU" scheme="http://liujunming.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about SIOV</title>
    <link href="http://liujunming.github.io/2022/03/31/Notes-about-SIOV/"/>
    <id>http://liujunming.github.io/2022/03/31/Notes-about-SIOV/</id>
    <published>2022-03-31T07:17:54.000Z</published>
    <updated>2022-03-31T14:33:11.078Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要转载自:</p><ol><li><a href="http://blog.chinaunix.net/uid-28541347-id-5854292.html" target="_blank" rel="noopener">Scalable IOV技术详解</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4NTcwODg4NA==&amp;mid=2247483943&amp;idx=1&amp;sn=95f267a48893894d520d370a8758f3a4&amp;chksm=cfa5831cf8d20a0a3a6bbc5f3fc6ae6c3434f02e71cf935b93d2b35bf1abc82139106b511880&amp;mpshare=1&amp;scene=1&amp;srcid=0326ubuKIgOfRoXUsG8y8NWx&amp;sharer_sharetime=1648449427829&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AcIRgEgW1RVJwfGNas9nLEM%3D&amp;acctmode=0&amp;pass_ticket=a6RZLn0bY3VEyzyN0m6198%2FdvFlHI6%2BFJg7xUTXpn1mV5TdnwOdDYHcBYHEZUhPU&amp;wx_header=0#rd" target="_blank" rel="noopener">聊聊intel平台io虚拟化技术之 SIOV</a></li><li><a href="https://01.org/blogs/ashokraj/2018/recent-enhancements-intel-virtualization-technology-directed-i/o-intel-vt-d" target="_blank" rel="noopener">RECENT ENHANCEMENTS IN INTEL® VIRTUALIZATION TECHNOLOGY FOR DIRECTED I/O (INTEL® VT-D)</a>。</li></ol><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Scalable IO Virtualization(SIOV)是IO虚拟化技术的一个演进，是SR-IOV的进一步发展。为了提升虚拟机的IO性能，intel 的VT-d解决了设备直通问题，使虚拟机可以直接访问硬件设备从而提高性能，而SR-IOV则提供了设备共享的能力，通过将设备硬件虚拟化出多个VF给不同的虚拟机使用。</p><p>首先我们回顾一下SR-IOV技术，如下图所示，SR-IOV引入了两种设备PF和VF。其中PF具有完整的PCIe功能，包括VF的管理（创建/删除/配置），VF则是一种轻量的PCIe设备，只有部分数据传输功能，不包含资源管理和配置管理。但VF也是标准的PCIe设备，既有唯一的BDF（Bus，Device，Function）来标识这个设备，拥有属于自己的PCIe配置空间。<br><img src="/images/2022/03/38.png" alt></p><h2 id="2-Limitation-of-SR-IOV"><a href="#2-Limitation-of-SR-IOV" class="headerlink" title="2. Limitation of SR-IOV"></a>2. Limitation of SR-IOV</h2><p>While SR-IOV enabled the ability to partition a device and provide direct access to VMs, it also imposed scalability limitations to modern cloud and containerized environments. For instance:</p><ul><li>Device BARs and headers must be duplicated for every VF.</li><li>PCIe limits resources such as MSI-X to maximum of 2048 vectors.</li><li>BIOS Firmware must reserve a number of resources such as MMIO ranges, bus ranges to accommodate devices of any capability to be hotplugged.</li></ul><p>SR-IOV implementations typically provide only a small number of VFs due to the above resource requirements. Typical SR-IOV devices only support 64 or less VFs per physical device. Light-weight containerized usages in modern cloud environments expect to have thousands of containers and therefore will put pressure on potentially scarce resources. In these environments, SR-IOV will not scale.</p><p>Limitations of SR-IOV based implementations include:</p><ul><li>Scalability - Unable to scale to hyperscale usages (1000+ VMs/Containers) due to cost implications for having increased memory on board and limitations on BUS numbers in certain platforms.</li><li>Flexible Resource Management - SR-IOV requires resources such as BUS numbers and MMIO ranges to use the newly created VFs. Typically, the resources are spread evenly between each of the VFs. Although it’s possible to conceive of such variable resource assignments to different VFs, it imposes hardware complexity which would increase hardware cost. For instance, being able to create a device with 2 hardware queues for one VF, and 4 queues on the same physical device for another VF is generally not implemented.</li><li>Composability - the motivation of SR-IOV is to enable direct VF pass-through. The guest driver has full control on the assigned VF device which the host/hypervisor has no insight into. This makes it difficult to perform live migration or snapshot VF device state.</li></ul><p>Even with these limitations, SR-IOV has worked well in traditional VM usage. However, this approach no longer meets the scaling requirements for containerized environments.</p><h2 id="3-Scalable-IOV"><a href="#3-Scalable-IOV" class="headerlink" title="3. Scalable IOV"></a>3. Scalable IOV</h2><p>Intel introduced the recent update to Intel® VT-d that allows for fine-grained capacity allocation. More specifically, it allows software to compose virtual devices with different capacity or capability. For instance, it’s not required to replicate hardware like SR-IOV devices. Intel® Scalable IOV allows software to compose a virtual device on demand. The virtual device provisioned via software allows most device access to be separated into slow path (configuration) and fast path (I/O). Any activity that involves configuration and control is done by software mediation. Fast path I/O is performed directly to hardware with no software intervention. This allows resources such as queues to be bundled on demand and such usage can fit either full machine virtualization or native container type usages.</p><p>Intel® Scalable IOV requires changes in the following areas:</p><ul><li>Device Support - A device should support Process Address Space ID (PASID). The PASID is a 20 bit value that is used in conjunction with the Requester ID. PASID granular resource allocation and proper isolation requirements are identified in the Intel® Scalable I/O Virtualization Technical Specification.<ul><li>The Interrupt Message Store (IMS) provides devices the flexibility to dictate how interrupts are specified without limitations on how many and where the message address and data are stored.</li></ul></li><li>Platform Support - DMA remapping hardware should support PASID granular DMA isolation capability.</li><li>System Software - Support in the Operating System to provide abstractions that allow such devices to be provisioned for a Guest OS, or native process consumption.</li></ul><p>Intel® Scalable IOV addresses the aforementioned limitations observed on PCIe* SR-IOV:</p><ul><li>Scalability - supports finer-grained device sharing. For example, on a NIC with 1024 TX/RX queue pairs, each queue pair can now be independently assignable.</li><li>Flexible Resource management - software fully manages and maps backend resources to virtual devices. This provides great flexibility for heterogeneous configurations (different resources, different capabilities, and others.)</li><li>Composability - mediation of the slow-path allows the host/hypervisor to capture the virtual device state to enable live migration or snapshot usages. Also state save/restore is required only on a small piece of device resource (queue, context, etc.), which can be implemented more easily on a device as compared to requiring the entire VF state to be migratable.</li></ul><p><img src="/images/2022/03/44.png" alt></p><p>针对SR-IOV的一些局限性，intel推出了Scalable IOV技术。它主要包含一些几个技术特性：</p><ol><li>硬件辅助的直通架构，具体来说就是<ol><li>慢速路径有软件模拟完成，所谓慢速路径一般指设备的配置，接口的管理，而快速路径则是指IO的数据传输路径。在SR-IOV中，慢速路径和快速路径都是通过硬件直通的方式完成的；</li><li>快速路径资源可以动态分配，映射；</li><li>硬件保证快速路径的资源在DMA时是完全隔离的，保证不同虚拟设备的安全隔离；</li></ol></li><li>更加细粒度的动态资源配置。具体来说就是可以按照PCIe设备上的tx/rx queue pair来切分虚拟设备，而不是VF，从而实现更细粒度的资源分配；</li><li>利用PASID（Process Address Space ID）的PCIe能力，PASID技术也是PCI协议的一个补充，它颠覆了传统通过BDF（Bus，Device，Function）来唯一标识一个PCIe设备的方式，以BDF+PASID在一个PCIe设备内细分更多的虚拟设备；</li><li>支持各种IO设备，包括网卡，存储设备，GPU，各种加速器等；</li><li>支持虚拟机，裸金属，容器等多种应用场景；</li></ol><p>以上就是Scalable IOV的主要技术特征，可以看出和SR-IOV类似，它不仅仅是PCIe设备侧的一次革新，更是硬件设备，BIOS，操作系统，hypervisor，CPU，IOMMU等整个硬件的一次革新。<br><img src="/images/2022/03/39.png" alt></p><ul><li>Over-provisioning: 两个VDEV之间的Queue资源是可以share的</li><li>Generational Compatability: vmm可以使用VDCM(Virtual Device Composition Module)在不同代的硬件设备上呈现相同的VDEV功能，这样即使在部署了不同代的SIOV设备的host之间虚拟机也能正常迁移</li></ul><h2 id="4-整体架构"><a href="#4-整体架构" class="headerlink" title="4. 整体架构"></a>4. 整体架构</h2><p>Scalable IOV的整体架构和构成如下图所示。<br><img src="/images/2022/03/40.png" alt></p><h2 id="5-硬件架构"><a href="#5-硬件架构" class="headerlink" title="5. 硬件架构"></a>5. 硬件架构</h2><p><img src="/images/2022/03/45.png" alt></p><p>SIOV 主要是以queue为粒度来给上层应用提供服务，因此设备层提出了一种叫ADI（Assignable Device Interfaces）的接口概念，这个有些类似于SR-IOV中的VF，ADI指作为一种独立的单元进行分配、配置和组织的一组后端资源。它和VF有两点不同之处：</p><ol><li>没有PCI配置空间，所有ADI设备共享PF的配置空间；</li><li>通过PASID标识，而不是BDF</li></ol><p>同时ADI作为一个可用随时分配的设备，又具备以下特点：</p><ol><li>ADI设备之间是完全隔离的，不共享任何资源；</li><li>不同的ADI设备的MMIO寄存器是以物理页为单位隔离，保证进行MMIO页映射时在不同的页，避免MMIO被不同的进程共享；</li><li>所有ADI的DMA操作通过PASID进行，因此IOMMU可以根据每个设备DMA的PASID查找不同的页表，保证物理上ADI是安全隔离的；</li><li>采用了Interrupt Message Storage（IMS）技术。其实IMS和ADI不是绑定的，ADI采用IMS是由于往往ADI设备较多，每个ADI设备的每个queue都会产生中断，为了支持大量的中断消息存储使用了IMS技术。至于IMS具体的存储格式和位置是和具体设备实现相关的。此外ADI中断不支持共享，而且只支持MSI/MSI-X，不支持lagacy中断；</li><li>每个ADI设备可以独立的进行reset操作；</li></ol><h3 id="5-1-PCI配置空间"><a href="#5-1-PCI配置空间" class="headerlink" title="5.1 PCI配置空间"></a>5.1 PCI配置空间</h3><p>对PCIe设备进行初始化和枚举时，需要配置空间能够发现设备是否支持Scalable IOV技术，intel定义了一个Designated Vendor Specific Extended Capability (DVSEC) 域用于发现和配置支持Scalable IOV技术的设备。具体如下图所示：<br><img src="/images/2022/03/41.png" alt></p><h3 id="5-2-MMIO"><a href="#5-2-MMIO" class="headerlink" title="5.2 MMIO"></a>5.2 MMIO</h3><p>ADI的MMIO，它是位于PF bar地址空间的一段连续的按页大小对齐的地址范围。每个ADI设备的MMIO是相互独立的，ADI设备的MMIO register又分为两类，一类是访问频率比较高的比如硬件层的doorbell，一类是不经常访问的或者慢路径访问的比如用来进行一些设备配置和管理等。</p><h3 id="5-3-PASID-区分来自不同ADI设备的DMA请求"><a href="#5-3-PASID-区分来自不同ADI设备的DMA请求" class="headerlink" title="5.3 PASID(区分来自不同ADI设备的DMA请求)"></a>5.3 PASID(区分来自不同ADI设备的DMA请求)</h3><p>IOMMU提供DMA remapping的操作，进行地址转换，将不同的IO设备提供的IOVA地址转换成物理地址，用于设备的DMA。在intel IOMMU中，每个IO设备通过BDF找到属于自己的页表。为了支持Scalable IOV，DMA remapping增加了PASID的支持，其多级页表也进行了重新设计，具体如下图所示：<br><img src="/images/2022/03/42.png" alt></p><h3 id="5-4-IMS"><a href="#5-4-IMS" class="headerlink" title="5.4 IMS"></a>5.4 IMS</h3><p>一个PCIe设备即使在MSI-X的情况下，它支持的最大中断数目也只能到2048，那如果一个PF上支持的ADI数量所使用的总的中断数量超过了这个limit将如何处理呢？<br>为了解决这个中断limit的问题，SIOV引入了新的中断存储机制叫IMS(Iinterrupt Message Storage)，理论上IMS在支持的中断数量是没有上限的，从实现原理上来讲其仍然是message 格式的中断触发机制，每个message有一个DWORD 大小的payload和64-bit的address。这些message存储在 IMS的table里面，这个table可以有全部缓存在硬件上，也可以全部放在host memory里面。</p><h2 id="6-软件架构"><a href="#6-软件架构" class="headerlink" title="6. 软件架构"></a>6. 软件架构</h2><p><img src="/images/2022/03/47.png" alt><br><img src="/images/2022/03/48.png" alt></p><h3 id="6-1-VDCM"><a href="#6-1-VDCM" class="headerlink" title="6.1 VDCM"></a>6.1 VDCM</h3><p>VDCM (Virtual Device Composition Module)主要负责在ADI和虚拟设备（VDEV）之间建立映射关系，处理和仿真慢速路径的操作(负责一些trap到后端的MMIO的解释执行)，另外就是ADI设备的一些操作比如Reset和配置等。</p><h3 id="6-2-VDEV"><a href="#6-2-VDEV" class="headerlink" title="6.2 VDEV"></a>6.2 VDEV</h3><p>其实上面也已经讲了它是由一个或者多个ADI设备组成，在guest里面看到的就是一个标准的PCIe 设备。每个VDEV都有虚拟的requester id, config space, memory BAR，MSI-X table等，它们都是由VDCM来进行模拟的。</p><h3 id="6-3-VDEV-MMIO-and-interrupts"><a href="#6-3-VDEV-MMIO-and-interrupts" class="headerlink" title="6.3 VDEV MMIO and interrupts"></a>6.3 VDEV MMIO and interrupts</h3><p>从上面的分析来看，VDCM在整个软件架构上扮演着非常重要的角色，下面我们结合一张图来看一下相关实现：<br><img src="/images/2022/03/46.png" alt></p><p>结合上图我们来分析一些细节的东西，比如vdev 的MMIO，中断等 。</p><h4 id="6-3-1-VDEV-MMIO"><a href="#6-3-1-VDEV-MMIO" class="headerlink" title="6.3.1 VDEV MMIO"></a>6.3.1 VDEV MMIO</h4><p>从图中可以看到VDEV MMIO实现分为三类：</p><ol><li>直接map到 ADI的 MMIO，类似SR-IOV场景下将硬件的MMIO通过EPT的方式直接让guest访问，避免大量的VM Exit；</li><li>通过VDCM 模拟的MMIO，guest里面在写这个MMIO的时候会trap到 VDCM，然后需要VDCM进行解释和模拟相关的action，通常这类MMIO是要是一些控制面的数据交互；</li><li>map到host侧的memory上，这类MMIO通常存储的是一些参数或者数据，这样就避免了在读取或者写入的时候VDCM侧的解释和指令模拟。</li></ol><h4 id="6-3-2-VDEV-interrupts"><a href="#6-3-2-VDEV-interrupts" class="headerlink" title="6.3.2 VDEV interrupts"></a>6.3.2 VDEV interrupts</h4><p>VDEV 会通过VDCM 虚拟出MSI或者MSI-X的能力呈现给guest，当guest driver 去programs MSI 或者MSI-X的时候会被VDCM截获到然后做相关的中断虚拟化操作。这里需要说明的是慢路径上的中断是可以通过VMM提供的中断注入接口来触发，而快路径或者说是数据面上的中断是通IOMMU的post interrupt来注入的。</p><h3 id="7-MISC"><a href="#7-MISC" class="headerlink" title="7. MISC"></a>7. MISC</h3><h3 id="7-1-Hardware-Assisted-Mediated-Pass-Through"><a href="#7-1-Hardware-Assisted-Mediated-Pass-Through" class="headerlink" title="7.1 Hardware-Assisted Mediated Pass-Through"></a>7.1 Hardware-Assisted Mediated Pass-Through</h3><p><img src="/images/2022/03/35.PNG" alt><br><img src="/images/2022/03/36.PNG" alt><br><img src="/images/2022/03/37.PNG" alt></p><p>另外一个视角来看SIOV：同时结合了SR-IOV和Mediated pass-through的优点。</p><hr><p>参考资料:</p><ol><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4NTcwODg4NA==&amp;mid=2247483943&amp;idx=1&amp;sn=95f267a48893894d520d370a8758f3a4&amp;chksm=cfa5831cf8d20a0a3a6bbc5f3fc6ae6c3434f02e71cf935b93d2b35bf1abc82139106b511880&amp;mpshare=1&amp;scene=1&amp;srcid=0326ubuKIgOfRoXUsG8y8NWx&amp;sharer_sharetime=1648449427829&amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;exportkey=AcIRgEgW1RVJwfGNas9nLEM%3D&amp;acctmode=0&amp;pass_ticket=a6RZLn0bY3VEyzyN0m6198%2FdvFlHI6%2BFJg7xUTXpn1mV5TdnwOdDYHcBYHEZUhPU&amp;wx_header=0#rd" target="_blank" rel="noopener">聊聊intel平台io虚拟化技术之 SIOV</a></li><li><a href="http://blog.chinaunix.net/uid-28541347-id-5854292.html" target="_blank" rel="noopener">Scalable IOV技术详解</a></li><li><a href="https://01.org/blogs/ashokraj/2018/recent-enhancements-intel-virtualization-technology-directed-i/o-intel-vt-d" target="_blank" rel="noopener">RECENT ENHANCEMENTS IN INTEL® VIRTUALIZATION TECHNOLOGY FOR DIRECTED I/O (INTEL® VT-D)</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/introducing-intel-scalable-io-virtualization.html" target="_blank" rel="noopener">Introducing Intel® Scalable I/O Virtualization</a></li><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/scalable-i-o-virtualized-servers-paper.pdf" target="_blank" rel="noopener">White Paper</a></li><li><a href="https://01.org/blogs/2019/assignable-interfaces-intel-scalable-i/o-virtualization-linux" target="_blank" rel="noopener">ASSIGNABLE INTERFACES IN INTEL® SCALABLE I/O VIRTUALIZATION IN LINUX</a></li><li><a href="https://www.opencompute.org/documents/ocp-scalable-io-virtualization-technical-specification-revision-1-v1-2-pdf" target="_blank" rel="noopener">Version 1.2 SPEC</a></li><li><a href="http://news.eeworld.com.cn/wltx/ic567173.html" target="_blank" rel="noopener">英特尔携手微软打造全新I/O虚拟化架构,提升加速器和I/O设备的可扩展性</a></li><li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/12/Hardware-Assisted-Mediated-Pass-Through-with-VFIO-Kevin-Tian-Intel.pdf" target="_blank" rel="noopener">Hardware-Assisted Mediated Pass-Through with VFIO</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Intel%C2%AE-Scalable-I_O-Virtualization_Kevin-Tian.pdf" target="_blank" rel="noopener">Intel® Scalable I/O Virtualization</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要转载自:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-28541347-id-5854292.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scalable IOV技术详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=Mzg4NTcwODg4NA==&amp;amp;mid=2247483943&amp;amp;idx=1&amp;amp;sn=95f267a48893894d520d370a8758f3a4&amp;amp;chksm=cfa5831cf8d20a0a3a6bbc5f3fc6ae6c3434f02e71cf935b93d2b35bf1abc82139106b511880&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0326ubuKIgOfRoXUsG8y8NWx&amp;amp;sharer_sharetime=1648449427829&amp;amp;sharer_shareid=fcd8378fa2afcbc997c8bd7f888f36e6&amp;amp;exportkey=AcIRgEgW1RVJwfGNas9nLEM%3D&amp;amp;acctmode=0&amp;amp;pass_ticket=a6RZLn0bY3VEyzyN0m6198%2FdvFlHI6%2BFJg7xUTXpn1mV5TdnwOdDYHcBYHEZUhPU&amp;amp;wx_header=0#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;聊聊intel平台io虚拟化技术之 SIOV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://01.org/blogs/ashokraj/2018/recent-enhancements-intel-virtualization-technology-directed-i/o-intel-vt-d&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RECENT ENHANCEMENTS IN INTEL® VIRTUALIZATION TECHNOLOGY FOR DIRECTED I/O (INTEL® VT-D)&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Misc about PCI&amp;&amp;PCIe(1)</title>
    <link href="http://liujunming.github.io/2022/03/31/Misc-about-PCI-PCIe-1/"/>
    <id>http://liujunming.github.io/2022/03/31/Misc-about-PCI-PCIe-1/</id>
    <published>2022-03-31T00:24:43.000Z</published>
    <updated>2022-03-31T02:07:43.110Z</updated>
    
    <content type="html"><![CDATA[<p>PCI和PCIe内容较杂，mark下相关的MSIC。 <a id="more"></a></p><h3 id="1-Lane"><a href="#1-Lane" class="headerlink" title="1. Lane"></a>1. Lane</h3><p>中文翻译：数据通路</p><p>A PCIe connection consists of one or more data-transmission <a href="https://superuser.com/questions/843344/what-is-a-pci-express-lane" target="_blank" rel="noopener">lanes</a>, connected serially. Each lane consists of two pairs of wires, one for receiving and one for transmitting. You can have one, four, eight, or sixteen lanes in a single consumer PCIe slot–denoted as x1, x4, x8, or x16.</p><p><img src="/images/2022/03/34.jpg" alt></p><p><img src="/images/2022/03/33.PNG" alt></p><h3 id="2-Bar-size"><a href="#2-Bar-size" class="headerlink" title="2. Bar size"></a>2. Bar size</h3><p><img src="/images/2022/03/32.PNG" alt></p><p>A BAR size: it can be get by writing <strong>0FFFF FFFFH</strong> to a physical BAR register first, then read it.  Assume that the register’s value to read is <strong>A</strong>, the BAR size is:  <strong>~(A &amp; ~0FH) + 1</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCI和PCIe内容较杂，mark下相关的MSIC。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Shared Virtual Memory</title>
    <link href="http://liujunming.github.io/2022/03/30/Introduction-to-Shared-Virtual-Memory/"/>
    <id>http://liujunming.github.io/2022/03/30/Introduction-to-Shared-Virtual-Memory/</id>
    <published>2022-03-30T07:15:58.000Z</published>
    <updated>2022-03-30T08:33:36.472Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要转载自：<a href="http://blog.chinaunix.net/uid-28541347-id-5854016.html" target="_blank" rel="noopener">Shared Virtual Memory（SVM）介绍</a>。<a id="more"></a></p><h3 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h3><p>Shared Virtual Addressing (SVA) allows the processor and device to use the same virtual addresses. SVA is what PCIe calls Shared Virtual Memory (SVM).<br><img src="/images/2022/03/23.PNG" alt><br><img src="/images/2022/03/24.PNG" alt></p><p>VT-d SVM: Extends root complex IOMMU to comprehend x86 page table formats<br><img src="/images/2022/03/31.PNG" alt></p><h3 id="2-History"><a href="#2-History" class="headerlink" title="2. History"></a>2. History</h3><p>共享虚拟内存（SVM）技术最初是为了解决在GPU场景下，设备（GPU）和host（CPU）之间共享内存的。目的是在设备GPU和CPU之间可以直接传递指针（地址），为了让设备可以直接使用进程空间的地址，简化编程模型。我们知道通常host侧采用的地址是主机的虚拟地址（VA），而设备侧通常使用的是物理地址（PA）或IOVA。</p><p>如下图，传统内存访问的三个途径：</p><ol><li>CPU访存通过MMU（CPU的页表）将VA转为PA访问物理地址</li><li>GPU访存通过GPU的页表访问物理地址</li><li>PCIe设备直接使用物理地址PA访问</li></ol><p><img src="/images/2022/03/25.png" alt></p><p>在引入了iommu（VT-d）后，如下图，PCIe设备也可以使用虚拟地址（IOVA）来访存了，也有设备自己对应的页表（iommu页表）完成IOVA到物理地址（PA）的映射。<br><img src="/images/2022/03/26.png" alt></p><p>这种情况下CPU进程和设备的内存通信一般采用如下流程：</p><ol><li>CPU进程分配一块内存，并采用系统调用syscall或ioctl请求内核准备接收操作</li><li>内核初始化设备的DMA操作，这里面有两种情况：一种是内核重新分配一块内核空间的内存，将其物理地址传递给设备进行DMA，另一种是如果应用程序将用户空间的内存pin住（这段内存的虚拟地址空间和物理地址空间不会发生变化）则可直接将用户空间的buffer地址传递给设备进行DMA</li><li>设备将数据DMA到内存，对应上面这里也有两种情况，如果是内核设置的内核buffer的地址，则设备会先将数据DMA到内核buffer，再由内核将数据由内核空间拷贝到用户空间的buffer（我们通常使用内核协议栈进行收发报文的应用程序就是这种），另一种如果用户空间直接将内存pin住，则设备直接将数据DMA到应用程序的buffer（我们采用DPDK收发报文就是这种）</li></ol><p><img src="/images/2022/03/27.png" alt></p><h3 id="3-引入SVM后的变化"><a href="#3-引入SVM后的变化" class="headerlink" title="3. 引入SVM后的变化"></a>3. 引入SVM后的变化</h3><p>下面我们看引入SVM后的效果，最大的区别是设备访问地址在经过iommu的DMAR转换时会参考引用CPU的mmu页表，在地址缺页时同样会产生缺页中断。为什么要这样设计呢？因为要想设备直接使用进程空间的虚拟地址可能采用的有两种方法。一种是把整个进程地址空间全部pin住，但这点一般是不现实的，除非类似DPDK应用程序全部采用静态内存，否则如果进程动态分配一个内存，那么这个地址设备是不感知的。另一种方法就是采用动态映射，就像进程访问虚拟地址一样，mmu发现缺页就会动态映射，所以从设备发来的地址请求也会经过CPU缺页处理，并将映射关系同步到iommu的页表中。<br><img src="/images/2022/03/28.png" alt></p><p>有了以上的流程，CPU和设备的内存交互流程就变成了如下图所示。主要是第三步的变化，设备直接将数据DMA到进程空间的地址，并且不需要进程pin内存，而是通过page fault触发缺页处理进行映射。<br><img src="/images/2022/03/29.png" alt></p><h3 id="4-支持SVM的条件"><a href="#4-支持SVM的条件" class="headerlink" title="4. 支持SVM的条件"></a>4. 支持SVM的条件</h3><p>那么支持SVM需要软硬件具备什么条件呢。首先是设备角度：</p><ol><li>要支持<a href="/2021/11/09/Notes%20about-PASID-Process-Address-Space-ID/">PASID</a>，因为一个设备会被多个进程访问，对应多个设备DMAR页表，需要通过PASID来区分采用哪个页表</li><li>支持<a href="/2022/03/30/Notes-about-PCIe-Page-Request-Interface/">dma page fault</a>处理，当访问的虚拟地址引发缺页时能够等待或重试</li></ol><p>从驱动角度来说，</p><ol><li>操作设备的API需要通过PASID来区分不同进程</li></ol><p><img src="/images/2022/03/30.PNG" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.kernel.org/doc/html/latest/x86/sva.html" target="_blank" rel="noopener">Shared Virtual Addressing (SVA) with ENQCMD</a></li><li><a href="https://archive.fosdem.org/2016/schedule/event/intel_svm/attachments/slides/1269/export/events/attachments/intel_svm/slides/1269/FOSDEM_2016___SVM_on_Intel_Graphics.pdf" target="_blank" rel="noopener">SVM on Intel Graphics</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2018/52/kvm-forum-vSVA-yliu-jpan-jean-eric.pdf" target="_blank" rel="noopener">Shared Virtual Addressing in KVM kvm forum 2018</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Shared-Virtual-Memory-in-KVM_Yi-Liu.pdf" target="_blank" rel="noopener">Shared Virtual Addressing in KVM</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要转载自：&lt;a href=&quot;http://blog.chinaunix.net/uid-28541347-id-5854016.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Shared Virtual Memory（SVM）介绍&lt;/a&gt;。
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to PCIe Page Request Interface</title>
    <link href="http://liujunming.github.io/2022/03/30/Notes-about-PCIe-Page-Request-Interface/"/>
    <id>http://liujunming.github.io/2022/03/30/Notes-about-PCIe-Page-Request-Interface/</id>
    <published>2022-03-30T01:45:26.000Z</published>
    <updated>2022-03-30T06:48:37.556Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录PCIe中PRI(Page Request Interface)相关知识点。<a id="more"></a></p><p>阅读本文前，读者需要对<a href="/2019/11/24/Introduction-to-PCIe-Address-Translation-Services/">ATS</a>和<a href="/2021/07/29/Notes-about-guest-memory-pinning-when-direct-assignment-of-I-0-devices/">guest memory pinning when direct assignment of I/O devices</a>有一定的了解。</p><p>PRI(Page Request Interface) allows functions(BDF中的F) to raise page faults to the IOMMU.</p><h3 id="1-DMA-Page-Fault-Support"><a href="#1-DMA-Page-Fault-Support" class="headerlink" title="1. DMA Page Fault Support"></a>1. DMA Page Fault Support</h3><p>Description from PCIe spec:<br><img src="/images/2022/03/21.PNG" alt></p><p>Description from ASPLOS ’17 paper:<br><img src="/images/2022/03/20.PNG" alt></p><h3 id="2-Page-Request-Services"><a href="#2-Page-Request-Services" class="headerlink" title="2. Page Request Services"></a>2. Page Request Services</h3><p>The general model for a page request is as follows:</p><ol><li>A Function determines that it requires access to a page for which an ATS translation is not available.</li><li>The Function causes the associated Page Request Interface to send a Page Request Message to its RC. A Page Request Message contains a page address and a Page Request Group (PRG) index. The PRG index is used to identify the transaction and is used to match requests with responses.</li><li>When the RC determines its response to the request (which will typically be to make the requested page resident), it sends a PRG Response Message back to the requesting Function.</li><li>The Function can then employ ATS to request a translation for the requested page(s). </li></ol><h3 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h3><p><img src="/images/2022/03/22.PNG" alt></p><hr><p>参考资料：</p><ol><li><a href="https://composter.com.ua/documents/ats_r1.1_26Jan09.pdf" target="_blank" rel="noopener">Address Translation Services Revision 1.1</a></li><li><a href="https://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Shared-Virtual-Memory-in-KVM_Yi-Liu.pdf" target="_blank" rel="noopener">Shared Virtual Memory in KVM</a></li><li>Page Fault Support for Network Controllers, ASPLOS ’17</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录PCIe中PRI(Page Request Interface)相关知识点。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel I/OAT</title>
    <link href="http://liujunming.github.io/2022/03/29/Introduction-to-Intel-I-OAT/"/>
    <id>http://liujunming.github.io/2022/03/29/Introduction-to-Intel-I-OAT/</id>
    <published>2022-03-29T05:34:04.000Z</published>
    <updated>2022-03-29T12:21:10.541Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel I/OAT(I/O Acceleration Technology)相关知识点。<a id="more"></a></p><h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>Intel I/OAT is actually a set of technologies that each contributes to increased performance.</p><p>The features of Intel I/OAT enhance data acceleration across the computing platform.</p><ul><li><strong>Intel® QuickData Technology</strong> enables data copy by the chipset instead of the CPU, to move data more efficiently through the server and provide fast, scalable, and reliable throughput.</li><li><strong>Direct Cache Access (DCA)</strong> allows a capable I/O device, such as a network controller, to place data directly into CPU cache, reducing cache misses and improving application response times.</li><li><strong>Extended Message Signaled Interrupts (MSI-X)</strong> distributes I/O interrupts to multiple CPUs and cores, for higher efficiency, better CPU utilization, and higher application performance.</li><li><strong>Receive Side Coalescing (RSC)</strong> aggregates packets from the same TCP/IP flow into one larger packet, reducing per-packet processing costs for faster TCP/IP processing.</li><li><strong>Low latency interrupts</strong> tune interrupt interval times depending on the latency sensitivity of the data, using criteria such as port number or packet size, for higher processing efficiency.</li></ul><p><img src="/images/2022/03/16.PNG" alt></p><p>本文只介绍QuickData Technology和DCA。</p><h3 id="2-Intel®-QuickData-Technology"><a href="#2-Intel®-QuickData-Technology" class="headerlink" title="2. Intel® QuickData Technology"></a>2. Intel® QuickData Technology</h3><p><img src="/images/2022/03/17.PNG" alt></p><p><img src="/images/2022/03/15.PNG" alt><br><img src="/images/2022/03/19.PNG" alt></p><p><img src="/images/2022/03/18.PNG" alt></p><p><a href="https://01.org/blogs/2019/introducing-intel-data-streaming-accelerator" target="_blank" rel="noopener">Intel® DSA</a> replaces the Intel® QuickData Technology.</p><h3 id="3-Direct-Cache-Access-DCA"><a href="#3-Direct-Cache-Access-DCA" class="headerlink" title="3. Direct Cache Access (DCA)"></a>3. Direct Cache Access (DCA)</h3><p><img src="/images/2022/03/13.PNG" alt><br><img src="/images/2022/03/14.PNG" alt></p><p><a href="/2022/03/28/Introduction-to-Intel-DDIO-technology/">Introduction to Intel DDIO technology</a></p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/io/i-o-acceleration-technology-paper.html" target="_blank" rel="noopener">White Paper</a></li><li><a href="https://www.intel.com/content/www/us/en/wireless-network/accel-technology.html" target="_blank" rel="noopener">Intel® I/O Acceleration Technology</a></li><li><a href="https://www.usenix.org/system/files/atc20-farshin.pdf" target="_blank" rel="noopener">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi hundred-gigabit Networks</a></li><li><a href="https://insujang.github.io/2021-04-26/using-intel-ioat-dma/" target="_blank" rel="noopener">Using Intel IOAT DMA</a></li><li><a href="https://www.snia.org/sites/default/files/SDC/2016/presentations/persistent_memory/Tanveer_Alam_Enterprise_Storage_RAS_Augmented_native_Intel_Platform_Storage_Extensions.pdf" target="_blank" rel="noopener">Tanveer_Alam_Enterprise_Storage_RAS_Augmented_native_Intel_Platform_Storage_Extensions.pdf</a></li><li><a href="https://landley.net/kdocs/ols/2005/ols2005v1-pages-289-296.pdf" target="_blank" rel="noopener">Accelerating Network Receive Processing</a></li><li><a href="https://0xffff.one/d/934" target="_blank" rel="noopener">关于intel的IOAT技术</a></li><li><a href="http://nowlab.cse.ohio-state.edu/static/media/publications/abstract/vaidyana-cluster07.pdf" target="_blank" rel="noopener">Efficient Asynchronous Memory Copy Operations on Multi-Core Systems and I/OAT</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel I/OAT(I/O Acceleration Technology)相关知识点。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Compute Express Link (CXL)</title>
    <link href="http://liujunming.github.io/2022/03/28/Introduction-to-Compute-Express-Link-CXL/"/>
    <id>http://liujunming.github.io/2022/03/28/Introduction-to-Compute-Express-Link-CXL/</id>
    <published>2022-03-28T11:15:28.000Z</published>
    <updated>2022-03-28T15:59:00.766Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Compute Express Link (CXL)相关知识点。<a id="more"></a></p><h3 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h3><p>Compute Express Link™ (CXL™) is an industry-supported <strong>Cache-Coherent Interconnect</strong> for <em>Processors</em>, <em>Memory Expansion</em> and <em>Accelerators</em>. CXL technology maintains <strong>memory coherency</strong> between the CPU memory space and memory on attached devices, which allows resource sharing for higher performance, reduced software stack complexity, and lower overall system cost. This permits users to simply focus on target workloads as opposed to the redundant memory management hardware in their accelerators. </p><p>CXL is designed to be an industry open standard interface for <strong>high-speed communications</strong>, as accelerators are increasingly used to complement CPUs in support of emerging applications such as Artificial Intelligence and Machine Learning.</p><h3 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h3><p>通俗地说，有了CXL，Host在访问Device Memory时，可以得到像访问本地Memory一样的体验；同样的，Device访问host Memory时，也像是在访问Device Memory一样。</p><p>我们已经有了PCIe这样的高速串行总线，为什么还要再搞出一个新的CXL呢 ？主要是因为<strong>PCIe不支持cache的一致性</strong>，这会导致每次Device去访问Host上的内存时，即便已经访问了多次而且内存也没有变化的情况下，都要重新访问，这样导致性能很差。另外因为人工智能和机器学习的兴起，FPGA/GPU 卡上会有大量的内存，在不进行运算时闲置的话，会造成资源浪费。而因为PCIe不支持Cache的一致性，Host访问设备上的内存也会非常的慢(CPU访问设备的内存是不cache的，意味着这次访问完而且设备内存也没有变化的情况下，下次还要重新访问。为什么不cache 呢?因为设备的内存不能汇报自己的改变)。所以Intel就发明了CXL，它在PCIe的基础上加入了<strong>Cache一致性</strong>的支持以用来<strong>提高设备和主机之间内存互相访问的速度</strong>。</p><h3 id="3-Components"><a href="#3-Components" class="headerlink" title="3. Components"></a>3. Components</h3><p><img src="/images/2022/03/11.PNG" alt><br>CXL在PCIe 5.0的基础上复用三种类型的协议:</p><ul><li>CXL.io: Provides discovery, configuration, register access, interrupts, etc.</li><li>CXL.cache: Provides the CXL device access to the processor memory</li><li>CXL.memory: Provides the Processor access to the CXL device attached memory</li></ul><p>Each of these are illustrated in the block below.<br><img src="/images/2022/03/09.webp" alt></p><h3 id="4-Usage"><a href="#4-Usage" class="headerlink" title="4. Usage"></a>4. Usage</h3><p><img src="/images/2022/03/10.PNG" alt><br>HBM(High Bandwidth Memory)</p><h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><p><img src="/images/2022/03/12.PNG" alt></p><p>In short, CXL is an open industry standard interconnect offering <strong>high-bandwidth</strong>, <strong>low-latency</strong> connectivity between the host processor and devices including accelerators, memory expansion, and smart I/O devices. CXL utilizes the PCI Express® (PCIe®) 5.0 physical layer infrastructure and the PCIe alternate protocol to address the demanding needs of high-performance computational workloads in Artificial Intelligence, Machine Learning, communication systems, and HPC through the enablement of <strong>coherency and memory semantics</strong> across heterogeneous processing and memory systems.</p><hr><p>参考资料:</p><ol><li><a href="https://zhuanlan.zhihu.com/p/65435956" target="_blank" rel="noopener">基于PCIe 5.0的CXL是什么？</a></li><li><a href="https://zhuanlan.zhihu.com/p/383878879" target="_blank" rel="noopener">CXL简介</a></li><li><a href="http://www.360doc.com/content/20/0519/10/36367108_913231044.shtml" target="_blank" rel="noopener">强力科普PCIe/CXL</a></li><li><a href="https://www.computeexpresslink.org/about-cxl" target="_blank" rel="noopener">About CXL™</a></li><li><a href="https://www.computeexpresslink.org/post/introduction-to-compute-express-link-cxl-the-cpu-to-device-interconnect-breakthrough" target="_blank" rel="noopener">Introduction to Compute Express Link (CXL): The CPU-To-Device Interconnect Breakthrough</a></li><li><a href="https://docs.wixstatic.com/ugd/0c1418_27f700c09d4648c4bede5dec99a20824.pdf" target="_blank" rel="noopener">Compute Express Link® (CXL):A Coherent Interface for Ultra-High-Speed Transfers</a></li><li><a href="https://docs.wixstatic.com/ugd/0c1418_d9878707bbb7427786b70c3c91d5fbd1.pdf" target="_blank" rel="noopener">Compute Express Link</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Compute Express Link (CXL)相关知识点。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel DDIO technology</title>
    <link href="http://liujunming.github.io/2022/03/28/Introduction-to-Intel-DDIO-technology/"/>
    <id>http://liujunming.github.io/2022/03/28/Introduction-to-Intel-DDIO-technology/</id>
    <published>2022-03-28T01:29:57.000Z</published>
    <updated>2022-03-28T06:24:15.604Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel DDIO(Data Direct I/O) technology。<a id="more"></a></p><h2 id="1-What-is-Intel®-Data-Direct-I-O"><a href="#1-What-is-Intel®-Data-Direct-I-O" class="headerlink" title="1. What is  Intel® Data Direct I/O?"></a>1. What is  Intel® Data Direct I/O?</h2><p>Intel® Data Direct I/O (Intel® DDIO) is a feature to be introduced on the Intel® E5 Xeon® processors. Intel’s LAN Access Division (LAD) worked for the incorporation of Intel DDIO into the Xeon E5 processor because of its benefits for LAN I/O in terms of performance and system power consumption. With Intel DDIO, Intel’s Ethernet server NICs and controllers talk directly to the processor cache without a detour via system memory. Intel DDIO makes the processor cache the primary destination and source of I/O data rather than main memory. By avoiding system memory, Intel DDIO reduces latency, increases system I/O bandwidth, and reduces power consumption due to memory reads and writes.<br><img src="/images/2022/03/06.PNG" alt></p><h2 id="2-How-does-it-work"><a href="#2-How-does-it-work" class="headerlink" title="2. How does it work?"></a>2. How does it work?</h2><p>Read and Writes的视角是NIC。<br>详情可以阅读<a href="https://blog.csdn.net/qq_40500045/article/details/109272627" target="_blank" rel="noopener">谈谈DDIO你该知道的事</a>。</p><h3 id="2-1-NIC-Reads"><a href="#2-1-NIC-Reads" class="headerlink" title="2.1 NIC Reads"></a>2.1 NIC Reads</h3><p><img src="/images/2022/03/07.PNG" alt></p><h4 id="2-1-1-Without-DDIO"><a href="#2-1-1-Without-DDIO" class="headerlink" title="2.1.1 Without DDIO"></a>2.1.1 Without DDIO</h4><ol><li>处理器更新报文和控制结构体。由于分配的缓冲区在内存中， 因此会触发一次Cache不命中，处理器把内存读取到Cache中，然后更新控制结构体和报文信息。之后通知NIC来读取报文。</li><li>NIC收到有报文需要传递到网络上的通知后，它首先需要读取控制结构体进而知道从哪里获取报文。由于之前处理器刚把该缓冲区从内存读到Cache中并且做了更新，很有可能Cache还没有来得及把更新的内容写回到内存中。因此，当NIC发起一个对内存的读请求时，很有可能这个请求会发送到Cache系统中，Cache系统会把数据写回到内存中，然后内存控制器再把数据写到PCI总线上去。因此，一个读内存的操作会产生多次内存的读写。</li></ol><h4 id="2-1-2-With-DDIO"><a href="#2-1-2-With-DDIO" class="headerlink" title="2.1.2 With DDIO"></a>2.1.2 With DDIO</h4><ol><li>处理器更新报文和控制结构体。这个步骤和没有DDIO的技术类似，但是由于DDIO的引入，处理器会开始就把内存中的缓冲区和控制结构体预取到Cache，因此减少了内存读的时间。</li><li>NIC收到有报文需要传递到网络上的通知后，通过PCI总线把控制结构体和报文送到NIC内部。利用DDIO技术，I/O访问可以直接将Cache的内容送到PCI总线上。这样，就减少了Cache写回时等待的时间。</li></ol><p>由此可以看出，由于DDIO技术的引入，网卡的读操作减少了访问内存的次数，因而提高了访问效率，减少了报文转发的延迟。在理想状况下，NIC和处理器无需访问内存，直接通过访问Cache就可以完成更新数据，把数据送到NIC内部，进而送到网络上的所有操作。</p><h3 id="2-2-NIC-Writes"><a href="#2-2-NIC-Writes" class="headerlink" title="2.2 NIC Writes"></a>2.2 NIC Writes</h3><p><img src="/images/2022/03/08.PNG" alt></p><h4 id="2-2-1-Without-DDIO"><a href="#2-2-1-Without-DDIO" class="headerlink" title="2.2.1 Without DDIO"></a>2.2.1 Without DDIO</h4><ol><li>报文和控制结构体通过PCI总线送到指定的内存中。如果该内存恰好缓存在Cache中(有可能之前处理器有对该内存进行过读写操作)，则需要等待Cache把内容先写回到内存中，然后才能把报文和控制结构体写到内存中。</li><li>运行在处理器上的驱动程序或者软件得到通知收到新报文，去内存中读取控制结构体和相应的报文，Cache不命中。之所以Cache一定不会命中，是因为即使该内存地址在Cache中，在步骤1中也被强制写回到内存中。因此，只能从内存中读取控制结构体和报文。</li></ol><h4 id="2-2-2-With-DDIO"><a href="#2-2-2-With-DDIO" class="headerlink" title="2.2.2 With DDIO"></a>2.2.2 With DDIO</h4><p>这时，报文和控制结构体通过PCI总线直接送到Cache中。这时有两种情形:</p><ol><li>a) 如果该内存恰好缓存在Cache中(有可能之前处理器有对该内存进行过读写操作)，则直接在Cache中更新内容，覆盖原有内容。<br>b) 如果该内存没有缓存在Cache中，则在最后一级Cache中分配一块区域，并相应更新Cache表，表明该内容是对应于内存中的某个地址的。</li><li>运行在处理器上的驱动或者软件被通知到有报文到达，其产生一个内存读操作，由于该内容已经在Cache中，因此直接从Cache中读。</li></ol><p>由此可以看出，DDIO技术在处理器和外设之间交换数据时，减少了处理器和外设访问内存的次数，也减少了Cache写回的等待，提高了系统的吞吐率和数据的交换延迟。</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology-brief.html" target="_blank" rel="noopener">Intel® Data Direct I/O Technology (Intel® DDIO): A Primer</a></li><li><a href="https://www.intel.com/content/www/us/en/io/data-direct-i-o-faq.html" target="_blank" rel="noopener">Intel Data Direct I/O (Intel DDIO): Frequently Asked Questions</a></li><li><a href="https://blog.csdn.net/qq_40500045/article/details/109272627" target="_blank" rel="noopener">谈谈DDIO你该知道的事</a></li><li><a href="https://www.usenix.org/system/files/atc20-farshin.pdf" target="_blank" rel="noopener">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel DDIO(Data Direct I/O) technology。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>thread core module die package</title>
    <link href="http://liujunming.github.io/2022/03/16/thread-core-module-die-package/"/>
    <id>http://liujunming.github.io/2022/03/16/thread-core-module-die-package/</id>
    <published>2022-03-16T06:42:02.000Z</published>
    <updated>2022-03-16T11:58:04.885Z</updated>
    
    <content type="html"><![CDATA[<p>Terms: thread core module die package.<a id="more"></a><br><img src="/images/2022/03/03.PNG" alt></p><p>Per SDM Vol4:<br><img src="/images/2022/03/02.PNG" alt></p><h3 id="thread"><a href="#thread" class="headerlink" title="thread"></a>thread</h3><p>Individual cores can support multiple <em>hardware threads</em> of execution. These are also known as <em>logical processors</em>. This technique has multiple names, including <em>simultaneous multithreading</em> (SMT) and <em>Hyper-Threading Technology </em>(HT).</p><h3 id="core"><a href="#core" class="headerlink" title="core"></a>core</h3><p><strong>(Physical) processor core</strong> is an independent execution unit that can run one program thread at a time in parallel with other cores.</p><h3 id="module"><a href="#module" class="headerlink" title="module"></a>module</h3><p>Intel Atom processors also have the concept of CPU modules. In these processors, two cores share a large L2 cache. The modules interface with the CPU fabric rather than the cores interfacing directly.</p><h3 id="die"><a href="#die" class="headerlink" title="die"></a>die</h3><p><strong>Processor die</strong> is a single continuous piece of semiconductor material (usually silicon). A die can contain any number of cores. Up to 15 are available on the Intel product line. Processor die is where the transistors making up the CPU actually reside.</p><p><img src="/images/2022/03/05.jpg" alt><br>One die with multiple cores</p><h3 id="package"><a href="#package" class="headerlink" title="package"></a>package</h3><p><strong>Processor package</strong> is what you get when you buy a single processor. It contains one or more dies, plastic/ceramic housing for dies and gold-plated contacts that match those on your motherboard.</p><p><img src="/images/2022/03/04.jpg" alt><br>CPU Package containing 2 separate DIEs</p><hr><p>参考资料:</p><ol><li><a href="https://link.springer.com/chapter/10.1007/978-1-4302-6638-9_2" target="_blank" rel="noopener">CPU Power Management</a></li><li><a href="https://superuser.com/questions/324284/what-is-meant-by-the-terms-cpu-core-die-and-package" target="_blank" rel="noopener">What is meant by the terms CPU, Core, Die and Package?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Terms: thread core module die package.
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>How to setup proxy for docker</title>
    <link href="http://liujunming.github.io/2022/03/15/How-to-setup-proxy-for-docker/"/>
    <id>http://liujunming.github.io/2022/03/15/How-to-setup-proxy-for-docker/</id>
    <published>2022-03-15T06:02:49.000Z</published>
    <updated>2022-03-15T08:23:12.113Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍docker中的代理设置。<a id="more"></a></p><h3 id="1-Docker-daemon"><a href="#1-Docker-daemon" class="headerlink" title="1. Docker daemon"></a>1. Docker daemon</h3><p><a href="https://stackoverflow.com/questions/23111631/cannot-download-docker-images-behind-a-proxy" target="_blank" rel="noopener">Cannot download Docker images behind a proxy</a></p><p>In order to download container images from the outside world when running commands like <code>docker pull busybox</code> ,set the proxies by:</p><ul><li>Create directory</li></ul><p><code>mkdir /etc/systemd/system/docker.service.d</code></p><ul><li>Create file</li></ul><p><code>vi /etc/systemd/system/docker.service.d/http-proxy.conf</code></p><ul><li>File content</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=&quot;HTTP_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:80/&quot;</span><br></pre></td></tr></table></figure><p>If you have internal Docker registries that you need to contact without proxying you can specify them via the <code>NO_PROXY</code> environment variable:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Environment=&quot;HTTP_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:80/&quot;</span><br><span class="line">Environment=&quot;NO_PROXY=localhost,127.0.0.0/8,docker-registry.somecorporation.com&quot;</span><br></pre></td></tr></table></figure></p><ul><li>Flush changes</li></ul><p><code>systemctl daemon-reload</code></p><ul><li>Verify that the configuration has been loaded</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show --property Environment docker</span><br><span class="line">Environment=HTTP_PROXY=http://proxy.example.com:80/</span><br><span class="line">Environment=HTTPS_PROXY=http://proxy.example.com:80/</span><br></pre></td></tr></table></figure><ul><li>Restart docker</li></ul><p><code>systemctl restart docker</code></p><h3 id="2-Running-container"><a href="#2-Running-container" class="headerlink" title="2. Running container"></a>2. Running container</h3><p>In order for a running container to access the internet you need to fix the dns names since Google’s are the default and they don’t work. We need to update the dns names by using <a href="https://docs.docker.com/engine/reference/commandline/dockerd/" target="_blank" rel="noopener">daemon configuration file</a>.</p><ul><li>Create file</li></ul><p><code>/etc/docker/daemon.json</code></p><ul><li>File content</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;dns&quot;:[</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">&quot;*.*.*.*&quot;,</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reload daemon</li></ul><p><code>systemctl daemon-reload</code></p><ul><li>Restart docker</li></ul><p><code>systemctl restart docker</code></p><ul><li>Now you can run a container and pass the proxies like this</li></ul><p><code>docker run --env http_proxy=http://proxy.example.com:80/ --env https_proxy=http://proxy.example.com:80/ -ti ubuntu bash</code></p><h3 id="3-Building-the-container"><a href="#3-Building-the-container" class="headerlink" title="3. Building the container"></a>3. Building the container</h3><ul><li>You can use ENV in the docker file:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">FROM ubuntu:16.04</span><br><span class="line"> </span><br><span class="line">ENV http_proxy http://proxy.example.com:80/</span><br><span class="line">ENV https_proxy http://proxy.example.com:80/</span><br><span class="line">...</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://elegantinfrastructure.com/docker/ultimate-guide-to-docker-http-proxy-configuration/" target="_blank" rel="noopener">Ultimate Guide to Docker HTTP Proxy Configuration</a></li><li><a href="https://docs.docker.com/network/proxy/" target="_blank" rel="noopener">Configure Docker to use a proxy server</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍docker中的代理设置。
    
    </summary>
    
      <category term="工具" scheme="http://liujunming.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to VT-x Page-Modification Logging</title>
    <link href="http://liujunming.github.io/2022/03/02/Introduction-to-VT-x-Page-Modification-Logging/"/>
    <id>http://liujunming.github.io/2022/03/02/Introduction-to-VT-x-Page-Modification-Logging/</id>
    <published>2022-03-02T07:41:04.000Z</published>
    <updated>2022-03-02T10:39:30.417Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍VT-x中的Page-Modification Logging(PML)技术。<a id="more"></a></p><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h3><p>在没有PML前，VMM要监控GPA的修改时，需要将EPT的页面结构设置为not-present或者read-only，这样会触发许多EPT violations,开销非常大。</p><p>PML建立在CPU对EPT中的accessed与dirty标志位支持上。<br>当启用PML时，对EPT中设置了dirty标志位的写操作都会产生一条in-memory记录，报告写操作的GPA，当记录写满时，触发一次VM Exit，然后VMM就可以监控被修改的页面。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h3><p>PML is a new feature on Intel’s Boardwell server platfrom targeted to reduce overhead of dirty logging mechanism.</p><p>The specification can be found at: <a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/page-modification-logging-vmm-white-paper.pdf" target="_blank" rel="noopener">Page Modification Logging for Virtual Machine Monitor White Paper</a></p><p>Currently, dirty logging is done by write protection, which write protects guest memory, and mark dirty GFN to dirty_bitmap in subsequent write fault. This works fine, except with overhead of additional write fault for logging each dirty GFN. The overhead can be large if the write operations from guest is intensive.</p><p>PML is a hardware-assisted efficient way for dirty logging. PML logs dirty GPA automatically to a 4K PML memory buffer <strong>when CPU changes EPT table’s D-bit from 0 to 1</strong>. To do this, A new 4K PML buffer base address, and a PML index were added to VMCS. Initially PML index is set to 512 (8 bytes for each GPA), and CPU decreases PML index after logging one GPA, and eventually a PML buffer full VMEXIT happens when PML buffer is fully logged.</p><p><img src="/images/2022/03/01.PNG" alt></p><p>With PML, we don’t have to use write protection so the intensive write fault EPT violation can be avoided, with an additional PML buffer full VMEXIT for 512 dirty GPAs. Theoretically, this can reduce hypervisor overhead when guest is in dirty logging mode, and therefore more CPU cycles can be allocated to guest, so it’s expected benchmarks in guest will have better performance comparing to non-PML.</p><h3 id="3-KVM-design"><a href="#3-KVM-design" class="headerlink" title="3. KVM design"></a>3. KVM design</h3><h4 id="3-1-Enable-Disable-PML"><a href="#3-1-Enable-Disable-PML" class="headerlink" title="3.1 Enable/Disable PML"></a>3.1 Enable/Disable PML</h4><p>PML is per-vcpu (per-VMCS), while EPT table can be shared by vcpus, so we need to enable/disable PML for all vcpus of guest. A dedicated 4K page will be allocated for each vcpu when PML is enabled for that vcpu.</p><p>Currently, we choose to always enable PML for guest, which means we enables PML when creating VCPU, and never disable it during guest’s life time. This avoids the complicated logic to enable PML by demand when guest is running. And to eliminate potential unnecessary GPA logging in non-dirty logging mode, we set D-bit manually for the slots with dirty logging disabled(<a href="https://lore.kernel.org/kvm/1422413668-3509-4-git-send-email-kai.huang@linux.intel.com/" target="_blank" rel="noopener">KVM: MMU: Explicitly set D-bit for writable spte.</a>).</p><h4 id="3-2-Flush-PML-buffer"><a href="#3-2-Flush-PML-buffer" class="headerlink" title="3.2 Flush PML buffer"></a>3.2 Flush PML buffer</h4><p>When userspace querys dirty_bitmap, it’s possible that there are GPAs logged in vcpu’s PML buffer, but as PML buffer is not full, so no VMEXIT happens. In this case, we’d better to manually flush PML buffer for all vcpus and update the dirty GPAs to dirty_bitmap.</p><p>We do PML buffer flush at the beginning of each VMEXIT, this makes dirty_bitmap more updated, and also makes logic of flushing PML buffer for all vcpus easier– we only need to kick all vcpus out of guest and PML buffer for each vcpu will be flushed automatically.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.</span></span><br><span class="line"><span class="comment"> * Called before reporting dirty_bitmap to userspace.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">kvm_flush_pml_buffers</span><span class="params">(struct kvm *kvm)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_vcpu</span> *<span class="title">vcpu</span>;</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * We only need to kick vcpu out of guest mode here, as PML buffer</span></span><br><span class="line"><span class="comment"> * is flushed at beginning of all VMEXITs, and it's obvious that only</span></span><br><span class="line"><span class="comment"> * vcpus running in guest are possible to have unflushed GPAs in PML</span></span><br><span class="line"><span class="comment"> * buffer.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">kvm_for_each_vcpu(i, vcpu, kvm)</span><br><span class="line">kvm_vcpu_kick(vcpu);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/page-modification-logging-vmm-white-paper.pdf" target="_blank" rel="noopener">Page Modification Logging for Virtual Machine Monitor White Paper</a></li><li><a href="https://lore.kernel.org/kvm/1422413668-3509-1-git-send-email-kai.huang@linux.intel.com/" target="_blank" rel="noopener">KVM: VMX: Page Modification Logging (PML) support</a></li><li><a href="https://diting0x.github.io/20170821/intel-pml/" target="_blank" rel="noopener">Intel VT 页面修改记录(PML)</a></li><li><a href="https://arxiv.org/pdf/2001.09991.pdf" target="_blank" rel="noopener">Intel Page Modification Logging, a hardware virtualization feature: study and improvement for virtual machine working set estimation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍VT-x中的Page-Modification Logging(PML)技术。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to umonitor/umwait/tpause instructions</title>
    <link href="http://liujunming.github.io/2022/03/01/Introduction-to-umonitor-umwait-tpause-instructions/"/>
    <id>http://liujunming.github.io/2022/03/01/Introduction-to-umonitor-umwait-tpause-instructions/</id>
    <published>2022-03-01T01:36:39.000Z</published>
    <updated>2022-03-01T10:42:53.651Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍<code>umonitor</code>、<code>umwait</code>和<code>tpause</code> 这三个指令的相关知识点。<a id="more"></a></p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>最权威的描述当然来自于SDM。</p><h4 id="1-1-UMONITOR—User-Level-Set-Up-Monitor-Address"><a href="#1-1-UMONITOR—User-Level-Set-Up-Monitor-Address" class="headerlink" title="1.1 UMONITOR—User Level Set Up Monitor Address"></a>1.1 UMONITOR—User Level Set Up Monitor Address</h4><p>The UMONITOR instruction arms address <strong>monitoring hardware</strong> using an address specified in the source register(the address range that the monitoring hardware checks for store operations can be determined by using the CPUID monitor leaf function, EAX=05H). A store to an address within the specified address range triggers the monitoring hardware. The state of monitor hardware is used by <strong>UMWAIT</strong>.</p><p>UMONITOR sets up an address range for the monitor hardware using the content of source register as an effective address and puts the monitor hardware in armed state. A store to the specified address range will <strong>trigger</strong> the monitor hardware.</p><h4 id="1-2-UMWAIT—User-Level-Monitor-Wait"><a href="#1-2-UMWAIT—User-Level-Monitor-Wait" class="headerlink" title="1.2 UMWAIT—User Level Monitor Wait"></a>1.2 UMWAIT—User Level Monitor Wait</h4><p>A hint that allows the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events.</p><p>a class of events:</p><ul><li>the monitoring hardware is triggered</li><li>when the time-stamp counter reaches or exceeds the implicit EDX:EAX 64-bit input value(if the monitoring hardware did not trigger beforehand)</li></ul><p>UMWAIT instructs the processor to enter an implementation-dependent optimized state while monitoring a range of addresses. The optimized state may be either a light-weight power/performance optimized state or an improved power/performance optimized state. The selection between the two states is governed by the explicit input register bit[0] source operand.</p><h4 id="1-3-Timed-PAUSE"><a href="#1-3-Timed-PAUSE" class="headerlink" title="1.3 Timed PAUSE"></a>1.3 Timed PAUSE</h4><p>Directs the processor to enter an implementation dependent optimized state until the TSC reaches the value in EDX:EAX.</p><p>TPAUSE instructs the processor to enter an implementation-dependent optimized state. There are two such optimized states to choose from: light-weight power/performance optimized state, and improved power/performance optimized state. The selection between the two is governed by the explicit input register bit[0] source operand.</p><h3 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2. Usage"></a>2. Usage</h3><h4 id="2-1-spin-lock"><a href="#2-1-spin-lock" class="headerlink" title="2.1 spin-lock"></a>2.1 spin-lock</h4><p>Today, if an application needs to wait for a very short duration they have to have spinloops. Spinloops consume more power and continue to use execution resources that could hurt its thread siblings in a core with hyperthreads(HT). New instructions <code>umonitor</code>, <code>umwait</code> and <code>tpause</code> allow a low power alternative waiting at the same time could improve the HT sibling perform while giving it any power headroom. These instructions can be used in both user space and kernel space.</p><p>A new MSR IA32_UMWAIT_CONTROL allows kernel to set a time limit(how long the <code>umwait</code> and <code>tpause</code> instructions can wait before normal execution continues) in TSC-quanta that prevents user applications from waiting for a long time.</p><p>The processor supports two levels of optimized states: a light-weight power/performance optimized state (C0.1 state) or an improved power/performance optimized state (C0.2 state with deeper power saving and higher exit latency). It is conceivable that system administrators might not want to allow the system to go into C0.2 if, for example, it is handling workloads with realtime response requirements. </p><h4 id="2-2-DPDK"><a href="#2-2-DPDK" class="headerlink" title="2.2 DPDK"></a>2.2 DPDK</h4><p><a href="https://lore.kernel.org/dpdk-dev/1639360.KZQzHLtdKU@thomas/T/#m1cfc1a8d08b92f52416ac81fc0559dfebf59f55d" target="_blank" rel="noopener">Power-optimized RX for Ethernet devices</a></p><p>This patchset proposes a simple API for Ethernet drivers to cause the CPU to enter a power-optimized state while waiting for packets to arrive, along with a set of(hopefully generic) intrinsics that facilitate that. This is achieved through cooperation with the NIC driver that will allow us to know address of the next NIC RX(Receive) ring packet descriptor, and wait for writes on it.</p><hr><p>参考资料:</p><ol><li><a href="https://lwn.net/Articles/790920/" target="_blank" rel="noopener">Short waits with umwait</a></li><li><a href="https://lore.kernel.org/lkml/1560994438-235698-1-git-send-email-fenghua.yu@intel.com/" target="_blank" rel="noopener">x86/umwait: Enable user wait instructions</a></li><li><a href="https://lore.kernel.org/lkml/20190716065551.27264-1-tao3.xu@intel.com/" target="_blank" rel="noopener">KVM: x86: Enable user wait instructions</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍&lt;code&gt;umonitor&lt;/code&gt;、&lt;code&gt;umwait&lt;/code&gt;和&lt;code&gt;tpause&lt;/code&gt; 这三个指令的相关知识点。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Time-Stamp Counter Virtualization</title>
    <link href="http://liujunming.github.io/2022/02/28/Time-Stamp-Counter-virtualization/"/>
    <id>http://liujunming.github.io/2022/02/28/Time-Stamp-Counter-virtualization/</id>
    <published>2022-02-28T06:56:25.000Z</published>
    <updated>2022-02-28T10:17:23.634Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录Time-Stamp Counter Virtualization相关内容。<a id="more"></a></p><h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><p><img src="/images/2022/02/28.PNG" alt></p><p>The product means <em>the result obtained by multiplying two or more quantities together</em>.</p><h3 id="TSC-offsetting"><a href="#TSC-offsetting" class="headerlink" title="TSC offsetting"></a>TSC offsetting</h3><p><strong>Timestamp-counter offsetting</strong> (<strong>TSC offsetting</strong>) is an existing feature that allows VMM software to specify a value (the <strong>TSC offset</strong>) that is added to the TSC when it is read by guest software. A VMM can use this feature to provide guest software with the illusion that it is operating at a time later or earlier than that represented by the current TSC value.</p><h3 id="TSC-scaling"><a href="#TSC-scaling" class="headerlink" title="TSC scaling"></a>TSC scaling</h3><p>With TSC offsetting, guest software perceives a TSC that is offset from the real hardware, but which advances at the same rate. That may be adequate for usages in which the offset is used to account for execution time before virtual machine was created. But it might not suffice if the VMM migrates a virtual machine between platforms on which the TSC moves at different rates.</p><p>TSC scaling provides VMM software with a mechanism by which is it can adjust the TSC rate perceived by guest software. When TSC scaling and TSC offsetting are both enabled, reads from the TSC in VMX nonroot operation multiply the actual TSC value by a new <strong>TSC multiplier</strong>, add the TSC offset to the product, and return the sum to guest software.</p><p>With both TSC offsetting and TSC scaling, a VMM that migrates a virtual machine from one platform to another can configure the TSC offset and the TSC multiplier on the new platform so that the TSC (as perceived by the guest) appears to proceed from the same value that it had before the migration <strong>and at the same rate</strong>.</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/timestamp-counter-scaling-virtualization-white-paper.pdf" target="_blank" rel="noopener">Timestamp-Counter Scaling (TSC scaling) for Virtualization</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录Time-Stamp Counter Virtualization相关内容。
    
    </summary>
    
      <category term="VT-x" scheme="http://liujunming.github.io/categories/VT-x/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-x" scheme="http://liujunming.github.io/tags/VT-x/"/>
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
  </entry>
  
  <entry>
    <title>Dive into Time-Stamp Counter</title>
    <link href="http://liujunming.github.io/2022/02/28/Dive-into-Time-Stamp-Counter/"/>
    <id>http://liujunming.github.io/2022/02/28/Dive-into-Time-Stamp-Counter/</id>
    <published>2022-02-28T06:23:39.000Z</published>
    <updated>2022-02-28T10:09:54.798Z</updated>
    
    <content type="html"><![CDATA[<p>本文将深入研究Time-Stamp Counter。<a id="more"></a></p><p><img src="/images/2022/02/24.PNG" alt></p><p><img src="/images/2022/02/25.PNG" alt></p><p><img src="/images/2022/02/26.PNG" alt></p><p><img src="/images/2022/02/27.PNG" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将深入研究Time-Stamp Counter。
    
    </summary>
    
      <category term="Time" scheme="http://liujunming.github.io/categories/Time/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
      <category term="Time" scheme="http://liujunming.github.io/tags/Time/"/>
    
  </entry>
  
  <entry>
    <title>What&#39;s serializing instruction in Intel terminology</title>
    <link href="http://liujunming.github.io/2022/02/28/What-s-serializing-instruction-in-Intel-terminology/"/>
    <id>http://liujunming.github.io/2022/02/28/What-s-serializing-instruction-in-Intel-terminology/</id>
    <published>2022-02-28T05:53:10.000Z</published>
    <updated>2022-02-28T10:09:54.799Z</updated>
    
    <content type="html"><![CDATA[<p>What’s serializing instruction in Intel terminology?<a id="more"></a></p><p><img src="/images/2022/02/23.PNG" alt></p><p>Serializing instructions <strong>force the processor to complete all modifications to flags, registers, and memory by previous instructions and to drain all buffered writes to memory before the next instruction is fetched and executed</strong>. For example, when a MOV to control register instruction is used to load a new value into control register CR0 to enable protected mode, the processor must perform a serializing operation before it enters protected mode. This serializing operation ensures that all operations that were started while the processor was in real-address mode are completed before the switch to protected mode is made.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;What’s serializing instruction in Intel terminology?
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to PKS</title>
    <link href="http://liujunming.github.io/2022/02/27/Introduction-to-PKS/"/>
    <id>http://liujunming.github.io/2022/02/27/Introduction-to-PKS/</id>
    <published>2022-02-27T05:54:21.000Z</published>
    <updated>2022-02-27T06:58:14.794Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍PKS(<strong>P</strong>rotection <strong>K</strong>eys for <strong>S</strong>upervisor Pages).<a id="more"></a></p><h3 id="1-Prerequisite"><a href="#1-Prerequisite" class="headerlink" title="1. Prerequisite"></a>1. Prerequisite</h3><p><a href="/2020/03/07/Introduction-to-pkeys/">Introduction to PKU</a></p><h3 id="2-SPEC"><a href="#2-SPEC" class="headerlink" title="2. SPEC"></a>2. SPEC</h3><p><img src="/images/2022/02/21.PNG" alt></p><p><img src="/images/2022/02/22.PNG" alt></p><h3 id="3-Description"><a href="#3-Description" class="headerlink" title="3. Description"></a>3. Description</h3><p>参见<a href="https://lpc.events/event/11/contributions/907/attachments/787/1699/lpc-2021-PKS-22-Sept-2021.pdf" target="_blank" rel="noopener">Protection Keys, Supervisor (PKS)</a>中的<strong>PKS Hardware Overview</strong>一节。</p><p>Protection Keys for Supervisor Pages(PKS) is a feature that extends the Protection Keys architecture to support thread-specific permission restrictions on supervisor pages.</p><p>PKS works similar to an existing feature named PKU(protecting user pages). They both perform an additional check after normal paging permission checks are done. Access or Writes can be disabled via a MSR update without TLB flushes when permissions changes. If violating this addional check, #PF occurs and PFEC.PK bit will be set.</p><p>PKS introduces MSR IA32_PKRS to manage supervisor protection key rights. The MSR contains 16 pairs of ADi and WDi bits. Each pair advertises on a group of pages with the same key which is set in the leaf paging-structure entries(bits[62:59]). Currently, IA32_PKRS is not supported by XSAVES architecture.</p><hr><p>参考资料:</p><ol><li><a href="https://lpc.events/event/11/contributions/907/attachments/787/1699/lpc-2021-PKS-22-Sept-2021.pdf" target="_blank" rel="noopener">Protection Keys, Supervisor (PKS)</a></li><li><a href="https://lore.kernel.org/lkml/20210505003032.489164-1-rick.p.edgecombe@intel.com/" target="_blank" rel="noopener">PKS write protected page tables</a></li><li><a href="https://lore.kernel.org/lkml/20220127175505.851391-1-ira.weiny@intel.com/" target="_blank" rel="noopener">PKS/PMEM: Add Stray Write Protection</a></li><li><a href="https://lore.kernel.org/lkml/20220221080840.7369-1-chenyi.qiang@intel.com/" target="_blank" rel="noopener">KVM: PKS Virtualization support</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍PKS(&lt;strong&gt;P&lt;/strong&gt;rotection &lt;strong&gt;K&lt;/strong&gt;eys for &lt;strong&gt;S&lt;/strong&gt;upervisor Pages).
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel VT-rp</title>
    <link href="http://liujunming.github.io/2022/02/16/Introduction-to-Intel-VT-rp/"/>
    <id>http://liujunming.github.io/2022/02/16/Introduction-to-Intel-VT-rp/</id>
    <published>2022-02-16T08:50:20.000Z</published>
    <updated>2022-02-16T13:01:30.095Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel的VT-rp技术。<a id="more"></a></p><h3 id="1-Material"><a href="#1-Material" class="headerlink" title="1. Material"></a>1. Material</h3><ul><li>SPEC:<a href="https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html" target="_blank" rel="noopener">ISE</a></li><li><a href="https://static.sched.com/hosted_files/kvmforum2020/34/kvm2020_hypervisor-managed%20linear%20address%20translation_v3.pdf" target="_blank" rel="noopener">Hypervisor-managed Linear Address Translation by Chao Gao slides</a></li><li><a href="https://www.youtube.com/watch?v=j2T90htYSko" target="_blank" rel="noopener">Hypervisor-managed Linear Address Translation by Chao Gao video</a></li></ul><h3 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h3><p><img src="/images/2022/02/08.png" alt><br>对于write-protecting CR3 page table leads to high performance penalty的解释如下：<br>VMMs could enforce the integrity of these specific guest linear to guest physical mappings (paging structures) by using legacy EPT permissions to mark the guest physical memory containing the relevant guest paging structures as read-only. The intent of marking these guest paging structures as read-only is to ensure an invalid mapping is not created by guest software. However, such page-table edit control techniques are known to cause very high overheads(EPT violation VM Exit)  due to the requirement that the VMM must monitor all paging contexts created by the (Guest) operating system.<br><img src="/images/2022/02/09.png" alt></p><h3 id="3-Terms"><a href="#3-Terms" class="headerlink" title="3. Terms"></a>3. Terms</h3><ul><li>Intel VT-rp(Intel Virtualization Technology - Redirect Protection)</li><li>HLAT(Hypervisor-managed Linear Address Translation)</li><li>PLR(Protected Linear Range)</li><li>PW(Paging Write)</li><li>VPW(Verify Paging-Write)</li></ul><h3 id="4-VT-rp"><a href="#4-VT-rp" class="headerlink" title="4. VT-rp"></a>4. VT-rp</h3><p><img src="/images/2022/02/10.png" alt></p><h4 id="4-1-HLAT"><a href="#4-1-HLAT" class="headerlink" title="4.1 HLAT"></a>4.1 HLAT</h4><p><img src="/images/2022/02/11.png" alt><br><img src="/images/2022/02/12.png" alt><br><img src="/images/2022/02/13.png" alt></p><h4 id="4-2-EPT-Control-Bit-“Paging-Write”"><a href="#4-2-EPT-Control-Bit-“Paging-Write”" class="headerlink" title="4.2 EPT Control Bit “Paging-Write”"></a>4.2 EPT Control Bit “Paging-Write”</h4><p>A new EPT control bit called <strong>“Paging-Write”</strong> specified in EPT leaf entries. </p><p><img src="/images/2022/02/16.png" alt></p><p><img src="/images/2022/02/14.png" alt></p><p>硬件会更新guest paging structure pages的A/D bits without EPT violation VM exits</p><h4 id="4-3-EPT-Control-Bit-“Verify-Paging-write”"><a href="#4-3-EPT-Control-Bit-“Verify-Paging-write”" class="headerlink" title="4.3 EPT Control Bit “Verify Paging-write”"></a>4.3 EPT Control Bit “Verify Paging-write”</h4><p>A new EPT control bit called <strong>“Verify Paging-Write”</strong> specified in EPT leaf entries (that refer to the final host physical page in the translation).</p><p><img src="/images/2022/02/17.png" alt></p><p><img src="/images/2022/02/15.png" alt></p><h4 id="4-4-Prevent-Alias-Mapping-with-PW-amp-VPW"><a href="#4-4-Prevent-Alias-Mapping-with-PW-amp-VPW" class="headerlink" title="4.4 Prevent Alias Mapping with PW &amp; VPW"></a>4.4 Prevent Alias Mapping with PW &amp; VPW</h4><p><img src="/images/2022/02/19.png" alt></p><h3 id="5-Implementaion"><a href="#5-Implementaion" class="headerlink" title="5. Implementaion"></a>5. Implementaion</h3><p><img src="/images/2022/02/18.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel的VT-rp技术。
    
    </summary>
    
      <category term="SDM" scheme="http://liujunming.github.io/categories/SDM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
      <category term="Security" scheme="http://liujunming.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to IPIv</title>
    <link href="http://liujunming.github.io/2022/02/15/Introduction-to-IPIv/"/>
    <id>http://liujunming.github.io/2022/02/15/Introduction-to-IPIv/</id>
    <published>2022-02-15T07:17:34.000Z</published>
    <updated>2022-02-16T13:01:30.095Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍Intel的IPIv(IPI Virtualization)硬件技术。<a id="more"></a></p><h3 id="1-Previous-software-technology"><a href="#1-Previous-software-technology" class="headerlink" title="1. Previous software technology"></a>1. Previous software technology</h3><ul><li><a href="/2022/02/12/Introduction-to-PV-IPI/">PV IPI</a></li><li><a href="/2022/02/13/Introduction-to-Passthrough-IPI/">Passthrough IPI</a></li></ul><p>软件方案终究不够完美，还是需要Intel推出硬件方案来解决该问题。</p><h3 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h3><p>Currently, issuing an IPI except self-ipi in guest on Intel CPU always causes a VM-exit. It can lead to non-negligible overhead to some workloads involving frequent IPIs when running in VMs.</p><p>IPI virtualization is a new VT-x feature, targeting to eliminate VM-exits on source vCPUs <strong>when issuing unicast, physical-addressing IPIs</strong>. Once it is enabled, the processor virtualizes following kinds of operations that send IPIs without causing VM-exits:</p><ul><li>Memory-mapped ICR writes</li><li>MSR-mapped ICR writes</li><li>SENDUIPI execution</li></ul><h3 id="3-Spec"><a href="#3-Spec" class="headerlink" title="3. Spec"></a>3. Spec</h3><p>latest <a href="https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html" target="_blank" rel="noopener">Intel Architecture Instruction Set Extensions Programming Reference</a><br><img src="/images/2022/02/05.png" alt></p><p>Idea:</p><ul><li>The processor uses a data structure called the PID-pointer table. Each entry in the PID-pointer table contains the 64-bit physical address of a PID.</li><li>The processor indexes into a PID-pointer table using a virtual APIC ID<br><img src="/images/2022/02/06.png" alt></li></ul><p>不是所有类型的IPI都可以利用IPIv的，只有满足一定条件时，硬件的IPIv才能生效，否则，依然需要发生VM Exit。<br><img src="/images/2022/02/07.png" alt><br>这也是为什么 <a href="https://lore.kernel.org/kvm/20211231142849.611-1-guang.zeng@intel.com/" target="_blank" rel="noopener">IPI virtualization support for VM</a>的cover letter中有这样的描述:<em>when issuing unicast, physical-addressing IPIs</em>. 像SIPI/NMI/INIT等IPI就不能使用IPIv。</p><h3 id="4-Implementation"><a href="#4-Implementation" class="headerlink" title="4. Implementation"></a>4. Implementation</h3><p>mailing patch: <a href="https://lore.kernel.org/kvm/20211231142849.611-1-guang.zeng@intel.com/" target="_blank" rel="noopener">IPI virtualization support for VM</a><br>等以后IPIv feature upstream了再更新吧，说白了，patch的功能就是让硬件happy。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍Intel的IPIv(IPI Virtualization)硬件技术。
    
    </summary>
    
      <category term="中断" scheme="http://liujunming.github.io/categories/%E4%B8%AD%E6%96%AD/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="中断" scheme="http://liujunming.github.io/tags/%E4%B8%AD%E6%96%AD/"/>
    
  </entry>
  
</feed>
