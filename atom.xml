<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2021-10-28T10:05:35.381Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Dive into irqfd(KVM side) mechanism</title>
    <link href="http://liujunming.github.io/2021/10/27/Dive-into-irqfd-KVM-side-mechanism/"/>
    <id>http://liujunming.github.io/2021/10/27/Dive-into-irqfd-KVM-side-mechanism/</id>
    <published>2021-10-27T01:53:42.000Z</published>
    <updated>2021-10-28T10:05:35.381Z</updated>
    
    <content type="html"><![CDATA[<p>本文将深入理解irqfd机制，偏向于KVM side。为了便于理清irqfd机制，本文只介绍patch <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=721eecbf4fe995ca94a9edec0c9843b1cc0eaaf3" target="_blank" rel="noopener">KVM: irqfd</a>中的内容。<a id="more"></a></p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>irqfd in KVM is implemented based on eventfd in Linux.</p><p>As its name shows, irqfd is basically a fd that is bound to an interrupt in the virtual machine. Here the fd must be an eventfd. The delivery path is single direction, say, interrupt is delivered from outside world into the guest. </p><p>With irqfd, if we want to trigger an interrupt we have setup, what we need to do is only write to that corresponding eventfd. To write it in userspace, a simple <code>write()</code> syscall would suffice (actually there is a libc call named <code>eventfd_write()</code>, however that’s merely a wrapper of the <code>write()</code> system call). To do it in kernel, we can use <code>eventfd_signal()</code> instead.</p><h3 id="2-Overview"><a href="#2-Overview" class="headerlink" title="2. Overview"></a>2. Overview</h3><p><img src="/images/2021/10/16.png" alt></p><p>irqfd基于eventfd机制，qemu中将一个gsi(全局系统中断号)与eventfd捆绑后，向kvm发送注册irqfd请求，kvm收到请求后将带有gsi信息的eventfd加入到与irqfd有关的等待队列中，一旦有进程向该eventfd写入，等待队列中的元素就会唤醒，并调用相应的唤醒函数(<code>irqfd_wakeup</code>)向guest注入中断(<code>irqfd_inject</code>)。</p><h3 id="3-Details"><a href="#3-Details" class="headerlink" title="3. Details"></a>3. Details</h3><p><img src="/images/2021/10/17.png" alt></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_irqfd_assign</span><span class="params">(struct kvm *kvm, struct kvm_irqfd *args)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_kernel_irqfd</span> *<span class="title">irqfd</span>;</span></span><br><span class="line">...</span><br><span class="line">irqfd = kzalloc(<span class="keyword">sizeof</span>(*irqfd), GFP_KERNEL_ACCOUNT);</span><br><span class="line">irqfd-&gt;kvm = kvm;</span><br><span class="line">irqfd-&gt;gsi = args-&gt;gsi;</span><br><span class="line">INIT_LIST_HEAD(&amp;irqfd-&gt;<span class="built_in">list</span>);</span><br><span class="line">INIT_WORK(&amp;irqfd-&gt;inject, irqfd_inject);</span><br><span class="line">INIT_WORK(&amp;irqfd-&gt;shutdown, irqfd_shutdown);</span><br><span class="line">...</span><br><span class="line">irqfd-&gt;eventfd = eventfd;</span><br><span class="line">...</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Install our own custom wake-up handling so we are notified via</span></span><br><span class="line"><span class="comment"> * a callback whenever someone signals the underlying eventfd</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">init_waitqueue_func_entry(&amp;irqfd-&gt;wait, irqfd_wakeup);</span><br><span class="line">...</span><br><span class="line">init_poll_funcptr(&amp;irqfd-&gt;pt, irqfd_ptable_queue_proc);</span><br><span class="line">events = vfs_poll(f.file, &amp;irqfd-&gt;pt);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>kvm_kernel_irqfd结构中有2个work_struct，inject和shutdown，分别负责触发中断和关闭中断，这两个work_struct各自对应的操作函数分别为irqfd_inject和irqfd_shutdown。</p><p>kvm_irqfd_assign调用init_waitqueue_func_entry将<strong>irqfd_wakeup</strong>函数注册为irqfd中<strong>wait queue entry</strong>激活时的处理函数。<strong>这样任何写入该irqfd对应的eventfd的行为都将触发这个函数。</strong></p><p>kvm_irqfd_assign利用init_poll_funcptr将irqfd_ptable_queue_proc函数注册为irqfd中的poll table的处理函数。<strong>irqfd_ptable_queue_proc会将poll table中对应的wait queue entry加入到waitqueue中去。</strong></p><p>kvm_irq_assign以irqfd-&gt;pt为参数，调用eventfd的poll函数，也就是eventfd_poll；eventfd_poll会调用poll_wait函数；poll_wait会回调之前为poll table注册的irqfd_ptable_queue_proc函数。</p><hr><p>参考资料:</p><ol><li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=721eecbf4fe995ca94a9edec0c9843b1cc0eaaf3" target="_blank" rel="noopener">KVM: irqfd</a></li><li><a href="https://blog.csdn.net/LoyenWang/article/details/115805007?spm=1001.2014.3001.5501" target="_blank" rel="noopener">Linux虚拟化KVM-Qemu分析（十二）之ioeventfd与irqfd</a></li><li><a href="https://www.cnblogs.com/haiyonghao/p/14440723.html" target="_blank" rel="noopener">qemu-kvm的irqfd机制</a></li><li><a href="http://xzpeter.org/htmls/2017_12_07_kvm_irqfd/kvm_irqfd_implementation.html" target="_blank" rel="noopener">KVM Irqfd Introduction</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将深入理解irqfd机制，偏向于KVM side。为了便于理清irqfd机制，本文只介绍patch &lt;a href=&quot;https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=721eecbf4fe995ca94a9edec0c9843b1cc0eaaf3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;KVM: irqfd&lt;/a&gt;中的内容。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>Dive into ioeventfd(KVM side) mechanism</title>
    <link href="http://liujunming.github.io/2021/10/26/Dive-into-ioeventfd(kvm%20side)-mechanism/"/>
    <id>http://liujunming.github.io/2021/10/26/Dive-into-ioeventfd(kvm side)-mechanism/</id>
    <published>2021-10-26T07:07:25.000Z</published>
    <updated>2021-10-26T15:12:22.036Z</updated>
    
    <content type="html"><![CDATA[<p>本文将深入理解ioeventfd机制，偏向于KVM side。<a id="more"></a></p><h3 id="1-Prerequisite"><a href="#1-Prerequisite" class="headerlink" title="1. Prerequisite"></a>1. Prerequisite</h3><p><a href="/2021/10/25/eventfd-system-call内核实现/">eventfd system call内核实现</a></p><h3 id="2-Introduction-and-Motivation"><a href="#2-Introduction-and-Motivation" class="headerlink" title="2. Introduction and Motivation"></a>2. Introduction and Motivation</h3><p><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=d34e6b175e61821026893ec5298cc8e7558df43a" target="_blank" rel="noopener">KVM: add ioeventfd support</a>中的commit message很好地阐述了ioeventfd的motivation。</p><p>ioeventfd is a mechanism to <strong>register PIO/MMIO regions to trigger an eventfd signal when written to by a guest</strong>.  Host userspace can register any arbitrary IO address with a corresponding eventfd and then pass the eventfd to a specific end-point of interest for handling.</p><p>Normal IO requires a blocking round-trip since the operation may cause side-effects in the emulated model or may return data to the caller. Therefore, an IO in KVM traps from the guest to the host, causes a VMX/SVM “heavy-weight” exit back to userspace, and is ultimately serviced by qemu’s device model <strong>synchronously</strong> before returning control back to the vcpu.</p><p>However, there is a subclass of IO which acts purely as a trigger for other IO (such as to kick off an out-of-band DMA request, etc).  For these patterns, the synchronous call is particularly expensive since we really only want to simply get our notification transmitted <strong>asychronously</strong> and return as quickly as possible.  All the sychronous infrastructure to ensure proper data-dependencies are met in the normal IO case are just unecessary overhead for signalling.  This adds additional computational load on the system, as well as latency to the signalling path.</p><p>Therefore, we provide a mechanism for registration of an in-kernel trigger point that allows the VCPU to only require a very brief, lightweight exit just long enough to signal an eventfd.  This also means that any clients compatible with the eventfd interface (which includes userspace and kernelspace equally well) can now register to be notified. The end result should be a more flexible and higher performance notification API for the backend KVM hypervisor and perhipheral components.</p><p>读者如果对<em>kick off an out-of-band DMA request</em>这句话不够理解，可以阅读:<a href="/2021/09/12/深入理解DMA-part1/">深入理解DMA part1</a>和<a href="/2021/09/13/深入理解DMA-part2/">深入理解DMA part2</a></p><h3 id="3-Overview"><a href="#3-Overview" class="headerlink" title="3. Overview"></a>3. Overview</h3><p><img src="/images/2021/10/15.jpeg" alt></p><p>Sequences:</p><ol><li>QEMU将一段PIO/MMIO region与eventfd绑定(具体来说，就是填好<code>struct kvm_ioeventfd</code>)，并设置好notification的handler;</li><li>通过ioctl将<code>struct kvm_ioeventfd</code>结构体传给KVM;</li><li>KVM根据信息，注册PIO/MMIO region的handler为<code>ioeventfd_ops</code>;</li><li>Guest写PIO/MMIO region时，会发生VM Exit，KVM最终会调用<code>ioeventfd_write</code> to trigger an event to QEMU;</li><li>QEMU监测到ioeventfd上出现了event，调用相应的handler处理IO.</li></ol><h3 id="4-Details"><a href="#4-Details" class="headerlink" title="4. Details"></a>4. Details</h3><p>用户态传入的参数如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_ioeventfd</span> &#123;</span></span><br><span class="line">    __u64 datamatch;<span class="comment">/* 1 */</span></span><br><span class="line">    __u64 addr;        <span class="comment">/* legal pio/mmio address */</span></span><br><span class="line">    __u32 len;         <span class="comment">/* 0, 1, 2, 4, or 8 bytes    */</span></span><br><span class="line">    __s32 fd;<span class="comment">/* 2 */</span></span><br><span class="line">    __u32 flags;</span><br><span class="line">    __u8  pad[<span class="number">36</span>];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>如果<code>flags</code>设置了<code>KVM_IOEVENTFD_FLAG_DATAMATCH</code>，只有当guest向addr地址写入的值与<code>datamatch</code>值相等时，才会触发event。</p><p>用户态信息<code>kvm_ioeventfd</code>需要转化成内核态存放。ioeventfd内核态结构体基于eventfd，如下所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * --------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"> * ioeventfd: translate a PIO/MMIO memory write to an eventfd signal.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * userspace can register a PIO/MMIO address with an eventfd for receiving</span></span><br><span class="line"><span class="comment"> * notification when the memory has been touched.</span></span><br><span class="line"><span class="comment"> * --------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">ioeventfd</span> &#123;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">list_head</span>     <span class="title">list</span>;</span></span><br><span class="line">u64                  addr;</span><br><span class="line"><span class="keyword">int</span>                  length;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventfd_ctx</span>  *<span class="title">eventfd</span>;</span></span><br><span class="line">u64                  datamatch;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_io_device</span> <span class="title">dev</span>;</span></span><br><span class="line">u8                   bus_idx;</span><br><span class="line"><span class="keyword">bool</span>                 wildcard;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li><p>list用于将当前ioeventfd链接到kvm的ioeventfd链表中去.</p></li><li><p>addr是ioeventfd对应的IO地址.</p></li><li><p>length是eventfd关联的长度.</p></li><li><p>eventfd是该ioeventfd对应的eventfd.</p></li><li><p>datamatch上文已经介绍过了.</p></li><li><p>dev用于将该ioeventfd与guest关联起来(通过注册该dev到guest).</p></li><li><p>bus_idx是该ioeventfd要注册到kvm的哪个总线上.</p><ul><li>kvm中将ioeventfd注册的地址分为4类，可以认为每类地址有独立的地址空间，它们被抽象成4个bus上的地址。分别是kvm_bus所列出的MMIO，PIO，VIRTIO_CCW_NOTIFY，FAST_MMIO。MMIO和FAST_MMIO的区别是，MMIO需要检查写入地址的值长度是否和ioeventfd指定的长度相等，FAST_MMIO则不需要检查长度。</li></ul></li><li>wildcard与datamatch互斥，如果kvm_ioeventfd中datamatch为false，则_ioeventfd-&gt;wildcard设为true.</li></ul><p>所以<code>_ioeventfd</code>描述了一个ioeventfd要注册到kvm中的所有信息，其中包含了ioeventfd信息和需要注册到guest的总线和设备信息。</p><hr><p>KVM中的函数调用链如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kvm_ioeventfd</span><br><span class="line">kvm_assign_ioeventfd</span><br><span class="line">kvm_assign_ioeventfd_idx</span><br><span class="line">kvm_iodevice_init(&amp;p-&gt;dev, &amp;ioeventfd_ops)</span><br><span class="line">kvm_io_bus_register_dev</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_io_bus_write</span><br><span class="line">__kvm_io_bus_write</span><br><span class="line">kvm_iodevice_write</span><br><span class="line">dev-&gt;ops-&gt;write(ioeventfd_write)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">kvm_io_device_ops</span> <span class="title">ioeventfd_ops</span> = &#123;</span></span><br><span class="line">.write      = ioeventfd_write,</span><br><span class="line">.destructor = ioeventfd_destructor,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><blockquote><p>需要注意的是，ioeventfd对应的文件操作只有write操作，而没有read操作。</p><p>write操作对应guest中写入ioeventfd对应的IO地址时触发的操作，也就是guest执行OUT类汇编指令时触发的操作，相反read操作就是guest执行IN类汇编指令时触发的操作，OUT类指令只是简单向外部输出数据，无需等待QEMU处理完成即可继续运行guest，但IN指令需要从外部获取数据，必须要等待QEMU处理完成IO请求再继续运行guest。</p><p>ioeventfd设计的初衷就是节省guest运行OUT类指令时的时间，IN类指令执行时间无法节省，因此这里的ioeventfd 文件操作中只有write而没有read。</p></blockquote><p>剩下的事情就留给读者了，结合着源码与参考资料，去发现更多的细节吧!</p><hr><p>参考资料:</p><ol><li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=d34e6b175e61821026893ec5298cc8e7558df43a" target="_blank" rel="noopener">KVM: add ioeventfd support</a></li><li><a href="https://www.cnblogs.com/haiyonghao/p/14440743.html" target="_blank" rel="noopener">qemu-kvm的ioeventfd机制</a></li><li><a href="https://blog.csdn.net/huang987246510/article/details/105618557" target="_blank" rel="noopener">qemu中的eventfd——ioeventfd</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将深入理解ioeventfd机制，偏向于KVM side。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>eventfd system call内核实现</title>
    <link href="http://liujunming.github.io/2021/10/25/eventfd-system-call%E5%86%85%E6%A0%B8%E5%AE%9E%E7%8E%B0/"/>
    <id>http://liujunming.github.io/2021/10/25/eventfd-system-call内核实现/</id>
    <published>2021-10-25T00:55:15.000Z</published>
    <updated>2021-10-25T10:06:37.351Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍<code>eventfd</code>这个system call的内核实现。<a id="more"></a></p><h3 id="1-Prerequisite"><a href="#1-Prerequisite" class="headerlink" title="1. Prerequisite"></a>1. Prerequisite</h3><ul><li><code>man eventfd</code></li><li><a href="/2019/08/24/linux-kernel中eventfd的使用/">linux kernel中eventfd的使用</a></li></ul><p>summary(仅考虑eventfd的flags为0 ，同时eventfd counter 没有 exceed the maximum):</p><ol><li>the eventfd counter has a nonzero value, then a <code>read</code> returns 8 bytes containing that value, and the counter’s value is reset to zero;</li><li>If the eventfd counter is zero at the time of the call to <code>read</code>, then the call blocks until the counter becomes nonzero;</li><li>A  <code>write</code>  call  adds  the 8-byte integer value supplied in its buffer to the counter;</li><li><code>eventfd_signal(struct eventfd_ctx *ctx, __u64 n)</code>: Adds @n to the eventfd counter.</li></ol><h3 id="2-Data-struct"><a href="#2-Data-struct" class="headerlink" title="2. Data struct"></a>2. Data struct</h3><p><code>eventfd_ctx</code>结构的形式如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventfd_ctx</span> &#123;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kref</span> <span class="title">kref</span>;</span></span><br><span class="line"><span class="keyword">wait_queue_head_t</span> wqh;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Every time that a write(2) is performed on an eventfd, the</span></span><br><span class="line"><span class="comment"> * value of the __u64 being written is added to "count" and a</span></span><br><span class="line"><span class="comment"> * wakeup is performed on "wqh". A read(2) will return the "count"</span></span><br><span class="line"><span class="comment"> * value to userspace, and will reset "count" to zero. The kernel</span></span><br><span class="line"><span class="comment"> * side eventfd_signal() also, adds to the "count" counter and</span></span><br><span class="line"><span class="comment"> * issue a wakeup.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">__u64 count;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> flags;</span><br><span class="line"><span class="keyword">int</span> id;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>在一个eventfd上执行<code>write</code>系统调用，会向<code>count</code>加上被写入的值，并唤醒等待队列<code>wqh</code>中的元素。内核中的<code>eventfd_signal</code>函数也会增加<code>count</code>的值并唤醒<code>wqh</code>中的元素。</p><p>在eventfd上执行<code>read</code>系统调用，会向用户空间返回<code>count</code>的值，并且该eventfd对应的<code>eventfd_ctx</code>结构中的<code>count</code>会被清0。</p><p><code>kref</code>是一个内核中的通用变量，一般插入到结构体中，用于记录该结构体被内核各处引用的次数，当<code>kref-&gt;refcount</code>为0时，该结构体不再被引用，需要进行释放。</p><p><code>flags</code>由调用者传入，可能取值为<code>EFD_CLOEXEC</code>、<code>EFD_NONBLOCK</code>、<code>EFD_SEMAPHORE</code>三者的任意或组合。</p><p><code>id</code>即eventfd的id，用于唯一标识一个eventfd。</p><h3 id="3-Core-function"><a href="#3-Core-function" class="headerlink" title="3. Core function"></a>3. Core function</h3><h4 id="3-1-系统调用的定义"><a href="#3-1-系统调用的定义" class="headerlink" title="3.1 系统调用的定义"></a>3.1 系统调用的定义</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">SYSCALL_DEFINE1(eventfd, <span class="keyword">unsigned</span> <span class="keyword">int</span>, count)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> do_eventfd(count, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">do_eventfd</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> count, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventfd_ctx</span> *<span class="title">ctx</span>;</span></span><br><span class="line"><span class="keyword">int</span> fd;</span><br><span class="line">...</span><br><span class="line">ctx = kmalloc(<span class="keyword">sizeof</span>(*ctx), GFP_KERNEL);</span><br><span class="line">...</span><br><span class="line">kref_init(&amp;ctx-&gt;kref);</span><br><span class="line">init_waitqueue_head(&amp;ctx-&gt;wqh);</span><br><span class="line">ctx-&gt;count = count;</span><br><span class="line">ctx-&gt;flags = flags;</span><br><span class="line">ctx-&gt;id = ida_simple_get(&amp;eventfd_ida, <span class="number">0</span>, <span class="number">0</span>, GFP_KERNEL);</span><br><span class="line"></span><br><span class="line">fd = anon_inode_getfd(<span class="string">"[eventfd]"</span>, &amp;eventfd_fops, ctx,</span><br><span class="line">      O_RDWR | (flags &amp; EFD_SHARED_FCNTL_FLAGS));</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> fd;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>分配一个eventfd_ctx结构用于存储eventfd相关信息</li><li>设置eventfd_ctx-&gt;kref中的值为1，表明内核正在引用该eventfd</li><li>初始化eventfd_ctx结构中的等待队列</li><li>为eventfd_ctx结构中的count(读写eventfd时要操作的量)赋上系统调用传入的count</li><li>通过Linux提供的<strong>ida机制</strong>为eventfd_ctx结构中的id申请一个id</li><li>通过anon_inode_getfd创建一个文件实例，该文件的操作方法为eventfd_fops，fd-&gt;private_data为eventfd_ctx，文件实例名为eventfd。</li><li>返回该文件实例的文件描述符</li></ol><h4 id="3-2-eventfd-read"><a href="#3-2-eventfd-read" class="headerlink" title="3.2 eventfd_read"></a>3.2 eventfd_read</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> ssize_t <span class="title">eventfd_read</span><span class="params">(struct file *file, <span class="keyword">char</span> __user *buf, <span class="keyword">size_t</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">loff_t</span> *ppos)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventfd_ctx</span> *<span class="title">ctx</span> = <span class="title">file</span>-&gt;<span class="title">private_data</span>;</span></span><br><span class="line"><span class="keyword">ssize_t</span> res;</span><br><span class="line">__u64 ucnt = <span class="number">0</span>;</span><br><span class="line">DECLARE_WAITQUEUE(wait, current);</span><br><span class="line"></span><br><span class="line">res = -EAGAIN;</span><br><span class="line"><span class="keyword">if</span> (ctx-&gt;count &gt; <span class="number">0</span>)</span><br><span class="line">res = <span class="keyword">sizeof</span>(ucnt);</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!(file-&gt;f_flags &amp; O_NONBLOCK)) &#123;</span><br><span class="line"><span class="comment">/*add to wait queue*/</span></span><br><span class="line">__add_wait_queue(&amp;ctx-&gt;wqh, &amp;wait);</span><br><span class="line"><span class="keyword">for</span> (;;) &#123;</span><br><span class="line">set_current_state(TASK_INTERRUPTIBLE);</span><br><span class="line"><span class="keyword">if</span> (ctx-&gt;count &gt; <span class="number">0</span>) &#123;</span><br><span class="line">res = <span class="keyword">sizeof</span>(ucnt);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"><span class="comment">/*触发调度器，执行调度*/</span></span><br><span class="line">schedule();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*remove from the wait queue*/</span></span><br><span class="line">__remove_wait_queue(&amp;ctx-&gt;wqh, &amp;wait);</span><br><span class="line">__set_current_state(TASK_RUNNING);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (likely(res &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">eventfd_ctx_do_read(ctx, &amp;ucnt);</span><br><span class="line"><span class="comment">/*judge whether wait queue is empty*/</span></span><br><span class="line"><span class="keyword">if</span> (waitqueue_active(&amp;ctx-&gt;wqh)) <span class="comment">//在该eventfd上write阻塞的进程</span></span><br><span class="line">wake_up_locked_poll(&amp;ctx-&gt;wqh, EPOLLOUT); <span class="comment">//唤醒对应的进程</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">eventfd_ctx_do_read</span><span class="params">(struct eventfd_ctx *ctx, __u64 *cnt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">*cnt = (ctx-&gt;flags &amp; EFD_SEMAPHORE) ? <span class="number">1</span> : ctx-&gt;count;</span><br><span class="line">ctx-&gt;count -= *cnt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>只有eventfd_ctx-&gt;count大于0时，eventfd才是可读的，此时调用eventfd_ctx_do_read对eventfd_ctx的count进行处理，如果eventfd_ctx-&gt;flags中的EFD_SEMAPHORE为0，就将count变量置0，并激活在等待队列中EPOLLOUT(write阻塞)的进程。</p></li><li><p>如果eventfd_ctx-&gt;count等于0，即该eventfd当前不可读，如果eventfd_ctx-&gt;flags中的O_NONBLOCK没有置位，那么将发起读eventfd动作的进程放入eventfd_ctx中的等待队列，并重新调度新的进程运行。</p></li></ul><h4 id="3-3-eventfd-write"><a href="#3-3-eventfd-write" class="headerlink" title="3.3 eventfd_write"></a>3.3 eventfd_write</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> ssize_t <span class="title">eventfd_write</span><span class="params">(struct file *file, <span class="keyword">const</span> <span class="keyword">char</span> __user *buf, <span class="keyword">size_t</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">loff_t</span> *ppos)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventfd_ctx</span> *<span class="title">ctx</span> = <span class="title">file</span>-&gt;<span class="title">private_data</span>;</span></span><br><span class="line"><span class="keyword">ssize_t</span> res;</span><br><span class="line">__u64 ucnt;</span><br><span class="line">DECLARE_WAITQUEUE(wait, current);</span><br><span class="line"></span><br><span class="line">copy_from_user(&amp;ucnt, buf, <span class="keyword">sizeof</span>(ucnt));</span><br><span class="line">...</span><br><span class="line">res = -EAGAIN;</span><br><span class="line"><span class="keyword">if</span> (ULLONG_MAX - ctx-&gt;count &gt; ucnt)</span><br><span class="line">res = <span class="keyword">sizeof</span>(ucnt);</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!(file-&gt;f_flags &amp; O_NONBLOCK)) &#123;</span><br><span class="line">__add_wait_queue(&amp;ctx-&gt;wqh, &amp;wait);</span><br><span class="line"><span class="keyword">for</span> (res = <span class="number">0</span>;;) &#123;</span><br><span class="line">set_current_state(TASK_INTERRUPTIBLE);</span><br><span class="line"><span class="keyword">if</span> (ULLONG_MAX - ctx-&gt;count &gt; ucnt) &#123;</span><br><span class="line">res = <span class="keyword">sizeof</span>(ucnt);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">schedule();</span><br><span class="line">&#125;</span><br><span class="line">__remove_wait_queue(&amp;ctx-&gt;wqh, &amp;wait);</span><br><span class="line">__set_current_state(TASK_RUNNING);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (likely(res &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">ctx-&gt;count += ucnt;</span><br><span class="line"><span class="keyword">if</span> (waitqueue_active(&amp;ctx-&gt;wqh)) <span class="comment">//在该eventfd上read阻塞的进程</span></span><br><span class="line">wake_up_locked_poll(&amp;ctx-&gt;wqh, EPOLLIN);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将想要写入eventfd的value赋值到ucnt，判断ULLONG_MAX - eventfd_ctx-&gt;count 与ucnt的大小，确认eventfd中是否还有足够空间用于写入。</p><ul><li><p>如果有足够空间用于写入，ctx-&gt;count += ucnt，并激活在等待队列中EPOLLIN(read阻塞)的进程。</p></li><li><p>如果没有足够空间用于写入，则将发起写eventfd动作的进程放入eventfd_ctx中的等待队列，并重新调度新的进程运行。</p></li></ul><h4 id="3-4-eventfd-signal"><a href="#3-4-eventfd-signal" class="headerlink" title="3.4 eventfd_signal"></a>3.4 eventfd_signal</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * eventfd_signal - Adds @n to the eventfd counter.</span></span><br><span class="line"><span class="comment"> * @ctx: [in] Pointer to the eventfd context.</span></span><br><span class="line"><span class="comment"> * @n: [in] Value of the counter to be added to the eventfd internal counter.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">__<span class="function">u64 <span class="title">eventfd_signal</span><span class="params">(struct eventfd_ctx *ctx, __u64 n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> (ULLONG_MAX - ctx-&gt;count &lt; n)</span><br><span class="line">n = ULLONG_MAX - ctx-&gt;count;</span><br><span class="line">ctx-&gt;count += n;</span><br><span class="line"><span class="keyword">if</span> (waitqueue_active(&amp;ctx-&gt;wqh)) <span class="comment">//在该eventfd上read阻塞的进程</span></span><br><span class="line">wake_up_locked_poll(&amp;ctx-&gt;wqh, EPOLLIN);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://www.cnblogs.com/haiyonghao/p/14440737.html" target="_blank" rel="noopener">Linux的eventfd机制</a></li><li><a href="https://huazq.github.io/2019/08/08/eventfd%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">eventfd分析</a></li><li><a href="https://www.cnblogs.com/ck1020/p/7214310.html" target="_blank" rel="noopener">Linux eventfd分析</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍&lt;code&gt;eventfd&lt;/code&gt;这个system call的内核实现。
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>x86-64 Instruction Encoding</title>
    <link href="http://liujunming.github.io/2021/10/22/x86-64-Instruction-Encoding/"/>
    <id>http://liujunming.github.io/2021/10/22/x86-64-Instruction-Encoding/</id>
    <published>2021-10-22T03:20:48.000Z</published>
    <updated>2021-10-22T07:16:48.160Z</updated>
    
    <content type="html"><![CDATA[<p>本文将借鉴<a href="https://www.systutorials.com/beginners-guide-x86-64-instruction-encoding/" target="_blank" rel="noopener">A Beginners’ Guide to x86-64 Instruction Encoding</a>，并补充相关材料，以一个具体的例子来介绍Intel Instruction Encoding。<br><a id="more"></a></p><h3 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h3><p>以一个典型的memory reference来引入Instruction Encoding。<br><code>[base + index*scale + disp]</code></p><p><code>base</code>和<code>index</code>是寄存器，<code>disp</code>是偏移量，<code>scale</code>是系数。<br><img src="/images/2021/10/09.png" alt></p><p><center>Figure1</center><br>Figure1中，SIB中的Scale，Index，Base与<code>scale</code>,<code>index</code>,<code>base</code>相对应。 Displacement与<code>disp</code>相对应。</p><h3 id="2-Tools-and-tips-for-finding-out-an-x86-64-instruction’s-encoding"><a href="#2-Tools-and-tips-for-finding-out-an-x86-64-instruction’s-encoding" class="headerlink" title="2. Tools and tips for finding out an x86-64 instruction’s encoding"></a>2. Tools and tips for finding out an x86-64 instruction’s encoding</h3><p>To quickly find out the encoding of an instruction, you can use the GNU assembler  <code>as</code> and the <code>objdump</code> tool together. For example, to find out the encoding of the instruction <code>addq 10(%rdi), %r8</code>, you can do it as follows.</p><p>First, create a file add.s containing one line<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">addq 10(%rdi), %r8</span><br></pre></td></tr></table></figure></p><p>Second, assemble the add.s to object file by<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ as add.s -o add.o</span><br></pre></td></tr></table></figure></p><p>Last, deassemble the object file by</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ objdump -d add.o</span><br></pre></td></tr></table></figure><p>It will print out</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">add.o:     file format elf64-x86-64</span><br><span class="line"></span><br><span class="line">Disassembly of section .text:</span><br><span class="line"></span><br><span class="line">0000000000000000 &lt;.text&gt;:</span><br><span class="line">   0:   4c 03 47 0a             add    0xa(%rdi),%r8</span><br></pre></td></tr></table></figure><p>Here <code>4c 03 47 0a</code> is the 4-byte encoding of the <code>addq</code> instruction.</p><h3 id="3-Brief-introduction-to-x86-64-instruction-encoding"><a href="#3-Brief-introduction-to-x86-64-instruction-encoding" class="headerlink" title="3. Brief introduction to x86-64 instruction encoding"></a>3. Brief introduction to x86-64 instruction encoding</h3><p>The x86-64 instructions are encoded one by one as a variable number of bytes for each. Each instruction’s encoding consists of:</p><ul><li>an opcode</li><li>a register and/or address mode specifier consisting of the ModR/M byte and sometimes the scale-index-base (SIB) byte (if required)</li><li>a displacement and an immediate data field (if required)</li></ul><p>Please refer to Figure1 for more information.</p><h3 id="4-An-example-manually-encode-an-x86-64-instruction"><a href="#4-An-example-manually-encode-an-x86-64-instruction" class="headerlink" title="4. An example: manually encode an x86-64 instruction"></a>4. An example: manually encode an x86-64 instruction</h3><p>Let’s take a look at the encoding of an instruction <code>add r8,QWORD PTR [rdi+0xa]</code> (in Intel syntax) in the previous part. Let’s see how it is encoded to <code>4c 03 47 0a</code>.</p><p>From the “add” instruction reference from “ADD”, “INSTRUCTION SET REFERENCE” in the ISA reference Volume 2A., find the line for the encoding of the <code>ADD r64, r/m64</code> corresponding to this instruction</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Opcode      Instruction     Op/  64-bit Compat/   Description</span><br><span class="line">                            En   Mode   Leg Mode</span><br><span class="line">REX.W+03/r  ADD r64,r/m64   RM   Valid  N.E.      Add r/m64 to r64.</span><br></pre></td></tr></table></figure><p>REX info:</p><p><img src="/images/2021/10/13.png" alt></p><p>and, from the REX description</p><blockquote><p>In 64-bit mode, the instruction’s default operation size is 32 bits. … Using a REX prefix in the form of REX.W promotes operation to 64 bits.</p></blockquote><p>So, we get</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">REX.W = 1</span><br></pre></td></tr></table></figure><p>The ‘R’, ‘X’ and ‘B’ bits are related to the operand encoding (check “Table 2-4. REX Prefix Fields [BITS: 0100WRXB]” of the reference volume 2A).</p><blockquote><p>REX.X bit modifies the SIB index field.</p></blockquote><p>SIB is not used in this instruction. Hence,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">REX.X = 0</span><br></pre></td></tr></table></figure><p>Let’s further look at the encoding of the operands. From the “Instruction Operand Encoding” for the <code>add</code> instruction:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Op/En Operand 1      Operand 2    Operand 3 Operand 4</span><br><span class="line">RM    ModRM:reg(r,w) ModRM:r/m(r) NA        NA</span><br></pre></td></tr></table></figure><p>There will be 2 operand parts for the <code>RM</code> encoding. The first part will be <code>ModRM:reg(r,w)</code> and the second part will be <code>ModRM:r/m(r)</code>. “Figure 2-4. Memory Addressing Without an SIB Byte; REX.X Not Used” from Volume 2 shows the encoding for this case.</p><p><img src="/images/2021/10/12.jpg" alt></p><p><img src="/images/2021/10/14.png" alt></p><p>The REX.R and REX.B bits and the ModeRM byte will be decided accordingly. There are 3 parts in the ModRM byte: ‘mod’, ‘reg’ and ‘r/m’.</p><p>There is a table “Table 2-2. 32-Bit Addressing Forms with the ModR/M Byte” (it is for 32-bit operands. But from 2.2.1.1, “In 64-bit mode, these formats do not change. Bits needed to<br>define fields in the 64-bit context are provided by the addition of REX prefixes” and hence the same value can be used) in Volume 2 which shows mapping of the operands combinations to the bits values of ‘mod’.</p><p>Although the table applies to 64-bit modes too, it does not show the additional registers like <code>r8</code>. Hence, we only use it to find out bits for ‘Mod’ only for the <code>addq</code> instruction we are encoding it. As <code>0xa</code> can be encoded in a byte, we can use <code>disp8</code> to keep the instruction encoding short. From the row of <code>[EDI]+disp8</code> (actually, all <code>disp8</code> ones share the same ‘Mod’ bits),</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mod = 01 (in bits)</span><br></pre></td></tr></table></figure><p>For the encoding of the registers, I compiled a table for the general purpose 64-bit registers for your reference:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">_.Reg  Register</span><br><span class="line">----------------</span><br><span class="line">0.000   RAX</span><br><span class="line">0.001   RCX</span><br><span class="line">0.010   RDX</span><br><span class="line">0.011   RBX</span><br><span class="line">0.100   RSP</span><br><span class="line">0.101   RBP</span><br><span class="line">0.110   RSI</span><br><span class="line">0.111   RDI</span><br><span class="line">1.000   R8</span><br><span class="line">1.001   R9</span><br><span class="line">1.010   R10</span><br><span class="line">1.011   R11</span><br><span class="line">1.100   R12</span><br><span class="line">1.101   R13</span><br><span class="line">1.110   R14</span><br><span class="line">1.111   R15</span><br></pre></td></tr></table></figure><p>The ‘_‘ in the ‘_.Reg’ are usually a bit in the REX prefix, such as REX.B and REX.R, depending on specific instructions and operand combinations.</p><p>For the <code>addq</code> instruction in this case, <code>r8</code> is <code>1.000</code> and <code>rdi</code> is <code>0.111</code>. Hence, in bits, we get</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reg = 000</span><br><span class="line">r/m = 111</span><br><span class="line">REX.B = 0 (from `rdi`)</span><br><span class="line">REX.R = 1 (from `r8`)</span><br></pre></td></tr></table></figure><p>Now, let’s put them together.</p><p>By putting the ‘WRXB’ bits (<code>[BITS: 0100WRXB]</code>) together, we get the REX prefix for this instruction is</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0100 1100</span><br></pre></td></tr></table></figure><p>Together with the <code>03</code> in <code>REX.W+03/r</code> from the reference for the <code>ADD</code> instruction, the opcode part, in hexadecimal, is</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4c 03</span><br></pre></td></tr></table></figure><p>By putting the <code>mod</code>, <code>reg</code> and <code>r/m</code> together, we get the ModRM byte (in bits)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">01 000 111</span><br></pre></td></tr></table></figure><p>which is, in hexadecimal,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">47</span><br></pre></td></tr></table></figure><p>Following the ModRM byte is the displacement is <code>0xa</code>(<code>10</code>‘s hexadecimal representation) in one byte (<code>disp8</code>).</p><p>Putting all these together, we finally get the encoding of <code>add r8,[rdi+0xa]</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4c 03 47 0a</span><br></pre></td></tr></table></figure><p>In this example, to show the process, I have shown how to manually do an instruction’s encoding which is usually done by the assembler. You may use the same method to encode all other instruction by checking the reference documents for details of every instruction/operand combinations’ cases.</p><h3 id="5-Tips"><a href="#5-Tips" class="headerlink" title="5. Tips"></a>5. Tips</h3><ul><li><a href="https://wiki.osdev.org/X86-64_Instruction_Encoding" target="_blank" rel="noopener">X86-64 Instruction Encoding</a> </li></ul><p>is a very good page from OSDev as a quick reference.</p><ul><li><a href="https://events.static.linuxfound.org/sites/events/files/slides/bpetkov-x86-hacks.pdf" target="_blank" rel="noopener">x86 Instruction Encoding</a> </li></ul><p>可以快速扫一下内容，例如:</p><p><img src="/images/2021/10/10.png" alt></p><p>有些内容还是比较形象直观的。</p><ul><li>Intel SDM vol2 2.1 INSTRUCTION FORMAT FOR PROTECTED MODE, REAL-ADDRESS MODE,<br>AND VIRTUAL-8086 MODE </li></ul><p><img src="/images/2021/10/11.png" alt></p><p>以上内容是对Figure1的补充说明。</p><ul><li>Intel SDM vol2 3.1 INTERPRETING THE INSTRUCTION REFERENCE PAGES</li></ul><p>Intel SDM vol2中有具体的指令说明，需要先扫一下3.1 INTERPRETING THE INSTRUCTION REFERENCE PAGES中的内容。This section describes the format of information contained in the instruction reference pages in this chapter. It explains notational conventions and abbreviations used in these sections.</p><p>For example:</p><p><img src="/images/2021/10/8.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://www.systutorials.com/beginners-guide-x86-64-instruction-encoding/" target="_blank" rel="noopener">A Beginners’ Guide to x86-64 Instruction Encoding</a></li><li><a href="https://wiki.osdev.org/X86-64_Instruction_Encoding" target="_blank" rel="noopener">X86-64 Instruction Encoding</a></li><li><a href="https://events.static.linuxfound.org/sites/events/files/slides/bpetkov-x86-hacks.pdf" target="_blank" rel="noopener">x86 Instruction Encoding</a></li><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf" target="_blank" rel="noopener">Intel SDM</a></li><li><a href="https://sourceware.org/binutils/docs/as/i386_002dMemory.html" target="_blank" rel="noopener">Memory References</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将借鉴&lt;a href=&quot;https://www.systutorials.com/beginners-guide-x86-64-instruction-encoding/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A Beginners’ Guide to x86-64 Instruction Encoding&lt;/a&gt;，并补充相关材料，以一个具体的例子来介绍Intel Instruction Encoding。&lt;br&gt;
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>PIO virtualization in QEMU/KVM</title>
    <link href="http://liujunming.github.io/2021/10/19/PIO-virtualization-in-QEMU-KVM/"/>
    <id>http://liujunming.github.io/2021/10/19/PIO-virtualization-in-QEMU-KVM/</id>
    <published>2021-10-19T05:56:31.000Z</published>
    <updated>2021-10-20T22:10:04.052Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍：当guest执行<code>in</code>或<code>out</code>指令时，QEMU与KVM源码中的实现细节。<a id="more"></a></p><h3 id="1-PIO-background"><a href="#1-PIO-background" class="headerlink" title="1. PIO background"></a>1. PIO background</h3><p>Intel的I/O指令使得处理器可以访问I/O端口，以便从外设输入数据，或者向外设发送数据。这些指令有一个指定I/O空间端口地址的操作数。有两类的I/O指令：</p><ol><li>在寄存器指定的地址传送一个数据（字节、字、双字）。</li><li>传送指定内存中的一串数据（字节串、字串、双字串）。这些被称作为“串 I/O指令”或者说“块I/O指令”。</li></ol><p>有<code>IN</code>/<code>OUT</code> <code>INS</code>/<code>OUTS</code>指令</p><h3 id="2-PIO-configuration-in-VMCS"><a href="#2-PIO-configuration-in-VMCS" class="headerlink" title="2. PIO configuration in VMCS"></a>2. PIO configuration in VMCS</h3><p>SDM中的description如下:</p><p><img src="/images/2021/10/06.png" alt></p><p>KVM在Primary Processor-Based VM-Execution Controls 设置了Unconditional I/O exiting位，并且没有设置Use I/O bitmaps 位。因此，一旦guest执行了PIO指令，一定会发生VM Exit。</p><p>详情请阅读patch <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8eb73e2d410f00d383023fe41c0c25c6195b7389" target="_blank" rel="noopener">KVM: VMX: drop I/O permission bitmaps</a> </p><h3 id="3-Warm-up"><a href="#3-Warm-up" class="headerlink" title="3. Warm-up"></a>3. Warm-up</h3><h4 id="3-1-VM-Exit-Qualification-for-I-O-Instructions"><a href="#3-1-VM-Exit-Qualification-for-I-O-Instructions" class="headerlink" title="3.1 VM Exit Qualification for I/O Instructions"></a>3.1 VM Exit Qualification for I/O Instructions</h4><p>当guest执行PIO指令时，触发<a href="https://github.com/torvalds/linux/blob/v5.10/arch/x86/kvm/vmx/vmx.c#L5931" target="_blank" rel="noopener">vmx_handle_exit</a>，根据<a href="https://github.com/torvalds/linux/blob/v5.10/arch/x86/kvm/vmx/vmx.c#L5633" target="_blank" rel="noopener">EXIT_REASON_IO_INSTRUCTION</a>执行<a href="https://github.com/torvalds/linux/blob/v5.10/arch/x86/kvm/vmx/vmx.c#L4899" target="_blank" rel="noopener">handle_io</a>函数。</p><p><img src="/images/2021/10/07.png" alt></p><p><code>handle_io</code>会解析Exit Qualification，代码如下: </p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">handle_io</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> exit_qualification;</span><br><span class="line"><span class="keyword">int</span> size, in, <span class="built_in">string</span>;</span><br><span class="line"><span class="keyword">unsigned</span> port;</span><br><span class="line"></span><br><span class="line">exit_qualification = vmx_get_exit_qual(vcpu);</span><br><span class="line"><span class="built_in">string</span> = (exit_qualification &amp; <span class="number">16</span>) != <span class="number">0</span>;</span><br><span class="line">...</span><br><span class="line">port = exit_qualification &gt;&gt; <span class="number">16</span>;</span><br><span class="line">size = (exit_qualification &amp; <span class="number">7</span>) + <span class="number">1</span>;</span><br><span class="line">in = (exit_qualification &amp; <span class="number">8</span>) != <span class="number">0</span>;</span><br><span class="line">   ...</span><br><span class="line">   <span class="keyword">return</span> kvm_fast_pio(vcpu, size, port, in);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-2-misc"><a href="#3-2-misc" class="headerlink" title="3.2 misc"></a>3.2 misc</h4><ul><li>本文只讨论guest执行<code>in</code>或<code>out</code>指令时的情况，guest执行串 I/O指令这一情况不做介绍；</li><li>本文不考虑KVM模拟I/O指令的情况，即假设<code>kernel_pio</code>的返回值不为0。</li></ul><h3 id="4-PIO中out的处理流程"><a href="#4-PIO中out的处理流程" class="headerlink" title="4.  PIO中out的处理流程"></a>4.  PIO中out的处理流程</h3><p>KVM函数调用链如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_fast_pio</span><br><span class="line">kvm_fast_pio_out</span><br><span class="line">emulator_pio_out_emulated</span><br><span class="line">emulator_pio_in_out</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_fast_pio_out</span><span class="params">(struct kvm_vcpu *vcpu, <span class="keyword">int</span> size,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">unsigned</span> <span class="keyword">short</span> port)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> val = kvm_rax_read(vcpu); <span class="comment">//获取vcpu中rax寄存器的值</span></span><br><span class="line">    <span class="keyword">int</span> ret = emulator_pio_out_emulated(&amp;vcpu-&gt;arch.emulate_ctxt,</span><br><span class="line">    size, port, &amp;val, <span class="number">1</span>);</span><br><span class="line">    ...</span><br><span class="line">    vcpu-&gt;arch.pio.linear_rip = kvm_get_linear_rip(vcpu);<span class="comment">//获取guest中rip寄存器的值</span></span><br><span class="line">    vcpu-&gt;arch.complete_userspace_io = complete_fast_pio_out;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>complete_userspace_io</code>的细节后面再描述。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">emulator_pio_out_emulated</span><span class="params">(struct x86_emulate_ctxt *ctxt,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">int</span> size, <span class="keyword">unsigned</span> <span class="keyword">short</span> port,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">const</span> <span class="keyword">void</span> *val, <span class="keyword">unsigned</span> <span class="keyword">int</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_vcpu</span> *<span class="title">vcpu</span> = <span class="title">emul_to_vcpu</span>(<span class="title">ctxt</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">memcpy</span>(vcpu-&gt;arch.pio_data, val, size * count);</span><br><span class="line"><span class="keyword">return</span> emulator_pio_in_out(vcpu, size, port, (<span class="keyword">void</span> *)val, count, <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">emulator_pio_in_out</span><span class="params">(struct kvm_vcpu *vcpu, <span class="keyword">int</span> size,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">unsigned</span> <span class="keyword">short</span> port, <span class="keyword">void</span> *val,</span></span></span><br><span class="line"><span class="function"><span class="params">       <span class="keyword">unsigned</span> <span class="keyword">int</span> count, <span class="keyword">bool</span> in)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">vcpu-&gt;arch.pio.port = port;</span><br><span class="line">vcpu-&gt;arch.pio.in = in;</span><br><span class="line">vcpu-&gt;arch.pio.count  = count;</span><br><span class="line">vcpu-&gt;arch.pio.size = size;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_IO;</span><br><span class="line">vcpu-&gt;run-&gt;io.direction = in ? KVM_EXIT_IO_IN : KVM_EXIT_IO_OUT;</span><br><span class="line">vcpu-&gt;run-&gt;io.size = size;</span><br><span class="line">vcpu-&gt;run-&gt;io.data_offset = KVM_PIO_PAGE_OFFSET * PAGE_SIZE;</span><br><span class="line">vcpu-&gt;run-&gt;io.count = count;</span><br><span class="line">vcpu-&gt;run-&gt;io.port = port;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到<code>vcpu-&gt;run-&gt;io.data_offset</code>被设置为4096了，<code>emulator_pio_out_emulated</code>已经把guest向端口写的值拷贝到了<code>vpuc-&gt;arch.pio_data</code>中去了。 <code>vcpu-&gt;arch.pio_data</code>就在<code>kvm_run</code>后面一个页的位置，这可以从<code>kvm_vcpu_init</code>中看出来。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_vcpu_init</span><span class="params">(struct kvm_vcpu *vcpu, struct kvm *kvm, <span class="keyword">unsigned</span> id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    page = alloc_page(GFP_KERNEL | __GFP_ZERO);</span><br><span class="line">    vcpu-&gt;run = page_address(page);</span><br><span class="line">    ...</span><br><span class="line">    kvm_arch_vcpu_init(vcpu);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_arch_vcpu_init</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    page = alloc_page(GFP_KERNEL | __GFP_ZERO);</span><br><span class="line">    vcpu-&gt;arch.pio_data = page_address(page);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>KVM处理完后，返回到QEMU。此时，QEMU的执行代码:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_cpu_exec</span><span class="params">(CPUState *cpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">switch</span> (run-&gt;exit_reason) &#123;</span><br><span class="line"><span class="keyword">case</span> KVM_EXIT_IO:</span><br><span class="line">            DPRINTF(<span class="string">"handle_io\n"</span>);</span><br><span class="line">            <span class="comment">/* Called outside BQL */</span></span><br><span class="line">            kvm_handle_io(run-&gt;io.port, attrs,</span><br><span class="line">                (<span class="keyword">uint8_t</span> *)run + run-&gt;io.data_offset,</span><br><span class="line">                run-&gt;io.direction,</span><br><span class="line">                run-&gt;io.size,</span><br><span class="line">                run-&gt;io.count);</span><br><span class="line">            ret = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>QEMU处理完后，返回到KVM。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_arch_vcpu_ioctl_run</span><span class="params">(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> (unlikely(vcpu-&gt;arch.complete_userspace_io)) &#123;</span><br><span class="line"><span class="keyword">int</span> (*cui)(struct kvm_vcpu *) = vcpu-&gt;arch.complete_userspace_io;</span><br><span class="line">vcpu-&gt;arch.complete_userspace_io = <span class="literal">NULL</span>;</span><br><span class="line">   r = cui(vcpu);</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">vcpu_run(vcpu);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>kvm_fast_pio_out</code>已将<code>complete_userspace_io</code> 赋值为<code>complete_fast_pio_out</code>;</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int complete_fast_pio_out(struct kvm_vcpu *vcpu)</span><br><span class="line">&#123;</span><br><span class="line">vcpu-&gt;arch.pio.count = 0;</span><br><span class="line">...</span><br><span class="line">return kvm_skip_emulated_instruction(vcpu);//主要功能是让guest的RIP跳过一个指令</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-PIO中in的处理流程"><a href="#5-PIO中in的处理流程" class="headerlink" title="5.  PIO中in的处理流程"></a>5.  PIO中in的处理流程</h3><p>KVM函数调用链如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm_fast_pio</span><br><span class="line">kvm_fast_pio_in</span><br><span class="line">emulator_pio_in_emulated</span><br><span class="line">emulator_pio_in_out</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kvm_fast_pio_in</span><span class="params">(struct kvm_vcpu *vcpu, <span class="keyword">int</span> size, <span class="keyword">unsigned</span> <span class="keyword">short</span> port)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> val;</span><br><span class="line">...</span><br><span class="line">emulator_pio_in_emulated(&amp;vcpu-&gt;arch.emulate_ctxt, size, port,</span><br><span class="line">       &amp;val, <span class="number">1</span>);</span><br><span class="line">   ...</span><br><span class="line">vcpu-&gt;arch.pio.linear_rip = kvm_get_linear_rip(vcpu);</span><br><span class="line">vcpu-&gt;arch.complete_userspace_io = complete_fast_pio_in;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">emulator_pio_in_emulated</span><span class="params">(struct x86_emulate_ctxt *ctxt,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> size, <span class="keyword">unsigned</span> <span class="keyword">short</span> port, <span class="keyword">void</span> *val,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">unsigned</span> <span class="keyword">int</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">kvm_vcpu</span> *<span class="title">vcpu</span> = <span class="title">emul_to_vcpu</span>(<span class="title">ctxt</span>);</span></span><br><span class="line"><span class="keyword">int</span> ret;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vcpu-&gt;arch.pio.count)</span><br><span class="line"><span class="keyword">goto</span> data_avail;</span><br><span class="line"></span><br><span class="line"><span class="built_in">memset</span>(vcpu-&gt;arch.pio_data, <span class="number">0</span>, size * count);</span><br><span class="line"></span><br><span class="line">ret = emulator_pio_in_out(vcpu, size, port, val, count, <span class="literal">true</span>);</span><br><span class="line"><span class="keyword">if</span> (ret) &#123;</span><br><span class="line">data_avail:</span><br><span class="line"><span class="built_in">memcpy</span>(val, vcpu-&gt;arch.pio_data, size * count);</span><br><span class="line">vcpu-&gt;arch.pio.count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在<code>emulator_pio_in_emulated</code>中，由于<code>vcpu-&gt;arch.pio.count</code>此时还没有数据（需要QEMU提供），所以会执行 <code>emulator_pio_in_out</code>，之前已经看过这个函数了，就是设置<code>kvm_run</code>的相关数据，然后由QEMU来填充。</p><p>回到QEMU后，QEMU会往<code>kvm_run</code>填入数据。</p><p>回到KVM后，<code>kvm_arch_vcpu_ioctl_run</code>会回调<code>complete_fast_pio_in</code>函数。</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">complete_fast_pio_in</span><span class="params">(struct kvm_vcpu *vcpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> val;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* We should only ever be called with arch.pio.count equal to 1 */</span></span><br><span class="line">BUG_ON(vcpu-&gt;arch.pio.count != <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Since vcpu-&gt;arch.pio.count == 1 let emulator_pio_in_emulated perform</span></span><br><span class="line"><span class="comment"> * the copy and tracing</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">emulator_pio_in_emulated(&amp;vcpu-&gt;arch.emulate_ctxt, vcpu-&gt;arch.pio.size,</span><br><span class="line"> vcpu-&gt;arch.pio.port, &amp;val, <span class="number">1</span>);</span><br><span class="line">kvm_rax_write(vcpu, val);<span class="comment">//将值写入到vcpu的rax寄存器中</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> kvm_skip_emulated_instruction(vcpu);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在最终的<code>emulator_pio_in_emulated</code>中，由于这个时候<code>vcpu-&gt;arch.pio.count</code>已经有值了，表示数据可用了。</p><p><code>emulator_pio_in_emulated</code>中的执行代码为:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">emulator_pio_in_emulated</span><span class="params">(struct x86_emulate_ctxt *ctxt,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> size, <span class="keyword">unsigned</span> <span class="keyword">short</span> port, <span class="keyword">void</span> *val,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">unsigned</span> <span class="keyword">int</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">memcpy</span>(val, vcpu-&gt;arch.pio_data, size * count);<span class="comment">//拷贝QEMU填充的值</span></span><br><span class="line">    vcpu-&gt;arch.pio.count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2017/07/10/kvm-pio" target="_blank" rel="noopener">QEMU-KVM中的PIO处理</a></li><li><a href="https://oenhan.com/kvm-src-5-io-pio" target="_blank" rel="noopener">KVM源代码分析5:IO虚拟化之PIO</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍：当guest执行&lt;code&gt;in&lt;/code&gt;或&lt;code&gt;out&lt;/code&gt;指令时，QEMU与KVM源码中的实现细节。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>KVM MMIO Emulation</title>
    <link href="http://liujunming.github.io/2021/10/17/KVM-MMIO-Emulation/"/>
    <id>http://liujunming.github.io/2021/10/17/KVM-MMIO-Emulation/</id>
    <published>2021-10-17T05:04:06.000Z</published>
    <updated>2021-10-17T07:37:34.474Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要汇总KVM中MMIO Emulation的过程。<a id="more"></a></p><h3 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h3><p><a href="/2020/07/12/Introduction-to-ept-misconfig/">Introduction to ept misconfig</a></p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>For a summary, the following shows the process of MMIO implementation:</p><ol><li>QEMU declares a memory region(but not allocate ram or commit it to kvm)</li><li>Guest first access the MMIO address, cause a EPT violation VM-exit</li><li>KVM construct the EPT page table and marks the page table entry with special mark(110b)</li><li>Later the guest access these MMIO, it will be processed by EPT misconfig VM-exit handler</li></ol><h3 id="QEMU-part"><a href="#QEMU-part" class="headerlink" title="QEMU part"></a>QEMU part</h3><p>这里以e1000网卡模拟为例，设备初始化MMIO时候时候注册的MemoryRegion为IO类型（不是RAM类型）。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span></span><br><span class="line">e1000_mmio_setup(E1000State *d)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">uint32_t</span> excluded_regs[] = &#123;</span><br><span class="line">        E1000_MDIC, E1000_ICR, E1000_ICS, E1000_IMS,</span><br><span class="line">        E1000_IMC, E1000_TCTL, E1000_TDT, PNPMMIO_SIZE</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// 这里注册MMIO，调用memory_region_init_io，mr-&gt;ram = false！！！</span></span><br><span class="line">    memory_region_init_io(&amp;d-&gt;mmio, OBJECT(d), &amp;e1000_mmio_ops, d,</span><br><span class="line">                          <span class="string">"e1000-mmio"</span>, PNPMMIO_SIZE);</span><br><span class="line">    memory_region_add_coalescing(&amp;d-&gt;mmio, <span class="number">0</span>, excluded_regs[<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; excluded_regs[i] != PNPMMIO_SIZE; i++)</span><br><span class="line">        memory_region_add_coalescing(&amp;d-&gt;mmio, excluded_regs[i] + <span class="number">4</span>,</span><br><span class="line">                                     excluded_regs[i+<span class="number">1</span>] - excluded_regs[i] - <span class="number">4</span>);</span><br><span class="line">    memory_region_init_io(&amp;d-&gt;io, OBJECT(d), &amp;e1000_io_ops, d, <span class="string">"e1000-io"</span>, IOPORT_SIZE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>QEMU uses function <code>memory_region_init_io</code> to declare a MMIO region. Here we can see the <code>mr-&gt;ram</code> is false so no really memory is allocated.</p><p>QEMU调用<code>kvm_set_phys_mem</code>注册虚拟机的物理内存到KVM相关的数据结构中的时候，会调用<code>memory_region_is_ram</code>来判断该段物理地址空间是否是RAM设备， 如果不是RAM设备直接return了．</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">kvm_set_phys_mem</span><span class="params">(KVMMemoryListener *kml,</span></span></span><br><span class="line"><span class="function"><span class="params">                             MemoryRegionSection *section, <span class="keyword">bool</span> add)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">if</span> (!memory_region_is_ram(mr)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (writeable || !kvm_readonly_mem_allowed) &#123;</span><br><span class="line">            <span class="keyword">return</span>;     <span class="comment">// 设备MR不是RAM但可以写，那么这里直接return不注册到kvm里面</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!mr-&gt;romd_mode) &#123;</span><br><span class="line">            <span class="comment">/* If the memory device is not in romd_mode, then we actually want</span></span><br><span class="line"><span class="comment">             * to remove the kvm memory slot so all accesses will trap. */</span></span><br><span class="line">            add = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="KVM-part"><a href="#KVM-part" class="headerlink" title="KVM part"></a>KVM part</h3><p>In <code>vmx_init</code>, when ept enabled, it calls <code>ept_set_mmio_spte_mask</code>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ept_set_mmio_spte_mask</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * EPT Misconfigurations can be generated if the value of bits 2:0</span></span><br><span class="line"><span class="comment"> * of an EPT paging-structure entry is 110b (write/execute).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">kvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,</span><br><span class="line">   VMX_EPT_MISCONFIG_WX_VALUE, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">kvm_mmu_set_mmio_spte_mask</span><span class="params">(u64 mmio_mask, u64 mmio_value, u64 access_mask)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">shadow_mmio_mask = mmio_mask | SPTE_SPECIAL_MASK;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Here set <code>shadow_mmio_mask</code>.</p><p>We the guest access the MMIO address, the VM will exit caused by ept violation and <code>tdp_page_fault</code> will be called. <code>__direct_map</code> will be called to construct the EPT page table.</p><p>After the long call-chain, the final function <code>mark_mmio_spte</code> will be called to set the spte with <code>shadow_mmio_mask</code> which as we already know is set when the vmx initialization.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__direct_map</span><br><span class="line">mmu_set_spte</span><br><span class="line">set_spte</span><br><span class="line">set_mmio_spte</span><br><span class="line">mark_mmio_spte</span><br></pre></td></tr></table></figure></p><p>The condition to call <code>mark_mmio_spte</code> is <code>is_noslot_pfn</code>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">bool</span> <span class="title">set_mmio_spte</span><span class="params">(struct kvm *kvm, u64 *sptep, <span class="keyword">gfn_t</span> gfn,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">pfn_t</span> pfn, <span class="keyword">unsigned</span> access)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (unlikely(is_noslot_pfn(pfn))) &#123;</span><br><span class="line">mark_mmio_spte(kvm, sptep, gfn, access);</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">is_noslot_pfn</span><span class="params">(<span class="keyword">pfn_t</span> pfn)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> pfn == KVM_PFN_NOSLOT;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>As we know the QEMU doesn’t commit the MMIO memory region, so pfn is <code>KVM_PFN_NOSLOT</code> and then mark the spte with <code>shadow_mmio_mask</code>.</p><p>When the guest later access this MMIO page, as it’s ept page table entry is 110b, this will cause the VM exit by EPT misconfig, any how can a page be write/execute but no read permission. In the handler <code>handle_ept_misconfig</code> it first process the MMIO case, this will dispatch to the QEMU part.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vcpu_run</span><br><span class="line">vcpu_enter_guest</span><br><span class="line">kvm_x86_ops-&gt;run(vcpu) (run the guest!)</span><br><span class="line">handle_exit_irqoff()</span><br><span class="line">       handle_exit() which is vmx_handle_exit</span><br><span class="line">                    handle all the vmexit, fill in the KVM_EXIT reasons</span><br><span class="line">                    (kvm_vmx_exit_handlers[exit_reason](vcpu))</span><br><span class="line">                        handle_ept_misconfig (just one of many handlers!)</span><br><span class="line">                            kvm_mmu_page_fault</span><br><span class="line">                                x86_emulate_instruction</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x86_emulate_instruction</span><br><span class="line">x86_emulate_insn</span><br><span class="line">writeback</span><br><span class="line">segmented_write</span><br><span class="line">write_emulated[emulator_write_emulated]</span><br><span class="line">emulator_read_write</span><br><span class="line">emulator_read_write_onepage</span><br><span class="line">ops-&gt;read_write_mmio[write_mmio]</span><br><span class="line">vcpu_mmio_write</span><br><span class="line">kvm_io_bus_write</span><br><span class="line">__kvm_io_bus_write</span><br><span class="line">kvm_iodevice_write</span><br><span class="line">ops-&gt;write[ioeventfd_write]</span><br></pre></td></tr></table></figure><p>最后会调用到<code>ioeventfd_write</code>，写eventfd给QEMU发送通知事件。</p><hr><p>参考资料:</p><ol><li><a href="https://kernelgo.org/mmio.html" target="_blank" rel="noopener">MMIO Emulation</a></li><li><a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/09/03/kvm-mmio" target="_blank" rel="noopener">KVM MMIO implementation</a></li><li><a href="http://lastweek.io/pubs/virt_note.pdf" target="_blank" rel="noopener">Notes on Virtualization Stack</a></li><li><a href="https://blog.csdn.net/lpstc123/article/details/45111949" target="_blank" rel="noopener">Qemu-kvm的ioeventfd创建与触发的大致流程</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要汇总KVM中MMIO Emulation的过程。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>KVM notes - part2</title>
    <link href="http://liujunming.github.io/2021/10/10/KVM-notes-part2/"/>
    <id>http://liujunming.github.io/2021/10/10/KVM-notes-part2/</id>
    <published>2021-10-10T01:47:05.000Z</published>
    <updated>2021-10-27T10:02:05.210Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about using the KVM API.<a id="more"></a></p><h3 id="kvm-hello-world"><a href="#kvm-hello-world" class="headerlink" title="kvm-hello-world"></a>kvm-hello-world</h3><p>首先建议研究下<a href="https://github.com/dpw/kvm-hello-world" target="_blank" rel="noopener">kvm-hello-world</a>这一项目。运行并研究其代码。<br>整体来说比较简单，就是对<code>ioctl(vm-&gt;fd, KVM_SET_TSS_ADDR, 0xfffbd000)</code>这行代码有困惑。搜了下资料，<a href="https://www.kernel.org/doc/Documentation/virtual/kvm/api.txt" target="_blank" rel="noopener">Documentation/virtual/kvm/api.txt</a>解释如下:</p><blockquote><p>Capability: KVM_CAP_SET_TSS_ADDR<br>Architectures: x86<br>Type: vm ioctl<br>Parameters: unsigned long tss_address (in)<br>Returns: 0 on success, -1 on error</p><p>This ioctl defines the physical address of a three-page region in the guest physical address space.  The region must be within the first 4GB of the guest physical address space and must not conflict with any memory slot or any mmio address.  The guest may malfunction if it accesses this memory region.</p><p>This ioctl is required on Intel-based hosts.  This is needed on Intel hardware because of a quirk in the virtualization implementation (see the internals documentation when it pops into existence).</p></blockquote><h3 id="LWN-Using-the-KVM-API"><a href="#LWN-Using-the-KVM-API" class="headerlink" title="LWN: Using the KVM API"></a>LWN: Using the KVM API</h3><p><a href="https://lwn.net/Articles/658511/" target="_blank" rel="noopener">LWN: Using the KVM API</a></p><p>好文。值得细细品读。</p><p>Notes 如下:</p><ul><li><a href="https://www.kernel.org/doc/Documentation/virtual/kvm/api.txt" target="_blank" rel="noopener">Documentation/virtual/kvm/api.txt</a></li><li><a href="https://lwn.net/Articles/658512/" target="_blank" rel="noopener">fully functional sample program</a></li><li><code>KVM_EXIT_FAIL_ENTRY</code>: in particular, shows up often when changing the initial conditions of the VM; it indicates that the underlying hardware virtualization mechanism (VT in this case) can’t start the VM because the initial conditions don’t match its requirements.</li><li><code>KVM_EXIT_INTERNAL_ERROR</code> indicates an error from the Linux KVM subsystem rather than from the hardware. </li></ul><h4 id="Additional-KVM-API-features"><a href="#Additional-KVM-API-features" class="headerlink" title="Additional KVM API features"></a>Additional KVM API features</h4><p>Prospective implementers of memory-mapped I/O devices will want to look at the <code>exit_reason</code> <code>KVM_EXIT_MMIO</code>, as well as the <code>KVM_CAP_COALESCED_MMIO</code> extension to reduce vmexits, and the <code>ioeventfd</code> mechanism to process I/O asynchronously.</p><p>For hardware interrupts, see the <code>irqfd</code> mechanism, using the <code>KVM_CAP_IRQFD</code> extension capability. This provides a file descriptor that can inject a hardware interrupt into the KVM virtual machine without stopping it first. A virtual machine may thus write to this from a separate event loop or device-handling thread, and threads running <code>KVM_RUN</code> for a virtual CPU will process that interrupt at the next available opportunity.</p><p>x86 virtual machines will likely want to support <a href="https://en.wikipedia.org/wiki/CPUID" target="_blank" rel="noopener">CPUID</a> and <a href="https://en.wikipedia.org/wiki/Model-specific_register" target="_blank" rel="noopener">model-specific registers (MSRs)</a>, both of which have architecture-specific <code>ioctl()</code>s that minimize vmexits.</p><h4 id="Applications-of-the-KVM-API"><a href="#Applications-of-the-KVM-API" class="headerlink" title="Applications of the KVM API"></a>Applications of the KVM API</h4><p>Other than learning, debugging a virtual machine implementation, or as a party trick, why use <code>/dev/kvm</code> directly?</p><p>Virtual machines like <code>qemu-kvm</code> or <code>kvmtool</code> typically emulate the standard hardware of the target architecture; for instance, a standard x86 PC. While they can support other devices and <a href="https://lwn.net/Articles/580186/" target="_blank" rel="noopener">virtio</a> hardware, if you want to emulate a completely different type of system that shares little more than the instruction set architecture, you might want to implement a new VM instead. And even within an existing virtual machine implementation, authors of a new class of virtio hardware device will want a clear understanding of the KVM API.</p><p>Efforts like <a href="https://github.com/google/novm" target="_blank" rel="noopener">novm</a> and <a href="https://git.kernel.org/cgit/linux/kernel/git/will/kvmtool.git/" target="_blank" rel="noopener">kvmtool</a> use the KVM API to construct a lightweight VM, dedicated to running Linux rather than an arbitrary OS. More recently, the Clear Containers project uses kvmtool to <a href="https://lwn.net/Articles/644675/" target="_blank" rel="noopener">run containers using hardware virtualization</a>.</p><p>Alternatively, a VM need not run an OS at all. A KVM-based VM could instead implement a hardware-assisted sandbox with no virtual hardware devices and no OS, providing arbitrary virtual “hardware” devices as the API between the sandbox and the sandboxing VM.</p><p>While running a full virtual machine remains the primary use case for hardware virtualization, we’ve seen many innovative uses of the KVM API recently, and we can certainly expect more in the future.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about using the KVM API.
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>KVM notes - part1</title>
    <link href="http://liujunming.github.io/2021/10/07/KVM-notes-part1/"/>
    <id>http://liujunming.github.io/2021/10/07/KVM-notes-part1/</id>
    <published>2021-10-07T04:17:17.000Z</published>
    <updated>2021-09-20T16:54:13.524Z</updated>
    
    <content type="html"><![CDATA[<p>Notes about KVM in the full picture.<a id="more"></a></p><h3 id="How-VT-x-KVM-and-QEMU-Work-Together"><a href="#How-VT-x-KVM-and-QEMU-Work-Together" class="headerlink" title="How VT-x, KVM and QEMU Work Together"></a>How VT-x, KVM and QEMU Work Together</h3><p><a href="https://binarydebt.wordpress.com/2018/10/14/intel-virtualisation-how-vt-x-kvm-and-qemu-work-together/" target="_blank" rel="noopener">How VT-x, KVM and QEMU Work Together</a></p><p><img src="/images/2021/10/01.png" alt></p><h3 id="kvm-the-Linux-Virtual-Machine-Monitor"><a href="#kvm-the-Linux-Virtual-Machine-Monitor" class="headerlink" title="kvm: the Linux Virtual Machine Monitor"></a>kvm: the Linux Virtual Machine Monitor</h3><p><a href="https://www.kernel.org/doc/ols/2007/ols2007v1-pages-225-230.pdf" target="_blank" rel="noopener">kvm: the Linux Virtual Machine Monitor</a></p><p><img src="/images/2021/10/04.png" alt></p><p><img src="/images/2021/10/2.png" alt></p><p><img src="/images/2021/10/3.png" alt></p><h3 id="Architecture-of-the-Kernel-based-Virtual-Machine-KVM-2010"><a href="#Architecture-of-the-Kernel-based-Virtual-Machine-KVM-2010" class="headerlink" title="Architecture of the Kernel-based Virtual Machine (KVM), 2010"></a>Architecture of the Kernel-based Virtual Machine (KVM), 2010</h3><p><a href="http://www.linux-kongress.org/2010/slides/KVM-Architecture-LK2010.pdf" target="_blank" rel="noopener">Architecture of the Kernel-based Virtual Machine (KVM), 2010</a></p><p><img src="/images/2021/10/05.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Notes about KVM in the full picture.
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="KVM" scheme="http://liujunming.github.io/tags/KVM/"/>
    
  </entry>
  
  <entry>
    <title>深入理解DMA part2</title>
    <link href="http://liujunming.github.io/2021/09/13/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3DMA-part2/"/>
    <id>http://liujunming.github.io/2021/09/13/深入理解DMA-part2/</id>
    <published>2021-09-13T11:06:35.000Z</published>
    <updated>2021-09-13T11:55:25.945Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍DMA controller的相关知识点。<br><a id="more"></a></p><h3 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h3><h4 id="1-1-Third-party"><a href="#1-1-Third-party" class="headerlink" title="1.1 Third-party"></a>1.1 Third-party</h4><p>Standard DMA, also called third-party DMA, uses a DMA controller. A DMA controller can generate memory addresses and initiate memory read or write cycles. It contains several hardware registers that can be written and read by the CPU. These include a memory address register, a byte count register, and one or more control registers. Depending on what features the DMA controller provides, these control registers might specify some combination of the source, the destination, the direction of the transfer (reading from the I/O device or writing to the I/O device), the size of the transfer unit, and/or the number of bytes to transfer in one burst.</p><p>To carry out an input, output or memory-to-memory operation, the host processor initializes the DMA controller with a count of the number of words to transfer, and the memory address to use. The CPU then commands the peripheral device to initiate a data transfer. The DMA controller then provides addresses and read/write control lines to the system memory. Each time a byte of data is ready to be transferred between the peripheral device and memory, the DMA controller increments its internal address register until the full block of data is transferred.</p><h4 id="1-2-Bus-mastering"><a href="#1-2-Bus-mastering" class="headerlink" title="1.2 Bus mastering"></a>1.2 Bus mastering</h4><p>In a bus mastering system, also known as a first-party DMA system, the CPU and peripherals can each be granted control of the memory bus. Where a peripheral can become a bus master, it can directly write to system memory without the involvement of the CPU, providing memory address and control signals as required. Some measures must be provided to put the processor into a hold condition so that bus contention does not occur</p><h3 id="2-Real-world-DMA-Controllers"><a href="#2-Real-world-DMA-Controllers" class="headerlink" title="2. Real-world DMA Controllers"></a>2. Real-world DMA Controllers</h3><p><img src="/images/2021/09/15.png" alt><br>DMA channels are system pathways used by many devices to transfer information directly to and from memory.</p><h4 id="2-1-DMA-channels"><a href="#2-1-DMA-channels" class="headerlink" title="2.1 DMA channels"></a>2.1 DMA channels</h4><p>To find out what DMA channels your system uses, you can use the cat <code>/proc/dma</code> command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/dma</span><br><span class="line"> 4: cascade</span><br></pre></td></tr></table></figure></p><p><img src="/images/2021/09/14.png" alt><br>这里的DMA channels可不用深究，了解即可。</p><h3 id="3-Bus-Mastering-DMA"><a href="#3-Bus-Mastering-DMA" class="headerlink" title="3. Bus Mastering DMA"></a>3. Bus Mastering DMA</h3><p><img src="/images/2021/09/16.png" alt></p><p><img src="/images/2021/09/17.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://ucilnica.fri.uni-lj.si/pluginfile.php/168345/mod_resource/content/0/ORS_book-DMA.pdf" target="_blank" rel="noopener">Basic Components Of a Computer System</a></li><li><a href="https://en.wikipedia.org/wiki/Direct_memory_access" target="_blank" rel="noopener">Direct memory access wikipedia</a></li><li><a href="https://www.angelfire.com/scifi/hardware/ref/sys-dma.htm" target="_blank" rel="noopener">Direct Memory Access (DMA) Channels</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍DMA controller的相关知识点。&lt;br&gt;
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入理解DMA part1</title>
    <link href="http://liujunming.github.io/2021/09/12/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3DMA-part1/"/>
    <id>http://liujunming.github.io/2021/09/12/深入理解DMA-part1/</id>
    <published>2021-09-12T08:06:21.000Z</published>
    <updated>2021-09-12T10:38:12.354Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍DMA的基础概念和use interface from system programmer’s perspective。<br><a id="more"></a></p><h3 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h3><p><img src="/images/2021/09/12.png" alt><br><img src="/images/2021/09/11.png" alt><br><img src="/images/2021/09/13.png" alt></p><h3 id="2-Overview"><a href="#2-Overview" class="headerlink" title="2. Overview"></a>2. Overview</h3><p>Let’s review how a DMA transfer takes place, considering only input transfers to simplify the discussion.</p><p>Data transfer can be triggered in two ways: either the software asks for data (via a function such as <em>read</em>) or the hardware asynchronously pushes data to the system.</p><p>In the first case, the steps involved can be summarized as follows:</p><ol><li>When a process calls <em>read</em>, the driver method allocates a DMA buffer and instructs the hardware to transfer its data into that buffer. The process is put to sleep.</li><li>The hardware writes data to the DMA buffer and raises an interrupt when it’s done.</li><li>The interrupt handler gets the input data, acknowledges the interrupt, and awakens the process, which is now able to read data.</li></ol><p>The second case comes about when DMA is used asynchronously. This happens, for example, with data acquisition devices that go on pushing data even if nobody is reading them. In this case, the driver should maintain a buffer so that a subsequent <em>read</em> call will return all the accumulated data to user space. The steps involved in this kind of transfer are slightly different:</p><ol><li>The hardware raises an interrupt to announce that new data has arrived.</li><li>The interrupt handler allocates a buffer and tells the hardware where to transfer its data.</li><li>The peripheral device writes the data to the buffer and raises another interrupt when it’s done.</li><li>The handler dispatches the new data, wakes any relevant process, and takes care of housekeeping.</li></ol><h3 id="3-A-simple-PCI-DMA-example"><a href="#3-A-simple-PCI-DMA-example" class="headerlink" title="3. A simple PCI DMA example"></a>3. A simple PCI DMA example</h3><p>As an example of how the DMA mappings might be used, we present a simple example of DMA coding for a PCI device. The actual form of DMA operations on the PCI bus is very dependent on the device being driven. Thus, this example does not apply to any real device; instead, it is part of a hypothetical driver called <em>dad</em> (DMA Acquisition Device). A driver for this device might define a transfer function like this:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dad_transfer</span><span class="params">(struct dad_dev *dev, <span class="keyword">int</span> write, <span class="keyword">void</span> *buffer,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">size_t</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">dma_addr_t</span> bus_addr;</span><br><span class="line">    <span class="comment">/* Map the buffer for DMA */</span></span><br><span class="line">    dev-&gt;dma_dir = (write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);</span><br><span class="line">    dev-&gt;dma_size = count;</span><br><span class="line">    bus_addr = dma_map_single(&amp;dev-&gt;pci_dev-&gt;dev, buffer, count,</span><br><span class="line">    dev-&gt;dma_dir);</span><br><span class="line">    dev-&gt;dma_addr = bus_addr;</span><br><span class="line">    <span class="comment">/* Set up the device */</span></span><br><span class="line">    writeb(dev-&gt;registers.command, DAD_CMD_DISABLEDMA);</span><br><span class="line">    writeb(dev-&gt;registers.command, write ? DAD_CMD_WR : DAD_CMD_RD);</span><br><span class="line">    writel(dev-&gt;registers.addr, cpu_to_le32(bus_addr));</span><br><span class="line">    writel(dev-&gt;registers.len, cpu_to_le32(count));</span><br><span class="line">    <span class="comment">/* Start the operation */</span></span><br><span class="line">    writeb(dev-&gt;registers.command, DAD_CMD_ENABLEDMA);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>This function maps the buffer to be transferred and starts the device operation. The other half of the job must be done in the interrupt service routine, which looks something like this:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dad_interrupt</span><span class="params">(<span class="keyword">int</span> irq, <span class="keyword">void</span> *dev_id, struct pt_regs *regs)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">dad_dev</span> *<span class="title">dev</span> = (<span class="title">struct</span> <span class="title">dad_dev</span> *) <span class="title">dev_id</span>;</span></span><br><span class="line">    <span class="comment">/* Make sure it's really our device interrupting */</span></span><br><span class="line">    <span class="comment">/* Unmap the DMA buffer */</span></span><br><span class="line">    dma_unmap_single(dev-&gt;pci_dev-&gt;dev, dev-&gt;dma_addr,</span><br><span class="line">    dev-&gt;dma_size, dev-&gt;dma_dir);</span><br><span class="line">    <span class="comment">/* Only now it is safe to access the buffer, copy to user, etc. */</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="3-1-more-detailed-explanation"><a href="#3-1-more-detailed-explanation" class="headerlink" title="3.1 more detailed explanation"></a>3.1 more detailed explanation</h4><p>The steps involved to transfer the data to the device could be summarized as follows :</p><ol><li>Assume that you have the data in a buffer.</li><li>The driver creates a DMA mapping for this buffer (say using <code>pci_alloc_consistent()</code> or the newer <code>dma_alloc_coherent()</code>), and returns the corresponding DMA bus address(physical address).</li><li>This DMA bus address is to be informed to the device. This is done by writing into the correct DMA registers of the device through <code>writel()</code> (assuming that the device registers are memory mapped).</li><li>The device also needs to be informed about the amount of data that is being transferred and such (by writing to the appropriate registers of the device using <code>writel()</code>)</li><li>Now issue the command to the device to start the DMA transactions by writing to one of its control registers (again possibly using <code>writel()</code>).</li><li>Once the data transaction is completed, the device issues an interrupt.</li><li>In the interrupt handler, the driver may unallocate the buffer which was used for transaction and might as well perform DMA unmapping.</li></ol><p>And there you have it. The data is transferred to the device!</p><hr><p>参考资料:</p><ol><li><a href="https://softwareengineering.stackexchange.com/questions/272470/how-does-a-dma-controller-work" target="_blank" rel="noopener">How does a DMA controller work?</a></li><li><a href="https://stackoverflow.com/questions/25161555/dma-and-i-o-memory-region-under-linux" target="_blank" rel="noopener">DMA and I/O memory region under Linux</a></li><li>Linux Device Drivers, Third Edition</li><li>MODERN OPERATING SYSTEMS, FOURTH EDITION by ANDREW S. TANENBAUM</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍DMA的基础概念和use interface from system programmer’s perspective。&lt;br&gt;
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>How to use Power Management capability to reset PCI device function</title>
    <link href="http://liujunming.github.io/2021/09/12/How-to-use-Power-Management-capability-to-reset-PCI-device-function/"/>
    <id>http://liujunming.github.io/2021/09/12/How-to-use-Power-Management-capability-to-reset-PCI-device-function/</id>
    <published>2021-09-11T17:26:28.000Z</published>
    <updated>2021-09-11T12:51:18.330Z</updated>
    
    <content type="html"><![CDATA[<p>本文将结合spec与kernel代码来介绍：利用Power Management capability to reset PCI device function。<a id="more"></a></p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>PCIe设备的PM状态:</p><p><img src="/images/2021/09/10.png" alt></p><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><p>核心思想: Put device into D3 state and back into D0 state.</p><p><img src="/images/2021/09/7.PNG" alt></p><ul><li><a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci.c#L4664" target="_blank" rel="noopener">检查PMCSR的No_Soft_Reset bit</a></li></ul><p><img src="/images/2021/09/8.PNG" alt></p><ul><li><a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci.c#L4675" target="_blank" rel="noopener">Put device into D3 state</a></li></ul><p><img src="/images/2021/09/9.PNG" alt></p><ul><li><a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci.c#L4680" target="_blank" rel="noopener">Put device back into D0 state</a></li></ul><hr><p>参考资料:</p><ol><li><a href="http://blog.chinaaet.com/justlxy/p/5100061872" target="_blank" rel="noopener">PCIe扫盲——Power Management概述</a></li><li><a href="https://lekensteyn.nl/files/docs/PCI_Power_Management_12.pdf" target="_blank" rel="noopener">PCI Bus Power Management Interface Specification Revision 1.2 </a></li><li><a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci.c#L4656" target="_blank" rel="noopener">pci_pm_reset</a></li><li><a href="https://static.lwn.net/2001/0704/a/pcipm-doc.php3" target="_blank" rel="noopener">RFC: PCI Power Management Documentation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将结合spec与kernel代码来介绍：利用Power Management capability to reset PCI device function。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about kernel compile and config</title>
    <link href="http://liujunming.github.io/2021/09/11/Notes-about-kernel-compile/"/>
    <id>http://liujunming.github.io/2021/09/11/Notes-about-kernel-compile/</id>
    <published>2021-09-11T02:40:20.000Z</published>
    <updated>2021-09-11T10:23:43.034Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录内核编译配置的相关内容。<a id="more"></a></p><h3 id="proc-config-gz"><a href="#proc-config-gz" class="headerlink" title="/proc/config.gz"></a>/proc/config.gz</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zcat /proc/config.gz</span><br></pre></td></tr></table></figure><p>Linux 可以在内核本身存储用于内核构建的 gzip 内核配置文件副本，并通过 /proc/config.gz 提供给用户。也就是说，/proc/config.gz 就是当前的 Linux 内核配置文件，并且是用 gzip 格式压缩过的。</p><p>但不是所有的 Linux 发行版都有 /proc/config.gz 文件，大部分常见的 Linux 发行版就没有提供，比如 Ubuntu。只有当内核配置 <code>CONFIG_IKCONFIG</code> 和 <code>CONFIG_IKCONFIG_PROC</code> 为<code>y</code>，才会在 /proc 中出现 config.gz 文件。当然，即使大多数发行版没有提供 /proc/config.gz，仍然可以通过 /boot 查看内核配置信息。</p><h3 id="boot-内核配置信息"><a href="#boot-内核配置信息" class="headerlink" title="/boot 内核配置信息"></a>/boot 内核配置信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /boot/config-5.4.0-80-generic</span><br><span class="line"></span><br><span class="line">cat /boot/config-$(uname -r)</span><br></pre></td></tr></table></figure><h3 id="make-oldconfig"><a href="#make-oldconfig" class="headerlink" title="make oldconfig"></a>make oldconfig</h3><p>It reads the existing <code>.config</code> file that was used for an old kernel and prompts the user for options in the current kernel source that are not found in the file. This is useful when taking an existing configuration and moving it to a new kernel.</p><p>使用其他 (通常是较旧的) 内核版本的 <code>.config</code> 文件时，需要先更新它。可以使用 <code>make oldconfig</code> 命令，以交互方式询问对新配置的选择。</p><p><code>make olddefconfig</code> 的含义为：采用已有的<code>.config</code>文件的参数作为默认参数，同时升级依赖属性，新属性设置为默认值不再提醒。</p><h3 id="mkinitramfs"><a href="#mkinitramfs" class="headerlink" title="mkinitramfs"></a>mkinitramfs</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkinitramfs -o /boot/initrd.img 2.6.24-16</span><br><span class="line"></span><br><span class="line">mkinitramfs -o &lt;full_path_to_initrd&gt; &lt;kernel_version&gt;</span><br></pre></td></tr></table></figure><p>Note: 2.6.24-16是需要创建initramfs的kernel版本号，如果是给当前kernel制作initramfs，可以用<code>uname -r</code>查看当前的版本号。提供kernel版本号的主要目的是为了在initramfs中添加指定kernel的驱动模块。<code>mkinitramfs</code>会把<code>/lib/modules/${kernel_version}/</code>目录下的一些启动会用到的模块添加到initramfs中。</p><h3 id="load-a-module-in-initrd"><a href="#load-a-module-in-initrd" class="headerlink" title="load a module in initrd"></a>load a module in initrd</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/initramfs-tools/modules</span><br><span class="line"># List of modules that you want to include in your initramfs.</span><br><span class="line"># They will be loaded at boot time in the order below.</span><br><span class="line">#</span><br><span class="line"># Syntax:  module_name [args ...]</span><br><span class="line">#</span><br><span class="line"># You must run update-initramfs(8) to effect this change.</span><br><span class="line">#</span><br><span class="line"># Examples:</span><br><span class="line">#</span><br><span class="line"># raid1</span><br><span class="line"># sd_mod</span><br></pre></td></tr></table></figure><p>Add the names of the modules to <code>/etc/initramfs-tools/modules</code> . This added the modules to the initrd file. Update the initrd file by  <code>update-initramfs -u</code> </p><p><code>update-initramfs -u</code>更新当前kernel的initramfs。在添加模块时，initramfs tools只会添加一些必要模块，用户可以通过在<code>/etc/initramfs-tools/modules</code>文件中加入模块名称来指定必须添加的模块。</p><h3 id="INSTALL-MOD-PATH"><a href="#INSTALL-MOD-PATH" class="headerlink" title="INSTALL_MOD_PATH"></a>INSTALL_MOD_PATH</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make modules_install INSTALL_MOD_PATH=out/</span><br></pre></td></tr></table></figure><p>The <code>INSTALL_MOD_PATH</code> variable is needed to install the modules in the target root filesystem instead of your host root filesystem.</p><h3 id="Linux-内核编译-LOCALVERSION-配置"><a href="#Linux-内核编译-LOCALVERSION-配置" class="headerlink" title="Linux 内核编译 LOCALVERSION 配置"></a>Linux 内核编译 LOCALVERSION 配置</h3><ul><li><p><code>LOCALVERSION</code> 可以在版本号之后追加后缀信息, 如果再定义 <code>CONFIG_LOCALVERSION_AUTO</code>, 将在最后进一步追加 <code>git</code> 版本号为后缀信息.</p></li><li><p>不定义<code>CONFIG_LOCALVERSION_AUTO</code> 将不显示 <code>git</code> 仓库信息, 如果此时 <code>LOCALVERSION</code> 变量也未定义, 将追加 “+”.</p></li><li><p>如果既不想添加后缀, 又不想有 <code>&quot;+&quot;</code> 号 : 不定义<code>CONFIG_LOCALVERSION_AUTO</code>, 将 <code>LOCALVERSION</code> 变量定义为空 : <code>LOCALVERSION=</code>.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j8 bindeb-pkg LOCALVERSION=</span><br></pre></td></tr></table></figure></li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.cnblogs.com/liuyawei/p/4024614.html" target="_blank" rel="noopener">Linux内核编译和替换小结</a></li><li><a href="https://blog.csdn.net/lu_embedded/article/details/108908577" target="_blank" rel="noopener">/proc/config.gz 是什么</a> </li><li><a href="https://stackoverflow.com/questions/4178526/what-does-make-oldconfig-do-exactly-in-the-linux-kernel-makefile" target="_blank" rel="noopener">What does “make oldconfig” do exactly in the Linux kernel makefile?</a></li><li><a href="https://askubuntu.com/questions/676707/how-to-load-a-module-in-initrd" target="_blank" rel="noopener">How to load a module in initrd?</a></li><li><a href="https://www.cnblogs.com/wwang/archive/2010/10/27/1862222.html" target="_blank" rel="noopener">制作initramfs/initrd镜像</a></li><li><a href="https://github.com/gatieme/LDD-LinuxDeviceDrivers/tree/master/study/problem/build/local_version" target="_blank" rel="noopener">Linux 内核编译 LOCALVERSION 配置(分析内核版本号自动添加的”+”号)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录内核编译配置的相关内容。
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>深入理解FLR</title>
    <link href="http://liujunming.github.io/2021/09/11/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3FLR/"/>
    <id>http://liujunming.github.io/2021/09/11/深入理解FLR/</id>
    <published>2021-09-11T00:21:12.000Z</published>
    <updated>2021-09-11T09:36:04.131Z</updated>
    
    <content type="html"><![CDATA[<p>本文将结合spec与kernel代码来介绍FLR。<a id="more"></a><br>FLR主要针对的是支持多个功能的PCIe设备（Multi-Fun PCIe Device），可以实现只对特定的Function复位，而其他的Function不受影响。</p><h3 id="check-FLR-capability"><a href="#check-FLR-capability" class="headerlink" title="check FLR capability"></a>check FLR capability</h3><p><img src="/images/2021/09/3.PNG" alt><br><img src="/images/2021/09/4.PNG" alt></p><p><a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci.c#L4556" target="_blank" rel="noopener">pcie_has_flr</a></p><h3 id="initiate-a-PCIe-function-level-reset"><a href="#initiate-a-PCIe-function-level-reset" class="headerlink" title="initiate a PCIe function level reset"></a>initiate a PCIe function level reset</h3><p><img src="/images/2021/09/5.PNG" alt><br><img src="/images/2021/09/6.PNG" alt></p><p><a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci.c#L4576" target="_blank" rel="noopener">pcie_flr</a></p><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci -vvvxxx -s 00:02.0</span><br></pre></td></tr></table></figure><p><img src="/images/2021/09/2.PNG" alt></p><hr><p>参考资料:</p><ol><li><a href="http://blog.chinaaet.com/justlxy/p/5100057845" target="_blank" rel="noopener">PCIe扫盲——复位机制介绍(FLR)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将结合spec与kernel代码来介绍FLR。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Summary of PCI&amp;PCIe device function reset</title>
    <link href="http://liujunming.github.io/2021/09/10/Summary-of-PCI-device-reset/"/>
    <id>http://liujunming.github.io/2021/09/10/Summary-of-PCI-device-reset/</id>
    <published>2021-09-10T10:20:31.000Z</published>
    <updated>2021-09-11T01:17:51.344Z</updated>
    
    <content type="html"><![CDATA[<p>本文将总结PCI&amp;PCIe device function reset的相关内容。<br><a id="more"></a></p><h3 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h3><p>当pass-thru device时，需要reset device function。<a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/vfio/pci/vfio_pci.c#L329" target="_blank" rel="noopener">code</a></p><p>内核中，reset a PCI device function的函数是<a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci.c#L5050" target="_blank" rel="noopener">__pci_reset_function_locked</a>，如有兴趣，可以深入研究。</p><h3 id="2-Spec"><a href="#2-Spec" class="headerlink" title="2. Spec"></a>2. Spec</h3><p><img src="/images/2021/09/1.PNG" alt></p><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>There are two main types of resets - conventional reset, and function-level reset. There are also two types of conventional resets, fundamental resets and non-fundamental resets. </p><h4 id="3-1-conventional-reset"><a href="#3-1-conventional-reset" class="headerlink" title="3.1 conventional reset"></a>3.1 conventional reset</h4><h5 id="3-1-1-cold-reset"><a href="#3-1-1-cold-reset" class="headerlink" title="3.1.1 cold reset"></a>3.1.1 cold reset</h5><p>A cold reset is a fundamental reset that takes place after power is applied to a PCIe device.<br>因为主电源断开后重新连接导致的复位。</p><h5 id="3-1-2-warm-reset"><a href="#3-1-2-warm-reset" class="headerlink" title="3.1.2 warm reset"></a>3.1.2 warm reset</h5><p>A warm reset is a fundamental reset that is triggered without disconnecting power from the device.<br>在不关闭主电源的情况下，产生的复位。</p><h5 id="3-1-3-hot-reset"><a href="#3-1-3-hot-reset" class="headerlink" title="3.1.3 hot reset"></a>3.1.3 hot reset</h5><p>A hot reset is a conventional reset that is triggered across a PCI express link. A hot reset is triggered either when a link is forced into electrical idle or by sending TS1 and TS2 ordered sets with the hot reset bit set. Software can initiate a hot reset by setting and then clearing the <strong>secondary bus reset</strong> bit in the bridge control register in the PCI configuration space of the bridge port upstream of the device.</p><h4 id="3-2-function-level-reset"><a href="#3-2-function-level-reset" class="headerlink" title="3.2 function-level reset"></a>3.2 function-level reset</h4><p>A function-level reset (FLR) is a reset that affects only a single function of a PCI express device. It must not reset the entire PCIe device. Implementing function-level resets is not required by the PCIe specification. A function-level reset is initiated by setting the initiate function-level reset bit in the function’s device control register in the PCI express capability structure in the PCI configuration space.<br>当PCIe设备使用FLR方式进行复位时，有些与PCIe链路相关的状态和寄存器并不会被复位；还有一些特殊的配置寄存器不能被FLR方式复位，如Max_Payload_Size、RCB和一些与电源管理、流量控制和链路控制直接相关的寄存器。</p><h3 id="4-sysfs-interface"><a href="#4-sysfs-interface" class="headerlink" title="4. sysfs interface"></a>4. sysfs interface</h3><p><a href="https://elixir.bootlin.com/linux/v5.10.63/source/Documentation/ABI/testing/sysfs-bus-pci#L124" target="_blank" rel="noopener">/sys/bus/pci/devices/…/reset</a></p><p><a href="https://elixir.bootlin.com/linux/v5.10.63/source/drivers/pci/pci-sysfs.c#L1312" target="_blank" rel="noopener">reset_store</a></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reset_store</span><br><span class="line">pci_reset_function</span><br><span class="line">__pci_reset_function_locked</span><br></pre></td></tr></table></figure><hr><p>参考资料：</p><ol><li><a href="http://blog.chinaaet.com/justlxy/p/5100057844" target="_blank" rel="noopener">PCIe扫盲——复位机制介绍</a></li><li><a href="https://unix.stackexchange.com/questions/73908/how-to-reset-cycle-power-to-a-pcie-device" target="_blank" rel="noopener">How to Reset/Cycle Power to a PCIe Device?</a></li><li><a href="https://www.pinlue.com/article/2020/03/2519/4210053341853.html" target="_blank" rel="noopener">PCIe总线的两种复位方式</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将总结PCI&amp;amp;PCIe device function reset的相关内容。&lt;br&gt;
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about guest memory pinning when direct assignment of I/O devices</title>
    <link href="http://liujunming.github.io/2021/07/29/Notes-about-guest-memory-pinning-when-direct-assignment-of-I-0-devices/"/>
    <id>http://liujunming.github.io/2021/07/29/Notes-about-guest-memory-pinning-when-direct-assignment-of-I-0-devices/</id>
    <published>2021-07-29T11:36:36.000Z</published>
    <updated>2021-07-29T13:56:48.075Z</updated>
    
    <content type="html"><![CDATA[<p>当pass-thru device时，Hypervisor会建立dma remapping，但是存在一个问题就是:it requires the hypervisor to statically pin the entire guest memory.<a id="more"></a> 原因如下:</p><p><img src="/images/2021/07/24.PNG" alt><br>[ATC’11 vIOMMU: Efficient IOMMU Emulation]</p><p><img src="/images/2021/07/22.PNG" alt></p><p><img src="/images/2021/07/23.PNG" alt><br><a href="http://awilliam.github.io/presentations/KVM-Forum-2016/#/2/19" target="_blank" rel="noopener">http://awilliam.github.io/presentations/KVM-Forum-2016/#/2/19</a></p><p><img src="/images/2021/07/25.PNG" alt></p><p>优化的工作有:</p><ol><li>ATC’11 vIOMMU: Efficient IOMMU Emulation</li><li>ATC’20 coIOMMU: A Virtual IOMMU with Cooperative DMA Buffer Tracking for Efficient Memory Management in Direct I/O</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当pass-thru device时，Hypervisor会建立dma remapping，但是存在一个问题就是:it requires the hypervisor to statically pin the entire guest memory.
    
    </summary>
    
      <category term="VT-d" scheme="http://liujunming.github.io/categories/VT-d/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="VT-d" scheme="http://liujunming.github.io/tags/VT-d/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Intel VMCS Shadowing technology</title>
    <link href="http://liujunming.github.io/2021/07/22/Introduction-to-Intel-VMCS-Shadowing-technology/"/>
    <id>http://liujunming.github.io/2021/07/22/Introduction-to-Intel-VMCS-Shadowing-technology/</id>
    <published>2021-07-22T01:46:58.000Z</published>
    <updated>2021-07-22T12:47:52.756Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/2021/07/21/Introduction-to-nested-virtualization/">Introduction to nested virtualization</a>一文介绍了嵌套虚拟化的基本概念。本文介绍的Intel VMCS Shadowing technology这一硬件技术，正是为了提高嵌套虚拟化系统的性能而应运而生的。<a id="more"></a></p><h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>The motivation of VMCS Shadowing: Eliminate VM Exits on guest VMCS accesses</p><p><img src="/images/2021/07/19.PNG" alt></p><p><img src="/images/2021/07/20.PNG" alt></p><p><img src="/images/2021/07/21.PNG" alt></p><h3 id="2-implementation"><a href="#2-implementation" class="headerlink" title="2. implementation"></a>2. implementation</h3><ul><li>Shadow VMCS is processor-dependent and must be accessed by L0 or L1 using VMREAD and VMWRITE instructions only</li><li>To avoid hardware dependencies:<ul><li>Software defined VMCS1→2 format is part of L1 address space</li><li>Processor-specific shadow VMCS format is part of L0 address space</li></ul></li><li>L0 synchronize the shadow VMCS content with the software-controlled VMCS1→2 format</li><li>Design simplifies live migration of L1, which does not depended on the shadow VMCS layout</li></ul><h3 id="3-sync-process"><a href="#3-sync-process" class="headerlink" title="3. sync process"></a>3. sync process</h3><ul><li>Before running L2 after switching from L1 we need to update all the changes L1 did, from the shadow VMCS to VMCS1→2</li><li>Before switching back to L1 after running L2 we need to sync from VMCS1→2 to the shadow VMCS</li></ul><h3 id="4-reducing-syncing-cost"><a href="#4-reducing-syncing-cost" class="headerlink" title="4. reducing syncing cost"></a>4. reducing syncing cost</h3><p>When Intel VMCS shadowing is used, the L0 VMM has no idea which of the more than 130 VMCS fields were accessed, since it was not involved in those accessed. The L0 VMM must therefore synchronize every filed that could have possibly been accessed, even though most of the fields are never touched.</p><p>Results from Intel Labs profiling across a wide variety of VMMs, show that approximately 90% of VMCS fields are never read and more than 95% percent are never written. As a result, for most VMMs, a full VMCS synchronization can take approximately 15 times longer than necessary.</p><p><strong>Idea: Shadow only the necessary fields</strong></p><p>To reduce this synchronization overhead, Intel incorporated an addtional feature into Intel VMCS Shadowing called VMREAD and VMWRITE bitmaps. These bitmaps allow for selective access to the shadow VMCS. The L0 VMM can tune the bitmaps so that the 5-10 percent of VMCS fields that are commonly accessed are written directly to the shadow VMCS, while the very rarely accessed fields are synchronized through the slower path that is managed by the L0 VMM.</p><p>By using the VMREAD/VMWRITE bitmaps, the L0 VMM gets the best of both worlds. Nearly all of the accesses go directly to the fast shadow VMCS and very few extraneous fields need to be synchronized. </p><hr><p>参考资料:</p><ol><li><a href="https://events.static.linuxfound.org/sites/events/files/cojp13_nakajima.pdf" target="_blank" rel="noopener">Making Nested Virtualization Real by Using Hardware Virtualization Features</a></li><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/intel-vmcs-shadowing-paper.pdf" target="_blank" rel="noopener">intel-vmcs-shadowing-paper</a></li><li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/12/Improving-KVM-x86-Nested-Virtualization-Liran-Alon-Oracle.pdf" target="_blank" rel="noopener">Improving-KVM-x86-Nested-Virtualization-Liran-Alon-Oracle</a></li><li><a href="https://docs.google.com/file/d/0BzyAwvVlQckeMTd2T2RBT2cweDg/edit?resourcekey=0-FfuT4IueJf7OFC4KmykJdA" target="_blank" rel="noopener">KVM forum 2013 Nested virtualization:shadow turtles</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/2021/07/21/Introduction-to-nested-virtualization/&quot;&gt;Introduction to nested virtualization&lt;/a&gt;一文介绍了嵌套虚拟化的基本概念。本文介绍的Intel VMCS Shadowing technology这一硬件技术，正是为了提高嵌套虚拟化系统的性能而应运而生的。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="嵌套虚拟化" scheme="http://liujunming.github.io/tags/%E5%B5%8C%E5%A5%97%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to nested virtualization</title>
    <link href="http://liujunming.github.io/2021/07/21/Introduction-to-nested-virtualization/"/>
    <id>http://liujunming.github.io/2021/07/21/Introduction-to-nested-virtualization/</id>
    <published>2021-07-21T11:20:41.000Z</published>
    <updated>2021-07-22T12:47:52.757Z</updated>
    
    <content type="html"><![CDATA[<p>一直听闻嵌套虚拟化这一技术，但是从未深入研究过其底层原理。借此机会，本文将一探嵌套虚拟化。</p><p>嵌套虚拟化的绝佳入门材料为ODSI’10上的论文<a href="https://www.usenix.org/conference/osdi10/turtles-project-design-and-implementation-nested-virtualization" target="_blank" rel="noopener">The Turtles Project: Design and Implementation of Nested Virtualization</a>。本文内容主要是<a href="https://www.usenix.org/legacy/events/osdi10/tech/full_papers/Ben-Yehuda.pdf" target="_blank" rel="noopener">paper</a>和<a href="https://www.usenix.org/legacy/events/osdi10/tech/slides/ben-yehuda.pdf" target="_blank" rel="noopener">slides</a>的notes。<a id="more"></a></p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>In nested virtualization, a hypervisor can run multiple other hypervisors with their associated virtual machines. 通俗地来说，套娃。</p><p>接下来将从CPU虚拟化，内存虚拟化，IO虚拟化这三个方面去介绍嵌套虚拟化的原理。</p><h3 id="2-CPU-Nested-VMX-Virtualization"><a href="#2-CPU-Nested-VMX-Virtualization" class="headerlink" title="2. CPU: Nested VMX Virtualization"></a>2. CPU: Nested VMX Virtualization</h3><p>Approach for nested virtualization <em>multiplexes</em> multiple levels of virtualization (multiple hypervisors) on the single level of architectural support available. </p><p><img src="/images/2021/07/3.PNG" alt></p><p><img src="/images/2021/07/4.PNG" alt></p><p><img src="/images/2021/07/5.PNG" alt></p><p><img src="/images/2021/07/6.PNG" alt></p><p><img src="/images/2021/07/7.PNG" alt></p><h4 id="2-1-VMX-Trap-and-Emulate"><a href="#2-1-VMX-Trap-and-Emulate" class="headerlink" title="2.1 VMX Trap and Emulate"></a>2.1 VMX Trap and Emulate</h4><p><img src="/images/2021/07/15.PNG" alt></p><h4 id="2-2-VMCS-Shadowing"><a href="#2-2-VMCS-Shadowing" class="headerlink" title="2.2 VMCS Shadowing"></a>2.2 VMCS Shadowing</h4><p><img src="/images/2021/07/16.PNG" alt></p><p><img src="/images/2021/07/17.PNG" alt></p><h4 id="2-3-VMEntry-and-VMExit-Emulation"><a href="#2-3-VMEntry-and-VMExit-Emulation" class="headerlink" title="2.3 VMEntry and VMExit Emulation"></a>2.3 VMEntry and VMExit Emulation</h4><p><img src="/images/2021/07/18.PNG" alt></p><h4 id="2-4-summary"><a href="#2-4-summary" class="headerlink" title="2.4 summary"></a>2.4 summary</h4><p><img src="/images/2021/07/8.PNG" alt></p><p><img src="/images/2021/07/9.PNG" alt></p><h3 id="3-MMU-Multi-dimensional-Paging"><a href="#3-MMU-Multi-dimensional-Paging" class="headerlink" title="3. MMU: Multi-dimensional Paging"></a>3. MMU: Multi-dimensional Paging</h3><p><img src="/images/2021/07/10.PNG" alt></p><p><img src="/images/2021/07/11.PNG" alt></p><p><img src="/images/2021/07/12.PNG" alt></p><h3 id="4-I-O-Multi-level-Device-Assignment"><a href="#4-I-O-Multi-level-Device-Assignment" class="headerlink" title="4. I/O: Multi-level Device Assignment"></a>4. I/O: Multi-level Device Assignment</h3><p><img src="/images/2021/07/13.PNG" alt></p><p><img src="/images/2021/07/14.PNG" alt></p><h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><ol><li><p>Approach for CPU virtualization works by having the lowest hypervisor inspect the trap and forward it to the hypervisors above it for emulation.  </p></li><li><p>For efficient memory virtualization, we developed multi-dimensional paging, which collapses the different memory translation tables into the one or two tables provided by the MMU.</p></li><li><p>For efficient I/O virtualization, we bypass multiple levels of hypervisor I/O stacks to provide nested guests with direct assignment of I/O devices via multilevel device assignment. </p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一直听闻嵌套虚拟化这一技术，但是从未深入研究过其底层原理。借此机会，本文将一探嵌套虚拟化。&lt;/p&gt;
&lt;p&gt;嵌套虚拟化的绝佳入门材料为ODSI’10上的论文&lt;a href=&quot;https://www.usenix.org/conference/osdi10/turtles-project-design-and-implementation-nested-virtualization&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Turtles Project: Design and Implementation of Nested Virtualization&lt;/a&gt;。本文内容主要是&lt;a href=&quot;https://www.usenix.org/legacy/events/osdi10/tech/full_papers/Ben-Yehuda.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper&lt;/a&gt;和&lt;a href=&quot;https://www.usenix.org/legacy/events/osdi10/tech/slides/ben-yehuda.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;slides&lt;/a&gt;的notes。
    
    </summary>
    
      <category term="虚拟化" scheme="http://liujunming.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="嵌套虚拟化" scheme="http://liujunming.github.io/tags/%E5%B5%8C%E5%A5%97%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PCI Express Configuration Space</title>
    <link href="http://liujunming.github.io/2021/07/02/Notes-about-extended-PCIE-configuration-space/"/>
    <id>http://liujunming.github.io/2021/07/02/Notes-about-extended-PCIE-configuration-space/</id>
    <published>2021-07-02T08:12:43.000Z</published>
    <updated>2021-07-02T14:48:47.135Z</updated>
    
    <content type="html"><![CDATA[<p>本文将记录PCI Express Configuration Space相关笔记。<a id="more"></a></p><h3 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h3><blockquote><p><strong>The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes</strong>. This extended configuration space <em>cannot</em> be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC).</p></blockquote><p>In the beginning there was a configuration space, for each PCI device function, of 256 bytes.<br>This space was accessed using the PCI legacy mechanism (we can ignore the fact there were two mechanisms) at ports 0xcf8 and 0xcfc.</p><p>The PCIe extended this space from 256 bytes to 4KiB and introduced a new mechanism to access the configuration space (<em>all</em> of it).</p><p>So, to recap:</p><ul><li>There is a single PCI configuration space of 4KiB. It is divided into a <em>PCI 3.0 Compatible region</em> (from 0x000 to 0x0ff) and PCIe extended configuration region (from 0x100 to 0xfff).</li><li>There are two mechanism to access the PCI configuration space. One is the legacy mechanism at 0xcf8/0xcfc the other one is a memory mapped area.</li><li>The Legacy mechanism can only access the compatibility region (the first 256 bytes).</li><li>The ECAM can access all of the space.</li></ul><h3 id="2-ECAM"><a href="#2-ECAM" class="headerlink" title="2. ECAM"></a>2. ECAM</h3><p>Enhanced Configuration Access Mechanism (ECAM)</p><blockquote><p>PCI Express extends the Configuration Space to 4096 bytes per Function as compared to 256 bytes allowed by PCI Local Bus Specification.</p><p>PCI Express Configuration Space is divided into a PCI 3.0 compatible region, which consists of the first 256 bytes of a Function’s Configuration Space, and a PCI Express Extended Configuration Space which consists of the remaining Configuration Space .</p><p>The PCI 3.0 compatible Configuration Space can be accessed using either the mechanism defined in the PCI Local Bus Specification or the PCI Express Enhanced Configuration Access Mechanism (ECAM).</p><p><strong>Accesses made using either access mechanism are equivalent. The PCI Express Extended Configuration Space can only be accessed by using the ECAM.</strong></p></blockquote><p>The base address of the MMIO area for the configuration space of each PCIe devices in a PCI segment group is given in the <a href="https://wiki.osdev.org/PCI_Express#Enhanced_Configuration_Mechanism" target="_blank" rel="noopener">ACPI table MCFG</a>.</p><p>The MCFG table lists, for each PCI segment group, the first and last (inclusive) bus number of the PCI segment group and the base address of the PCI Express enhanced configuration space.</p><p>The MCFG table is setup by the BIOS/UEFI based upon the value of the <code>PCIEXBAR</code> (offset 60h) in the Host Bridge PCI configuration space.</p><h3 id="3-Verification"><a href="#3-Verification" class="headerlink" title="3. Verification"></a>3. Verification</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ lspci -xxx -s 00:0.0</span><br><span class="line">00:00.0 Host bridge: Intel Corporation Xeon E3-1200 v6/7th Gen Core Processor Host Bridge/DRAM Registers (rev 02)</span><br><span class="line">00: 86 80 04 59 06 00 90 20 02 00 00 06 00 00 00 00</span><br><span class="line">10: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">20: 00 00 00 00 00 00 00 00 00 00 00 00 86 80 70 20</span><br><span class="line">30: 00 00 00 00 e0 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">40: 01 90 d1 fe 00 00 00 00 01 00 d1 fe 00 00 00 00</span><br><span class="line">50: c1 02 00 00 31 80 00 00 47 00 f0 8f 01 00 00 8b</span><br><span class="line">60: 01 00 00 e0 00 00 00 00 01 80 d1 fe 00 00 00 00</span><br><span class="line">70: 00 00 00 fe 01 00 00 00 00 0c 00 fe 7f 00 00 00</span><br><span class="line">80: 11 00 00 00 00 00 00 00 1a 00 00 00 00 00 00 00</span><br><span class="line">90: 01 00 00 fe 01 00 00 00 01 00 f0 6d 02 00 00 00</span><br><span class="line">a0: 01 00 00 00 02 00 00 00 01 00 00 6e 02 00 00 00</span><br><span class="line">b0: 01 00 00 8c 01 00 80 8b 01 00 00 8b 01 00 00 90</span><br><span class="line">c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">e0: 09 00 10 01 21 60 61 7a dc 80 15 14 00 c0 06 00</span><br><span class="line">f0: 00 00 00 00 c8 0f 09 00 00 00 00 00 00 00 00 00</span><br></pre></td></tr></table></figure><p><img src="/images/2021/07/1.PNG" alt><br><img src="/images/2021/07/2.PNG" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/iomem | grep MMCONFIG</span><br><span class="line">e0000000-efffffff : PCI MMCONFIG 0000 [bus 00-ff]</span><br><span class="line"></span><br><span class="line">$ dmesg | grep -i MCFG</span><br><span class="line">[    0.011632] ACPI: MCFG 0x000000008A5D0060 00003C (v01 INTEL  NUC7i5DN 00000043 MSFT 00000097)</span><br><span class="line"></span><br><span class="line">$ dmesg | grep -i MMCONFIG</span><br><span class="line">[    0.149772] PCI: MMCONFIG for domain 0000 [bus 00-ff] at [mem 0xe0000000-0xefffffff] (base 0xe0000000)</span><br><span class="line">[    0.149772] PCI: MMCONFIG at [mem 0xe0000000-0xefffffff] reserved in E820</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://stackoverflow.com/questions/57457283/how-to-access-pcie-configuration-space-ecam" target="_blank" rel="noopener">How to access PCIe configuration space?</a></li><li><a href="https://stackoverflow.com/questions/52136259/how-to-access-pci-express-configuration-space-via-mmio" target="_blank" rel="noopener">How to access pci express configuration space via MMIO?</a></li><li><a href="https://stackoverflow.com/questions/6341540/how-to-read-extended-pcie-configuration-space-in-linux" target="_blank" rel="noopener">How to read extended PCIE configuration space in Linux?</a></li><li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/10th-gen-core-families-datasheet-vol-2-datasheet.pdf" target="_blank" rel="noopener">10th gen core families datasheet vol 2 datasheet</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将记录PCI Express Configuration Space相关笔记。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about EFI Shell</title>
    <link href="http://liujunming.github.io/2021/07/01/Notes-about-EFI-Shell/"/>
    <id>http://liujunming.github.io/2021/07/01/Notes-about-EFI-Shell/</id>
    <published>2021-07-01T01:54:26.000Z</published>
    <updated>2021-07-01T10:11:28.349Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录EFI Shell相关的materials与常用的一些指令。<a id="more"></a></p><h3 id="1-materials"><a href="#1-materials" class="headerlink" title="1. materials"></a>1. materials</h3><p><a href="https://docstore.mik.ua/manuals/hp-ux/en/5991-1247B/ch04s13.html" target="_blank" rel="noopener">Command Reference for EFI Shell Commands</a><br><a href="https://manuais.iessanclemente.net/images/a/a6/EFI-ShellCommandManual.pdf" target="_blank" rel="noopener">Shell Command Reference Manual</a><br><a href="https://www.intel.com/content/dam/support/us/en/documents/motherboards/server/sb/efi_instructions.pdf" target="_blank" rel="noopener">Basic Instructions for Using the Extensible Firmware Interface</a><br><a href="https://kb.stonegroup.co.uk/index.php?View=entry&amp;EntryID=84" target="_blank" rel="noopener">How to Access the EFI Shell to carry out Systems Diagnostics or Updates</a></p><h3 id="2-常用指令"><a href="#2-常用指令" class="headerlink" title="2. 常用指令"></a>2. 常用指令</h3><p>EFI commands are very similar to common DOS and Linux commands. The most frequently<br>used commands include: </p><ul><li><code>ls</code> (or <code>dir</code>): Lists the directory contents.</li><li><code>cd</code>: Changes the directory.</li><li><code>cp</code>: Copies one or more files/directories to another location.</li><li><code>move</code>: Moves one or more files/directories to the destination.</li><li><code>rm</code>: Deletes one or more files or directories.</li><li><code>map</code>: Displays, resets, or deletes mappings with the verbose option.</li><li><code>map –r</code>: Commonly used to refresh mapped drives.</li><li><code>edit</code>: Starts a basic text editor with on-screen function key help.</li><li><code>set</code>: Displays (set), creates (set sname value), deletes (set –d), or changes EFI environment variables.</li><li><code>cls</code>: Clears the screen and can also change the background color.</li><li><code>echo</code>: Displays results on the screen.</li><li><code>help</code>: Displays help information.</li><li><code>exit</code>: Leaves the EFI shell and returns to the BIOS utility.</li><li><code>reset</code>: Resets the system with a warm reboot or complete shutdown. </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drivers –b</span><br></pre></td></tr></table></figure><p>“-b” option for any UEFI command is a screen “pause” option so that you do not miss what you are trying to find because of it scrolling by.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录EFI Shell相关的materials与常用的一些指令。
    
    </summary>
    
      <category term="Firmware" scheme="http://liujunming.github.io/categories/Firmware/"/>
    
    
      <category term="Firmware" scheme="http://liujunming.github.io/tags/Firmware/"/>
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>PCI Expansion ROM</title>
    <link href="http://liujunming.github.io/2021/06/30/PCI-Expansion-ROM/"/>
    <id>http://liujunming.github.io/2021/06/30/PCI-Expansion-ROM/</id>
    <published>2021-06-30T07:01:55.000Z</published>
    <updated>2021-09-10T11:09:27.597Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍PCI Expansion ROM相关内容，转载于<a href="https://blog.csdn.net/pwl999/article/details/78208065" target="_blank" rel="noopener">PCI&amp;PCIE ExpansionOption ROM</a>。<a id="more"></a></p><h3 id="1-What-is-Expansion-ROM"><a href="#1-What-is-Expansion-ROM" class="headerlink" title="1. What is Expansion ROM"></a>1. What is Expansion ROM</h3><p>Expansion rom是pci/pcie设备可选的一个外接的eprom芯片，用来存储相应pci设备的初始化代码或者系统启动代码。BIOS在POST(Power-on Self Test)阶段，会扫描pci设备是否有expansion rom，有的话将其拷贝到ram中执行。在PCI规范中称为expansion rom，在BIOS术语里面称为option rom。</p><h3 id="2-PCI配置空间关于Expansion-ROM的定义"><a href="#2-PCI配置空间关于Expansion-ROM的定义" class="headerlink" title="2. PCI配置空间关于Expansion ROM的定义"></a>2. PCI配置空间关于Expansion ROM的定义</h3><p>The four-byte register at offset 30h in a type 00h predefined header is defined to handle the base address and size information for this expansion ROM.  </p><p><img src="/images/2021/06/20.PNG" alt></p><p>关于“Expansion rom base address”寄存器的具体定义细节如下：</p><p><img src="/images/2021/06/21.PNG" alt></p><p>bit11-bit31定义expansion rom映射到memeory空间的高位地址bit11-bit31。bit0表示是否使能expansion rom，1使能为使能，需要注意的是expansion rom和pci其他的bar空间是共享地址解码的，所以一旦使能expansion rom就不能对其他的bar空间进行操作。expansion rom空间大小的计算方法和其他的“base address register”一样，往基址寄存器写全1，然后再回读进行计算。</p><p>软件把“Expansion rom base address”寄存器的基地址配置成相应的memeory空间地址并使能expansion rom以后，就可以对设备的expansion rom进行读访问了。软件将expansion rom中包含的可执行代码拷贝到ram中执行，不用关心这些可执行代码具体干些什么事情。</p><h3 id="3-Expansion-ROM的组织结构"><a href="#3-Expansion-ROM的组织结构" class="headerlink" title="3. Expansion ROM的组织结构"></a>3. Expansion ROM的组织结构</h3><p>Expansion rom的组织结构有相应的规范。一个可能包含多个rom image，可以支持多个不同类型的pci设备，每种设备也可以支持不同架构cpu的可执行代码。多个image在一个expansion rom芯片中的组织如下图所示，其中每份image的开始地址都是以512bytes对齐的：</p><p><img src="/images/2021/06/22.PNG" alt><br>一份标准的image由两部分组成：PCI Expansion ROM Header Format和PCI Data Structure Format。具体的格式定义如下图:<br><img src="/images/2021/06/23.png" alt></p><h3 id="4-Expansion-ROM的初始化过程"><a href="#4-Expansion-ROM的初始化过程" class="headerlink" title="4. Expansion ROM的初始化过程"></a>4. Expansion ROM的初始化过程</h3><p>BIOS的POST阶段，扫描并执行pci设备的expansion rom的过程大概分以下几步：</p><ol><li>首先判断pci设备是否实现“Expansion rom base address”寄存器，有则进行下一步判断；</li><li>如果有实现了expansion rom的基址寄存器，则配置和使能expansion rom，然后查找expansion rom是否有”AA55”的标示字符，如果有则说明设备有真实的expansion rom芯片存在；</li><li>如果expansion rom已经存在，则扫描是否有适合本设备和本CPU架构的image代码存在；</li><li>如果有适合本环境的image代码存在，则把相应的代码拷贝到ram的合适位置，并跳入header format中指定的初始化入口执行；</li><li>最后关闭expansion rom的使能。</li></ol><p>关于image代码的长度有3个概念：一是Image size这部分包括整个image的长度；一是Initialization size，这部分是将expansion rom拷贝到ram中执行，需要拷贝的长度；一是Runtime size，这部分的程序可以驻留在内存中，供运行时系统软件和OS调用。为了降低常驻代码对ram的占用，常用的做法是:在init功能执行完成以后，执行代码自己把长度修改为常驻程序最少需要占用的Runtime size。如果不需要常驻程序，直接把Runtime size修改成0。<br>对应的关系是：Image size &gt;= Initialization size &gt;= Runtime size。相应的组织架构如下图：<br><img src="/images/2021/06/24.png" alt></p><hr><p>参考资料:</p><ol><li>PCI Local Bus Specification Revision 3.0</li><li><a href="https://blog.csdn.net/pwl999/article/details/78208065" target="_blank" rel="noopener">PCI&amp;PCIE ExpansionOption ROM</a></li><li><a href="https://blog.csdn.net/huangkangying/article/details/8932463" target="_blank" rel="noopener">BIOS之Option ROM详解</a></li><li>PCI Firmware Specification Revision 3.0 </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍PCI Expansion ROM相关内容，转载于&lt;a href=&quot;https://blog.csdn.net/pwl999/article/details/78208065&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PCI&amp;amp;PCIE ExpansionOption ROM&lt;/a&gt;。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
</feed>
