<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2025-02-16T03:30:29.868Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Notes about vCPU MPState</title>
    <link href="http://liujunming.github.io/2025/02/16/Notes-about-vCPU-MPState/"/>
    <id>http://liujunming.github.io/2025/02/16/Notes-about-vCPU-MPState/</id>
    <published>2025-02-16T03:20:47.000Z</published>
    <updated>2025-02-16T03:30:29.868Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下vCPU MPState(MultiProcessing State)的相关notes。<a id="more"></a></p><p>在热迁移时，qemu会执行<code>kvm_get_mp_state</code>函数:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">kvm_get_mp_state</span><span class="params">(X86CPU *cpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    CPUState *cs = CPU(cpu);</span><br><span class="line">    CPUX86State *env = &amp;cpu-&gt;env;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">kvm_mp_state</span> <span class="title">mp_state</span>;</span></span><br><span class="line">    <span class="keyword">int</span> ret;</span><br><span class="line"></span><br><span class="line">    ret = kvm_vcpu_ioctl(cs, KVM_GET_MP_STATE, &amp;mp_state);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    env-&gt;mp_state = mp_state.mp_state;</span><br><span class="line">    <span class="keyword">if</span> (kvm_irqchip_in_kernel()) &#123;</span><br><span class="line">        cs-&gt;halted = (mp_state.mp_state == KVM_MP_STATE_HALTED);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>此时<code>KVM_GET_MP_STATE</code>就是关键线索，搜索<a href="https://www.kernel.org/doc/Documentation/virt/kvm/api.txt" target="_blank" rel="noopener">kvm api Documentation</a>即可找到最终的答案。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">4.38 KVM_GET_MP_STATE</span><br><span class="line"></span><br><span class="line">Capability: KVM_CAP_MP_STATE</span><br><span class="line">Architectures: x86, s390, arm, arm64</span><br><span class="line">Type: vcpu ioctl</span><br><span class="line">Parameters: struct kvm_mp_state (out)</span><br><span class="line">Returns: 0 on success; -1 on error</span><br><span class="line"></span><br><span class="line">struct kvm_mp_state &#123;</span><br><span class="line">__u32 mp_state;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Returns the vcpu&apos;s current &quot;multiprocessing state&quot; (though also valid on</span><br><span class="line">uniprocessor guests).</span><br><span class="line"></span><br><span class="line">Possible values are:</span><br><span class="line"></span><br><span class="line"> - KVM_MP_STATE_RUNNABLE:        the vcpu is currently running [x86,arm/arm64]</span><br><span class="line"> - KVM_MP_STATE_UNINITIALIZED:   the vcpu is an application processor (AP)</span><br><span class="line">                                 which has not yet received an INIT signal [x86]</span><br><span class="line"> - KVM_MP_STATE_INIT_RECEIVED:   the vcpu has received an INIT signal, and is</span><br><span class="line">                                 now ready for a SIPI [x86]</span><br><span class="line"> - KVM_MP_STATE_HALTED:          the vcpu has executed a HLT instruction and</span><br><span class="line">                                 is waiting for an interrupt [x86]</span><br><span class="line"> - KVM_MP_STATE_SIPI_RECEIVED:   the vcpu has just received a SIPI (vector</span><br><span class="line">                                 accessible via KVM_GET_VCPU_EVENTS) [x86]</span><br><span class="line"> - KVM_MP_STATE_STOPPED:         the vcpu is stopped [s390,arm/arm64]</span><br><span class="line"> - KVM_MP_STATE_CHECK_STOP:      the vcpu is in a special error state [s390]</span><br><span class="line"> - KVM_MP_STATE_OPERATING:       the vcpu is operating (running or halted)</span><br><span class="line">                                 [s390]</span><br><span class="line"> - KVM_MP_STATE_LOAD:            the vcpu is in a special load/startup state</span><br><span class="line">                                 [s390]</span><br><span class="line"></span><br><span class="line">On x86, this ioctl is only useful after KVM_CREATE_IRQCHIP. Without an</span><br><span class="line">in-kernel irqchip, the multiprocessing state must be maintained by userspace on</span><br><span class="line">these architectures.</span><br><span class="line"></span><br><span class="line">For arm/arm64:</span><br><span class="line"></span><br><span class="line">The only states that are valid are KVM_MP_STATE_STOPPED and</span><br><span class="line">KVM_MP_STATE_RUNNABLE which reflect if the vcpu is paused or not.</span><br></pre></td></tr></table></figure><hr><p>参考资料:</p><ol><li><a href="https://crosvm.dev/doc/hypervisor/enum.MPState.html" target="_blank" rel="noopener">Enum hypervisor::MPState</a></li><li><a href="https://www.kernel.org/doc/Documentation/virt/kvm/api.txt" target="_blank" rel="noopener">The Definitive KVM (Kernel-based Virtual Machine) API Documentation</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下vCPU MPState(MultiProcessing State)的相关notes。
    
    </summary>
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/categories/QEMU-KVM/"/>
    
    
      <category term="虚拟化" scheme="http://liujunming.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="QEMU&amp;&amp;KVM" scheme="http://liujunming.github.io/tags/QEMU-KVM/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Content-Addressable Memory (CAM)</title>
    <link href="http://liujunming.github.io/2025/02/15/Notes-about-Content-addressable-memory-CAM/"/>
    <id>http://liujunming.github.io/2025/02/15/Notes-about-Content-addressable-memory-CAM/</id>
    <published>2025-02-15T10:50:28.000Z</published>
    <updated>2025-02-15T11:45:04.155Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Content-Addressable Memory(CAM)相关notes，整理下来自deepseek的回答。<a id="more"></a> </p><p>CAM是一种特殊类型的存储器，其核心特点是<strong>通过内容直接检索数据</strong>，而非传统存储器（如RAM）通过地址访问数据的方式。</p><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>传统存储器（如RAM）：输入地址，返回该地址存储的数据；CAM：输入待查找的数据（关键词），返回该数据所在的地址或关联的其他数据。</p><p>工作流程如下所示:</p><ol><li>用户提供待匹配的<strong>关键词</strong>（Search Key）。</li><li>CAM在存储的所有条目中<strong>并行比较</strong>关键词。</li><li>返回匹配条目的地址或关联数据（如优先级最高的匹配结果）。</li></ol><h2 id="CAM的类型"><a href="#CAM的类型" class="headerlink" title="CAM的类型"></a>CAM的类型</h2><ul><li><p>二元CAM（Binary CAM, BCAM）：</p><ul><li>仅支持精确匹配（0或1）</li><li>应用场景：精确查找，如MAC地址表</li></ul></li><li><p>三元CAM（Ternary CAM, TCAM）：</p><ul><li>支持0、1和<strong>“无关”（Don’t Care）</strong>三种状态（通常用掩码表示）</li><li>可实现<strong>部分匹配</strong>，适用于需要通配符的场景</li><li>典型应用：IP路由中的<strong>最长前缀匹配</strong>（Longest Prefix Match）</li></ul></li></ul><h2 id="核心优势"><a href="#核心优势" class="headerlink" title="核心优势"></a>核心优势</h2><ul><li>高速并行搜索：所有存储条目同时比较，搜索时间复杂度为<strong>O(1)</strong>，远快于软件算法（如哈希表、树结构）</li><li>硬件级效率：无需遍历数据或处理冲突，适合实时性要求高的场景</li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>网络设备：</p><ul><li>路由器/交换机：快速查找路由表、MAC地址表</li><li>防火墙：基于规则的实时流量过滤</li></ul><h2 id="缺点与挑战"><a href="#缺点与挑战" class="headerlink" title="缺点与挑战"></a>缺点与挑战</h2><ul><li>高功耗：并行比较所有条目导致大量晶体管同时切换，功耗显著高于传统存储器</li><li>高成本：每个存储单元需集成比较电路，芯片面积增大</li><li>容量限制：受限于功耗和成本，CAM通常用于小规模高速缓存场景</li></ul><h2 id="TCAM的“无关”位示例"><a href="#TCAM的“无关”位示例" class="headerlink" title="TCAM的“无关”位示例"></a>TCAM的“无关”位示例</h2><p>在IP路由中，TCAM允许将子网掩码未覆盖的位设为“无关”，例如：IP地址：<code>192.168.1.0/24</code>（二进制：<code>11000000.10101000.00000001.********</code>），TCAM条目：<code>11000000 10101000 00000001 XXXXXXXX</code>（<code>X</code>表示“无关”位），搜索时，只需匹配前24位，后8位忽略，实现高效的最长前缀匹配。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CAM通过硬件并行性实现了极速数据检索，是网络设备和高速缓存的核心组件，但其功耗和成本限制了大规模应用。</p><hr><p>参考资料:</p><ol><li><a href="https://www.xilinx.com/products/intellectual-property/ef-di-cam.html" target="_blank" rel="noopener">xilinx Content Addressable Memory (CAM)</a></li><li><a href="https://ieeexplore.ieee.org/document/7159147" target="_blank" rel="noopener">Emerging Trends in Design and Applications of Memory-Based Computing and Content-Addressable Memories</a></li><li><a href="https://arxiv.org/pdf/1804.02330" target="_blank" rel="noopener">An Efficient I/O Architecture for RAM-based Content-Addressable Memory on FPGA</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Content-Addressable Memory(CAM)相关notes，整理下来自deepseek的回答。
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about linux /proc/kcore</title>
    <link href="http://liujunming.github.io/2025/02/15/Notes-about-linux-proc-kcore/"/>
    <id>http://liujunming.github.io/2025/02/15/Notes-about-linux-proc-kcore/</id>
    <published>2025-02-15T02:18:25.000Z</published>
    <updated>2025-02-15T03:50:51.251Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下linux中<code>/proc/kcore</code>相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p><code>/proc/kcore</code> is a file in the virtual /proc filesystem of a Linux machine. It is created by the kernel in <a href="https://elixir.bootlin.com/linux/v5.0/source/fs/proc/kcore.c" target="_blank" rel="noopener">fs/proc/kcore.c</a> and <strong>allows read access to all the kernels virtual memory space from userland</strong>.</p><p>Internally it has the format of an ELF core dump file (ELF Type 4/ET_CORE). That means it has the same format as a core file from a crashed process; but instead of capturing the (static) state of a single process at the moment of the crash, <strong>it provides a real time view into the state of the whole system</strong>.</p><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><ul><li>The ELF header (<code>Elf64_Ehdr</code>): It’s at the start of every ELF file. We need two pieces of information from it: the location and number of entries of the program header table.</li><li>The Program headers (<code>Elf64_Phdr</code>): An ELF file contains an array of program header structures. There are various subtypes of program headers, but we care only about the ones marked as <code>PT_LOAD</code>. <u>Each of these headers describe a loadable segment - a part of the file that is loaded into memory</u>. In <code>/proc/kcore</code>, <strong>they describe where in the file each portion of the system memory can be found.</strong></li></ul><p>On x86-64 systems, Linux maintains a complete one-to-one map of all physical memory in the kernels virtual address space. So by reading the right ranges of kernel virtual memory, one can get a complete copy of the content of the physical memory of that system.</p><p><img src="/images/2025/02/003.png" alt></p><p>每个<code>PT_LOAD</code> header用来记录一段 memory region，并描述了这段 memory region对应的文件offset、内核虚拟地址和长度。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol><li>使用<code>open</code>系统调用打开<code>/proc/kcore</code>文件</li><li>遍历ELF文件中的程序头表(Program Header Table)，找到包含目标内核虚拟地址的段(每个段描述了内核虚拟地址到文件offset的映射关系)</li><li>根据计算出的文件offset，使用<code>lseek</code>定位到目标位置，使用<code>read</code>读取目标地址的内容</li></ol><hr><p>参考资料:</p><ol><li><a href="https://schlafwandler.github.io/posts/dumping-/proc/kcore/" target="_blank" rel="noopener">Dumping /proc/kcore in 2019</a></li><li><a href="https://blog.csdn.net/weixin_45030965/article/details/124863905" target="_blank" rel="noopener">Linux /proc/kcore详解（一）</a></li><li><a href="https://blog.csdn.net/weixin_45030965/article/details/125164642" target="_blank" rel="noopener">Linux /proc/kcore详解（二）</a></li><li><a href="https://superuser.com/questions/669462/will-applications-running-in-root-mode-be-able-to-overwrite-oss-or-other-progra" target="_blank" rel="noopener">Will applications running in root mode be able to overwrite OS’s or other program’s memory section?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下linux中&lt;code&gt;/proc/kcore&lt;/code&gt;相关notes。
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>SystemTap如何改变内核函数返回值</title>
    <link href="http://liujunming.github.io/2025/02/09/SystemTap%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98%E5%86%85%E6%A0%B8%E5%87%BD%E6%95%B0%E8%BF%94%E5%9B%9E%E5%80%BC/"/>
    <id>http://liujunming.github.io/2025/02/09/SystemTap如何改变内核函数返回值/</id>
    <published>2025-02-09T11:41:30.000Z</published>
    <updated>2025-02-09T12:29:55.906Z</updated>
    
    <content type="html"><![CDATA[<p><strong>利用SystemTap，判断内核函数的参数后，才更改返回值?</strong> <a id="more"></a></p><p><img src="/images/2025/02/002.png" alt></p><p>接下来将展示下<a href="https://chatgpt.com/" target="_blank" rel="noopener">chatgpt</a>给出的答案(笔者也没有运行，不保证正确性)。</p><p><img src="/images/2025/02/001.png" alt></p><h2 id="SystemTap-概述"><a href="#SystemTap-概述" class="headerlink" title="SystemTap 概述"></a>SystemTap 概述</h2><p>SystemTap 是一个动态追踪工具，可以用来监控内核和用户空间程序的行为。你可以通过编写脚本来插桩内核函数，并分析函数参数、返回值等信息。</p><p>在这个教程中，我们将演示如何创建一个 SystemTap 脚本，挂钩内核函数、检查参数，并在满足特定条件时修改函数的返回值。</p><h2 id="创建SystemTap-脚本"><a href="#创建SystemTap-脚本" class="headerlink" title="创建SystemTap 脚本"></a>创建SystemTap 脚本</h2><p>我们以<code>sys_open</code>系统调用为例。<code>sys_open</code>用于打开文件，它接受两个主要参数：路径和打开标志。我们将判断路径是否是<code>/etc/passwd</code>，如果是，则修改返回值（模拟无法打开该文件）。</p><p>创建一个新的 SystemTap 脚本文件，比如<code>check_open.stp</code>，并编辑该文件，在文件中输入以下内容:</p><figure class="highlight stp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">probe syscall.open &#123;</span><br><span class="line">    <span class="comment">// 获取传入的路径参数</span></span><br><span class="line">    filename = user_string($filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印文件路径</span></span><br><span class="line">    printf(<span class="string">"Trying to open: %s\n"</span>, filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断路径是否为 /etc/passwd</span></span><br><span class="line">    if (filename == <span class="string">"/etc/passwd"</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果是 /etc/passwd，则返回 -EACCES，表示访问被拒绝</span></span><br><span class="line">        printf(<span class="string">"Blocking access to /etc/passwd\n"</span>)</span><br><span class="line">        return -EACCES</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>probe syscall.open</code>：我们使用 <code>syscall.open</code> 来挂钩 <code>sys_open</code> 系统调用。这将允许我们监视所有尝试打开文件的操作。</li><li><code>user_string($filename)</code>：该函数提取文件路径参数。我们使用 <code>user_string()</code> 来获取用户空间传入的字符串。</li><li><code>if (filename == &quot;/etc/passwd&quot;)</code>：这里检查传入的文件路径是否为 <code>/etc/passwd</code>。</li><li><code>return -EACCES</code>：如果路径是 <code>/etc/passwd</code>，我们通过 <code>return</code> 语句修改返回值，返回 <code>-EACCES</code>，表示访问被拒绝。</li></ul><h2 id="运行-SystemTap-脚本"><a href="#运行-SystemTap-脚本" class="headerlink" title="运行 SystemTap 脚本"></a>运行 SystemTap 脚本</h2><p>在创建了脚本后，需要使用 <code>stap</code> 命令来运行脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo stap check_open.stp</span><br></pre></td></tr></table></figure><p>这将会启动脚本并开始跟踪 <code>sys_open</code> 调用。如果你尝试打开 <code>/etc/passwd</code>，脚本会修改返回值，导致文件无法打开。</p><h2 id="验证脚本效果"><a href="#验证脚本效果" class="headerlink" title="验证脚本效果"></a>验证脚本效果</h2><p>你可以验证脚本的效果，看看它是否按预期工作：</p><p>尝试打开 <code>/etc/passwd</code> 文件，例如使用 <code>cat</code> 命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd</span><br></pre></td></tr></table></figure></p><p>由于脚本的作用，<code>cat</code> 命令应该失败，并返回错误信息，如 <code>Permission denied</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>创建脚本：使用<code>probe</code>挂钩内核函数，获取相关参数。</li><li>条件判断：通过<code>if</code>语句判断参数值，执行相应的操作。</li><li>修改返回值：使用<code>return</code>修改函数的返回值。</li><li>运行脚本：使用<code>sudo stap</code>命令来执行脚本并进行监控。</li></ol><hr><p>参考资料:</p><ol><li><a href="https://lrita.github.io/images/posts/systemtap/SystemTap-II.pdf" target="_blank" rel="noopener">SystemTap Tutorial Part-II</a></li><li><a href="https://www.opensourceforu.com/2010/10/systemtap-tutorial-part-2/" target="_blank" rel="noopener">SystemTap Tutorial, Part-2</a></li><li><a href="https://sourceware.org/systemtap/tutorial.pdf" target="_blank" rel="noopener">Systemtap tutorial</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;利用SystemTap，判断内核函数的参数后，才更改返回值?&lt;/strong&gt;
    
    </summary>
    
      <category term="debug" scheme="http://liujunming.github.io/categories/debug/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="工具" scheme="http://liujunming.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="debug" scheme="http://liujunming.github.io/tags/debug/"/>
    
  </entry>
  
  <entry>
    <title>Notes about eMCA</title>
    <link href="http://liujunming.github.io/2025/01/05/Notes-about-eMCA-Gen2/"/>
    <id>http://liujunming.github.io/2025/01/05/Notes-about-eMCA-Gen2/</id>
    <published>2025-01-05T07:35:47.000Z</published>
    <updated>2025-01-05T08:28:01.365Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下eMCA(Enhanced Machine Check Architecture)相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>简单来讲，eMCA可以将MCE和CMCI转换成SMI，让Firmware（BIOS）可以先行处理，然后再丢给OS。</p><p>下图以CMCI为例，MCE的流程也是类似:<br><img src="/images/2025/01/020.png" alt></p><h2 id="SDM的描述"><a href="#SDM的描述" class="headerlink" title="SDM的描述"></a>SDM的描述</h2><p><img src="/images/2025/01/019.png" alt></p><ul><li><strong>MCG_EMC_P (Enhanced Machine Check Capability) flag, bit 25</strong> — Indicates (when set) that the processor supports enhanced machine check capabilities for firmware first signaling.</li><li><strong>MCG_ELOG_P (extended error logging) flag, bit 26</strong> — Indicates (when set) that the processor allows platform firmware to be invoked when an error is detected so that it may provide additional platform specific information in an ACPI format “Generic Error Data Entry” that augments the data included in machine check bank registers.</li></ul><h2 id="FFM-Firmware-First-Mode"><a href="#FFM-Firmware-First-Mode" class="headerlink" title="FFM (Firmware First Mode)"></a>FFM (Firmware First Mode)</h2><p><img src="/images/2025/01/021.jpg" alt></p><p>FFM allows firmware to provide additional error information to os, synchronous with MCE or CMCI. </p><p>The hardware generates an SMI upon error. The SMI handler pre-processes the error and constructs Error Log in memory prior to continuing with the MCE or CMCI.<br><img src="/images/2025/01/022.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://peterhu.github.io/posts/2020/12/26/RAS%E7%AE%80%E4%BB%8B.html" target="_blank" rel="noopener">peterhu:RAS简介</a></li><li><a href="https://www.intel.com/content/www/us/en/quality/reliability-availability-serviceability-xeon-paper.html" target="_blank" rel="noopener">4th Gen Intel® Xeon® Scalable Processors: Reliability, Availability, and Serviceability (RAS) Technical Paper</a></li><li><a href="https://uefi.org/sites/default/files/resources/Spike%20Yuan-%20Server%20RAS%20and%20UEFI%20CPER_final.pdf" target="_blank" rel="noopener">Server RAS and UEFI CPER</a></li><li><a href="https://beyond-firmware.blogspot.com/2015/10/mca-machine-check-architecture.html" target="_blank" rel="noopener">Martin’s Coding Note:MCA</a></li><li><a href="https://www.intel.la/content/www/xl/es/content-details/671064/mca-enhancements-in-intel-xeon-processors.html" target="_blank" rel="noopener">MCA Enhancements in Intel® Xeon® Processors</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下eMCA(Enhanced Machine Check Architecture)相关notes。
    
    </summary>
    
      <category term="RAS" scheme="http://liujunming.github.io/categories/RAS/"/>
    
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about DRAM components</title>
    <link href="http://liujunming.github.io/2025/01/05/DRAM-components/"/>
    <id>http://liujunming.github.io/2025/01/05/DRAM-components/</id>
    <published>2025-01-05T00:52:40.000Z</published>
    <updated>2025-01-05T04:26:17.211Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下DRAM components相关notes。<br><img src="/images/2025/01/018.png" alt><br><a id="more"></a><br>In modern cloud servers, a CPU(socket) has several memory controllers. Each controller communicates with DIMMs through high-speed memory channels. Usually, a memory channel is shared by several DIMM slots. A DIMM has several ranks, and each is composed of sevaral DRAM chips. For typical DDR4 DIMMs, a rank is composed of 16 chips for data bits and 2 additional chips for ECC bits. <strong>A chip consists of multiple banks, which enables the access parallesim</strong>. A DRAM bank is structured as a two-dimensional cell array indexed by rows and columns. At the micro-level, a cell can store multiple bits of data, and the number of data bits stored in a cell is called the data width of a chip, which is usually denoted as x4, x8 or x16,etc.</p><ul><li>socket</li><li>memory controller</li><li>channel</li><li>DIMM(Dual In-Line Memory Module)</li><li>rank</li><li>chip</li><li>bank</li><li>cell (row, column)</li></ul><hr><p>参考资料:</p><ol><li>Predicting DRAM-Caused Node Unavailability in Hyper-Scale Clouds(DSN’22)</li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001137649751.html" target="_blank" rel="noopener">内存结构</a></li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001090907934.html" target="_blank" rel="noopener">内存基本概念</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下DRAM components相关notes。&lt;br&gt;&lt;img src=&quot;/images/2025/01/018.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Memory Scrubbing</title>
    <link href="http://liujunming.github.io/2025/01/04/Notes-about-Memory-Scrubbing/"/>
    <id>http://liujunming.github.io/2025/01/04/Notes-about-Memory-Scrubbing/</id>
    <published>2025-01-04T11:38:43.000Z</published>
    <updated>2025-01-04T13:26:49.114Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Memory Scrubbing相关notes。<a id="more"></a></p><h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><p>Memory scrubbing refers to the process of correcting or ‘scrubbing’ erroneously flipped bits in memory as a result of transient faults such as those caused by physical phenomena. Scrubbing is considered a RAS feature.</p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>Memory scrubbing将错误纠正后回写，可以防止内存条上的单bit错误逐渐累积形成多bit不可纠正错误。These actions provide software with early visibility for possible preventive measures such as page off-lining based on the rate of error corrections.</p><h2 id="When"><a href="#When" class="headerlink" title="When"></a>When</h2><ul><li>Patrol scrubbing proactively searches the system memory(由硬件而非软件来做), repairing correctable errors. It prevents accumulation of single-bit errors.</li><li>Demand scrubbing is the ability to write corrected data back to the memory once a correctable error is detected on a read transaction.</li></ul><h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p><strong>Patrol Scrubbing consists in reading memory, checking it against ECC for errors, and overwriting with the corrected memory words when an error is discovered</strong>. </p><p>Patrol scrubbing is done using a hardware engine, on either the platform or on the memory device, which generates requests to memory addresses on the memory device. The engine generates memory requests at a predefined frequency. Given enough time, it will eventually access every memory address. The frequency in which patrol scrub generates requests produces no noticeable impact on the memory device’s quality of service.</p><p>By generating read requests to memory addresses, the patrol scrubber allows the hardware an opportunity to run ECC on a memory address and correct any correctable errors before they can become uncorrectable errors. <u>Optionally, if an uncorrectable error is discovered, the patrol scrubber can trigger a hardware interrupt and notify the software layer of its memory address</u>.</p><hr><p>参考资料:</p><ol><li><a href="https://www.intel.com/content/www/us/en/quality/reliability-availability-serviceability-xeon-paper.html" target="_blank" rel="noopener">4th Gen Intel® Xeon® Scalable Processors: Reliability, Availability, and Serviceability (RAS) Technical Paper</a></li><li><a href="https://en.wikichip.org/wiki/memory_scrubbing" target="_blank" rel="noopener">https://en.wikichip.org/wiki/memory_scrubbing</a></li><li><a href="https://community.intel.com/t5/Server-Products/Uncorrectable-Memory-Error-amp-Patrol-Scrub/m-p/545123" target="_blank" rel="noopener">Uncorrectable Memory Error &amp; Patrol Scrub</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/pmem-RAS.html" target="_blank" rel="noopener">Reliability, Availability, and Serviceability (RAS)</a></li><li><a href="https://uefi.org/htmlspecs/ACPI_Spec_6_4_html/05_ACPI_Software_Programming_Model/ACPI_Software_Programming_Model.html" target="_blank" rel="noopener">ACPI Software Programming Model</a></li><li><a href="https://info.support.huawei.com/compute/docs/zh-cn/kunpeng-knowledge/typical-scenarios-1/zh-cn_topic_0000001108627660.html" target="_blank" rel="noopener">Demand Scrubbing/Patrol Scrubbing（内存巡检）</a></li><li><a href="https://www.cnblogs.com/xyjk1002-rejuvenation/p/16479297.html" target="_blank" rel="noopener">内存错误和服务器内存RAS功能-DELL篇-1</a></li><li><a href="https://superuser.com/questions/372422/can-linux-scrub-memory" target="_blank" rel="noopener">Can Linux scrub memory?</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Memory Scrubbing相关notes。
    
    </summary>
    
      <category term="RAS" scheme="http://liujunming.github.io/categories/RAS/"/>
    
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about SDM MCA</title>
    <link href="http://liujunming.github.io/2025/01/04/Notes-about-SDM-MCA/"/>
    <id>http://liujunming.github.io/2025/01/04/Notes-about-SDM-MCA/</id>
    <published>2025-01-04T03:35:29.000Z</published>
    <updated>2025-01-04T06:38:01.401Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下SDM中MCA相关notes。<a id="more"></a></p><h2 id="1-Architecture"><a href="#1-Architecture" class="headerlink" title="1. Architecture"></a>1. Architecture</h2><ul><li>processors implement a machine-check architecture that provides a mechanism for detecting and reporting hardware (machine) errors, such as: system bus errors, ECC errors, parity errors, cache errors, and TLB errors. It consists of a set of model-specific registers (MSRs) that are used to set up machine checking and additional banks of MSRs used for recording errors that are detected.</li><li>the processor can report information on corrected machine-check errors and deliver a programmable interrupt for software to respond to MC errors, referred to as corrected machine-check error interrupt (CMCI).</li><li>Intel 64 processors support for software recovery from certain uncorrected recoverable machine check errors.</li></ul><h2 id="2-MSRs"><a href="#2-MSRs" class="headerlink" title="2. MSRs"></a>2. MSRs</h2><p><img src="/images/2025/01/005.png" alt></p><h3 id="2-1-Machine-Check-Global-Control-MSRs"><a href="#2-1-Machine-Check-Global-Control-MSRs" class="headerlink" title="2.1 Machine-Check Global Control MSRs"></a>2.1 Machine-Check Global Control MSRs</h3><h4 id="2-1-1-IA32-MCG-CAP-MSR"><a href="#2-1-1-IA32-MCG-CAP-MSR" class="headerlink" title="2.1.1 IA32_MCG_CAP MSR"></a>2.1.1 IA32_MCG_CAP MSR</h4><p>The IA32_MCG_CAP MSR is a read-only register that provides information about the machine-check architecture of the processor.</p><p><img src="/images/2025/01/006.png" alt></p><p>深入了解各个field后，其实就可以对MCA整体架构有个全局的认识了。</p><h4 id="2-1-2-IA32-MCG-STATUS-MSR"><a href="#2-1-2-IA32-MCG-STATUS-MSR" class="headerlink" title="2.1.2 IA32_MCG_STATUS MSR"></a>2.1.2 IA32_MCG_STATUS MSR</h4><p>The IA32_MCG_STATUS MSR describes the current state of the processor after a machine-check exception has occurred.</p><p><img src="/images/2025/01/007.png" alt></p><h4 id="2-1-3-IA32-MCG-CTL-MSR"><a href="#2-1-3-IA32-MCG-CTL-MSR" class="headerlink" title="2.1.3 IA32_MCG_CTL MSR"></a>2.1.3 IA32_MCG_CTL MSR</h4><p>IA32_MCG_CTL controls the reporting of machine-check exceptions.</p><h4 id="2-1-4-IA32-MCG-EXT-CTL-MSR"><a href="#2-1-4-IA32-MCG-EXT-CTL-MSR" class="headerlink" title="2.1.4 IA32_MCG_EXT_CTL MSR"></a>2.1.4 IA32_MCG_EXT_CTL MSR</h4><p>IA32_MCG_EXT_CTL.LMCE_EN (bit 0) allows the processor to signal some MCEs to only a single logical processor in the system.</p><p><img src="/images/2025/01/008.png" alt></p><h4 id="2-1-5-Enabling-Local-Machine-Check"><a href="#2-1-5-Enabling-Local-Machine-Check" class="headerlink" title="2.1.5 Enabling Local Machine Check"></a>2.1.5 Enabling Local Machine Check</h4><p>When system software has enabled LMCE, then hardware will determine if a particular error can be delivered only to a single logical processor. Software should make no assumptions about the type of error that hardware can choose to deliver as LMCE.</p><h3 id="2-2-Error-Reporting-Register-Banks"><a href="#2-2-Error-Reporting-Register-Banks" class="headerlink" title="2.2 Error-Reporting Register Banks"></a>2.2 Error-Reporting Register Banks</h3><h4 id="2-2-1-IA32-MCi-CTL-MSRs"><a href="#2-2-1-IA32-MCi-CTL-MSRs" class="headerlink" title="2.2.1 IA32_MCi_CTL MSRs"></a>2.2.1 IA32_MCi_CTL MSRs</h4><p><img src="/images/2025/01/009.png" alt></p><h4 id="2-2-2-IA32-MCi-STATUS-MSRs"><a href="#2-2-2-IA32-MCi-STATUS-MSRs" class="headerlink" title="2.2.2 IA32_MCi_STATUS MSRs"></a>2.2.2 IA32_MCi_STATUS MSRs</h4><p><img src="/images/2025/01/010.png" alt></p><p><img src="/images/2025/01/011.png" alt></p><h4 id="2-2-3-IA32-MCi-ADDR-MSRs"><a href="#2-2-3-IA32-MCi-ADDR-MSRs" class="headerlink" title="2.2.3 IA32_MCi_ADDR MSRs"></a>2.2.3 IA32_MCi_ADDR MSRs</h4><p><img src="/images/2025/01/012.png" alt></p><h4 id="2-2-4-IA32-MCi-MISC-MSRs"><a href="#2-2-4-IA32-MCi-MISC-MSRs" class="headerlink" title="2.2.4 IA32_MCi_MISC MSRs"></a>2.2.4 IA32_MCi_MISC MSRs</h4><p><img src="/images/2025/01/013.png" alt></p><h4 id="2-2-5-IA32-MCi-CTL2-MSRs"><a href="#2-2-5-IA32-MCi-CTL2-MSRs" class="headerlink" title="2.2.5 IA32_MCi_CTL2 MSRs"></a>2.2.5 IA32_MCi_CTL2 MSRs</h4><p><img src="/images/2025/01/014.png" alt></p><h2 id="3-Enhanced-Cache-Error-reporting"><a href="#3-Enhanced-Cache-Error-reporting" class="headerlink" title="3. Enhanced Cache Error reporting"></a>3. Enhanced Cache Error reporting</h2><ul><li>In earlier Intel processors, cache status was based on the number of correction events that occurred in a cache.</li><li>In “threshold-based error status”, cache status is based on the number of lines (ECC blocks) in a cache that incur repeated corrections.</li><li>A processor that supports enhanced cache error reporting contains hardware that tracks the operating status of certain caches and provides an indicator of their “health”.<ul><li>The hardware reports a “green” status when the number of lines that incur repeated corrections is at or below a pre-defined threshold</li><li>a “yellow” status when the number of affected lines exceeds the threshold. Yellow status means that the cache reporting the event is operating correctly, but you should schedule the system for servicing within a few weeks.</li></ul></li></ul><h2 id="4-Corrected-Machine-Check-Error-Interrupt"><a href="#4-Corrected-Machine-Check-Error-Interrupt" class="headerlink" title="4. Corrected Machine Check Error Interrupt"></a>4. Corrected Machine Check Error Interrupt</h2><p>待另择篇幅整理</p><h2 id="5-Recovery-of-Uncorrected-Recoverable-UCR-Errors"><a href="#5-Recovery-of-Uncorrected-Recoverable-UCR-Errors" class="headerlink" title="5. Recovery of Uncorrected Recoverable(UCR) Errors"></a>5. Recovery of Uncorrected Recoverable(UCR) Errors</h2><p>Recovery of uncorrected recoverable machine check errors is an enhancement in machine-check architecture. <strong>This allow system software to perform recovery action on certain class of uncorrected errors and continue execution.</strong></p><h3 id="5-1-Detection-of-Software-Error-Recovery-Support"><a href="#5-1-Detection-of-Software-Error-Recovery-Support" class="headerlink" title="5.1 Detection of Software Error Recovery Support"></a>5.1 Detection of Software Error Recovery Support</h3><p>The new class of architectural MCA errors from which system software can attempt recovery is called <u><strong>Uncorrected Recoverable (UCR)</strong></u> Errors. <u>UCR errors are uncorrected errors that have been detected and signaled but have not corrupted the processor context</u>. For certain UCR errors, this means that once system software has performed a certain recovery action, it is possible to continue execution on this processor. UCR error reporting provides an error containment mechanism for data poisoning. The machine check handler will use the error log information from the error reporting registers to analyze and implement specific error recovery actions for UCR errors.</p><h3 id="5-2-UCR-Error-Reporting-and-Logging"><a href="#5-2-UCR-Error-Reporting-and-Logging" class="headerlink" title="5.2 UCR Error Reporting and Logging"></a>5.2 UCR Error Reporting and Logging</h3><p>IA32_MCi_STATUS MSR is used for reporting UCR errors and existing corrected or uncorrected errors.<br>When IA32_MCG_CAP[24] is set, a UCR error is indicated by the following bit settings in the IA32_MCi_STATUS register:</p><ul><li>Valid (bit 63) = 1</li><li>UC(bit61)=1</li><li>PCC(bit57)=0</li></ul><p>In addition, the IA32_MCi_STATUS register bit fields, bits 56:55, are defined (see Figure 16-6) to provide additional information to help system software to properly identify the necessary recovery action for the UCR error:</p><ul><li>S (Signaling) flag, bit 56</li><li>AR (Action Required) flag, bit 55 </li></ul><h3 id="5-3-UCR-Error-Classification"><a href="#5-3-UCR-Error-Classification" class="headerlink" title="5.3 UCR Error Classification"></a>5.3 UCR Error Classification</h3><ul><li><u>Uncorrected no action required (UCNA)</u> - is a UCR error that is not signaled via a machine check exception and, instead, is reported to system software as a corrected machine check error. </li><li><u>Software recoverable action optional (SRAO)</u> - a UCR error is signaled either via a machine check exception or CMCI. System software recovery action is optional and not required to continue execution from this machine check exception.</li><li><u>Software recoverable action required (SRAR)</u> - a UCR error that requires system software to take a recovery action on this processor before scheduling another stream of execution on this processor. </li></ul><p><img src="/images/2025/01/015.png" alt></p><h3 id="5-4-UCR-Error-Overwrite-Rules"><a href="#5-4-UCR-Error-Overwrite-Rules" class="headerlink" title="5.4 UCR Error Overwrite Rules"></a>5.4 UCR Error Overwrite Rules</h3><p>In general, the overwrite rules are as follows:</p><ul><li>UCR errors will overwrite corrected errors.</li><li>Uncorrected (PCC=1) errors overwrite UCR (PCC=0) errors.</li><li>UCR errors are not written over previous UCR errors.</li><li>Corrected errors do not write over previous UCR errors.</li></ul><h2 id="6-Interpreting-the-MCA-Error-Codes"><a href="#6-Interpreting-the-MCA-Error-Codes" class="headerlink" title="6. Interpreting the MCA Error Codes"></a>6. Interpreting the MCA Error Codes</h2><p>When the processor detects a machine-check error condition, it writes a 16-bit error code to the MCA error code field of one of the IA32_MCi_STATUS registers and sets the VAL (valid) flag in that register. The processor may also write a 16-bit model-specific error code in the IA32_MCi_STATUS register depending on the implementation of the machine-check architecture of the processor.</p><h3 id="6-1-Simple-Error-Codes"><a href="#6-1-Simple-Error-Codes" class="headerlink" title="6.1 Simple Error Codes"></a>6.1 Simple Error Codes</h3><p>Simple error codes indicate global error information.</p><p><img src="/images/2025/01/016.png" alt></p><h3 id="6-2-Compound-Error-Codes"><a href="#6-2-Compound-Error-Codes" class="headerlink" title="6.2 Compound Error Codes"></a>6.2 Compound Error Codes</h3><p>Compound error codes describe errors related to the TLBs, memory, caches, bus and interconnect logic, and internal timer. <strong>A set of sub-fields is common to all of compound errors. These sub-fields describe the type of access, level in the cache hierarchy, and type of request</strong>.</p><p><img src="/images/2025/01/017.png" alt></p><ul><li>Transaction Type (TT) Sub-Field</li><li>Level (LL) Sub-Field</li><li>Request (RRRR) Sub-Field</li><li>Bus and Interconnect Errors</li><li>Memory Controller and Extended Memory Errors</li></ul><h3 id="6-3-Architecturally-Defined-UCR-Errors"><a href="#6-3-Architecturally-Defined-UCR-Errors" class="headerlink" title="6.3 Architecturally Defined UCR Errors"></a>6.3 Architecturally Defined UCR Errors</h3><ul><li>Architecturally Defined SRAO Errors</li><li>Architecturally Defined SRAR Errors</li></ul><h3 id="6-4-Multiple-MCA-Errors"><a href="#6-4-Multiple-MCA-Errors" class="headerlink" title="6.4 Multiple MCA Errors"></a>6.4 Multiple MCA Errors</h3><p>When multiple MCA errors are detected within a certain detection window, the processor may aggregate the reporting of these errors together as a single event, i.e., a single machine exception condition. If this occurs, system software <u>may find multiple MCA errors logged in different MC banks on one logical processor</u> or <u>find multiple MCA errors logged across different processors for a single machine check broadcast event</u>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下SDM中MCA相关notes。
    
    </summary>
    
      <category term="RAS" scheme="http://liujunming.github.io/categories/RAS/"/>
    
    
      <category term="SDM" scheme="http://liujunming.github.io/tags/SDM/"/>
    
      <category term="RAS" scheme="http://liujunming.github.io/tags/RAS/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Device Memory TCP</title>
    <link href="http://liujunming.github.io/2025/01/01/Notes-about-Device-Memory-TCP/"/>
    <id>http://liujunming.github.io/2025/01/01/Notes-about-Device-Memory-TCP/</id>
    <published>2025-01-01T11:10:46.000Z</published>
    <updated>2025-01-01T12:04:42.997Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Device Memory TCP相关notes。<a id="more"></a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Device memory TCP is a proposal for transferring data to and/or from device memory efficiently, without bouncing the data to a host memory buffer.</p><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p><img src="/images/2025/01/001.png" alt></p><p><img src="/images/2025/01/002.png" alt></p><p><img src="/images/2025/01/003.png" alt></p><p>A large amount of data transfers have device memory as the source and/or destination. Accelerators drastically increased the volume of such transfers. Some examples include:</p><ul><li>ML accelerators transferring large amounts of training data from storage into GPU/TPU memory. In some cases ML training setup time can be as long as 50% of TPU compute time, improving data transfer throughput &amp; efficiency can help improving GPU/TPU utilization.</li><li>Distributed training, where ML accelerators, such as GPUs on different hosts, exchange data among them.</li><li>Distributed raw block storage applications transfer large amounts of data with remote SSDs, much of this data does not require host processing.</li></ul><p>Today, the majority of the Device-to-Device data transfers the network are implemented as the following low level operations: Device-to-Host copy, Host-to-Host network transfer, and Host-to-Device copy.</p><p>The implementation is suboptimal, especially for bulk data transfers, and can put significant strains on system resources, such as host memory bandwidth, PCIe bandwidth, etc.</p><h2 id="Proposal"><a href="#Proposal" class="headerlink" title="Proposal"></a>Proposal</h2><p><img src="/images/2025/01/004.png" alt></p><p>We attempt to optimize this use case by implementing socket APIs that enable the user to:</p><ol><li>send device memory across the network directly, and</li><li>receive incoming network packets directly into device memory.</li></ol><p>Packet payloads go directly from the NIC to device memory for receive and from device memory to NIC for transmit. Packet headers go to/from host memory and are processed by the TCP/IP stack normally.</p><p>The NIC must support header split to achieve this. i.e. the capability to split incoming packets into a header + payload and to put each into a separate buffer. Device Memory works by using device memory for the packet payload, and host memory for the packet headers.</p><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ul><li>Alleviate host memory bandwidth pressure, compared to existing network-transfer + device-copy semantics.</li><li>Alleviate PCIe BW pressure, by limiting data transfer to the lowest level of the PCIe tree, compared to traditional path which sends data through the root complex.</li></ul><hr><p>参考资料:</p><ol><li><a href="https://docs.kernel.org/networking/devmem.html" target="_blank" rel="noopener">docs.kernel:Device Memory TCP</a></li><li><a href="https://netdevconf.org/0x17/sessions/talk/device-memory-tcp.html" target="_blank" rel="noopener">netdevconf:Device Memory TCP</a></li><li><a href="https://lore.kernel.org/netdev/20240831004313.3713467-1-almasrymina@google.com/" target="_blank" rel="noopener">[PATCH net-next v24 00/13] Device Memory TCP</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Device Memory TCP相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about AMD Processor Hierarchy</title>
    <link href="http://liujunming.github.io/2024/12/22/Notes-about-AMD-Processor-Hierarchy/"/>
    <id>http://liujunming.github.io/2024/12/22/Notes-about-AMD-Processor-Hierarchy/</id>
    <published>2024-12-22T10:14:50.000Z</published>
    <updated>2024-12-22T11:05:21.491Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2024/12/008.jpg" alt><a id="more"></a></p><p>This is a simplified configuration of an EPYC Rome node with two sockets. Each socket contains eight <strong>Core Complex Dies</strong> (CCDs, each enclosed in a green box) and one I/O die (IOD, enclosed in a yellow box). The infinity sign (♾) represents the Infinity Fabric. Each CCD contains two <strong>Core Complexes</strong> (CCXs). Each CCX has 4 cores and 16 MB of L3 cache. Thus, there are 64 cores per socket and 128 cores per node.</p><p>The Rome processor hierarchy is as follows:</p><ul><li>Core: A CPU core has private L1I, L1D, and L2 caches, which are shared by two hyperthreads on the core.</li><li>CCX: A core complex includes four cores and a common L3 cache of 16 MB. Different CCXs do not share L3.</li><li>CCD: A core complex die includes two CCXs and an Infinity Link to the I/O die (IOD). The CCDs connect to memory, I/O, and each other through the IOD.</li><li>Socket: A socket includes eight CCDs (total of 64 cores), a common centralized I/O die (includes eight unified memory controllers and eight IO x16 PCIe 4.0 lanes—total of 128 lanes), and a link to the network interface controller (NIC).</li><li>Node: A node includes two sockets and a network interface controller (NIC).</li></ul><hr><p>参考资料:</p><ol><li><a href="https://www.nas.nasa.gov/hecc/support/kb/amd-rome-processors_658.html" target="_blank" rel="noopener">AMD Rome Processors</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2024/12/008.jpg&quot; alt&gt;
    
    </summary>
    
      <category term="AMD" scheme="http://liujunming.github.io/categories/AMD/"/>
    
    
      <category term="AMD" scheme="http://liujunming.github.io/tags/AMD/"/>
    
  </entry>
  
  <entry>
    <title>Notes about FUSE filesystem</title>
    <link href="http://liujunming.github.io/2024/12/21/Notes-about-FUSE-filesystem/"/>
    <id>http://liujunming.github.io/2024/12/21/Notes-about-FUSE-filesystem/</id>
    <published>2024-12-21T11:07:07.000Z</published>
    <updated>2025-01-12T12:30:54.361Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/2024/12/005.png" alt><a id="more"></a></p><h2 id="tutorial"><a href="#tutorial" class="headerlink" title="tutorial"></a>tutorial</h2><p><img src="/images/2024/12/007.png" alt></p><p>very nice tutorial: <a href="https://maastaar.net/fuse/linux/filesystem/c/2016/05/21/writing-a-simple-filesystem-using-fuse/" target="_blank" rel="noopener">Writing a Simple Filesystem Using FUSE in C</a></p><h3 id="Making-a-call-into-a-FUSE-file-system"><a href="#Making-a-call-into-a-FUSE-file-system" class="headerlink" title="Making a call into a FUSE file system"></a>Making a call into a FUSE file system</h3><p><img src="/images/2024/12/006.gif" alt></p><ol><li>A program, such as ls, mkdir makes a call to a file system routine. For example, open(“/test/fuse/file1.txt”). This call gets sent to the kernel.</li><li>If this file is in a FUSE volume, the kernel passes it on to the FUSE kernel module, which then passes it on to the implementation of that file system.</li><li>The implementation of open then refers to the actual data structures that represent the file system and returns a file handle. It is open’s job to take a concrete view of data (bits stored on a hard drive) and present an abstract view (a hierarchically organized file system).</li><li>The kernel returns the result of the open function to the program that originally made the call.</li></ol><p>Cited From <a href="https://www.cs.cmu.edu/~fp/courses/15213-s07/lectures/15-filesys/index.html" target="_blank" rel="noopener">File Systems and FUSE</a>.</p><h3 id="simple-fuse-example"><a href="#simple-fuse-example" class="headerlink" title="simple fuse example"></a>simple fuse example</h3><ul><li><a href="https://github.com/JulesWang/helloworld-fuse/tree/master" target="_blank" rel="noopener">helloworld-fuse</a></li><li><a href="https://github.com/libfuse/libfuse/blob/master/example/hello.c" target="_blank" rel="noopener">libfuse/example/hello.c</a></li></ul><h3 id="辅助资料"><a href="#辅助资料" class="headerlink" title="辅助资料"></a>辅助资料</h3><ul><li><a href="https://www.cs.hmc.edu/~geoff/classes/hmc.cs135.201109/homework/fuse/fuse_doc.html" target="_blank" rel="noopener">CS135 FUSE Documentation</a></li><li><a href="https://www.cs.nmsu.edu/~pfeiffer/fuse-tutorial/" target="_blank" rel="noopener">Writing a FUSE Filesystem: a Tutorial</a></li><li><a href="https://github.com/osxfuse/fuse/blob/master/doc/how-fuse-works" target="_blank" rel="noopener">How Fuse-1.3 Works</a></li><li><a href="https://www.bilibili.com/video/BV1NS4y1L7Me/" target="_blank" rel="noopener">linux内核开发第38讲：linux基于fuse实现自定义文件系统整体架构</a></li></ul><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>FAST’17 paper: <a href="https://www.usenix.org/system/files/conference/fast17/fast17-vangoor.pdf" target="_blank" rel="noopener">To FUSE or Not to FUSE: Performance of User-Space File Systems</a>，<a href="https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_vangoor.pdf" target="_blank" rel="noopener">slides</a>也非常硬核!</p><h3 id="辅助资料-1"><a href="#辅助资料-1" class="headerlink" title="辅助资料"></a>辅助资料</h3><ul><li><a href="https://georgesims21.github.io/fuse/" target="_blank" rel="noopener">George’s Blog FUSE</a></li><li><a href="https://www.bilibili.com/video/BV1r24y157gm/" target="_blank" rel="noopener">FUSE 文件系统浅析 - 张老师</a></li><li><a href="https://zhuanlan.zhihu.com/p/143256077" target="_blank" rel="noopener">用户态文件系统 - FUSE</a></li><li><a href="https://www.kernel.org/doc/html/next/filesystems/fuse.html" target="_blank" rel="noopener">kernel doc fuse</a></li><li><a href="https://zhuanlan.zhihu.com/p/17059519212" target="_blank" rel="noopener">FUSE读写流程梳理</a></li></ul><h2 id="Manual"><a href="#Manual" class="headerlink" title="Manual"></a>Manual</h2><ul><li><a href="https://man7.org/linux/man-pages/man4/fuse.4.html" target="_blank" rel="noopener">fuse(4)</a></li><li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/uapi/linux/fuse.h" target="_blank" rel="noopener">include/uapi/linux/fuse.h</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/2024/12/005.png&quot; alt&gt;
    
    </summary>
    
      <category term="文件系统" scheme="http://liujunming.github.io/categories/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="文件系统" scheme="http://liujunming.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Linux Transmit Packet Steering(XPS) technology</title>
    <link href="http://liujunming.github.io/2024/12/21/Notes-about-Linux-Transmit-Packet-Steering-XPS-technology/"/>
    <id>http://liujunming.github.io/2024/12/21/Notes-about-Linux-Transmit-Packet-Steering-XPS-technology/</id>
    <published>2024-12-21T00:07:27.000Z</published>
    <updated>2024-12-21T05:01:50.936Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux XPS(Transmit Packet Steering)相关notes。<a id="more"></a></p><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><blockquote><p>The Linux network stack maps each core C to a different Tx queue Q, such that Q’s memory is allocated from C’s node. Additionally, memory allocations of packets transmitted via Q are likewise fulfilled using the same node. Cores can then transmit simultaneously through their individual queues in an uncoordinated, NU(D)MA-friendly manner while avoiding synchronization overheads. When a thread T that executes on C issues a system call to open a socket file descriptor S, the network stack associates Q with S, saving Q’s identifier in the socket data structure. After that, whenever T transmits through S, the network stack checks that T still runs on C. If it does not, the network stack updates S to point to the queue of T ’s new core. (The actual modification happens after Q is drained from any outstanding packets that originated from S, to avoid out-of-order transmissions.)</p></blockquote><h2 id="2-Optimization"><a href="#2-Optimization" class="headerlink" title="2. Optimization"></a>2. Optimization</h2><h3 id="2-1-reduce-contention"><a href="#2-1-reduce-contention" class="headerlink" title="2.1 reduce contention"></a>2.1 reduce contention</h3><p>contention on the device queue lock is significantly reduced since fewer CPUs contend for the same queue(contention can be eliminated completely if each CPU has its own transmit queue).</p><h3 id="2-2-reduce-cache-miss-rate-on-transmit-completion"><a href="#2-2-reduce-cache-miss-rate-on-transmit-completion" class="headerlink" title="2.2 reduce cache miss rate on transmit completion"></a>2.2 reduce cache miss rate on transmit completion</h3><p>cache miss rate on transmit completion is reduced, in particular for data cache lines that hold the <code>sk_buff</code> structures.</p><p>网卡发完包后，会给CPU发送中断；接着linux内核协议栈就会调用<code>kfree_skb</code>，此时就会访问到<code>sk_buff</code> structures。如果发送数据包的core与调用<code>kfree_skb</code>的core一样，那么<code>sk_buff</code> structures的cache miss rate就会降低。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kfree_skb</span><br><span class="line">└── kfree_skb_reason</span><br><span class="line">    └── skb_unref</span><br><span class="line">        └── skb-&gt;users</span><br></pre></td></tr></table></figure><h3 id="2-3-DMA-Buffer-NUMA-Affinity"><a href="#2-3-DMA-Buffer-NUMA-Affinity" class="headerlink" title="2.3 DMA Buffer NUMA Affinity"></a>2.3 DMA Buffer NUMA Affinity</h3><p>网卡发包时，DMA本node的内存即可，无需跨numa node，可以提升DMA的性能。详情可以参考<a href="/2024/11/17/%E8%BD%AC%E8%BD%BD-Linux-NUMA-Optimization-1/#3-2-1-DMA-Buffer-NUMA-Affinity">DMA Buffer NUMA Affinity</a>。</p><hr><p>参考资料:</p><ol><li>IOctopus: Outsmarting Nonuniform DMA(ASPLOS’20)</li><li><a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt" target="_blank" rel="noopener">Scaling in the Linux Networking Stack</a></li><li><a href="https://zhuanlan.zhihu.com/p/148756667" target="_blank" rel="noopener">Linux网络栈的性能缩放</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux XPS(Transmit Packet Steering)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about network RFS and aRFS</title>
    <link href="http://liujunming.github.io/2024/12/08/Notes-about-network-RFS-and-aRFS/"/>
    <id>http://liujunming.github.io/2024/12/08/Notes-about-network-RFS-and-aRFS/</id>
    <published>2024-12-08T04:49:12.000Z</published>
    <updated>2024-12-08T13:24:24.082Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下network的RFS(Receive Flow Steering)和aRFS(Accelerated Receive Flow Steering)相关notes。<a id="more"></a></p><h3 id="RFS"><a href="#RFS" class="headerlink" title="RFS"></a>RFS</h3><p><a href="/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/">RSS</a>/<a href="/2024/12/01/Notes-about-Linux-Receive-Packet-Steering-RPS-technology/">RPS</a>很好地保证了数据包处理的负载均衡，可以将处理任务合理的分配到所有的 CPU 上。</p><p>但是，有时候我们还需要考虑其他的因素。比如，网卡收到了属于一个运行在 CPU0 上的进程的数据包，那么这些数据包被 CPU0 处理会比其他 CPU 处理更高效。</p><p>原因很简单直观，数据在 CPU 内传递比跨 CPU 传递要更节省时间。因此，我们<strong>希望数据包尽量能够被其所属的进程所在的CPU处理，至少能够被同属一个 NUMA域的CPU处理</strong>。</p><p>RFS 机制就是为了实现这一点。使用 hash 函数根据包头信息计算得到一个 hash 值，然后作为索引查表。RFS 所查找的匹配表（<code>rps_sock_flow_table</code>）中存储的是数据包所属进程所在的 CPU。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hash value] : [CPU id]</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>如果能查找到有效的CPU，就将数据包入队到CPU对应的backlog队列中；如果查找不到，那么就直接按照 RPS 机制转发。</p><p>与 RPS 预先配置好的 CPU 列表不同，<code>rps_sock_flow</code>表是动态更新的。如果有数据包的收发操作，如 <code>inet_recvmsg()</code>, <code>inet_sendmsg()</code>, <code>inet_sendpage()</code>, <code>tcp_splice_read()</code> 等操作，则会插入新的值。类似的情况还发生在进程被调度到新的 CPU 的时候。这时候就需要更新匹配表中的值。如果原来的 CPU 队列上还有未处理完的数据包，那么就会发生乱序。</p><p>为了避免乱序，RFS 使用了另一个表 —— <code>rps_dev_flow</code> 表，每个网卡队列对应一个该表。该表的索引依旧是包头的 hash 值，每个表项对应两个字段：1) 现在的 CPU（也就是该数据包所属流已经把数据包放在其队列上等待其内核处理的 CPU）号。2) 当该流最后一个数据包到达后，该 CPU 的 backlog 队列的尾计数器值(用来判断原CPU 队列上有没有未处理完的包)。</p><p>当进程切换 CPU 时，先判断原 CPU 队列上有没有未处理完的包，如果有就不切换；如果没有，就切换。</p><p><img src="/images/2024/12/003.png" alt></p><h3 id="aRFS"><a href="#aRFS" class="headerlink" title="aRFS"></a>aRFS</h3><p>Accelerated RFS 之于 RFS 相当于 RSS 之于 RPS。Accelerated RFS 在硬件上就可以选择正确的队列，随后触发该数据包所属流所在的 CPU 的中断。由此可见，如果想要在硬件上实现队列选择，我们需要一个从流到硬件队列的对应关系。</p><p><strong>aRFS允许网卡在选择队列时，直接将数据包放入应用程序所在CPU对应的队列</strong>。</p><ol><li>维护规则: 内核自动维护socket五元组、socket应用程序所在的CPU和CPU对应的接收队列的映射关系<ul><li>socket五元组 -&gt; CPU(从流到 CPU 的映射关系，记录在 <code>rps_dev_flow</code> 表中)</li><li>CPU -&gt; RX queue(CPU 和硬件队列的关系，通过 <code>/proc/irq/&lt;irq_num&gt;/smp_affinity</code> 进行配置)</li></ul></li><li>下发规则: 内核根据上述映射关系，在用户态接收数据时，将五元组与CPU对应的规则下发到网卡 </li><li>匹配规则: 网卡接收到的所有报文会根据aRFS规则进行匹配并转发到相应的接收队列</li></ol><p><img src="/images/2024/12/004.png" alt></p><p>每当<code>rps_dev_flow</code>表中的条目被更新，网络协议栈就会调用驱动中的<code>ndo_rx_flow_steer</code>函数来更新流到硬件队列的对应关系。</p><p>Modern NICs support aRFS by (1) providing the OS with an API that allows it to associate networking flows with Rx queues, and by (2) steering incoming packets accordingly. When the OS migrates thread T away from  core C, the OS updates the NIC regarding thread T ’s new queue using the aRFS API. The actual update is delayed until the original queue is drained from packets of socket file descriptor S(flow在old core上的排空), to avoid out-of-order receives.</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>理解aRFS或许更为关键</li><li>aRFS允许网卡在选择队列时，直接将数据包放入应用程序所在CPU对应的队列</li></ul><hr><p>参考资料:</p><ol><li><a href="https://borispis.github.io/files/2020-ioctopus.pdf" target="_blank" rel="noopener">IOctopus: Outsmarting Nonuniform DMA</a></li><li><a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt" target="_blank" rel="noopener">Scaling in the Linux Networking Stack</a></li><li><a href="https://blog.csdn.net/dog250/article/details/80025959" target="_blank" rel="noopener">Linux RPS/RFS 实现原理浅析</a></li><li><a href="https://garycplin.blogspot.com/2017/06/linux-network-scaling-receives-packets.html" target="_blank" rel="noopener">Linux Network Scaling: Receiving Packets</a></li><li><a href="https://blog.csdn.net/weixin_45485072/article/details/133248630" target="_blank" rel="noopener">Understanding Host Network Stack Overheads论文阅读笔记</a></li><li><a href="https://blog.luckyoung.org/2023/23-02-13_network-parameters/" target="_blank" rel="noopener">网络参数 RSS、RPS、RFS、aRFS 学习总结</a></li><li><a href="https://arthurchiao.art/blog/linux-net-stack-implementation-rx-zh/#684-arfs-hardware-accelerated-rfs" target="_blank" rel="noopener">Linux 网络栈接收数据（RX）：原理及内核实现（2022）</a></li><li><a href="https://zhuanlan.zhihu.com/p/148756667" target="_blank" rel="noopener">Linux网络栈的性能缩放</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下network的RFS(Receive Flow Steering)和aRFS(Accelerated Receive Flow Steering)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Notes about Linux Receive Packet Steering(RPS) technology</title>
    <link href="http://liujunming.github.io/2024/12/01/Notes-about-Linux-Receive-Packet-Steering-RPS-technology/"/>
    <id>http://liujunming.github.io/2024/12/01/Notes-about-Linux-Receive-Packet-Steering-RPS-technology/</id>
    <published>2024-11-30T16:56:52.000Z</published>
    <updated>2024-12-01T08:59:55.237Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下Linux RPS(Receive Packet Steering)相关notes。<a id="more"></a></p><h3 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h3><p><a href="/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/">RSS(Receive Side Scaling)</a></p><h3 id="What"><a href="#What" class="headerlink" title="What"></a>What</h3><p>Receive Packet Steering (RPS) is logically a software implementation of RSS.</p><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><p>RPS has some advantages over RSS: </p><ol><li>it can be used with any NIC</li><li>software filters can easily be added to hash over new protocols</li><li>it does not increase hardware device interrupt rate (although it does IPIs)</li></ol><p>在没有RSS功能的网卡中，RPS还是有价值的。</p><h3 id="How"><a href="#How" class="headerlink" title="How"></a>How</h3><p><img src="/images/2024/12/001.png" alt></p><p>RPS其实就是一个<strong>软件对CPU负载重分发</strong>的机制。其使能的作用点在CPU开始处理软中断，即下面的地方：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netif_rx_internal</span><br><span class="line">netif_receive_skb_internal</span><br></pre></td></tr></table></figure></p><p><img src="/images/2024/12/002.png" alt></p><p>Whereas RSS selects the queue and hence CPU that will run the hardware interrupt handler, RPS selects the CPU to perform protocol processing above the interrupt handler. This is accomplished by placing the packet on the desired CPU backlog queue and waking up the CPU for processing.(RSS选择队列，从而选择了运行硬件中断处理程序的CPU；而RPS在中断处理程序之上选择CPU执行协议处理。这是通过将数据包放置在所需的CPU积压队列中并唤醒CPU进行处理来实现的。)</p><hr><p>参考资料:</p><ol><li><a href="https://blog.csdn.net/dog250/article/details/80025959" target="_blank" rel="noopener">Linux RPS/RFS 实现原理浅析</a></li><li><a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt" target="_blank" rel="noopener">Scaling in the Linux Networking Stack</a></li><li><a href="https://zhuanlan.zhihu.com/p/148756667" target="_blank" rel="noopener">Linux网络栈的性能缩放</a></li><li><a href="https://garycplin.blogspot.com/2017/06/linux-network-scaling-receives-packets.html" target="_blank" rel="noopener">Linux Network Scaling: Receiving Packets</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下Linux RPS(Receive Packet Steering)相关notes。
    
    </summary>
    
      <category term="计算机网络" scheme="http://liujunming.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>virtio-net RSS Inner Header Hash</title>
    <link href="http://liujunming.github.io/2024/11/24/virtio-net-Inner-Header-Hash/"/>
    <id>http://liujunming.github.io/2024/11/24/virtio-net-Inner-Header-Hash/</id>
    <published>2024-11-24T02:03:40.000Z</published>
    <updated>2024-11-24T06:16:48.781Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio-net RSS Inner Header Hash的相关notes。<a id="more"></a></p><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p><a href="/2024/11/23/Notes-about-virtio-net-RSS-featute/">virtio-net RSS feature</a></p><p>需要理解网络<a href="https://www.wikiwand.com/en/articles/Tunneling_protocol" target="_blank" rel="noopener">Tunnel协议</a>，清楚Outer Header与Inner Header的概念。<br><img src="/images/2024/11/021.png" alt></p><h2 id="distribute-different-flows"><a href="#distribute-different-flows" class="headerlink" title="distribute different flows"></a>distribute different flows</h2><p>传统隧道协议由于在外头部中缺少足够的熵，导致收包时报文汇聚到单个队列上，无法发挥多队列收包的优势。</p><p>For legacy systems, they may lack entropy fields which modern protocols have in the outer header, resulting in multiple flows with the same outer header but different inner headers being directed to the same receive queue. This results in poor receive performance.</p><p>To address this limitation, inner header hash can be used to enable the device to advertise the capability to calculate the hash for the inner packet, regaining better receive performance.</p><blockquote><p>Legacy tunneling protocols, lacking the outer header entropy, can use RSS with the inner header hash to distribute flows with identical outer but different inner headers across various queues, improving performance.</p></blockquote><h2 id="identify-same-flow"><a href="#identify-same-flow" class="headerlink" title="identify same flow"></a>identify same flow</h2><blockquote><p>Identify an inner flow distributed across multiple outer tunnels.</p></blockquote><p>现代隧道协议在某些场景需要通过将同一条流接收在同一个队列上以获得性能收益，而外头部不容易做到。</p><p>Currently, a received encapsulated packet has an outer and an inner header, but the virtio device is unable to calculate the hash for the inner header. The same flow can traverse through different tunnels, resulting in the encapsulated packets being spread across multiple receive queues (refer to the figure below). However, in certain scenarios, we may need to direct these encapsulated packets of the same flow to a single receive queue. This facilitates the processing of the flow by the same CPU to improve performance (warm caches, less locking, etc.).</p><pre><code>client1                    client2   |        +-------+         |   +-------&gt;|tunnels|&lt;--------+            +-------+               |  |               v  v       +-----------------+       | monitoring host |       +-----------------+</code></pre><p>To achieve this, the device can calculate a symmetric hash based on the inner headers of the same flow.</p><h3 id="symmetric-hash"><a href="#symmetric-hash" class="headerlink" title="symmetric hash"></a>symmetric hash</h3><blockquote><p>symmetric hash确保同一个五元组(无论方向)哈希到同一个桶中，即当源IP和目标IP、源端口和目标端口互换时，哈希值仍然能保持一致。</p></blockquote><p><img src="/images/2024/11/022.png" alt></p><p><img src="/images/2024/11/023.png" alt></p><p><a href="https://lore.kernel.org/virtio-dev/e573702a-9a2e-d210-f13a-f0b241442991@linux.alibaba.com/" target="_blank" rel="noopener">https://lore.kernel.org/virtio-dev/e573702a-9a2e-d210-f13a-f0b241442991@linux.alibaba.com/</a></p><h2 id="Spec描述"><a href="#Spec描述" class="headerlink" title="Spec描述"></a>Spec描述</h2><p>VIRTIO_NET_F_HASH_TUNNEL</p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2620004" target="_blank" rel="noopener">5.1.6.4.4 Inner Header Hash</a></p><blockquote><p>5.1.6.4.4.1 Encapsulated packet<br>Multiple tunneling protocols allow encapsulating an inner, payload packet in an outer, encapsulated packet. The encapsulated packet thus contains an outer header and an inner header, and the device calculates the hash over either the inner header or the outer header.<br>If VIRTIO_NET_F_HASH_TUNNEL is negotiated and a received encapsulated packet’s outer header matches one of the encapsulation types enabled in enabled_tunnel_types, then the device uses the inner header for hash calculations (only a single level of encapsulation is currently supported).<br>If VIRTIO_NET_F_HASH_TUNNEL is negotiated and a received packet’s (outer) header does not match any encapsulation types enabled in enabled_tunnel_types, then the device uses the outer header for hash calculations.</p></blockquote><hr><p>参考资料:</p><ol><li><a href="https://networkdirection.net/articles/routingandswitching/gretunnels/" target="_blank" rel="noopener">GRE Tunnels</a></li><li><a href="https://lore.kernel.org/virtio-dev/20230703152711.106008-1-hengqi@linux.alibaba.com/" target="_blank" rel="noopener">[PATCH v21] virtio-net: support inner header hash</a></li><li><a href="https://developer.aliyun.com/article/1257786" target="_blank" rel="noopener">高性能网络 SIG 月度动态：联合 IBM 就 SMC v2.1 协议升级达成一致，ANCK 率先完成支持</a></li><li><a href="https://developer.aliyun.com/article/1305988" target="_blank" rel="noopener">高性能网络 SIG 月度动态：ANCK 首次支持 SMCv2.1，virtio 规范支持隧道报文内头部哈希</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio-net RSS Inner Header Hash的相关notes。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about virtio-net RSS feature</title>
    <link href="http://liujunming.github.io/2024/11/23/Notes-about-virtio-net-RSS-featute/"/>
    <id>http://liujunming.github.io/2024/11/23/Notes-about-virtio-net-RSS-featute/</id>
    <published>2024-11-23T10:33:03.000Z</published>
    <updated>2024-11-24T00:25:05.878Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下virtio-net中<a href="/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/">RSS(Receive Side Scaling)</a>的具体实现。<a id="more"></a></p><h2 id="Spec描述"><a href="#Spec描述" class="headerlink" title="Spec描述"></a>Spec描述</h2><p>VIRTIO_NET_F_RSS</p><p><img src="/images/2024/11/020.png" alt></p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2570003" target="_blank" rel="noopener">5.1.6.4.3 Hash calculation for incoming packets</a></p><p><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2580001" target="_blank" rel="noopener">5.1.6.4.3.1 Supported/enabled hash types</a></p><p>RSS需要通过ctrl q去下发配置参数，所以VIRTIO_NET_F_RSS Requires VIRTIO_NET_F_CTRL_VQ，需要ctrl q。<br><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html#x1-2890007" target="_blank" rel="noopener">5.1.6.5.7 Receive-side scaling (RSS)</a></p><h2 id="struct-virtio-net-rss-config"><a href="#struct-virtio-net-rss-config" class="headerlink" title="struct virtio_net_rss_config"></a>struct virtio_net_rss_config</h2><p>研究明白<code>struct virtio_net_rss_config</code>即可理解virtio-net RSS的实现细节。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> &#123;</span> </span><br><span class="line">   le16 vq_index_1_16: <span class="number">15</span>; <span class="comment">/* Bits 1 to 16 of the virtqueue index */</span> </span><br><span class="line">   le16 reserved: <span class="number">1</span>; <span class="comment">/* Set to zero */</span> </span><br><span class="line">&#125;; </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">virtio_net_rss_config</span> &#123;</span> </span><br><span class="line">    le32 hash_types; </span><br><span class="line">    le16 indirection_table_mask; </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> <span class="title">unclassified_queue</span>;</span> </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> <span class="title">indirection_table</span>[<span class="title">indirection_table_length</span>];</span> </span><br><span class="line">    le16 max_tx_vq; </span><br><span class="line">    u8 hash_key_length; </span><br><span class="line">    u8 hash_key_data[hash_key_length]; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="hash-types"><a href="#hash-types" class="headerlink" title="hash_types"></a>hash_types</h3><p>通过hash_types，比如可以选择(Source IP address,Destination IP address,Source TCP port,Destination TCP port)这个四元组作为RSS Input Fields，也可以选择(Source IP address,Destination IP address)这个二元组作为RSS Input Fields。<br><img src="/images/2024/11/016.png" alt></p><h3 id="indirection-table-mask"><a href="#indirection-table-mask" class="headerlink" title="indirection_table_mask"></a>indirection_table_mask</h3><p><img src="/images/2024/11/017.png" alt><br>indirection_table_mask就是上图中的LSB。例如indirection_table_mask为0x111，那么RSS Redirection Table的size就是8。</p><h3 id="hash-key-length和hash-key-data"><a href="#hash-key-length和hash-key-data" class="headerlink" title="hash_key_length和hash_key_data"></a>hash_key_length和hash_key_data</h3><p><img src="/images/2024/11/018.png" alt></p><p>hash_key_length和hash_key_data就代表上图中的Hash Key。</p><h3 id="struct-rss-rq-id"><a href="#struct-rss-rq-id" class="headerlink" title="struct rss_rq_id"></a>struct rss_rq_id</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">rss_rq_id</span> &#123;</span> </span><br><span class="line">   le16 vq_index_1_16: <span class="number">15</span>; <span class="comment">/* Bits 1 to 16 of the virtqueue index */</span> </span><br><span class="line">   le16 reserved: <span class="number">1</span>; <span class="comment">/* Set to zero */</span> </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>rss_rq_id is a receive virtqueue id.vq_index_1_16 consists of bits 1 to 16 of a virtqueue index. For example, a vq_index_1_16 value of 3 corresponds to virtqueue index 6, which maps to receiveq4.</p><p><img src="/images/2024/11/019.png" alt></p><table><thead><tr><th>vq_id</th><th>vq</th></tr></thead><tbody><tr><td>0</td><td>rxq1</td></tr><tr><td>1</td><td>txq1</td></tr><tr><td>2</td><td>rxq2</td></tr><tr><td>3</td><td>txq2</td></tr><tr><td>4</td><td>rxq3</td></tr><tr><td>5</td><td>txq3</td></tr><tr><td>6</td><td>rxq4</td></tr><tr><td>7</td><td>txq4</td></tr></tbody></table><p>如果vq_index_1_16为3，那么rss_rq_id就是6(0x110)，对应于rxq4。</p><h3 id="unclassified-queue"><a href="#unclassified-queue" class="headerlink" title="unclassified_queue"></a>unclassified_queue</h3><p>Field unclassified_queue specifies the receive virtqueue id in which to place unclassified packets.</p><h3 id="indirection-table"><a href="#indirection-table" class="headerlink" title="indirection_table"></a>indirection_table</h3><p>Field indirection_table is an array of receive virtqueues ids.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>The device MUST determine the destination queue for a network packet as follows:</p><ol><li>Calculate the hash of the packet.</li><li>If the device did not calculate the hash for the specific packet, the device directs the packet to the receiveq specified by <code>unclassified_queue</code> of <code>virtio_net_rss_config</code> structure.</li><li>Apply <code>indirection_table_mask</code>to the calculated hash and use the result as the index in the indirection table to get the destination receive virtqueue id.</li><li>If the destination receive queue is being reset, the device MUST drop the packet.</li></ol><hr><p>参考资料:</p><ol><li><a href="https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01.html" target="_blank" rel="noopener">VIRTIO 1.3 spec</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下virtio-net中&lt;a href=&quot;/2024/05/05/Notes-about-RSS-Receive-Side-Scaling/&quot;&gt;RSS(Receive Side Scaling)&lt;/a&gt;的具体实现。
    
    </summary>
    
      <category term="virtio" scheme="http://liujunming.github.io/categories/virtio/"/>
    
    
      <category term="计算机网络" scheme="http://liujunming.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
      <category term="virtio" scheme="http://liujunming.github.io/tags/virtio/"/>
    
  </entry>
  
  <entry>
    <title>Notes about NUMA</title>
    <link href="http://liujunming.github.io/2024/11/17/%E8%BD%AC%E8%BD%BD-Linux-NUMA-Optimization-1/"/>
    <id>http://liujunming.github.io/2024/11/17/转载-Linux-NUMA-Optimization-1/</id>
    <published>2024-11-17T00:37:04.000Z</published>
    <updated>2024-11-17T08:26:21.697Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要转载自:<a href="https://oliveryang.net/2016/02/linux-numa-optimization-1/" target="_blank" rel="noopener">https://oliveryang.net/2016/02/linux-numa-optimization-1/</a><a id="more"></a></p><h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p>理解NUMA的概念首先要熟悉多处理器计算机系统的几个重要概念。</p><h3 id="1-1-SMP-vs-AMP"><a href="#1-1-SMP-vs-AMP" class="headerlink" title="1.1 SMP vs. AMP"></a>1.1 SMP vs. AMP</h3><p><a href="https://www.wikiwand.com/en/articles/Symmetric_multiprocessing" target="_blank" rel="noopener">SMP(Symmetric Multiprocessing)</a>， 即对称多处理器架构，是目前最常见的多处理器计算机架构。<br><a href="https://en.wikipedia.org/wiki/Asymmetric_multiprocessing" target="_blank" rel="noopener">AMP(Asymmetric Multiprocessing)</a>， 即非对称多处理器架构，则是与SMP相对的概念。</p><p>那么两者之间的主要区别是什么呢？ 总结下来有这么几点，</p><ol><li>SMP的多个处理器都是同构的，使用相同架构的CPU；而AMP的多个处理器则可能是异构的。</li><li>SMP的多个处理器共享同一内存地址空间；而AMP的每个处理器则拥有自己独立的地址空间。</li><li>SMP的多个处理器操通常共享一个操作系统的实例；而AMP的每个处理器可以有或者没有运行操作系统， 运行操作系统的CPU也是在运行多个独立的实例。</li><li>SMP的多处理器之间可以通过共享内存来协同通信；而AMP则需要提供一种处理器间的通信机制。</li></ol><p>SMP和AMP的深入介绍很多经典文章书籍可参考，此处不再赘述。现今主流的x86多处理器服务器都是SMP架构的， 而很多嵌入式系统则是AMP架构的。</p><h3 id="1-2-NUMA-vs-UMA"><a href="#1-2-NUMA-vs-UMA" class="headerlink" title="1.2 NUMA vs. UMA"></a>1.2 NUMA vs. UMA</h3><p><a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access" target="_blank" rel="noopener">NUMA(Non-Uniform Memory Access)</a> 非均匀内存访问架构是指多处理器系统中，内存的访问时间是依赖于处理器和内存之间的相对位置的。 这种设计里存在和处理器相对近的内存，通常被称作本地内存；还有和处理器相对远的内存， 通常被称为非本地内存。</p><p><a href="https://en.wikipedia.org/wiki/Uniform_memory_access" target="_blank" rel="noopener">UMA(Uniform Memory Access)</a> 均匀内存访问架构则是与NUMA相反，所以处理器对共享内存的访问距离和时间是相同的。</p><p>由此可知，不论是NUMA还是UMA都是SMP架构的一种设计和实现上的选择。</p><p>阅读文档时，也常常能看到<strong>ccNUMA(Cache Coherent NUMA)</strong>，即缓存一致性NUMA架构。 这种架构主要是在NUMA架构之上保证了多处理器之间的缓存一致性。降低了系统程序的编写难度。</p><p>x86多处理器发展历史上，早期的多核和多处理器系统都是UMA架构的。这种架构下， 多个CPU通过同一个北桥(North Bridge)芯片与内存链接。北桥芯片里集成了内存控制器(Memory Controller)，</p><p>下图是一个典型的早期 x86 UMA 系统，四路处理器通过 FSB (前端系统总线) 和主板上的内存控制器芯片 (MCH) 相连，DRAM 是以 UMA 方式组织的，延迟并无访问差异，<br><img src="/images/2024/11/011.png" alt></p><p>在 UMA 架构下，CPU 和内存控制器之间的前端总线 (FSB) 在系统 CPU 数量不断增加的前提下， 成为了系统性能的瓶颈。因此，AMD 在引入 64 位 x86 架构时，实现了 NUMA 架构。之后， Intel 也推出了 x64 的 Nehalem 架构，x86 终于全面进入到 NUMA 时代。x86 NUMA 目前的实现属于 ccNUMA。</p><p>从 Nehalem 架构开始，x86 开始转向 NUMA 架构，内存控制器芯片被集成到处理器内部，多个处理器通过 QPI 链路相连，从此 DRAM 有了远近之分。 而 Sandybridge 架构则更近一步，将片外的 IOH 芯片也集成到了处理器内部，至此，内存控制器和 PCIe Root Complex 全部在处理器内部了。 下图就是一个典型的 x86 的 NUMA 架构：</p><p><img src="/images/2024/11/012.png" alt></p><h2 id="2-NUMA-Hierarchy"><a href="#2-NUMA-Hierarchy" class="headerlink" title="2. NUMA Hierarchy"></a>2. NUMA Hierarchy</h2><p>NUMA Hierarchy就是NUMA的层级结构。一个Intel x86 NUMA系统就是由多个NUMA Node组成。</p><h3 id="2-1-NUMA-Node内部"><a href="#2-1-NUMA-Node内部" class="headerlink" title="2.1 NUMA Node内部"></a>2.1 NUMA Node内部</h3><p>一个NUMA Node内部是由一个<strong>物理CPU</strong>和它所有的<strong>本地内存(Local Memory)</strong>组成的。广义得讲， 一个NUMA Node内部还包含<strong>本地IO资源</strong>，对大多数Intel x86 NUMA平台来说，主要是PCIe总线资源。 ACPI规范就是这么抽象一个NUMA Node的。</p><h4 id="2-1-1-物理CPU"><a href="#2-1-1-物理CPU" class="headerlink" title="2.1.1 物理CPU"></a>2.1.1 物理CPU</h4><p>一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成。每个CPU Core内部又可以由两个CPU Thread组成。 每个CPU thread都是一个操作系统可见的逻辑CPU。对大多数操作系统来说，一个八核HT打开的CPU会被识别为16个CPU。 下面就说一说这里面相关的概念，</p><ul><li><p>Socket<br>一个Socket对应一个物理CPU。 这个词大概是从CPU在主板上的物理连接方式上来的。处理器通过主板的Socket来插到主板上。 尤其是有了多核(Multi-core)系统以后，Multi-socket系统被用来指明系统到底存在多少个物理CPU。</p></li><li><p>Core<br>CPU的运算核心。 x86的核包含了CPU运算的基本部件，如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存。 一个Socket里可以有多个Core。如今的多核时代，即使是Single Socket的系统， 也是逻辑上的SMP系统。但是，一个物理CPU的系统不存在非本地内存，因此相当于UMA系统。</p></li><li><p>Uncore<br>Intel x86物理CPU里没有放在Core里的部件都被叫做Uncore。Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能。 在Nehalem时代，内存控制器被集成到CPU里，叫做iMC(Integrated Memory Controller)。 而PCIe Root Complex还做为独立部件在IO Hub芯片里。到了SandyBridge时代，PCIe Root Complex也被集成到了CPU里。 现今的Uncore部分，除了iMC，PCIe Root Complex，还有QPI(QuickPath Interconnect)控制器， L3缓存，CBox(负责缓存一致性)，及其它外设控制器。</p></li><li><p>Threads<br>这里特指CPU的多线程技术。在Intel x86架构下，CPU的多线程技术被称作超线程(Hyper-Threading)技术。 Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor)， 每个逻辑处理器都有独立的处理器状态，但共享Core内部的计算资源，如ALU，FPU，L1，L2缓存。 这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能，提高了硬件使用效率。 x86的超线程技术出现早于NUMA架构。</p></li></ul><p>以下图为例，1 个 x86 CPU Socket 有 4 个物理 Core，每个 Core 有两个 HT (Hyper Thread)，L1 L2 Cache 被两个 HT 共享， 而 L3 Cache 则在 Socket 内，被所有 4 个 Core 共享，<br><img src="/images/2024/11/013.jpg" alt></p><h4 id="2-1-2-本地内存"><a href="#2-1-2-本地内存" class="headerlink" title="2.1.2 本地内存"></a>2.1.2 本地内存</h4><p>在Intel x86平台上，所谓本地内存，就是CPU可以经过Uncore部件里的iMC访问到的内存。而那些非本地的， 远程内存(Remote Memory)，则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问。 曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示，远程内存访问的延时是本地内存的一倍。</p><p>可以假设，操作系统应该尽量利用本地内存的低访问延迟特性来优化应用和系统的性能。</p><h4 id="2-1-3-本地IO资源"><a href="#2-1-3-本地IO资源" class="headerlink" title="2.1.3 本地IO资源"></a>2.1.3 本地IO资源</h4><p>如前所述，Intel自从SandyBridge处理器开始，已经把PCIe Root Complex集成到CPU里了。 正因为如此，从CPU直接引出PCIe Root Port的PCIe 3.0的链路可以直接与PCIe Switch或者PCIe Endpoint相连。 一个PCIe Endpoint就是一个PCIe外设。这就意味着，对某个PCIe外设来说，如果它直接与哪个CPU相连， 它就属于哪个CPU所在的NUMA Node。</p><p>与本地内存一样，所谓本地IO资源，就是CPU可以经过Uncore部件里的PCIe Root Complex直接访问到的IO资源。 如果是非本地IO资源，则需要经过QPI链路到该IO资源所属的CPU，再通过该CPU PCIe Root Complex访问。 如果同一个NUMA Node内的CPU和内存和另外一个NUMA Node的IO资源发生互操作，因为要跨越QPI链路， 会存在额外的访问延迟问题。</p><p>其它体系结构里，为降低外设访问延迟，也有将IB(Infiniband)总线集成到CPU里的。 这样IB设备也属于NUMA Node的一部分了。</p><p>可以假设，操作系统如果是NUMA Aware的话，应该会尽量针对本地IO资源低延迟的优点进行优化。</p><h3 id="2-2-NUMA-Node互联"><a href="#2-2-NUMA-Node互联" class="headerlink" title="2.2 NUMA Node互联"></a>2.2 NUMA Node互联</h3><p>在Intel x86上，NUMA Node之间的互联是通过<a href="https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect" target="_blank" rel="noopener">QPI(QuickPath Interconnect)</a> Link的。 CPU的Uncore部分有QPI的控制器来控制CPU到QPI的数据访问。</p><p>不借助第三方的 Node Controller，2 或 4 个 NUMA Node (取决于具体架构)可以通过 QPI(QuickPath Interconnect) 总线互联起来， 构成一个NUMA系统。例如，<a href="https://www.doit.com.cn/p/118059.html" target="_blank" rel="noopener">SGI UV计算机系统</a>， 它就是借助自家的 SGI NUMAlink® 互联技术来达到 4 到 256 个 CPU socket 扩展的能力的。这是一个 SMP 系统， 所以支持运行一个 Linux 操作系统实例去管理系统。</p><p>下图就是一个利用 QPI Switch 互联的 8 NUMA Node 的 x86 系统，</p><p><img src="/images/2024/11/014.png" alt></p><h2 id="3-NUMA-Affinity"><a href="#3-NUMA-Affinity" class="headerlink" title="3. NUMA Affinity"></a>3. NUMA Affinity</h2><p>NUMA Affinity(亲和性)是和NUMA Hierarchy(层级结构)直接相关的。对系统软件来说， 以下两个概念至关重要，</p><h3 id="3-1-CPU-NUMA-Affinity"><a href="#3-1-CPU-NUMA-Affinity" class="headerlink" title="3.1 CPU NUMA Affinity"></a>3.1 CPU NUMA Affinity</h3><p>CPU NUMA的亲和性是指从CPU角度看，哪些内存访问更快，有更低的延迟。如前所述， 和该CPU直接相连的本地内存是更快的。操作系统如果可以根据任务所在CPU去分配本地内存， 就是基于CPU NUMA亲和性的考虑。因此，CPU NUMA亲和性就是要尽量让任务运行在本地的NUMA Node里。</p><h3 id="3-2-Device-NUMA-Affinity"><a href="#3-2-Device-NUMA-Affinity" class="headerlink" title="3.2 Device NUMA Affinity"></a>3.2 Device NUMA Affinity</h3><p><img src="/images/2024/11/015.png" alt></p><p>设备NUMA亲和性是指从PCIe外设的角度看，如果和CPU和内存相关的IO活动都发生在外设所属的NUMA Node， 将会有更低延迟。这里有两种设备NUMA亲和性的问题，</p><h4 id="3-2-1-DMA-Buffer-NUMA-Affinity"><a href="#3-2-1-DMA-Buffer-NUMA-Affinity" class="headerlink" title="3.2.1 DMA Buffer NUMA Affinity"></a>3.2.1 DMA Buffer NUMA Affinity</h4><p>大部分PCIe设备支持DMA功能的。也就是说，设备可以直接把数据写入到位于内存中的DMA缓冲区。 显然，如果DMA缓冲区在PCIe外设所属的NUMA Node里分配，那么将会有最低的延迟。 否则，外设的DMA操作要跨越QPI链接去读写另外一个NUMA Node里的DMA缓冲区。 因此，操作系统如果可以根据PCIe设备所属的NUMA node分配DMA缓冲区， 将会有最好的DMA操作的性能。</p><h4 id="3-2-2-Interrupt-NUMA-Affinity"><a href="#3-2-2-Interrupt-NUMA-Affinity" class="headerlink" title="3.2.2 Interrupt NUMA Affinity"></a>3.2.2 Interrupt NUMA Affinity</h4><p>设备DMA操作完成后，需要在CPU上触发中断来通知驱动程序的中断处理例程(ISR)来读写DMA缓冲区。 很多时候，ISR触发下半部机制(SoftIRQ)来进入到协议栈相关(Network，Storage)的代码路径来传送数据。 对大部分操作系统来说，硬件中断(HardIRQ)和下半部机制的代码在同一个CPU上发生。 因此，DMA缓冲区的读写操作发生的位置和设备硬件中断(HardIRQ)密切相关。假设操作系统可以把设备的硬件中断绑定到自己所属的NUMA node， 那之后中断处理函数和协议栈代码对DMA缓冲区的读写将会有更低的延迟。</p><h2 id="4-Firmware接口"><a href="#4-Firmware接口" class="headerlink" title="4. Firmware接口"></a>4. Firmware接口</h2><p>由于NUMA的亲和性对应用的性能非常重要，那么硬件平台就需要给操作系统提供接口机制来感知硬件的NUMA层级结构。 在x86平台，ACPI规范提供了以下接口来让操作系统检测系统的NUMA层级结构。</p><p>ACPI 5.0a规范的第17章是有关NUMA的章节。ACPI规范里，NUMA Node被第9章定义的Module Device所描述。 ACPI规范里用<strong>Proximity Domain</strong>对NUMA Node做了抽象，两者的概念大多时候等同。</p><ul><li><p><strong>SRAT(System Resource Affinity Table)</strong><br>主要描述了系统boot时的CPU和内存都属于哪个Proximity Domain(NUMA Node)。 这个表格里的信息时静态的，如果是启动后热插拔，需要用OSPM的_PXM方法去获得相关信息。</p></li><li><p><strong>SLIT(System Locality Information Table)</strong><br>提供CPU和内存之间的位置远近信息。在SRAT表格里，只能告诉给定的CPU和内存是否在一个NUMA Node。 对某个CPU来说，不在本NUMA Node里的内存，即远程内存们是否都是一样的访问延迟取决于NUMA的拓扑有多复杂(QPI的跳数)。 总之，对于不能简单用远近来描述的NUMA系统(QPI存在0，1，2等不同跳数)， 需要SLIT表格给出进一步的说明。同样的，这个表格也是静态表格，热插拔需要使用OSPM的_SLI方法。</p></li><li><p><strong>DSDT(Differentiated System Description Table)</strong><br>从Device NUMA角度看，这个表格给出了系统boot时的外设都属于哪个Proximity Domain(NUMA Node)。</p></li></ul><p>ACPI规范OSPM(Operating System-directed configuration and Power Management) 和OSPM各种方法就是操作系统里的ACPI驱动和ACPI firmware之间的一个互动的接口。 x86启动OS后，没有ACPI之前，firmware(BIOS)的代码是无法被执行了，除非通过SMI中断处理程序。 但有了ACPI，BIOS提前把ACPI的一些静态表格和AML的bytecode代码装载到内存， 然后ACPI驱动就会加载AML的解释器，这样OS就可以通过ACPI驱动调用预先装载的AML代码。 AML(ACPI Machine Language)是和Java类似的一种虚拟机解释型语言，所以不同操作系统的ACPI驱动， 只要有相同的虚拟机解释器，就可以直接从操作系统调用ACPI写好的AML的代码了。 所以，前文所述的所有热插拔的OSPM方法，其实就是对应ACPI firmware的AML的一段函数代码而已。 (关于ACPI的简单介绍，这里给出两篇延伸阅读：<a href="http://rdist.root.org/2008/10/17/all-about-acpi/" target="_blank" rel="noopener">1</a> 和<a href="https://www.usenix.org/legacy/events/usenix02/tech/freenix/full_papers/watanabe/watanabe_html/index.html" target="_blank" rel="noopener">2</a>。)</p><p>至此，x86 NUMA平台所需的一些硬件知识基本就覆盖到了。需要说明的是， 虽然本文以Intel平台为例，但AMD平台的差异也只是CPU总线和内部结构的差异而已。 其它方面的NUMA概念AMD也是类似的。</p><hr><p>参考资料:</p><ol><li>IOctopus: outsmarting nonuniform DMA(ASPLOS’20)</li><li><a href="https://blog.csdn.net/qq_20817327/article/details/105925071" target="_blank" rel="noopener">NUMA架构详解</a></li><li><a href="https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="noopener">The MySQL “swap insanity” problem and the effects of the NUMA architecture</a></li><li><a href="https://frankdenneman.nl/2016/07/08/numa-deep-dive-part-2-system-architecture/" target="_blank" rel="noopener">NUMA Deep Dive Part 2: System Architecture</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要转载自:&lt;a href=&quot;https://oliveryang.net/2016/02/linux-numa-optimization-1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://oliveryang.net/2016/02/linux-numa-optimization-1/&lt;/a&gt;
    
    </summary>
    
      <category term="体系结构" scheme="http://liujunming.github.io/categories/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="体系结构" scheme="http://liujunming.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Notes about AMD IOMMU IRTCache机制</title>
    <link href="http://liujunming.github.io/2024/11/10/Notes-about-AMD-IOMMU-IRTCache/"/>
    <id>http://liujunming.github.io/2024/11/10/Notes-about-AMD-IOMMU-IRTCache/</id>
    <published>2024-11-10T10:52:35.000Z</published>
    <updated>2024-11-10T11:52:37.336Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下AMD IOMMU IRTCache机制的相关notes。<a id="more"></a></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>在没有IRTCache机制之前，设备的MSI data到IRTE(Interrupt Remapping Table Entry)的映射，需要硬件从内存中读取Interrupt Remapping Table来获取IRTE。<br><img src="/images/2024/11/009.png" alt></p><p>在引入IRTCache机制之后，IOMMU硬件中就会缓存设备MSI data到IRTE的映射了,这样就可以避免IOMMU硬件从内存中读取IRTE。</p><h3 id="How"><a href="#How" class="headerlink" title="How"></a>How</h3><p>For IOMMU AVIC, the IOMMU driver needs to keep track of vcpu scheduling changes, and updates interrupt remapping table entry (IRTE) accordingly. The IRTE is normally cached by the hardware, which requires the IOMMU driver to issue IOMMU IRT invalidation command and wait for completion everytime it updates the table.</p><p>Enabling IOMMU AVIC on a large scale system with lots of vcpus and VFIO pass-through devices running interrupt-intensive workload, it could result in high IRT invalidation rate. In such case, the overhead from IRT invalidation could outweigh the benefit of IRTE caching.</p><p>Therefore, introduce a new AMD IOMMU driver option “amd_iommu=irtcachedis” to allow disabling IRTE caching, and avoid the need for IRTE invalidation.</p><p><img src="/images/2024/11/008.png" alt></p><p><img src="/images/2024/11/010.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://lore.kernel.org/lkml/20230530141137.14376-1-suravee.suthikulpanit@amd.com/" target="_blank" rel="noopener">[PATCH v3 0/5] iommu/amd: AVIC Interrupt Remapping Improvements</a></li><li>AMD I/O Virtualization Technology (IOMMU) Specification, 48882</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下AMD IOMMU IRTCache机制的相关notes。
    
    </summary>
    
      <category term="IOMMU" scheme="http://liujunming.github.io/categories/IOMMU/"/>
    
    
      <category term="AMD" scheme="http://liujunming.github.io/tags/AMD/"/>
    
      <category term="IOMMU" scheme="http://liujunming.github.io/tags/IOMMU/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PCIe flow control机制</title>
    <link href="http://liujunming.github.io/2024/11/10/Notes-about-PCIe-credit%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2024/11/10/Notes-about-PCIe-credit机制/</id>
    <published>2024-11-10T02:16:57.000Z</published>
    <updated>2024-11-10T07:12:32.928Z</updated>
    
    <content type="html"><![CDATA[<p>本文将mark下PCIe flow control机制的相关notes。<a id="more"></a></p><p>The receiver of each port reports the size of its Flow Control buffers <strong>in units called credits</strong>.<br>在一条链路的两端设备分别向对方通知其自己可以使用的缓冲区大小或数量。这里通知的空间大小就是信用（credit）。</p><p><img src="/images/2024/11/007.png" alt></p><p>A定期的告诉B，我这里还有地方，来吧，来吧~。B因此知道A是有空余空间接收的。同样，B也是采用同样的方式告知A。流控信用会定期在两者间发送，这叫做更新流控信用（Update FC）。</p><p>The data link layer has a Flow Control (FC) mechanism, which makes sure that a TLP is transmitted only when the link partner has enough buffer space to accept it.</p><p>The Flow Control mechanism uses a credit‐based mechanism that allows the transmitting port to be aware of buffer space available at the receiving port. As part of its initialization, each receiver reports the size of its buffers to the transmitter on the other end of the Link, and then <strong>during run‐time it regularly updates the number of credits available using Flow Control DLLPs</strong>. Technically, of course, DLLPs are overhead because they don’t convey any data payload, but they are kept small to minimize their impact on performance.</p><hr><p>参考资料:</p><ol><li>PCI Express Technology(Mike Jackson, Ravi Budruk)</li><li><a href="https://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-2" target="_blank" rel="noopener">Down to the TLP: How PCI express devices talk (Part II)</a></li><li><a href="https://www.slideshare.net/slideshow/pciexpressbasicsbackgroundpdf/252660914#38" target="_blank" rel="noopener">PCI Express Basics Background</a></li><li><a href="https://mp.weixin.qq.com/s/WkRTqLOpqynHtOiaaOTplw" target="_blank" rel="noopener">Credit timeout &amp; Completion timeout</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将mark下PCIe flow control机制的相关notes。
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
  <entry>
    <title>Notes about PCIe prefetchable bar</title>
    <link href="http://liujunming.github.io/2024/11/03/Notes-about-PCIe-prefetchable-bar/"/>
    <id>http://liujunming.github.io/2024/11/03/Notes-about-PCIe-prefetchable-bar/</id>
    <published>2024-11-03T09:21:24.000Z</published>
    <updated>2024-11-03T09:43:15.866Z</updated>
    
    <content type="html"><![CDATA[<p>When a base address register is marked as <strong>Prefetchable</strong>, it means that:the region does not have read side effects (reading from that memory range doesn’t change any state), and it is allowed for the CPU to cache loads from that memory region and read it in bursts (typically cache line sized).<a id="more"></a> Hardware is also allowed to merge repeated stores to the same address into one store of the latest value. If you are using paging and want maximum performance, you should map prefetchable MMIO regions as WT (write-through) instead of UC (uncacheable). On x86, frame buffers are the exception, they should be almost always be mapped WC (write-combining).</p><p><img src="/images/2024/11/006.png" alt></p><hr><p>参考资料:</p><ol><li><a href="https://wiki.osdev.org/PCI" target="_blank" rel="noopener">wiki.osdev.org/PCI</a></li><li><a href="https://blog.csdn.net/redseazhaojianertao/article/details/79943494" target="_blank" rel="noopener">PCIE的prefetchable和nonprefetchable的理解</a></li><li>Intel SDM Vol3</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When a base address register is marked as &lt;strong&gt;Prefetchable&lt;/strong&gt;, it means that:the region does not have read side effects (reading from that memory range doesn’t change any state), and it is allowed for the CPU to cache loads from that memory region and read it in bursts (typically cache line sized).
    
    </summary>
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/categories/PCI-PCIe/"/>
    
    
      <category term="PCI&amp;PCIe" scheme="http://liujunming.github.io/tags/PCI-PCIe/"/>
    
  </entry>
  
</feed>
