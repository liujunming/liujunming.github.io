<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>L</title>
  
  <subtitle>make it simple, make it happen.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liujunming.github.io/"/>
  <updated>2018-12-28T05:39:02.369Z</updated>
  <id>http://liujunming.github.io/</id>
  
  <author>
    <name>liujunming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记-Memory Management</title>
    <link href="http://liujunming.github.io/2018/12/28/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Memory-Management/"/>
    <id>http://liujunming.github.io/2018/12/28/Understanding-the-Linux-Kernel-读书笔记-Memory-Management/</id>
    <published>2018-12-28T04:49:01.000Z</published>
    <updated>2018-12-28T05:39:02.369Z</updated>
    
    <content type="html"><![CDATA[<p>The sections “Page Frame Management” and “Memory Area Management” illustrate two different techniques for handling physically contiguous memory areas, while the section “Noncontiguous Memory Area Management” illustrates a third technique that handles noncontiguous memory areas. In these sections we’ll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.<a id="more"></a> </p><p><img src="/images/2017/10/8.gif" alt=""></p><h2 id="1-Page-Frame-Management"><a href="#1-Page-Frame-Management" class="headerlink" title="1 Page Frame Management"></a>1 Page Frame Management</h2><h3 id="1-1-Page-Descriptors"><a href="#1-1-Page-Descriptors" class="headerlink" title="1.1 Page Descriptors"></a>1.1 Page Descriptors</h3><p>State information of a page frame is kept in a page descriptor of type <code>page</code>.All page descriptors are stored in the <code>mem_map</code> array.</p><h3 id="1-2-Non-Uniform-Memory-Access-NUMA"><a href="#1-2-Non-Uniform-Memory-Access-NUMA" class="headerlink" title="1.2 Non-Uniform Memory Access (NUMA)"></a>1.2 Non-Uniform Memory Access (NUMA)</h3><p>The physical memory of the system is partitioned in several <em>nodes</em>.The physical memory inside each node can be split into several zones, each node has a descriptor of type <code>pg_data_t</code>.</p><h3 id="1-3-Memory-Zones"><a href="#1-3-Memory-Zones" class="headerlink" title="1.3 Memory Zones"></a>1.3 Memory Zones</h3><p>Each memory zone has its own descriptor of type <code>zone</code>.</p><h3 id="1-4-The-Pool-of-Reserved-Page-Frames"><a href="#1-4-The-Pool-of-Reserved-Page-Frames" class="headerlink" title="1.4 The Pool of Reserved Page Frames"></a>1.4 The Pool of Reserved Page Frames</h3><p>Some kernel control paths cannot be blocked while requesting memory—this happens, for instance, when handling an interrupt or when executing code inside a critical region. In these cases, a kernel control path should issue <em>atomic memory allocation requests</em>.  An atomic request never blocks: if there are not enough free pages, the allocation simply fails. The kernel reserves a pool of page frames for atomic memory allocation requests to be used only on low-on-memory conditions.</p><h3 id="1-5-The-Zoned-Page-Frame-Allocator"><a href="#1-5-The-Zoned-Page-Frame-Allocator" class="headerlink" title="1.5 The Zoned Page Frame Allocator"></a>1.5 The Zoned Page Frame Allocator</h3><p>The kernel subsystem that handles the memory allocation requests for groups of contiguous page frames is called the <em>zoned page frame allocator</em>. Its main components are shown in Figure 8-2.<br><img src="/images/2018/12/62.png" alt=""><br>The component named “zone allocator” receives the requests for allocation and deallocation of dynamic memory. In the case of allocation requests, the component searches a memory zone that includes a group of contiguous page frames that can satisfy the request. Inside each zone, page frames are handled by a component named “buddy system”. To get better system performance, a small number of page frames are kept in cache to quickly satisfy the allocation requests for single page frames.</p><h4 id="1-5-1-The-Zone-Allocator"><a href="#1-5-1-The-Zone-Allocator" class="headerlink" title="1.5.1 The Zone Allocator"></a>1.5.1 The Zone Allocator</h4><p>The zone allocator is the frontend of the kernel page frame allocator. This component must locate a memory zone that includes a number of free page frames large enough to satisfy the memory request. </p><h4 id="1-5-2-The-Buddy-System-Algorithm"><a href="#1-5-2-The-Buddy-System-Algorithm" class="headerlink" title="1.5.2 The Buddy System Algorithm"></a>1.5.2 The Buddy System Algorithm</h4><p>The kernel must establish a robust and efficient strategy for allocating groups of contiguous page frames. In doing so, it must deal with a well-known memory management problem called <em>external fragmentation</em>. The technique adopted by Linux to solve the external fragmentation problem is based on the well-known <em>buddy system</em> algorithm.</p><h3 id="1-5-3-The-Per-CPU-Page-Frame-Cache"><a href="#1-5-3-The-Per-CPU-Page-Frame-Cache" class="headerlink" title="1.5.3 The Per-CPU Page Frame Cache"></a>1.5.3 The Per-CPU Page Frame Cache</h3><p>The kernel often requests and releases single page frames. To boost system performance, each memory zone defines a <em>per-CPU page frame cache</em>. Each per-CPU cache includes some pre-allocated page frames to be used for single memory requests issued by the local CPU.</p><h3 id="1-6-Kernel-Mappings-of-High-Memory-Page-Frames"><a href="#1-6-Kernel-Mappings-of-High-Memory-Page-Frames" class="headerlink" title="1.6 Kernel Mappings of High-Memory Page Frames"></a>1.6 Kernel Mappings of High-Memory Page Frames</h3><p>见<a href="http://liujunming.top/2017/10/10/Linux%E5%86%85%E6%A0%B8%E9%AB%98%E7%AB%AF%E5%86%85%E5%AD%98/" target="_blank" rel="noopener">Linux内核高端内存</a></p><h2 id="2-Memory-Area-Management"><a href="#2-Memory-Area-Management" class="headerlink" title="2 Memory Area Management"></a>2 Memory Area Management</h2><p><em>Memory areas</em> is with sequences of memory cells having contiguous physical addresses and an arbitrary length.</p><h3 id="2-1-The-Slab-Allocator"><a href="#2-1-The-Slab-Allocator" class="headerlink" title="2.1 The Slab Allocator"></a>2.1 The Slab Allocator</h3><p>The buddy system algorithm adopts the page frame as the basic memory area. This is fine for dealing with relatively large memory requests, but how are we going to deal with requests for small memory areas, say a few tens or hundreds of bytes?</p><p>The slab allocator groups objects into <code>caches</code>. Each cache is a “store” of objects of the same type.</p><p>The area of main memory that contains a cache is divided into <code>slabs</code>; each slab consists of one or more contiguous page frames that contain both allocated and free objects.</p><p><img src="/images/2018/12/63.png" alt=""></p><p><img src="/images/2018/12/64.png" alt=""></p><h4 id="2-1-1-Local-Caches-of-Free-Slab-Objects"><a href="#2-1-1-Local-Caches-of-Free-Slab-Objects" class="headerlink" title="2.1.1 Local Caches of Free Slab Objects"></a>2.1.1 Local Caches of Free Slab Objects</h4><p>To reduce spin lock contention among processors and to make better use of the hardware caches, each cache of the slab allocator includes a per-CPU data structure consisting of a small array of pointers to freed objects called the <em>slab local cache</em>.</p><h3 id="2-2-General-Purpose-Objects"><a href="#2-2-General-Purpose-Objects" class="headerlink" title="2.2 General Purpose Objects"></a>2.2 General Purpose Objects</h3><p><code>kmalloc()</code></p><h3 id="2-3-Memory-Pools"><a href="#2-3-Memory-Pools" class="headerlink" title="2.3 Memory Pools"></a>2.3 Memory Pools</h3><p>A memory pool allows a kernel component—such as the block device subsystem—to allocate some dynamic memory to be used only in low-on-memory emergencies.</p><p>Memory pools should not be confused with the reserved page frames described in the earlier section “The Pool of Reserved Page Frames.” In fact, those page frames can be used only to satisfy atomic memory allocation requests issued by interrupt handlers or inside critical regions. Instead, a memory pool is a reserve of dynamic memory that can be used only by a specific kernel component.</p><p>Often, a memory pool is stacked over the slab allocator—that is, it is used to keep a reserve of slab objects. Generally speaking, however, a memory pool can be used to allocate every kind of dynamic memory.</p><h2 id="3-Noncontiguous-Memory-Area-Management"><a href="#3-Noncontiguous-Memory-Area-Management" class="headerlink" title="3 Noncontiguous Memory Area Management"></a>3 Noncontiguous Memory Area Management</h2><p><code>vmalloc()</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The sections “Page Frame Management” and “Memory Area Management” illustrate two different techniques for handling physically contiguous memory areas, while the section “Noncontiguous Memory Area Management” illustrates a third technique that handles noncontiguous memory areas. In these sections we’ll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Multiprocessor architecture</title>
    <link href="http://liujunming.github.io/2018/12/27/Multiprocessor-architecture/"/>
    <id>http://liujunming.github.io/2018/12/27/Multiprocessor-architecture/</id>
    <published>2018-12-27T07:10:40.000Z</published>
    <updated>2018-12-27T07:26:16.244Z</updated>
    
    <content type="html"><![CDATA[<p>Multiprocessor are classified by the way their memory is organized.<a id="more"></a></p><ul><li>UMA (Uniform Memory Access) multiprocessors<ul><li>UMA Bus-Based SMP Architectures</li><li>UMA Multiprocessors Using Crossbar Switches</li><li>UMA Multiprocessors Using Multistage Switching Networks </li></ul></li><li>NUMA (Nonuniform Memory Access) multiprocessors</li></ul><hr><p>参考资料：</p><ol><li><a href="https://www.cs.vu.nl/~ast/books/mos2/sample-8.pdf" target="_blank" rel="noopener">MULTIPLE PROCESSOR SYSTEMS</a></li><li><a href="https://www.slideshare.net/arpanbaishya/multiprocessor-architecture" target="_blank" rel="noopener">Multiprocessor architecture </a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Multiprocessor are classified by the way their memory is organized.
    
    </summary>
    
      <category term="architecture" scheme="http://liujunming.github.io/categories/architecture/"/>
    
    
      <category term="深计算机系统" scheme="http://liujunming.github.io/tags/%E6%B7%B1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="architecture" scheme="http://liujunming.github.io/tags/architecture/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Process Scheduling</title>
    <link href="http://liujunming.github.io/2018/12/27/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Scheduling/"/>
    <id>http://liujunming.github.io/2018/12/27/Understanding-the-Linux-Kernel-读书笔记-Process-Scheduling/</id>
    <published>2018-12-27T02:43:29.000Z</published>
    <updated>2018-12-27T06:43:51.834Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter deals with <em>scheduling</em>, which is concerned with when to switch and which process to choose.<a id="more"></a> </p><h2 id="1-Scheduling-Policy"><a href="#1-Scheduling-Policy" class="headerlink" title="1 Scheduling Policy"></a>1 Scheduling Policy</h2><p>The set of rules used to determine when and how to select a new process to run is called <em>scheduling policy</em>.<br>Linux scheduling is based on the <em>time sharing</em> technique: several processes run in “time multiplexing” because the CPU time is divided into <em>slices</em>, one for each runnable process.</p><p>Programmers may change the scheduling priorities by means of the system calls illustrated in Table 7-1.</p><p><img src="/images/2018/12/59.png" alt=""></p><h3 id="1-1-Process-Preemption"><a href="#1-1-Process-Preemption" class="headerlink" title="1.1 Process Preemption"></a>1.1 Process Preemption</h3><p>Linux processes are <em>preemptable</em>. When a process enters the <code>TASK_RUNNING</code> state, the kernel checks whether its dynamic priority is greater than the priority of the currently running process. If it is, the execution of <code>current</code> is interrupted and the scheduler is invoked to select another process to run (usually the process that just became runnable).</p><p>Be aware that a preempted process is not suspended, because it remains in the <code>TASK_ RUNNING</code> state; it simply no longer uses the CPU.</p><h3 id="1-2-How-Long-Must-a-Quantum-Last"><a href="#1-2-How-Long-Must-a-Quantum-Last" class="headerlink" title="1.2 How Long Must a Quantum Last?"></a>1.2 How Long Must a Quantum Last?</h3><p>The quantum[时间片] duration is critical for system performance: it should be neither too long nor too short.</p><h2 id="2-The-Scheduling-Algorithm"><a href="#2-The-Scheduling-Algorithm" class="headerlink" title="2 The Scheduling Algorithm"></a>2 The Scheduling Algorithm</h2><p><a href="http://liujunming.top/junming/os/8.cpu-sched-mlfq.pdf" target="_blank" rel="noopener">The Multi-Level Feedback Queue</a></p><h3 id="2-1-Scheduling-of-Conventional-Processes"><a href="#2-1-Scheduling-of-Conventional-Processes" class="headerlink" title="2.1 Scheduling of Conventional Processes"></a>2.1 Scheduling of Conventional Processes</h3><ul><li><strong>Base time quantum</strong></li><li><strong>Dynamic priority and average sleep time</strong></li><li><strong>Active and expired processes</strong></li></ul><h3 id="2-2-Scheduling-of-Real-Time-Processes"><a href="#2-2-Scheduling-of-Real-Time-Processes" class="headerlink" title="2.2 Scheduling of Real-Time Processes"></a>2.2 Scheduling of Real-Time Processes</h3><h2 id="3-Data-Structures-Used-by-the-Scheduler"><a href="#3-Data-Structures-Used-by-the-Scheduler" class="headerlink" title="3 Data Structures Used by the Scheduler"></a>3 Data Structures Used by the Scheduler</h2><p>The runqueue lists link the process descriptors of all runnable processes—that is, of those in a <code>TASK_RUNNING</code> state—except the <em>swapper</em> process (idle process).</p><h3 id="3-1-The-runqueue-Data-Structure"><a href="#3-1-The-runqueue-Data-Structure" class="headerlink" title="3.1 The runqueue Data Structure"></a>3.1 The <code>runqueue</code> Data Structure</h3><p>The <a href="https://elixir.bootlin.com/linux/v2.6.11/source/kernel/sched.c#L198" target="_blank" rel="noopener">runqueue</a> data structure.</p><p>Each CPU in the system has its own runqueue; all <code>runqueue</code> structures are stored in the <code>runqueues</code> per-CPU variable.</p><p>The most important fields of the <code>runqueue</code> data structure are those related to the lists of runnable processes.</p><p>The <code>arrays</code> field of the runqueue is an array consisting of two <code>prio_array_t</code> structures. Each data structure represents a set of runnable processes, and includes 140 doubly linked list heads (one list for each possible process priority), a priority bitmap, and a counter of the processes included in the set.<br><img src="/images/2018/12/60.png" alt=""></p><h3 id="3-2-Process-Descriptor"><a href="#3-2-Process-Descriptor" class="headerlink" title="3.2 Process Descriptor"></a>3.2 Process Descriptor</h3><p>Each process descriptor includes several fields related to scheduling.</p><h2 id="4-Functions-Used-by-the-Scheduler"><a href="#4-Functions-Used-by-the-Scheduler" class="headerlink" title="4 Functions Used by the Scheduler"></a>4 Functions Used by the Scheduler</h2><p>The scheduler relies on several functions in order to do its work; the most important are:</p><ul><li><code>scheduler_tick()</code></li></ul><p>Keeps the <code>time_slice</code> counter of <code>current</code> up-to-date</p><ul><li><code>try_to_wake_up()</code></li></ul><p>Awakens a sleeping process</p><ul><li><code>recalc_task_prio()</code></li></ul><p>Updates the dynamic priority of a process</p><ul><li><code>schedule()</code></li></ul><p>Selects a new process to be executed</p><ul><li><code>load_balance()</code></li></ul><p>Keeps the runqueues of a multiprocessor system balanced</p><h2 id="5-Runqueue-Balancing-in-Multiprocessor-Systems"><a href="#5-Runqueue-Balancing-in-Multiprocessor-Systems" class="headerlink" title="5 Runqueue Balancing in Multiprocessor Systems"></a>5 Runqueue Balancing in Multiprocessor Systems</h2><p>Multiprocessor machines come in many different flavors, and the scheduler behaves differently depending on the hardware characteristics. In particular, we will consider the following three types of multiprocessor machines:</p><ul><li><em>Classic multiprocessor architecture</em></li></ul><p>Until recently, this was the most common architecture for multiprocessor machines. These machines have a common set of RAM chips shared by all CPUs.</p><ul><li><em>Hyper-threading</em></li></ul><p>A hyper-threaded chip is a microprocessor that executes several threads of execution at once; it includes several copies of the internal registers and quickly switches between them. This technology allows the processor to exploit the machine cycles to execute another thread while the current thread is stalled for a memory access. A hyper-threaded physical CPU is seen by Linux as several different logical CPUs.</p><ul><li><em>NUMA</em></li></ul><hr><p>These basic kinds of multiprocessor systems are often combined. For instance, a motherboard that includes two different hyper-threaded CPUs is seen by the kernel as four logical CPUs.</p><p>A runnable process is always stored in exactly one runqueue: no runnable process ever appears in two or more runqueues. Therefore, until a process remains runnable, it is usually bound to one CPU.</p><p>This design choice is usually beneficial for system performance, because the hardware cache of every CPU is likely to be filled with data owned by the runnable processes in the runqueue. In some cases, however, binding a runnable process to a given CPU might induce a severe performance penalty. For instance, consider a large number of batch processes that make heavy use of the CPU: if most of them end up in the same runqueue, one CPU in the system will be overloaded, while the others will be nearly idle.</p><p>Therefore, the kernel periodically checks whether the workloads of the runqueues are balanced and, if necessary, moves some process from one runqueue to another. However, to get the best performance from a multiprocessor system, the load balancing algorithm should take into consideration the topology of the CPUs in the system. Linux sports a sophisticated runqueue balancing algorithm based on the notion of “scheduling domains.” Thanks to the scheduling domains, the algorithm can be easily tuned for all kinds of existing multiprocessor architectures.</p><h3 id="5-1-Scheduling-Domains"><a href="#5-1-Scheduling-Domains" class="headerlink" title="5.1 Scheduling Domains"></a>5.1 Scheduling Domains</h3><p>Essentially, a <em>scheduling domain</em> is a set of CPUs whose workloads should be kept balanced by the kernel. Generally speaking, scheduling domains are hierarchically organized: the top-most scheduling domain, which usually spans all CPUs in the system, includes children scheduling domains, each of which include a subset of the CPUs. Thanks to the hierarchy of scheduling domains, workload balancing can be done in a rather efficient way.</p><p>Figure 7-2 illustrates three examples of scheduling domain hierarchies, corresponding to the three main architectures of multiprocessor machines.</p><p><img src="/images/2018/12/61.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter deals with &lt;em&gt;scheduling&lt;/em&gt;, which is concerned with when to switch and which process to choose.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Interprocess Communication</title>
    <link href="http://liujunming.github.io/2018/12/26/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Interprocess-Communication/"/>
    <id>http://liujunming.github.io/2018/12/26/apue-读书笔记-Interprocess-Communication/</id>
    <published>2018-12-26T06:56:24.000Z</published>
    <updated>2018-12-26T07:46:37.942Z</updated>
    
    <content type="html"><![CDATA[<p>Interprocess Communication (IPC).<br>In this chapter, we examine classical IPC: pipes, FIFOs, message queues, semaphores, and shared memory.<a id="more"></a> </p><!-- ![](/images/2018/12/26.png) --><h2 id="1-Pipes"><a href="#1-Pipes" class="headerlink" title="1 Pipes"></a>1 Pipes</h2><p>Pipes are the oldest form of UNIX System IPC and are provided by all UNIX systems. Pipes have two limitations：</p><ol><li>Historically, they have been half duplex[半双工] (i.e., data flows in only one direction).</li><li>Pipes can be used only between processes that have a common ancestor.</li></ol><h2 id="2-FIFOs"><a href="#2-FIFOs" class="headerlink" title="2 FIFOs"></a>2 FIFOs</h2><p>FIFOs are sometimes called named pipes. Unnamed pipes can be used only between related processes when a common ancestor has created the pipe. With FIFOs, however, unrelated processes can exchange data.</p><h2 id="3-Message-Queues"><a href="#3-Message-Queues" class="headerlink" title="3 Message Queues"></a>3 Message Queues</h2><p>A message queue is a linked list of messages stored within the kernel and identified by a message queue identifier.</p><h2 id="4-Semaphores"><a href="#4-Semaphores" class="headerlink" title="4 Semaphores"></a>4 Semaphores</h2><p>A semaphore isn’t a form of IPC similar to the others that we’ve described (pipes, FIFOs, and message queues). A semaphore is a counter used to provide access to a shared data object for multiple processes.</p><h2 id="5-Shared-Memory"><a href="#5-Shared-Memory" class="headerlink" title="5 Shared Memory"></a>5 Shared Memory</h2><p>Shared memory allows two or more processes to share the same pages of memory. No kernel intervention is required to exchange data via shared memory. Once a process has copied data into a shared memory segment, that data is immediately visible to other processes. Shared memory provides fast IPC, although this speed advantage is somewhat offset by the fact that normally we must use some type of synchronization technique, such as a System V semaphore, to synchronize access to the shared memory.</p><p>A POSIX shared memory object is used to share a region of memory between unrelated processes without creating an underlying disk file.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Interprocess Communication (IPC).&lt;br&gt;In this chapter, we examine classical IPC: pipes, FIFOs, message queues, semaphores, and shared memory.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Advanced I/O</title>
    <link href="http://liujunming.github.io/2018/12/26/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Advanced-I-O/"/>
    <id>http://liujunming.github.io/2018/12/26/apue-读书笔记-Advanced-I-O/</id>
    <published>2018-12-26T02:47:52.000Z</published>
    <updated>2018-12-26T07:01:10.468Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter covers numerous topics and functions that we lump under the term <em>advanced I/O</em> : nonblocking I/O, record locking, I/O multiplexing (the <code>select</code> and <code>poll</code> functions), asynchronous I/O, the <code>readv</code> and <code>writev</code> functions, and memory-mapped I/O (<code>mmap</code>).<a id="more"></a> </p><h2 id="1-Nonblocking-I-O"><a href="#1-Nonblocking-I-O" class="headerlink" title="1 Nonblocking I/O"></a>1 Nonblocking I/O</h2><p>参考<a href="http://liujunming.top/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/" target="_blank" rel="noopener">IO - 同步，异步，阻塞，非阻塞</a></p><h2 id="2-Record-Locking"><a href="#2-Record-Locking" class="headerlink" title="2 Record Locking"></a>2 Record Locking</h2><p><em>Record locking</em> is the term normally used to describe the ability of a process to prevent other processes from modifying a region of a file while the first process is reading or modifying that portion of the file. Under the UNIX System, ‘‘record’’ is a misnomer; the UNIX kernel does not have a notion of records in a file. A better term is <em>byte-range locking</em>, given that it is a range of a file (possibly the entire file) that is locked.</p><p>demo可以参考<a href="https://blog.csdn.net/anonymalias/article/details/9197641" target="_blank" rel="noopener">Linux进程同步之记录锁</a>和书中示例。对于像锁的隐含继承与实现这些细节问题可参考书籍。</p><h2 id="3-I-O-Multiplexing"><a href="#3-I-O-Multiplexing" class="headerlink" title="3 I/O Multiplexing"></a>3 I/O Multiplexing</h2><p>参考<a href="http://liujunming.top/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/" target="_blank" rel="noopener">IO - 同步，异步，阻塞，非阻塞</a></p><ul><li><code>select</code> and <code>pselect</code> Functions</li><li><code>poll</code> Function</li></ul><h2 id="4-Asynchronous-I-O"><a href="#4-Asynchronous-I-O" class="headerlink" title="4 Asynchronous I/O"></a>4 Asynchronous I/O</h2><p>参考<a href="http://liujunming.top/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/" target="_blank" rel="noopener">IO - 同步，异步，阻塞，非阻塞</a></p><h2 id="5-readv-and-writev-Functions"><a href="#5-readv-and-writev-Functions" class="headerlink" title="5 readv and writev Functions"></a>5 <code>readv</code> and <code>writev</code> Functions</h2><p>The <code>readv</code> and <code>writev</code> functions let us read into and write from multiple noncontiguous buffers in a single function call. These operations are called <em>scatter read</em> and <em>gather write</em>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> readv(<span class="keyword">int</span> fd, <span class="keyword">const</span> struct iovec *iov, <span class="keyword">int</span> iovcnt); </span><br><span class="line"><span class="keyword">ssize_t</span> writev(<span class="keyword">int</span> fd, <span class="keyword">const</span> struct iovec *iov, <span class="keyword">int</span> iovcnt);</span><br></pre></td></tr></table></figure></p><p>The second argument to both functions is a pointer to an array of <code>iovec</code> structures:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> &#123;</span></span><br><span class="line">    <span class="keyword">void</span>   *iov_base;  <span class="comment">/* starting address of buffer */</span></span><br><span class="line">    <span class="keyword">size_t</span>  iov_len;   <span class="comment">/* size of buffer */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The number of elements in the iov array is specified by iovcnt.Figure 14.22 shows a diagram relating the arguments to these two functions and the <code>iovec</code> structure.<br><img src="/images/2018/12/58.png" alt=""></p><h2 id="6-readn-and-writen-Functions"><a href="#6-readn-and-writen-Functions" class="headerlink" title="6 readn and writen Functions"></a>6 <code>readn</code> and <code>writen</code> Functions</h2><p>Pipes, FIFOs, and some devices—notably terminals and networks—have the following two properties:</p><ol><li>A <code>read</code> operation may return less than asked for, even though we have not encountered the end of file. This is not an error, and we should simply continue reading from the device.</li><li>A <code>write</code> operation can return less than we specified. Again, it’s not an error, and we should continue writing the remainder of the data.</li></ol><p>Generally, when we read from or write to a pipe, network device, or terminal, we need to take these characteristics into consideration. We can use the <code>readn</code> and <code>writen</code> functions to read and write N bytes of data, respectively, letting these functions handle a return value that’s possibly less than requested. These two functions simply call read or write as many times as required to <code>read</code> or <code>write</code> the entire N bytes of data.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ssize_t</span> readn(<span class="keyword">int</span> fd, <span class="keyword">void</span> *buf, <span class="keyword">size_t</span> nbytes); </span><br><span class="line"><span class="keyword">ssize_t</span> writen(<span class="keyword">int</span> fd, <span class="keyword">void</span> *buf, <span class="keyword">size_t</span> nbytes);</span><br></pre></td></tr></table></figure></p><h2 id="7-Memory-Mapped-I-O"><a href="#7-Memory-Mapped-I-O" class="headerlink" title="7 Memory-Mapped I/O"></a>7 Memory-Mapped I/O</h2><p>Memory-mapped I/O lets us map a file on disk into a buffer in memory so that, when we fetch bytes from the buffer, the corresponding bytes of the file are read. Similarly, when we store data in the buffer, the corresponding bytes are automatically written to the file. This lets us perform I/O without using <code>read</code> or <code>write</code>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter covers numerous topics and functions that we lump under the term &lt;em&gt;advanced I/O&lt;/em&gt; : nonblocking I/O, record locking, I/O multiplexing (the &lt;code&gt;select&lt;/code&gt; and &lt;code&gt;poll&lt;/code&gt; functions), asynchronous I/O, the &lt;code&gt;readv&lt;/code&gt; and &lt;code&gt;writev&lt;/code&gt; functions, and memory-mapped I/O (&lt;code&gt;mmap&lt;/code&gt;).
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>IO - 同步，异步，阻塞，非阻塞</title>
    <link href="http://liujunming.github.io/2018/12/26/IO-%E5%90%8C%E6%AD%A5%EF%BC%8C%E5%BC%82%E6%AD%A5%EF%BC%8C%E9%98%BB%E5%A1%9E%EF%BC%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/"/>
    <id>http://liujunming.github.io/2018/12/26/IO-同步，异步，阻塞，非阻塞/</id>
    <published>2018-12-26T01:58:29.000Z</published>
    <updated>2018-12-26T06:57:52.226Z</updated>
    
    <content type="html"><![CDATA[<p>本文是对阻塞、非阻塞、同步、异步、I/O多路复用的总结。<a id="more"></a> </p><h2 id="1-阻塞-I-O（blocking-IO）"><a href="#1-阻塞-I-O（blocking-IO）" class="headerlink" title="1 阻塞 I/O（blocking IO）"></a>1 阻塞 I/O（blocking IO）</h2><p><img src="/images/2018/12/53.png" alt=""><br>blocking IO的特点就是在IO执行的两个阶段都被block了。</p><h2 id="2-非阻塞-I-O（nonblocking-IO）"><a href="#2-非阻塞-I-O（nonblocking-IO）" class="headerlink" title="2 非阻塞 I/O（nonblocking IO）"></a>2 非阻塞 I/O（nonblocking IO）</h2><p><img src="/images/2018/12/54.png" alt=""><br>nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。</p><h2 id="3-I-O-多路复用（-I-O-multiplexing）"><a href="#3-I-O-多路复用（-I-O-multiplexing）" class="headerlink" title="3 I/O 多路复用（ I/O multiplexing）"></a>3 I/O 多路复用（ I/O multiplexing）</h2><p><img src="/images/2018/12/55.png" alt=""><br>I/O multiplexing本质上是多条I/O路径共用同一个线程。<br>详情可以参考<a href="https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/" target="_blank" rel="noopener">select, poll, and epoll</a></p><h2 id="4-异步-I-O（asynchronous-IO）"><a href="#4-异步-I-O（asynchronous-IO）" class="headerlink" title="4 异步 I/O（asynchronous IO）"></a>4 异步 I/O（asynchronous IO）</h2><p><img src="/images/2018/12/56.png" alt=""><br>异步I/O通知可以采取两种方式：</p><ol><li>使用信号进行异步通知，如上图所示，示例可以参考<a href="http://www.informit.com/articles/article.aspx?p=607373&amp;seqNum=4" target="_blank" rel="noopener">POSIX Asynchronous I/O</a></li><li>使用回调函数进行异步通知，示例可以参考<a href="https://blog.csdn.net/Shreck66/article/details/48765533" target="_blank" rel="noopener">linux下aio异步读写详解与实例</a>和<a href="https://stackoverflow.com/questions/5153972/unix-linux-signal-handling-sigev-thread" target="_blank" rel="noopener">UNIX/Linux signal handling: SIGEV_THREAD</a></li></ol><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><h3 id="5-1-blocking和non-blocking的区别"><a href="#5-1-blocking和non-blocking的区别" class="headerlink" title="5.1 blocking和non-blocking的区别"></a>5.1 blocking和non-blocking的区别</h3><p>调用blocking IO会一直block住对应的进程，直到操作完成；而non-blocking IO在kernel还未准备好数据的情况下会立刻返回。</p><h3 id="5-2-synchronous-IO和asynchronous-IO的区别"><a href="#5-2-synchronous-IO和asynchronous-IO的区别" class="headerlink" title="5.2 synchronous IO和asynchronous IO的区别"></a>5.2 synchronous IO和asynchronous IO的区别</h3><p>在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的：</p><ul><li>A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;</li><li>An asynchronous I/O operation does not cause the requesting process to be blocked;</li></ul><p>两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。</p><p>有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的<code>read</code>这个system call。non-blocking IO在执行<code>read</code>这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，<code>read</code>会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。</p><p>而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。</p><h3 id="5-3-各个IO-Model的比较"><a href="#5-3-各个IO-Model的比较" class="headerlink" title="5.3 各个IO Model的比较"></a>5.3 各个IO Model的比较</h3><p><img src="/images/2018/12/57.png" alt=""><br>通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用<code>read</code>来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知或者调用回调函数。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。</p><hr><p>参考资料：</p><ol><li><a href="https://segmentfault.com/a/1190000003063859" target="_blank" rel="noopener">Linux IO模式及 select、poll、epoll详解</a></li><li><a href="https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/" target="_blank" rel="noopener">select, poll, and epoll</a></li><li><a href="http://davmac.org/davpage/linux/async-io.html" target="_blank" rel="noopener">Asynchronous I/O and event notification on linux</a></li><li><a href="https://fwheel.net/aio.html" target="_blank" rel="noopener">Asynchronous File I/O on Linux</a></li><li><a href="https://blog.csdn.net/Shreck66/article/details/48765533" target="_blank" rel="noopener">linux下aio异步读写详解与实例</a></li><li><a href="https://www.systutorials.com/docs/linux/man/7-aio/" target="_blank" rel="noopener">aio (7) - Linux Man Pages</a></li><li><a href="https://stackoverflow.com/questions/5153972/unix-linux-signal-handling-sigev-thread" target="_blank" rel="noopener">UNIX/Linux signal handling: SIGEV_THREAD</a></li><li><a href="http://www.informit.com/articles/article.aspx?p=607373&amp;seqNum=4" target="_blank" rel="noopener">POSIX Asynchronous I/O</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是对阻塞、非阻塞、同步、异步、I/O多路复用的总结。
    
    </summary>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/categories/I-O%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="I/O系统" scheme="http://liujunming.github.io/tags/I-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Signals</title>
    <link href="http://liujunming.github.io/2018/12/25/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Signals/"/>
    <id>http://liujunming.github.io/2018/12/25/apue-读书笔记-Signals/</id>
    <published>2018-12-25T01:42:04.000Z</published>
    <updated>2018-12-25T05:54:14.341Z</updated>
    
    <content type="html"><![CDATA[<p>Signals are software interrupts. Most nontrivial application programs need to deal with signals. Signals provide a way of handling asynchronous events—for example, a user at a terminal typing the interrupt key to stop a program.<a id="more"></a> </p><h2 id="1-Signal-Concepts"><a href="#1-Signal-Concepts" class="headerlink" title="1 Signal Concepts"></a>1 Signal Concepts</h2><p>First, every signal has a name. These names all begin with the three characters <code>SIG</code>. For example, <code>SIGABRT</code> is the abort signal that is generated when a process calls the <code>abort</code> function. <code>SIGALRM</code> is the alarm signal that is generated when the timer set by the <code>alarm</code> function goes off.</p><p>Signal names are all defined by positive integer constants (the signal number) in the header <code>&lt;signal.h&gt;</code>.</p><p>Numerous conditions can generate a signal:</p><ul><li>The terminal-generated signals occur when users press certain terminal keys. Pressing the Control-C key on the terminal normally causes the interrupt signal (<code>SIGINT</code>) to be generated. This is how to stop a runaway program.</li><li>Hardware exceptions generate signals: divide by 0, invalid memory reference, and the like. These conditions are usually detected by the hardware, and the kernel is notified. The kernel then generates the appropriate signal for the process that was running at the time the condition occurred. For example, <code>SIGSEGV</code> is generated for a process that executes an invalid memory reference.</li><li>The <code>kill</code> function allows a process to send any signal to another process or process group. Naturally, there are limitations: we have to be the owner of the process that we’re sending the signal to, or we have to be the superuser.</li><li>The <code>kill</code> command allows us to send signals to other processes. This command is often used to terminate a runaway background process.</li><li>Software conditions can generate signals when a process should be notified of various events. </li></ul><p>Signals are classic examples of asynchronous events. They occur at what appear to be random times to the process. The process can’t simply test a variable (such as <code>errno</code>) to see whether a signal has occurred; instead, the process has to tell the kernel ‘‘if and when this signal occurs, do the following.’’</p><p>We can tell the kernel to do one of three things when a signal occurs. We call this the <code>disposition</code> of the signal, or the <code>action</code> associated with a signal.</p><ol><li>Ignore the signal. </li><li>Catch the signal. To do this, we tell the kernel to call a function of ours whenever the signal occurs. In our function, we can do whatever we want to handle the condition.</li><li>Let the default action apply. Every signal has a default action, shown in Figure 10.1. Note that the default action for most signals is to terminate the process.</li></ol><p><img src="/images/2018/12/52.png" alt=""></p><p>When the default action is labeled ‘‘terminate+core,’’ it means that a memory image of the process is left in the file named <code>core</code> of the current working directory of the process. This file can be used with most UNIX System debuggers to examine the state of the process at the time it terminated.<br><a href="https://book.douban.com/annotation/15103265/" target="_blank" rel="noopener">core笔记</a></p><h2 id="2-signal-Function"><a href="#2-signal-Function" class="headerlink" title="2 signal Function"></a>2 <code>signal</code> Function</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> (*signal(<span class="keyword">int</span> signo, <span class="keyword">void</span> (*func)(<span class="keyword">int</span>)))(<span class="keyword">int</span>);</span><br></pre></td></tr></table></figure><p>The <code>signo</code> argument is just the name of the signal from Figure 10.1. The value of <code>func</code> is (a) the constant <code>SIG_IGN</code>, (b) the constant <code>SIG_DFL</code>, or (c) the address of a function to be called when the signal occurs. If we specify <code>SIG_IGN</code>, we are telling the system to ignore the signal. (Remember that we cannot ignore the two signals <code>SIGKILL</code> and <code>SIGSTOP</code>.) When we specify <code>SIG_DFL</code>, we are setting the action associated with the signal to its default value. When we specify the address of a function to be called when the signal occurs, we are arranging to ‘‘catch’’ the signal. We call the function either the <em>signal handler</em> or the <em>signal-catching function</em>.</p><h2 id="3-Unreliable-Signals"><a href="#3-Unreliable-Signals" class="headerlink" title="3 Unreliable Signals"></a>3 Unreliable Signals</h2><p>Signals were unreliable. By this we mean that signals could get lost: a signal could occur and the process would never know about it.</p><h2 id="4-Interrupted-System-Calls"><a href="#4-Interrupted-System-Calls" class="headerlink" title="4 Interrupted System Calls"></a>4 Interrupted System Calls</h2><h2 id="5-Reentrant-Functions"><a href="#5-Reentrant-Functions" class="headerlink" title="5 Reentrant Functions"></a>5 Reentrant Functions</h2><p><a href="https://stackoverflow.com/questions/34758863/what-is-reentrant-function-in-c" target="_blank" rel="noopener">re-entrancy</a></p><h2 id="6-Reliable-Signal-Terminology-and-Semantics"><a href="#6-Reliable-Signal-Terminology-and-Semantics" class="headerlink" title="6 Reliable-Signal Terminology and Semantics"></a>6 Reliable-Signal Terminology and Semantics</h2><p>First, a signal is <em>generated</em> for a process (or sent to a process) when the event that causes the signal occurs. The event could be a hardware exception (e.g., divide by 0), a software condition (e.g., an <code>alarm</code> timer expiring), a terminal-generated signal, or a call to the <code>kill</code> function. When the signal is generated, the kernel usually sets a flag of some form in the process table.</p><p>We say that a signal is <em>delivered</em> to a process when the action for a signal is taken. During the time between the generation of a signal and its delivery, the signal is said to be <em>pending</em>.</p><p>A process has the option of <em>blocking</em> the delivery of a signal. If a signal that is blocked is generated for a process, and if the action for that signal is either the default action or to catch the signal, then the signal remains pending for the process until the process either (a) unblocks the signal or (b) changes the action to ignore the signal.  The system determines what to do with a blocked signal when the signal is delivered, not when it’s generated.This allows the process to change the action for the signal before it’s delivered. The <code>sigpending</code> function  can be called by a process to determine which signals are blocked and pending.</p><p>Each process has a signal mask that defines the set of signals currently blocked from delivery to that process. We can think of this mask as having one bit for each possible signal. If the bit is on for a given signal, that signal is currently blocked.A process can examine and change its current signal mask by calling <code>sigprocmask</code>.</p><p>Since it is possible for the number of signals to exceed the number of bits in an integer, POSIX.1 defines a data type, called <code>sigset_t</code>, that holds a <em>signal set</em>. The signal mask, for example, is stored in one of these signal sets.</p><h2 id="7-kill-and-raise-Functions"><a href="#7-kill-and-raise-Functions" class="headerlink" title="7 kill and raise Functions"></a>7 <code>kill</code> and <code>raise</code> Functions</h2><p>The <code>kill</code> function sends a signal to a process or a group of processes. The <code>raise</code> function allows a process to send a signal to itself.</p><h2 id="8-alarm-and-pause-Functions"><a href="#8-alarm-and-pause-Functions" class="headerlink" title="8 alarm and pause Functions"></a>8 <code>alarm</code> and <code>pause</code> Functions</h2><p>The <code>alarm</code> function allows us to set a timer that will expire at a specified time in the future. When the timer expires, the <code>SIGALRM</code> signal is generated. If we ignore or don’t catch this signal, its default action is to terminate the process.</p><p>The <code>pause</code> function suspends the calling process until a signal is caught.</p><h2 id="9-Signal-Sets"><a href="#9-Signal-Sets" class="headerlink" title="9 Signal Sets"></a>9 Signal Sets</h2><h2 id="10-sigprocmask-Function"><a href="#10-sigprocmask-Function" class="headerlink" title="10 sigprocmask Function"></a>10 <code>sigprocmask</code> Function</h2><h2 id="11-sigpending-Function"><a href="#11-sigpending-Function" class="headerlink" title="11 sigpending Function"></a>11 <code>sigpending</code> Function</h2><h2 id="12-sigaction-Function"><a href="#12-sigaction-Function" class="headerlink" title="12 sigaction Function"></a>12 <code>sigaction</code> Function</h2><p>The <code>sigaction</code> function allows us to examine or modify (or both) the action associated with a particular signal.</p><h2 id="13-sigsetjmp-and-siglongjmp-Functions"><a href="#13-sigsetjmp-and-siglongjmp-Functions" class="headerlink" title="13 sigsetjmp and siglongjmp Functions"></a>13 <code>sigsetjmp</code> and <code>siglongjmp</code> Functions</h2><h2 id="14-sigsuspend-Function"><a href="#14-sigsuspend-Function" class="headerlink" title="14 sigsuspend Function"></a>14 <code>sigsuspend</code> Function</h2><h2 id="15-abort-Function"><a href="#15-abort-Function" class="headerlink" title="15 abort Function"></a>15 <code>abort</code> Function</h2><h2 id="16-system-Function"><a href="#16-system-Function" class="headerlink" title="16 system Function"></a>16 <code>system</code> Function</h2><h2 id="17-sleep-nanosleep-and-clock-nanosleep-Functions"><a href="#17-sleep-nanosleep-and-clock-nanosleep-Functions" class="headerlink" title="17 sleep, nanosleep, and clock_nanosleep Functions"></a>17 <code>sleep</code>, <code>nanosleep</code>, and <code>clock_nanosleep</code> Functions</h2><h2 id="18-sigqueue-Function"><a href="#18-sigqueue-Function" class="headerlink" title="18 sigqueue Function"></a>18 <code>sigqueue</code> Function</h2><h2 id="19-Job-Control-Signals"><a href="#19-Job-Control-Signals" class="headerlink" title="19 Job-Control Signals"></a>19 Job-Control Signals</h2><h2 id="20-Signal-Names-and-Numbers"><a href="#20-Signal-Names-and-Numbers" class="headerlink" title="20 Signal Names and Numbers"></a>20 Signal Names and Numbers</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Signals are software interrupts. Most nontrivial application programs need to deal with signals. Signals provide a way of handling asynchronous events—for example, a user at a terminal typing the interrupt key to stop a program.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Process Relationships</title>
    <link href="http://liujunming.github.io/2018/12/24/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Relationships/"/>
    <id>http://liujunming.github.io/2018/12/24/apue-读书笔记-Process-Relationships/</id>
    <published>2018-12-24T02:13:44.000Z</published>
    <updated>2018-12-25T01:52:01.137Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we’ll look at process groups in more detail and the concept of sessions that was introduced by POSIX.1. We’ll also look at the relationship between the login shell that is invoked for us when we login and all the processes that we start from our login shell.<a id="more"></a> </p><h2 id="1-Terminal-Logins"><a href="#1-Terminal-Logins" class="headerlink" title="1 Terminal Logins"></a>1 Terminal Logins</h2><p>Let’s start by looking at the programs that are executed when we login to a UNIX system.</p><p><strong>BSD Terminal Logins</strong><br>The system administrator creates a file, usually <code>/etc/ttys</code>, that has one line per terminal device.Each line specifies the name of the device and other parameters that are passed to the <code>getty</code> program. When the system is bootstrapped, the kernel creates process ID 1, the <code>init</code> process, and it is <code>init</code> that brings the system up in multiuser mode.The <code>init</code> process reads the file <code>/etc/ttys</code> and, for every terminal device that allows a login, does a <code>fork</code> followed by an <code>exec</code> of the program <code>getty</code>. This gives us the processes shown in Figure 9.1.<br><img src="/images/2018/12/47.png" alt=""></p><p>It is <code>getty</code> that calls <code>open</code> for the terminal device. The terminal is opened for reading and writing.Once the device is open, file descriptors 0, 1, and 2 are set to the device. Then <code>getty</code> outputs something like <code>login:</code> and waits for us to enter our user name.<br>When we enter our user name, <code>getty</code>’s job is complete, and it then invokes the <code>login</code> program, similar to:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">execle(<span class="string">"/bin/login"</span>, <span class="string">"login"</span>, <span class="string">"-p"</span>, username, (<span class="keyword">char</span> *)<span class="number">0</span>, envp);</span><br></pre></td></tr></table></figure></p><p>Figure 9.2 shows the state of these processes right after <code>login</code> has been invoked.<br><img src="/images/2018/12/48.png" alt=""><br><img src="/images/2018/12/49.png" alt=""></p><h2 id="2-Network-Logins"><a href="#2-Network-Logins" class="headerlink" title="2 Network Logins"></a>2 Network Logins</h2><p>The main (physical) difference between logging in to a system through a serial terminal and logging in to a system through a network is that the connection between the terminal and the computer isn’t point-to-point.</p><h2 id="3-Process-Groups"><a href="#3-Process-Groups" class="headerlink" title="3 Process Groups"></a>3 Process Groups</h2><p>In addition to having a process ID, each process belongs to a process group.</p><p>A process group is a collection of one or more processes, usually associated with the same job, that can receive signals from the same terminal. Each process group has a unique process group ID. Process group IDs are similar to process IDs: they are positive integers and can be stored in a <code>pid_t</code> data type.</p><p>Each process group can have a process group leader. The leader is identified by its process group ID being equal to its process ID.</p><p>It is possible for a process group leader to create a process group, create processes in the group, and then terminate.</p><p>A process joins an existing process group or creates a new process group by calling <code>setpgid</code>.</p><h2 id="4-Sessions"><a href="#4-Sessions" class="headerlink" title="4 Sessions"></a>4 Sessions</h2><p>A session is a collection of one or more process groups. For example, we could have the arrangement shown in Figure 9.6. Here we have three process groups in a single session.<br><img src="/images/2018/12/50.png" alt=""><br>The processes in a process group are usually placed there by a shell pipeline. For example, the arrangement shown in Figure 9.6 could have been generated by shell commands of the form<br><code>proc1 | proc2 &amp; proc3 | proc4 | proc5</code></p><p>A process establishes a new session by calling the <code>setsid</code> function.<br>If the calling process is not a process group leader, this function creates a new session. Three things happen.</p><ol><li>The process becomes the session leader of this new session. (A session leader is the process that creates a session.) The process is the only process in this new session.</li><li>The process becomes the process group leader of a new process group. The new process group ID is the process ID of the calling process.</li><li>The process has no controlling terminal. If the process had a controlling terminal before calling setsid, that association is broken.</li></ol><h2 id="5-Controlling-Terminal"><a href="#5-Controlling-Terminal" class="headerlink" title="5 Controlling Terminal"></a>5 Controlling Terminal</h2><p>Sessions and process groups have a few other characteristics.</p><ul><li>A session can have a single <em>controlling terminal</em>. This is usually the terminal device (in the case of a terminal login) or pseudo terminal device (in the case of a network login) on which we login.</li><li>The session leader that establishes the connection to the controlling terminal is called the <em>controlling process</em>.</li><li>The process groups within a session can be divided into a single <em>foreground process group</em> and one or more <em>background process groups</em>.</li><li>If a session has a controlling terminal, it has a single foreground process group and all other process groups in the session are background process groups.</li><li>Whenever we press the terminal’s interrupt key (often Control-C), the interrupt signal is sent to all processes in the foreground process group.</li><li>Whenever we press the terminal’s quit key (often Control-backslash), the quit signal is sent to all processes in the foreground process group.</li><li>If a modem (or network) disconnect is detected by the terminal interface, the hang-up signal is sent to the controlling process (the session leader).</li></ul><p>These characteristics are shown in Figure 9.7.<br><img src="/images/2018/12/51.png" alt=""></p><h2 id="6-tcgetpgrp-tcsetpgrp-and-tcgetsid-Functions"><a href="#6-tcgetpgrp-tcsetpgrp-and-tcgetsid-Functions" class="headerlink" title="6 tcgetpgrp, tcsetpgrp, and tcgetsid Functions"></a>6 <code>tcgetpgrp</code>, <code>tcsetpgrp</code>, and <code>tcgetsid</code> Functions</h2><p>We need a way to tell the kernel which process group is the foreground process group, so that the terminal device driver knows where to send the terminal input and the terminal-generated signals (Figure 9.7).</p><h2 id="7-Job-Control"><a href="#7-Job-Control" class="headerlink" title="7 Job Control"></a>7 Job Control</h2><p>Job control allows us to start multiple jobs (groups of processes) from a single terminal and to control which jobs can access the terminal and which jobs are run in the background. </p><h2 id="8-Shell-Execution-of-Programs"><a href="#8-Shell-Execution-of-Programs" class="headerlink" title="8 Shell Execution of Programs"></a>8 Shell Execution of Programs</h2><p>Let’s examine how the shells execute programs and how this relates to the concepts of process groups, controlling terminals, and sessions.</p><h2 id="9-Orphaned-Process-Groups"><a href="#9-Orphaned-Process-Groups" class="headerlink" title="9 Orphaned Process Groups"></a>9 Orphaned Process Groups</h2><p>We’ve mentioned that a process whose parent terminates is called an orphan and is inherited by the <code>init</code> process. We now look at entire process groups that can be orphaned and see how POSIX.1 handles this situation.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we’ll look at process groups in more detail and the concept of sessions that was introduced by POSIX.1. We’ll also look at the relationship between the login shell that is invoked for us when we login and all the processes that we start from our login shell.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记- Process Control</title>
    <link href="http://liujunming.github.io/2018/12/23/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Control/"/>
    <id>http://liujunming.github.io/2018/12/23/apue-读书笔记-Process-Control/</id>
    <published>2018-12-23T05:36:08.000Z</published>
    <updated>2018-12-24T10:25:33.274Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Process-Identifiers"><a href="#1-Process-Identifiers" class="headerlink" title="1 Process Identifiers"></a>1 Process Identifiers</h2><p>Every process has a unique process ID, a non-negative integer. The process ID is the only well-known identifier of a process that is always unique.<a id="more"></a> </p><p>Process ID 0 is usually the scheduler process.<br>Process ID 1 is usually the <code>init</code> process and is invoked by the kernel at the end of the bootstrap procedure. </p><h2 id="2-fork-Function"><a href="#2-fork-Function" class="headerlink" title="2 fork Function"></a>2 <code>fork</code> Function</h2><p>An existing process can create a new one by calling the <code>fork</code> function.<br>The new process created by <code>fork</code> is called the <em>child process</em>. This function is called once but returns twice. The only difference in the returns is that the return value in the child is 0, whereas the return value in the parent is the process ID of the new child.</p><p><strong>File Sharing</strong><br>One characteristic of <code>fork</code> is that all file descriptors that are open in the parent are duplicated in the child. We say ‘‘duplicated’’ because it’s as if the <code>dup</code> function had been called for each descriptor. The parent and the child share a file table entry for every open descriptor.</p><p>Consider a process that has three different files opened for standard input, standard output, and standard error. On return from <code>fork</code>, we have the arrangement shown in Figure 8.2.<br><img src="/images/2018/12/45.png" alt=""><br>It is important that the parent and the child share the same file offset.</p><h2 id="3-exit-Functions"><a href="#3-exit-Functions" class="headerlink" title="3 exit Functions"></a>3 <code>exit</code> Functions</h2><p>Regardless of how a process terminates, the same code in the kernel is eventually executed. This kernel code closes all the open descriptors for the process, releases the memory that it was using, and so on.</p><p>For any of the preceding cases, we want the terminating process to be able to notify its parent how it terminated. For the three exit functions (<code>exit</code>, <code>_exit</code>, and <code>_Exit</code>), this is done by passing an exit status as the argument to the function. In the case of an abnormal termination, however, the kernel—not the process—generates a termination status to indicate the reason for the abnormal termination. In any case, the parent of the process can obtain the termination status from either the <code>wait</code> or the <code>waitpid</code> function.</p><p>What happens if the parent terminates before the child? The answer is that the <code>init</code> process becomes the parent process of any process whose parent terminates. In such a case, we say that the process has been inherited by init.</p><p>Another condition we have to worry about is when a child terminates before its parent. If the child completely disappeared, the parent wouldn’t be able to fetch its termination status when and if the parent was finally ready to check if the child had terminated. The kernel keeps a small amount of information for every terminating process, so that the information is available when the parent of the terminating process calls <code>wait</code> or <code>waitpid</code>. Minimally, this information consists of the process ID, the termination status of the process, and the amount of CPU time taken by the process. The kernel can discard all the memory used by the process and close its open files. In UNIX System terminology, a process that has terminated, but whose parent has not yet waited for it, is called a <em>zombie</em>. The <code>ps</code> command prints the state of a zombie process as Z.</p><p>The final condition to consider is this: What happens when a process that has been inherited by <code>init</code> terminates? Does it become a zombie? The answer is ‘‘no,’’ because <code>init</code> is written so that whenever one of its children terminates, <code>init</code> calls one of the wait functions to fetch the termination status. By doing this, <code>init</code> prevents the system from being clogged by zombies.</p><h2 id="4-wait-and-waitpid-Functions"><a href="#4-wait-and-waitpid-Functions" class="headerlink" title="4 wait and waitpid Functions"></a>4 <code>wait</code> and <code>waitpid</code> Functions</h2><p>When a process terminates, either normally or abnormally, the kernel notifies the parent by sending the <code>SIGCHLD</code> signal to the parent. Because the termination of a child is an asynchronous event—it can happen at any time while the parent is running—this signal is the asynchronous notification from the kernel to the parent. The parent can choose to ignore this signal, or it can provide a function that is called when the signal occurs: a signal handler. The default action for this signal is to be ignored. For now, we need to be aware that a process that calls <code>wait</code> or <code>waitpid</code> can:</p><ul><li>Block, if all of its children are still running</li><li>Return immediately with the termination status of a child, if a child has terminated and is waiting for its termination status to be fetched</li><li>Return immediately with an error, if it doesn’t have any child processes</li></ul><p>If the process is calling <code>wait</code> because it received the <code>SIGCHLD</code> signal, we expect <code>wait</code> to return immediately. But if we call it at any random point in time, it can block.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pid_t</span> wait(<span class="keyword">int</span> *statloc);</span><br><span class="line"><span class="keyword">pid_t</span> waitpid(<span class="keyword">pid_t</span> pid, <span class="keyword">int</span> *statloc, <span class="keyword">int</span> options);</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/12/46.png" alt=""></p><p>The <code>waitpid</code> function provides three features that aren’t provided by the <code>wait</code><br>function.</p><ol><li>The <code>waitpid</code> function lets us wait for one particular process, whereas the <code>wait</code> function returns the status of any terminated child.</li><li>The <code>waitpid</code> function provides a nonblocking version of <code>wait</code>. There are times when we want to fetch a child’s status, but we don’t want to block.</li><li>The <code>waitpid</code> function provides support for job control with the <code>WUNTRACED</code> and <code>WCONTINUED</code> options.</li></ol><h2 id="5-waitid-Function"><a href="#5-waitid-Function" class="headerlink" title="5 waitid Function"></a>5 <code>waitid</code> Function</h2><h2 id="6-wait3-and-wait4-Functions"><a href="#6-wait3-and-wait4-Functions" class="headerlink" title="6 wait3 and wait4 Functions"></a>6 <code>wait3</code> and <code>wait4</code> Functions</h2><h2 id="7-exec-Functions"><a href="#7-exec-Functions" class="headerlink" title="7 exec Functions"></a>7 <code>exec</code> Functions</h2><p>One use of the <code>fork</code> function is to create a new process (the child) that then causes another program to be executed by calling one of the <code>exec</code> functions. When a process calls one of the <code>exec</code> functions, that process is completely replaced by the new program, and the new program starts executing at its <code>main</code> function. The process ID does not change across an <code>exec</code>, because a new process is not created; <code>exec</code> merely replaces the current process—its text, data, heap, and stack segments—with a brand-new program from disk.</p><p>With <code>fork</code>, we can create new processes; and with the <code>exec</code> functions, we can initiate new programs. The <code>exit</code> function and the <code>wait</code> functions handle termination and waiting for termination. These are the only process control primitives we need.</p><h2 id="8-Changing-User-IDs-and-Group-IDs"><a href="#8-Changing-User-IDs-and-Group-IDs" class="headerlink" title="8 Changing User IDs and Group IDs"></a>8 Changing User IDs and Group IDs</h2><h2 id="9-Interpreter-Files"><a href="#9-Interpreter-Files" class="headerlink" title="9 Interpreter Files"></a>9 Interpreter Files</h2><h2 id="10-system-Function"><a href="#10-system-Function" class="headerlink" title="10 system Function"></a>10 <code>system</code> Function</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">system</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *cmdstring)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="11-Process-Accounting"><a href="#11-Process-Accounting" class="headerlink" title="11 Process Accounting"></a>11 Process Accounting</h2><p>Most UNIX systems provide an option to do process accounting. When enabled, the kernel writes an accounting record each time a process terminates. These accounting records typically contain a small amount of binary data with the name of the command, the amount of CPU time used, the user ID and group ID, the starting time, and so on.</p><h2 id="12-User-Identification"><a href="#12-User-Identification" class="headerlink" title="12 User Identification"></a>12 User Identification</h2><h2 id="13-Process-Scheduling"><a href="#13-Process-Scheduling" class="headerlink" title="13 Process Scheduling"></a>13 Process Scheduling</h2><p>The UNIX System provided processes with only coarse control over their scheduling priority. The scheduling policy and priority were determined by the kernel. A process could choose to run with lower priority by adjusting its <em>nice value</em> (thus a process could be ‘‘nice’’ and reduce its share of the CPU by adjusting its nice value). Only a privileged process was allowed to increase its scheduling priority.</p><h2 id="14-Process-Times"><a href="#14-Process-Times" class="headerlink" title="14 Process Times"></a>14 Process Times</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">clock_t</span> times(struct tms *buf);</span><br><span class="line"><span class="comment">//Returns: elapsed wall clock time in clock ticks if OK, −1 on error</span></span><br></pre></td></tr></table></figure><p>This function fills in the <code>tms</code> structure pointed to by <code>buf</code> :<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">tms</span> &#123;</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_utime;  <span class="comment">/* user CPU time */</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_stime;  <span class="comment">/* system CPU time */</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_cutime; <span class="comment">/* user CPU time, terminated children */</span></span><br><span class="line">    <span class="keyword">clock_t</span>  tms_cstime; <span class="comment">/* system CPU time, terminated children */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Process-Identifiers&quot;&gt;&lt;a href=&quot;#1-Process-Identifiers&quot; class=&quot;headerlink&quot; title=&quot;1 Process Identifiers&quot;&gt;&lt;/a&gt;1 Process Identifiers&lt;/h2&gt;&lt;p&gt;Every process has a unique process ID, a non-negative integer. The process ID is the only well-known identifier of a process that is always unique.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Process Environment</title>
    <link href="http://liujunming.github.io/2018/12/23/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Process-Environment/"/>
    <id>http://liujunming.github.io/2018/12/23/apue-读书笔记-Process-Environment/</id>
    <published>2018-12-23T03:42:28.000Z</published>
    <updated>2018-12-23T05:45:27.532Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we’ll see how the <code>main</code> function is called when the program is executed, how command-line arguments are passed to the new program, what the typical memory layout looks like, how to allocate additional memory, how the process can use environment variables, and various ways for the process to terminate. Additionally, we’ll look at the <code>longjmp</code> and <code>setjmp</code> functions and their interaction with the stack. We finish the chapter by examining the resource limits of a process.<a id="more"></a> </p><h2 id="1-main-Function"><a href="#1-main-Function" class="headerlink" title="1 main Function"></a>1 <code>main</code> Function</h2><p>A C program starts execution with a function called <code>main</code>. The prototype for the <code>main</code> function is<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="2-Process-Termination"><a href="#2-Process-Termination" class="headerlink" title="2 Process Termination"></a>2 Process Termination</h2><p>There are eight ways for a process to terminate. Normal termination occurs in five ways:</p><ol><li>Return from <code>main</code></li><li>Calling <code>exit</code></li><li>Calling <code>_exit</code> or <code>_Exit</code></li><li>Return of the last thread from its start routine</li><li>Calling <code>pthread_exit</code> from the last thread</li></ol><p>Abnormal termination occurs in three ways:</p><ol start="6"><li>Calling <code>abort</code> </li><li>Receipt of a signal </li><li>Response of the last thread to a cancellation request </li></ol><h3 id="2-1-Exit-Functions"><a href="#2-1-Exit-Functions" class="headerlink" title="2.1 Exit Functions"></a>2.1 Exit Functions</h3><p>Three functions terminate a program normally: <code>_exit</code> and <code>_Exit</code>, which return to the kernel immediately, and <code>exit</code>, which performs certain cleanup processing and then returns to the kernel.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">exit</span><span class="params">(<span class="keyword">int</span> status)</span></span>; </span><br><span class="line"><span class="keyword">void</span> _Exit(<span class="keyword">int</span> status);</span><br><span class="line"><span class="keyword">void</span> _exit(<span class="keyword">int</span> status);</span><br></pre></td></tr></table></figure></p><p>Historically, the <code>exit</code> function has always performed a clean shutdown of the standard I/O library: the <code>fclose</code> function is called for all open streams.</p><p><code>exit(0)</code> is the same as <code>return(0)</code> from the main function.</p><h3 id="2-2-atexit-Function"><a href="#2-2-atexit-Function" class="headerlink" title="2.2 atexit Function"></a>2.2 <code>atexit</code> Function</h3><p>With ISO C, a process can register at least 32 functions that are automatically called by <code>exit</code>. These are called <em>exit handlers</em> and are registered by calling the <code>atexit</code> function.<br><img src="/images/2018/12/42.png" alt=""></p><h2 id="3-Command-Line-Arguments"><a href="#3-Command-Line-Arguments" class="headerlink" title="3 Command-Line Arguments"></a>3 Command-Line Arguments</h2><p>When a program is executed, the process that does the <code>exec</code> can pass command-line arguments to the new program.</p><h2 id="4-Environment-List"><a href="#4-Environment-List" class="headerlink" title="4 Environment List"></a>4 Environment List</h2><p>Each program is also passed an <em>environment list</em>. Like the argument list, the environment list is an array of character pointers, with each pointer containing the address of a null-terminated C string. The address of the array of pointers is contained in the global variable <code>environ</code>:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="keyword">char</span> **environ;</span><br></pre></td></tr></table></figure></p><p>For example, if the environment consisted of five strings, it could look like Figure 7.5. Here we explicitly show the null bytes at the end of each string. We’ll call <code>environ</code> the <em>environment pointer</em>, the array of pointers the environment list, and the strings they point to the <em>environment strings</em>.<br><img src="/images/2018/12/43.png" alt=""></p><h2 id="5-Memory-Layout-of-a-C-Program"><a href="#5-Memory-Layout-of-a-C-Program" class="headerlink" title="5 Memory Layout of a C Program"></a>5 Memory Layout of a C Program</h2><p><img src="/images/2018/12/44.png" alt=""></p><h2 id="6-Shared-Libraries"><a href="#6-Shared-Libraries" class="headerlink" title="6 Shared Libraries"></a>6 Shared Libraries</h2><p>Shared libraries remove the common library routines from the executable file, instead maintaining a single copy of the library routine somewhere in memory that all processes reference.This reduces the size of each executable file but may add some runtime overhead, either when the program is first executed or the first time each shared library function is called. Another advantage of shared libraries is that library functions can be replaced with new versions without having to relink edit every program that uses the library (assuming that the number and type of arguments haven’t changed).</p><h2 id="7-Memory-Allocation"><a href="#7-Memory-Allocation" class="headerlink" title="7 Memory Allocation"></a>7 Memory Allocation</h2><p>ISO C specifies three functions for memory allocation:</p><ol><li><code>malloc</code>, which allocates a specified number of bytes of memory. The initial value of the memory is indeterminate.</li><li><code>calloc</code>, which allocates space for a specified number of objects of a specified size. The space is initialized to all 0 bits.</li><li><code>realloc</code>, which increases or decreases the size of a previously allocated area. </li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">malloc</span><span class="params">(<span class="keyword">size_t</span> size)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">calloc</span><span class="params">(<span class="keyword">size_t</span> nobj, <span class="keyword">size_t</span> size)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">realloc</span><span class="params">(<span class="keyword">void</span> *ptr, <span class="keyword">size_t</span> newsize)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="8-Environment-Variables"><a href="#8-Environment-Variables" class="headerlink" title="8 Environment Variables"></a>8 Environment Variables</h2><p> the environment strings are usually of the form:</p><p> <code>name=value</code></p><p>The UNIX kernel never looks at these strings; their interpretation is up to the various applications. The shells, for example, use numerous environment variables. Some, such as <code>HOME</code> and <code>USER</code>, are set automatically at login; others are left for us to set. We normally set environment variables in a shell start-up file to control the shell’s actions.</p><p>ISO C defines a function that we can use to fetch values from the environment.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">getenv</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p>In addition to fetching the value of an environment variable, sometimes we may want to set an environment variable.</p><h2 id="9-setjmp-and-longjmp-Functions"><a href="#9-setjmp-and-longjmp-Functions" class="headerlink" title="9 setjmp and longjmp Functions"></a>9 <code>setjmp</code> and <code>longjmp</code> Functions</h2><p>In C, we can’t <code>goto</code> a label that’s in another function. Instead, we must use the <code>setjmp</code> and <code>longjmp</code> functions to perform this type of branching.</p><h2 id="10-getrlimit-and-setrlimit-Functions"><a href="#10-getrlimit-and-setrlimit-Functions" class="headerlink" title="10 getrlimit and setrlimit Functions"></a>10 <code>getrlimit</code> and <code>setrlimit</code> Functions</h2><p>Every process has a set of resource limits, some of which can be queried and changed by the <code>getrlimit</code> and <code>setrlimit</code> functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getrlimit</span><span class="params">(<span class="keyword">int</span> resource, struct rlimit *rlptr)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">setrlimit</span><span class="params">(<span class="keyword">int</span> resource, <span class="keyword">const</span> struct rlimit *rlptr)</span></span>;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we’ll see how the &lt;code&gt;main&lt;/code&gt; function is called when the program is executed, how command-line arguments are passed to the new program, what the typical memory layout looks like, how to allocate additional memory, how the process can use environment variables, and various ways for the process to terminate. Additionally, we’ll look at the &lt;code&gt;longjmp&lt;/code&gt; and &lt;code&gt;setjmp&lt;/code&gt; functions and their interaction with the stack. We finish the chapter by examining the resource limits of a process.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title> apue 读书笔记-System Data Files and Information</title>
    <link href="http://liujunming.github.io/2018/12/21/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-System-Data-Files-and-Information/"/>
    <id>http://liujunming.github.io/2018/12/21/apue-读书笔记-System-Data-Files-and-Information/</id>
    <published>2018-12-21T02:52:39.000Z</published>
    <updated>2018-12-21T10:08:25.988Z</updated>
    
    <content type="html"><![CDATA[<p>A UNIX system requires numerous data files for normal operation: the password file <code>/etc/passwd</code> and the group file <code>/etc/group</code> are two files that are frequently used by various programs. <a id="more"></a> </p><h2 id="1-Password-File"><a href="#1-Password-File" class="headerlink" title="1 Password File"></a>1 Password File</h2><p>The UNIX System’s password file contains the fields shown in Figure 6.1. These fields are contained in a <code>passwd</code> structure that is defined in <code>&lt;pwd.h&gt;</code>.<br><img src="/images/2018/12/35.png" alt=""><br>Historically, the password file has been stored in /etc/passwd and has been an ASCII file. Each line contains the fields described in Figure 6.1, separated by colons. For example, four lines from the <code>/etc/passwd</code> file on Linux could be:<br><img src="/images/2018/12/36.png" alt=""></p><ul><li>There is usually an entry with the user name <code>root</code>. This entry has a user ID of 0 (the superuser).</li><li>The encrypted password field contains a single character as a placeholder.</li><li>Some fields in a password file entry can be empty. If the encrypted password field is empty, it usually means that the user does not have a password. (This is not recommended.)</li><li>The shell field contains the name of the executable program to be used as the login shell for the user.</li><li>The <code>nobody</code> user name can be used to allow people to log in to a system, but with a user ID (65534) and group ID (65534) that provide no privileges. </li></ul><p>POSIX.1 defines two functions to fetch entries from the password file. These functions allow us to look up an entry given a user’s login name or numerical user ID.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct passwd *<span class="title">getpwuid</span><span class="params">(<span class="keyword">uid_t</span> uid)</span></span>;</span><br><span class="line"><span class="function">struct passwd *<span class="title">getpwnam</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>getpwuid</code> function is used by the <code>ls</code> program to map the numerical user ID contained in an i-node into a user’s login name. The <code>getpwnam</code> function is used by the <code>login</code> program when we enter our login name.</p><p>These two POSIX.1 functions are fine if we want to look up either a login name or a user ID, but some programs need to go through the entire password file. Three functions can be used for this purpose: <code>getpwent</code>, <code>setpwent</code>, and <code>endpwent</code>.</p><h2 id="2-Shadow-Passwords"><a href="#2-Shadow-Passwords" class="headerlink" title="2 Shadow Passwords"></a>2 Shadow Passwords</h2><p>The encrypted password is a copy of the user’s password that has been put through a one-way encryption algorithm. Because this algorithm is one-way, we can’t guess the original password from the encrypted version.</p><p>Linux store the encrypted password in another file, often called the <em>shadow password file</em>. Minimally, this file has to contain the user name and the encrypted password. Other information relating to the password is also stored here.<br><img src="/images/2018/12/37.png" alt=""></p><h2 id="3-Group-File"><a href="#3-Group-File" class="headerlink" title="3 Group File"></a>3 Group File</h2><p>The UNIX System’s group file contains the fields shown in Figure 6.4. These fields are contained in a <code>group</code> structure that is defined in <code>&lt;grp.h&gt;</code>.<br><img src="/images/2018/12/38.png" alt="">.</p><p>The field <code>gr_mem</code> is an array of pointers to the user names that belong to this group. This array is terminated by a null pointer.</p><p>We can look up either a group name or a numerical group ID with the following two functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct group *<span class="title">getgrgid</span><span class="params">(<span class="keyword">gid_t</span> gid)</span></span>;</span><br><span class="line"><span class="function">struct group *<span class="title">getgrnam</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p>If we want to search the entire group file, we need some additional functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct group *<span class="title">getgrent</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setgrent</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">endgrent</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="4-Supplementary-Group-IDs"><a href="#4-Supplementary-Group-IDs" class="headerlink" title="4 Supplementary Group IDs"></a>4 Supplementary Group IDs</h2><p>Not only did we belong to the group corresponding to the group ID in our password file entry, but we could also belong to as many as 16 additional groups. The file access permission checks were modified so that in addition to comparing the the file’s group ID to the process effective group ID, it was also compared to all the supplementary group IDs.<br>The advantage of using supplementary group IDs is that we no longer have to change groups explicitly. It is common to belong to multiple groups (i.e., participate in multiple projects) at the same time.</p><p>Three functions are provided to fetch and set the supplementary group IDs.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getgroups</span><span class="params">(<span class="keyword">int</span> gidsetsize, <span class="keyword">gid_t</span> grouplist[])</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">setgroups</span><span class="params">(<span class="keyword">int</span> ngroups, <span class="keyword">const</span> <span class="keyword">gid_t</span> grouplist[])</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">initgroups</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *username, <span class="keyword">gid_t</span> basegid)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="5-Other-Data-Files"><a href="#5-Other-Data-Files" class="headerlink" title="5 Other Data Files"></a>5 Other Data Files</h2><p>We’ve discussed only two of the system’s data files so far: the password file and the group file. Numerous other files are used by UNIX systems in normal day-to-day operation. </p><p>Figure 6.6 shows some of these routines, which are common to UNIX systems. In this figure, we show the functions for the password files and group file, which we discussed earlier in this chapter, and some of the networking functions. There are <code>get</code>, <code>set</code>, and <code>end</code> functions for all the data files in this figure.<br><img src="/images/2018/12/39.png" alt=""></p><h2 id="6-Login-Accounting"><a href="#6-Login-Accounting" class="headerlink" title="6 Login Accounting"></a>6 Login Accounting</h2><p>Two data files provided with most UNIX systems are the <code>utmp</code> file, which keeps track of all the users currently logged in, and the <code>wtmp</code> file, which keeps track of all logins and logouts.</p><h2 id="7-System-Identification"><a href="#7-System-Identification" class="headerlink" title="7 System Identification"></a>7 System Identification</h2><p>POSIX.1 defines the <code>uname</code> function to return information on the current host and operating system.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">uname</span><span class="params">(struct utsname *name)</span></span>;</span><br></pre></td></tr></table></figure></p><p><code>gethostname</code> function only return the name of the host.</p><h2 id="8-Time-and-Date-Routines"><a href="#8-Time-and-Date-Routines" class="headerlink" title="8 Time and Date Routines"></a>8 Time and Date Routines</h2><p>The <code>time</code> function returns the current time and date.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">time_t</span> time(<span class="keyword">time_t</span> *calptr);</span><br></pre></td></tr></table></figure></p><p>The real-time extensions to POSIX.1 added support for multiple system clocks.A clock is identified by the <code>clockid_t</code> type. Standard values are summarized in Figure 6.8.<br><img src="/images/2018/12/41.png" alt=""><br>The <code>clock_gettime</code> function can be used to get the time of the specified clock. The time is returned in a <code>timespec</code> structure which expresses time values in terms of seconds and nanoseconds.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">clock_gettime</span><span class="params">(<span class="keyword">clockid_t</span> clock_id, struct timespec *tsp)</span></span>;</span><br></pre></td></tr></table></figure></p><p>When the clock ID is set to <code>CLOCK_REALTIME</code>, the <code>clock_gettime</code> function provides similar functionality to the <code>time</code> function, except with <code>clock_gettime</code>, we might be able to get a higher-resolution time value if the system supports it.</p><p>Once we have the integer value that counts the number of seconds since the Epoch, we normally call a function to convert it to a broken-down time structure, and then call another function to generate a human-readable time and date. Figure 6.9 shows the relationships between the various time functions. (The three functions in this figure that are shown with dashed lines—<code>localtime</code>, <code>mktime</code>, and <code>strftime</code>—are all affected by the <code>TZ</code> environment variable. The dotted lines show how the calendar time is obtained from time-related structures.)<br><img src="/images/2018/12/40.png" alt=""><br>The two functions <code>localtime</code> and <code>gmtime</code> convert a calendar time into what’s called a broken-down time, a tm structure.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">tm</span> &#123;</span>      <span class="comment">/* a broken-down time */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_sec;   <span class="comment">/* seconds after the minute: [0 - 60] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_min;   <span class="comment">/* minutes after the hour: [0 - 59] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_hour;  <span class="comment">/* hours after midnight: [0 - 23] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_mday;  <span class="comment">/* day of the month: [1 - 31] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_mon;   <span class="comment">/* months since January: [0 - 11] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_year;  <span class="comment">/* years since 1900 */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_wday;  <span class="comment">/* days since Sunday: [0 - 6] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_yday;  <span class="comment">/* days since January 1: [0 - 365] */</span></span><br><span class="line">    <span class="keyword">int</span>  tm_isdst; <span class="comment">/* daylight saving time flag: &lt;0, 0, &gt;0 */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>The difference between <code>localtime</code> and <code>gmtime</code> is that the first converts the calendar time to the local time, taking into account the local time zone and daylight saving time flag, whereas the latter converts the calendar time into a broken-down time expressed as UTC.</p><p>The function <code>mktime</code> takes a broken-down time, expressed as a local time, and converts it into a <code>time_t</code> value.</p><p>The <code>strftime</code> function is a <code>printf-like</code> function for time values. It is complicated by the multitude of arguments available to customize the string it produces.</p><p>We mentioned that the three functions in Figure 6.9 with dashed lines were affected by the <code>TZ</code> environment variable:<code>localtime</code>,<code>mktime</code>,and <code>strftime</code>. If defined,the value of this environment variable is used by these functions instead of the default time zone. If the variable is defined to be a null string, such as <code>TZ=</code>, then UTC is normally used.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A UNIX system requires numerous data files for normal operation: the password file &lt;code&gt;/etc/passwd&lt;/code&gt; and the group file &lt;code&gt;/etc/group&lt;/code&gt; are two files that are frequently used by various programs.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Standard I/O Library</title>
    <link href="http://liujunming.github.io/2018/12/19/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Standard-I-O-Library/"/>
    <id>http://liujunming.github.io/2018/12/19/apue-读书笔记-Standard-I-O-Library/</id>
    <published>2018-12-19T03:50:28.000Z</published>
    <updated>2018-12-21T04:04:53.370Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we describe the standard I/O library. This library is specified by the ISO C standard because it has been implemented on many operating systems other than the UNIX System.<a id="more"></a> </p><p>The standard I/O library handles such details as buffer allocation and performing I/O in optimal-sized chunks, obviating our need to worry about using the correct block size.</p><h2 id="1-Streams-and-FILE-Objects"><a href="#1-Streams-and-FILE-Objects" class="headerlink" title="1 Streams and FILE Objects"></a>1 Streams and <code>FILE</code> Objects</h2><p>In <a href="http://liujunming.top/2018/12/17/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-File-I-O/" target="_blank" rel="noopener">File I/O</a>, all the I/O routines centered on file descriptors. When a file is opened, a file descriptor is returned, and that descriptor is then used for all subsequent I/O operations. With the standard I/O library, the discussion centers on <em>streams</em>. When we open or create a file with the standard I/O library, we say that we have associated a stream with the file.</p><p>With the ASCII character set, a single character is represented by a single byte. With international character sets, a character can be represented by more than one byte. Standard I/O file streams can be used with both single-byte and multibyte (‘‘wide’’) character sets. A stream’s orientation determines whether the characters that are read and written are single byte or multibyte. Initially, when a stream is created, it has no orientation. If a multibyte I/O function is used on a stream without orientation, the stream’s orientation is set to wide oriented. If a byte I/O function is used on a stream without orientation, the stream’s orientation is set to byte oriented. Only two functions can change the orientation once set. The <code>freopen</code> function will clear a stream’s orientation; the <code>fwide</code> function can be used to set a stream’s orientation.</p><p>When we open a stream, the standard I/O function <code>fopen</code> returns a pointer to a <code>FILE</code> object. This object is normally a structure that contains all the information required by the standard I/O library to manage the stream: the file descriptor used for actual I/O, a pointer to a buffer for the stream, the size of the buffer, a count of the number of characters currently in the buffer, an error flag, and the like.</p><h2 id="2-Standard-Input-Standard-Output-and-Standard-Error"><a href="#2-Standard-Input-Standard-Output-and-Standard-Error" class="headerlink" title="2 Standard Input, Standard Output, and Standard Error"></a>2 Standard Input, Standard Output, and Standard Error</h2><p>Three streams are predefined and automatically available to a process: standard input, standard output, and standard error. These streams refer to the same files as the file descriptors <code>STDIN_FILENO</code>, <code>STDOUT_FILENO</code>, and <code>STDERR_FILENO</code>, respectively.<br>These three standard I/O streams are referenced through the predefined file pointers <code>stdin</code>,<code>stdout</code>,and <code>stderr</code>. The file pointers are defined in the<code>&lt;stdio.h&gt;</code> header.</p><h2 id="3-Buffering"><a href="#3-Buffering" class="headerlink" title="3 Buffering"></a>3 Buffering</h2><p>The goal of the buffering provided by the standard I/O library is to use the minimum number of <code>read</code> and <code>write</code> calls.Also, this library tries to do its buffering automatically for each I/O stream, obviating the need for the application to worry about it.<br>Three types of buffering are provided:</p><ol><li>Fully buffered. In this case, actual I/O takes place when the standard I/O buffer is filled.</li><li>Line buffered. </li><li>Unbuffered.</li></ol><p>Linux buffering characteristics:</p><ul><li>Standard error is always unbuffered.</li><li>All other streams are line buffered if they refer to a terminal device; otherwise, they are fully buffered.</li></ul><p>If we don’t like these defaults for any given stream, we can change the buffering by calling either the <code>setbuf</code> or <code>setvbuf</code> function.</p><p>At any time, we can force a stream to be flushed.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fflush</span><span class="params">(FILE *fp)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>fflush</code> function causes any unwritten data for the stream to be passed to the kernel.</p><h2 id="4-Opening-a-Stream"><a href="#4-Opening-a-Stream" class="headerlink" title="4 Opening a Stream"></a>4 Opening a Stream</h2><p>The <code>fopen</code>, <code>freopen</code>, and <code>fdopen</code> functions open a standard I/O stream.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">FILE *<span class="title">fopen</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> type)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">freopen</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> type, FILE *<span class="keyword">restrict</span> fp)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">fdopen</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> *type)</span></span>;</span><br></pre></td></tr></table></figure></p><p>An open stream is closed by calling <code>fclose</code>.</p><h2 id="5-Reading-and-Writing-a-Stream"><a href="#5-Reading-and-Writing-a-Stream" class="headerlink" title="5 Reading and Writing a Stream"></a>5 Reading and Writing a Stream</h2><p>Once we open a stream, we can choose from among three types of unformatted I/O:</p><ol><li>Character-at-a-time I/O. We can read or write one character at a time, with the standard I/O functions handling all the buffering, if the stream is buffered.</li><li>Line-at-a-time I/O. If we want to read or write a line at a time, we use <code>fgets</code> and <code>fputs</code>.</li><li>Binary I/O. This type of I/O is supported by the <code>fread</code> and <code>fwrite</code> functions. For each I/O operation, we read or write some number of objects, where each object is of a specified size. These two functions are often used for binary files where we read or write a structure with each operation. </li></ol><h3 id="5-1-Character-at-a-time-I-O"><a href="#5-1-Character-at-a-time-I-O" class="headerlink" title="5.1 Character-at-a-time I/O"></a>5.1 Character-at-a-time I/O</h3><p>Three functions allow us to read one character at a time.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getc</span><span class="params">(FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fgetc</span><span class="params">(FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getchar</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><p>In most implementations, two flags are maintained for each stream in the <code>FILE</code> object:</p><ul><li>An error flag</li><li>An end-of-file flag</li></ul><p>Output functions are available that correspond to each of the input functions we’ve already described.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">putc</span><span class="params">(<span class="keyword">int</span> c, FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fputc</span><span class="params">(<span class="keyword">int</span> c, FILE *fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">putchar</span><span class="params">(<span class="keyword">int</span> c)</span></span>;</span><br></pre></td></tr></table></figure></p><h3 id="5-2-Line-at-a-Time-I-O"><a href="#5-2-Line-at-a-Time-I-O" class="headerlink" title="5.2 Line-at-a-Time I/O"></a>5.2 Line-at-a-Time I/O</h3><p>Line-at-a-time input is provided by the two functions, <code>fgets</code> and <code>gets</code>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">fgets</span><span class="params">(<span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">int</span> n, FILE *<span class="keyword">restrict</span> fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">gets</span><span class="params">(<span class="keyword">char</span> *buf)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>gets</code> function should never be used. The problem is that it doesn’t allow the caller to specify the buffer size. This allows the buffer to overflow if the line is longer than the buffer, writing over whatever happens to follow the buffer in memory.</p><p>Line-at-a-time output is provided by <code>fputs</code> and <code>puts</code>.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fputs</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> str, FILE *<span class="keyword">restrict</span> fp)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">puts</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *str)</span></span>;</span><br></pre></td></tr></table></figure></p><h3 id="5-3-Binary-I-O"><a href="#5-3-Binary-I-O" class="headerlink" title="5.3 Binary I/O"></a>5.3 Binary I/O</h3><p>If we’re doing binary I/O, we often would like to read or write an entire structure at a time.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">size_t</span> fread(<span class="keyword">void</span> *<span class="keyword">restrict</span> ptr, <span class="keyword">size_t</span> size, <span class="keyword">size_t</span> nobj,</span><br><span class="line">FILE *<span class="keyword">restrict</span> fp);</span><br><span class="line"><span class="keyword">size_t</span> fwrite(<span class="keyword">const</span> <span class="keyword">void</span> *<span class="keyword">restrict</span> ptr, <span class="keyword">size_t</span> size, <span class="keyword">size_t</span> nobj,</span><br><span class="line">FILE *<span class="keyword">restrict</span> fp);</span><br></pre></td></tr></table></figure></p><h2 id="6-Positioning-a-Stream"><a href="#6-Positioning-a-Stream" class="headerlink" title="6 Positioning a Stream"></a>6 Positioning a Stream</h2><p>There are three ways to position a standard I/O stream:</p><ol><li>The two functions <code>ftell</code> and <code>fseek</code>.</li><li>The two functions <code>ftello</code> and <code>fseeko</code>.</li><li>The two functions <code>fgetpos</code> and <code>fsetpos</code>.</li></ol><h2 id="7-Formatted-I-O"><a href="#7-Formatted-I-O" class="headerlink" title="7 Formatted I/O"></a>7 Formatted I/O</h2><h3 id="7-1-Formatted-Output"><a href="#7-1-Formatted-Output" class="headerlink" title="7.1 Formatted Output"></a>7.1 Formatted Output</h3><p>Formatted output is handled by the five <code>printf</code> functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">printf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fprintf</span><span class="params">(FILE *<span class="keyword">restrict</span> fp, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dprintf</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sprintf</span><span class="params">(<span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">snprintf</span><span class="params">(<span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">size_t</span> n, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br></pre></td></tr></table></figure></p><h3 id="7-2-Formatted-Input"><a href="#7-2-Formatted-Input" class="headerlink" title="7.2 Formatted Input"></a>7.2 Formatted Input</h3><p>Formatted input is handled by the three <code>scanf</code> functions.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">scanf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fscanf</span><span class="params">(FILE *<span class="keyword">restrict</span> fp, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sscanf</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> buf, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> format, ...)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="8-Implementation-Details"><a href="#8-Implementation-Details" class="headerlink" title="8 Implementation Details"></a>8 Implementation Details</h2><p>Under the UNIX System, the standard I/O library ends up calling the I/O routines that we described in <a href="http://liujunming.top/2018/12/17/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-File-I-O/" target="_blank" rel="noopener">File I/O</a>. Each standard I/O stream has an associated file descriptor, and we can obtain the descriptor for a stream by calling <code>fileno</code>.</p><h2 id="9-Temporary-Files"><a href="#9-Temporary-Files" class="headerlink" title="9 Temporary Files"></a>9 Temporary Files</h2><p>The ISO C standard defines two functions that are provided by the standard I/O library to assist in creating temporary files.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">tmpnam</span><span class="params">(<span class="keyword">char</span> *ptr)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">tmpfile</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><h2 id="10-Memory-Streams"><a href="#10-Memory-Streams" class="headerlink" title="10 Memory Streams"></a>10 Memory Streams</h2><p>The standard I/O library buffers data in memory, so operations such as character-at-a-time I/O and line-at-a-time I/O are more efficient. We can provide our own buffer for the library to use by calling <code>setbuf</code> or <code>setvbuf</code>.  <em>Memory streams</em> are standard I/O streams for which there are no underlying files, although they are still accessed with FILE pointers. All I/O is done by transferring bytes to and from buffers in main memory.</p><p>Three functions are available to create memory streams.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">FILE *<span class="title">fmemopen</span><span class="params">(<span class="keyword">void</span> *<span class="keyword">restrict</span> buf, <span class="keyword">size_t</span> size, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> type)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">open_memstream</span><span class="params">(<span class="keyword">char</span> **bufp, <span class="keyword">size_t</span> *sizep)</span></span>;</span><br><span class="line"><span class="function">FILE *<span class="title">open_wmemstream</span><span class="params">(<span class="keyword">wchar_t</span> **bufp, <span class="keyword">size_t</span> *sizep)</span></span>;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we describe the standard I/O library. This library is specified by the ISO C standard because it has been implemented on many operating systems other than the UNIX System.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-Files and Directories</title>
    <link href="http://liujunming.github.io/2018/12/18/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Files-and-Directories/"/>
    <id>http://liujunming.github.io/2018/12/18/apue-读书笔记-Files-and-Directories/</id>
    <published>2018-12-18T01:36:45.000Z</published>
    <updated>2018-12-18T08:57:30.141Z</updated>
    
    <content type="html"><![CDATA[<p>We’ll now look at additional features of the file system and the properties of a file.<br><a id="more"></a> </p><h2 id="1-stat-fstat-fstatat-and-lstat-Functions"><a href="#1-stat-fstat-fstatat-and-lstat-Functions" class="headerlink" title="1 stat, fstat, fstatat, and lstat Functions"></a>1 <code>stat</code>, <code>fstat</code>, <code>fstatat</code>, and <code>lstat</code> Functions</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">stat</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, struct stat *<span class="keyword">restrict</span> buf )</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fstat</span><span class="params">(<span class="keyword">int</span> fd, struct stat *buf)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">lstat</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, struct stat *<span class="keyword">restrict</span> buf )</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fstatat</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> *<span class="keyword">restrict</span> pathname, struct stat *<span class="keyword">restrict</span> buf, <span class="keyword">int</span> flag)</span></span>;</span><br></pre></td></tr></table></figure><p>Given a <code>pathname</code>, the <code>stat</code> function returns a structure of information about the named file.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">stat</span> &#123;</span></span><br><span class="line">    <span class="keyword">mode_t</span>    st_mode;          <span class="comment">/* file type &amp; mode (permissions) */</span></span><br><span class="line">    <span class="keyword">ino_t</span>    st_ino;            <span class="comment">/* i-node number (serial number) */</span></span><br><span class="line">    <span class="keyword">dev_t</span>    st_dev;            <span class="comment">/* device number (file system) */</span></span><br><span class="line">    <span class="keyword">dev_t</span>    st_rdev;           <span class="comment">/* device number for special files */</span></span><br><span class="line">    <span class="keyword">nlink_t</span>    st_nlink;        <span class="comment">/* number of links */</span></span><br><span class="line">    <span class="keyword">uid_t</span>    st_uid;            <span class="comment">/* user ID of the owner */</span></span><br><span class="line">    <span class="keyword">gid_t</span>    st_gid;            <span class="comment">/* group ID of the owner */</span></span><br><span class="line">    <span class="keyword">off_t</span>    st_size;           <span class="comment">/* size in bytes, for regular files */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>    <span class="title">st_atim</span>;</span> <span class="comment">/* time of last access */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>    <span class="title">st_mtim</span>;</span> <span class="comment">/* time of last modification */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>    <span class="title">st_ctim</span>;</span> <span class="comment">/* time of last file status change */</span></span><br><span class="line">    <span class="keyword">blksize_t</span>    st_blksize;    <span class="comment">/* best I/O block size*/</span></span><br><span class="line">    <span class="keyword">blkcnt_t</span>    st_blocks;      <span class="comment">/* number of disk blocks allocated */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-File-Types"><a href="#2-File-Types" class="headerlink" title="2 File Types"></a>2 File Types</h2><p>The types are:</p><ol><li>Regular file.</li><li>Directory file.</li><li>Block special file. A type of file providing buffered I/O access in fixed-size units to devices such as disk drives.</li><li>Character special file. A type of file providing unbuffered I/O access in variable-sized units to devices. All devices on a system are either block special files or character special files.</li><li>FIFO. A type of file used for communication between processes. </li><li>Socket.</li><li>Symbolic link. A type of file that points to another file.</li></ol><p>The type of a file is encoded in the <code>st_mode</code> member of the <code>stat</code> structure.</p><h2 id="3-Set-User-ID-and-Set-Group-ID"><a href="#3-Set-User-ID-and-Set-Group-ID" class="headerlink" title="3 Set-User-ID and Set-Group-ID"></a>3 Set-User-ID and Set-Group-ID</h2><p>Every process has six or more IDs associated with it. These are shown in Figure 4.5.<br><img src="/images/2018/12/29.png" alt=""></p><ul><li>The real user ID and real group ID identify who we really are. These two fields are taken from our entry in the password file when we log in. Normally, these values don’t change during a login session, although there are ways for a superuser process to change them.</li><li>The effective user ID, effective group ID, and supplementary group IDs determine our file access permissions.</li><li>The saved set-user-ID and saved set-group-ID contain copies of the effective user ID and the effective group ID, respectively, when a program is executed.</li></ul><p>Normally, the effective user ID equals the real user ID, and the effective group ID equals the real group ID.</p><p>When we execute a program file, the effective user ID of the process is usually the real user ID, and the effective group ID is usually the real group ID. However, we can also set a special flag in the file’s mode word (<code>st_mode</code>) that says, ‘‘When this file is executed, set the effective user ID of the process to be the owner of the file (<code>st_uid</code>).’’ Similarly, we can set another bit in the file’s mode word that causes the effective group ID to be the group owner of the file (<code>st_gid</code>). These two bits in the file’s mode word are called the <em>set-user-ID</em> bit and the <em>set-group-ID</em> bit.</p><h2 id="4-File-Access-Permissions"><a href="#4-File-Access-Permissions" class="headerlink" title="4 File Access Permissions"></a>4 File Access Permissions</h2><p>The <code>st_mode</code> value also encodes the access permission bits for the file.</p><p>There are nine permission bits for each file, divided into three categories. They are shown in Figure 4.6.<br><img src="/images/2018/12/30.png" alt=""><br>The term <code>user</code> in the first three rows in Figure 4.6 refers to the owner of the file. The <code>chmod</code> command, which is typically used to modify these nine permission bit.</p><h2 id="5-Ownership-of-New-Files-and-Directories"><a href="#5-Ownership-of-New-Files-and-Directories" class="headerlink" title="5 Ownership of New Files and Directories"></a>5 Ownership of New Files and Directories</h2><p>The user ID of a new file is set to the effective user ID of the process.</p><h2 id="6-access-and-faccessat-Functions"><a href="#6-access-and-faccessat-Functions" class="headerlink" title="6 access and faccessat Functions"></a>6 <code>access</code> and <code>faccessat</code> Functions</h2><p>As we described earlier, when we open a file, the kernel performs its access tests based on the effective user and group IDs. Sometimes, however, a process wants to test accessibility based on the real user and group IDs.</p><h2 id="7-umask-Function"><a href="#7-umask-Function" class="headerlink" title="7 umask Function"></a>7 <code>umask</code> Function</h2><p>Now that we’ve described the nine permission bits associated with every file, we can describe the <em>file mode creation mask</em> that is associated with every process.<br>The <code>umask</code> function sets the file mode creation mask for the process and returns the previous value.</p><p>For example, if we want to ensure that anyone can read a file, we should set the <code>umask</code> to 0.</p><h2 id="8-chmod-fchmod-and-fchmodat-Functions"><a href="#8-chmod-fchmod-and-fchmodat-Functions" class="headerlink" title="8 chmod, fchmod, and fchmodat Functions"></a>8 <code>chmod</code>, <code>fchmod</code>, and <code>fchmodat</code> Functions</h2><p>The <code>chmod</code>, <code>fchmod</code>, and <code>fchmodat</code> functions allow us to change the file access permissions for an existing file.</p><h2 id="9-chown-fchown-fchownat-and-lchown-Functions"><a href="#9-chown-fchown-fchownat-and-lchown-Functions" class="headerlink" title="9 chown, fchown, fchownat, and lchown Functions"></a>9 <code>chown</code>, <code>fchown</code>, <code>fchownat</code>, and <code>lchown</code> Functions</h2><p>The <code>chown</code> functions allow us to change a file’s user ID and group ID.</p><h2 id="10-File-Size"><a href="#10-File-Size" class="headerlink" title="10 File Size"></a>10 File Size</h2><p>The <code>st_size</code> member of the <code>stat</code> structure contains the size of the file in bytes. This field is meaningful only for regular files, directories, and symbolic links.</p><h2 id="11-File-Truncation"><a href="#11-File-Truncation" class="headerlink" title="11 File Truncation"></a>11 File Truncation</h2><p>Sometimes we would like to truncate a file by chopping off data at the end of the file.</p><h2 id="12-File-Systems"><a href="#12-File-Systems" class="headerlink" title="12 File Systems"></a>12 File Systems</h2><p>We can think of a disk drive being divided into one or more partitions. Each partition can contain a file system, as shown in Figure 4.13. The i-nodes are fixed-length entries that contain most of the information about a file.<br><img src="/images/2018/12/31.png" alt=""><br>If we examine the i-node and data block portion of a cylinder group in more detail, we could have the arrangement shown in Figure 4.14.<br><img src="/images/2018/12/32.png" alt=""></p><p>Assume that we make a new directory in the working directory, as in:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir testdir</span><br></pre></td></tr></table></figure></p><p>Figure 4.15 shows the result. Note that in this figure, we explicitly show the entries for dot and dot-dot.<br><img src="/images/2018/12/33.png" alt=""></p><h2 id="13-link-linkat-unlink-unlinkat-and-remove-Functions"><a href="#13-link-linkat-unlink-unlinkat-and-remove-Functions" class="headerlink" title="13 link, linkat, unlink, unlinkat, and remove Functions"></a>13 <code>link</code>, <code>linkat</code>, <code>unlink</code>, <code>unlinkat</code>, and <code>remove</code> Functions</h2><p>A file can have multiple directory entries pointing to its i-node. We can use either the <code>link</code> function or the <code>linkat</code> function to create a link to an existing file. To remove an existing directory entry, we call the <code>unlink</code> function.</p><h2 id="14-rename-and-renameat-Functions"><a href="#14-rename-and-renameat-Functions" class="headerlink" title="14 rename and renameat Functions"></a>14 <code>rename</code> and <code>renameat</code> Functions</h2><p>A file or a directory is renamed with either the <code>rename</code> or <code>renameat</code> function.</p><h2 id="15-Symbolic-Links"><a href="#15-Symbolic-Links" class="headerlink" title="15 Symbolic Links"></a>15 Symbolic Links</h2><p>A symbolic link is an indirect pointer to a file, unlike the hard links, which pointed directly to the i-node of the file.</p><h2 id="16-Creating-and-Reading-Symbolic-Links"><a href="#16-Creating-and-Reading-Symbolic-Links" class="headerlink" title="16 Creating and Reading Symbolic Links"></a>16 Creating and Reading Symbolic Links</h2><h2 id="17-File-Times"><a href="#17-File-Times" class="headerlink" title="17 File Times"></a>17 File Times</h2><h2 id="18-futimens-utimensat-and-utimes-Functions"><a href="#18-futimens-utimensat-and-utimes-Functions" class="headerlink" title="18 futimens, utimensat, and utimes Functions"></a>18 <code>futimens</code>, <code>utimensat</code>, and <code>utimes</code> Functions</h2><p>Several functions are available to change the access time and the modification time of a file.</p><h2 id="19-mkdir-mkdirat-and-rmdir-Functions"><a href="#19-mkdir-mkdirat-and-rmdir-Functions" class="headerlink" title="19 mkdir, mkdirat, and rmdir Functions"></a>19 <code>mkdir</code>, <code>mkdirat</code>, and <code>rmdir</code> Functions</h2><p>Directories are created with the <code>mkdir</code> and <code>mkdirat</code> functions, and deleted with the <code>rmdir</code> function.</p><h2 id="20-Reading-Directories"><a href="#20-Reading-Directories" class="headerlink" title="20 Reading Directories"></a>20 Reading Directories</h2><h2 id="21-chdir-fchdir-and-getcwd-Functions"><a href="#21-chdir-fchdir-and-getcwd-Functions" class="headerlink" title="21 chdir, fchdir, and getcwd Functions"></a>21 <code>chdir</code>, <code>fchdir</code>, and <code>getcwd</code> Functions</h2><p>Every process has a current working directory. This directory is where the search for all relative pathnames starts (i.e., with all pathnames that do not begin with a slash). When a user logs in to a UNIX system, the current working directory normally starts at the directory specified by the sixth field in the /etc/passwd file—the user’s home directory. The current working directory is an attribute of a process; the home directory is an attribute of a login name.</p><p>We can change the current working directory of the calling process by calling the <code>chdir</code> or <code>fchdir</code> function.</p><h2 id="22-Device-Special-Files"><a href="#22-Device-Special-Files" class="headerlink" title="22 Device Special Files"></a>22 Device Special Files</h2><p>The two fields <code>st_dev</code> and <code>st_rdev</code> are often confused.The rules for their use are simple.</p><ul><li>Every file system is known by its major and minor device numbers, which are encoded in the primitive system data type <code>dev_t</code>. The major number identifies the device driver and sometimes encodes which peripheral board to communicate with; the minor number identifies the specific subdevice. Recall from Figure 4.13 that a disk drive often contains several file systems. Each file system on the same disk drive would usually have the same major number, but a different minor number.</li><li>We can usually access the major and minor device numbers through two macros defined by most implementations: <code>major</code> and <code>minor</code>.</li><li>The <code>st_dev</code> value for every filename on a system is the device number of the file system containing that filename and its corresponding i-node.</li><li>Only character special files and block special files have an <code>st_rdev</code> value. This value contains the device number for the actual device.</li></ul><h2 id="23-Summary-of-File-Access-Permission-Bits"><a href="#23-Summary-of-File-Access-Permission-Bits" class="headerlink" title="23 Summary of File Access Permission Bits"></a>23 Summary of File Access Permission Bits</h2><p>We’ve covered all the file access permission bits, some of which serve multiple purposes. Figure 4.26 summarizes these permission bits and their interpretation when applied to a regular file and a directory.<br><img src="/images/2018/12/34.png" alt=""><br>The final nine constants can also be grouped into threes, as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">S_IRWXU = S_IRUSR | S_IWUSR | S_IXUSR</span><br><span class="line">S_IRWXG = S_IRGRP | S_IWGRP | S_IXGRP</span><br><span class="line">S_IRWXO = S_IROTH | S_IWOTH | S_IXOTH</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We’ll now look at additional features of the file system and the properties of a file.&lt;br&gt;
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>apue 读书笔记-File I/O</title>
    <link href="http://liujunming.github.io/2018/12/17/apue-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-File-I-O/"/>
    <id>http://liujunming.github.io/2018/12/17/apue-读书笔记-File-I-O/</id>
    <published>2018-12-17T01:15:52.000Z</published>
    <updated>2018-12-20T08:40:04.267Z</updated>
    
    <content type="html"><![CDATA[<p>The functions described in this chapter are often referred to as <em>unbuffered I/O</em>.The term <em>unbuffered</em> means that each <code>read</code> or <code>write</code> invokes a system call in the kernel.<a id="more"></a> </p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><h3 id="1-1-File-Descriptors"><a href="#1-1-File-Descriptors" class="headerlink" title="1.1 File Descriptors"></a>1.1 File Descriptors</h3><p>To the kernel, all open files are referred to by file descriptors. A file descriptor is a non-negative integer. When we open an existing file or create a new file, the kernel returns a file descriptor to the process.</p><p>By convention, UNIX System shells associate file descriptor 0 with the standard input of a process, file descriptor 1 with the standard output, and file descriptor 2 with the standard error.</p><p>Although their values are standardized by POSIX.1, the magic numbers 0, 1, and 2 should be replaced in POSIX-compliant applications with the symbolic constants <code>STDIN_FILENO</code>, <code>STDOUT_FILENO</code>, and <code>STDERR_FILENO</code> to improve readability.</p><h3 id="1-2-open-and-openat-Functions"><a href="#1-2-open-and-openat-Functions" class="headerlink" title="1.2 open and openat Functions"></a>1.2 <code>open</code> and <code>openat</code> Functions</h3><p>A file is opened or created by calling either the <code>open</code> function or the <code>openat</code> function.</p><h3 id="1-3-creat-Function"><a href="#1-3-creat-Function" class="headerlink" title="1.3 creat Function"></a>1.3 creat Function</h3><p>A new file can also be created by calling the <code>creat</code> function.</p><h3 id="1-4-close-Function"><a href="#1-4-close-Function" class="headerlink" title="1.4 close Function"></a>1.4 close Function</h3><p>An open file is closed by calling the close function.<br>When a process terminates, all of its open files are closed automatically by the kernel. Many programs take advantage of this fact and don’t explicitly close open files.</p><h3 id="1-5-lseek-Function"><a href="#1-5-lseek-Function" class="headerlink" title="1.5 lseek Function"></a>1.5 <code>lseek</code> Function</h3><p>Every open file has an associated <strong>current file offset</strong>, normally a non-negative integer that measures the number of bytes from the beginning of the file.<br>An open file’s offset can be set explicitly by calling <code>lseek</code>.</p><p><code>lseek</code> only records the current file offset within the kernel — it does not cause any I/O to take place. This offset is then used by the next read or write operation.<br>The file’s offset can be greater than the file’s current size, in which case the next write to the file will extend the file. This is referred to as creating a <em>hole</em> in a file and is allowed. Any bytes in a file that have not been written are read back as 0. A hole in a file isn’t required to have storage backing it on disk.</p><p>We use the <code>od</code> command to look at the contents of the file.</p><h3 id="1-6-read-Function"><a href="#1-6-read-Function" class="headerlink" title="1.6 read Function"></a>1.6 <code>read</code> Function</h3><p>Data is read from an open file with the <code>read</code> function.</p><h3 id="1-7-write-Function"><a href="#1-7-write-Function" class="headerlink" title="1.7 write Function"></a>1.7 <code>write</code> Function</h3><p>Data is written to an open file with the <code>write</code> function.</p><h2 id="2-I-O-Efficiency"><a href="#2-I-O-Efficiency" class="headerlink" title="2 I/O Efficiency"></a>2 I/O Efficiency</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"apue.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFSIZE 4096</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">char</span> buf[BUFFSIZE];</span><br><span class="line">    <span class="keyword">while</span> ((n = read(STDIN_FILENO, buf, BUFFSIZE)) &gt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (write(STDOUT_FILENO, buf, n) != n)</span><br><span class="line">            err_sys(<span class="string">"write error"</span>);</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">0</span>)</span><br><span class="line">            err_sys(<span class="string">"read error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Figure 3.6 shows the results for reading a 516,581,760-byte file, using 20 different buffer sizes.</p><center><img src="/images/2018/12/25.png" alt=""></center><p>This accounts for the minimum in the system time occurring at the few timing measurements starting around a BUFFSIZE of 4,096. Increasing the buffer size beyond this limit has little positive effect.</p><p>Most file systems support some kind of read-ahead to improve performance. When sequential reads are detected, the system tries to read in more data than an application requests, assuming that the application will read it shortly. The effect of read-ahead can be seen in Figure 3.6, where the elapsed time for buffer sizes as small as 32 bytes is as good as the elapsed time for larger buffer sizes.</p><h2 id="3-File-Sharing"><a href="#3-File-Sharing" class="headerlink" title="3 File Sharing"></a>3 File Sharing</h2><p>The UNIX System supports the sharing of open files among different processes.</p><ol><li>Every process has an entry in the process table. Within each process table entry is a table of open file descriptors, which we can think of as a vector, with one entry per descriptor. Associated with each file descriptor are:</li></ol><ul><li>The file descriptor flags </li><li>A pointer to a file table entry</li></ul><ol start="2"><li>The kernel maintains a file table for all open files. Each file table entry contains:</li></ol><ul><li>The file status flags for the file, such as read, write, append, sync, and nonblocking</li><li>The current file offset</li><li>A pointer to the v-node table entry for the file</li></ul><ol start="3"><li>Each open file (or device) has a v-node structure that contains information about the type of file and pointers to functions that operate on the file. For most files, the v-node also contains the i-node for the file. This information is read from disk when the file is opened, so that all the pertinent information about the file is readily available. For example, the i-node contains the owner of the file, the size of the file, pointers to where the actual data blocks for the file are located on disk, and so on.</li></ol><p><em>Linux has no v-node. Instead, a generic i-node structure is used</em>.</p><p>Figure 3.7 shows a pictorial arrangement of these three tables for a single process that has two different files open: one file is open on standard input (file descriptor 0), and the other is open on standard output (file descriptor 1).<br><img src="/images/2018/12/26.png" alt=""></p><p>If two independent processes have the same file open, we could have the arrangement shown in Figure 3.8.<br><img src="/images/2018/12/27.png" alt=""><br>We assume here that the first process has the file open on descriptor 3 and that the second process has that same file open on descriptor 4. Each process that opens the file gets its own file table entry, but only a single v-node table entry is required for a given file. One reason each process gets its own file table entry is so that each process has its own current offset for the file.</p><h2 id="4-Atomic-Operations"><a href="#4-Atomic-Operations" class="headerlink" title="4 Atomic Operations"></a>4 Atomic Operations</h2><h3 id="4-1-Appending-to-a-File"><a href="#4-1-Appending-to-a-File" class="headerlink" title="4.1 Appending to a File"></a>4.1 Appending to a File</h3><p>Consider a single process that wants to append to the end of a file.This works fine for a single process, but problems arise if multiple processes use this technique to append to the same file. (This scenario can arise if multiple instances of the same program are appending messages to a log file, for example.)</p><p>Assume that two independent processes, A and B, are appending to the same file. Each has opened the file but without the O_APPEND flag. This gives us the same picture as Figure 3.8. Each process has its own file table entry, but they share a single v-node table entry. Assume that process A does the lseek and that this sets the current offset for the file for process A to byte offset 1,500 (the current end of file). Then the kernel switches processes, and B continues running. Process B then does the lseek, which sets the current offset for the file for process B to byte offset 1,500 also (the current end of file). Then B calls write, which increments B’s current file offset for the file to 1,600. Because the file’s size has been extended, the kernel also updates the current file size in the v-node to 1,600. Then the kernel switches processes and A resumes. When A calls write, the data is written starting at the current file offset for A, which is byte offset 1,500. This overwrites the data that B wrote to the file.</p><p>The problem here is that our logical operation of ‘‘position to the end of file and write’’ requires two separate function calls (as we’ve shown it). The solution is to have the positioning to the current end of file and the write be an atomic operation with regard to other processes. Any operation that requires more than one function call cannot be atomic, as there is always the possibility that the kernel might temporarily suspend the process between the two function calls (as we assumed previously).</p><p>The UNIX System provides an atomic way to do this operation if we set the O_APPEND flag when a file is opened. As we described in the previous section, this causes the kernel to position the file to its current end of file before each write. We no longer have to call lseek before each write.</p><h3 id="4-2-pread-and-pwrite-Functions"><a href="#4-2-pread-and-pwrite-Functions" class="headerlink" title="4.2 pread and pwrite Functions"></a>4.2 <code>pread</code> and <code>pwrite</code> Functions</h3><p>Calling <code>pread</code> is equivalent to calling <code>lseek</code> followed by a call to <code>read</code>, with the following exceptions.</p><ul><li>There is no way to interrupt the two operations that occur when we call <code>pread</code>.</li><li>The current file offset is not updated.</li></ul><h3 id="4-3-Creating-a-File"><a href="#4-3-Creating-a-File" class="headerlink" title="4.3 Creating a File"></a>4.3 Creating a File</h3><p>When <code>O_CREAT</code> and <code>O_EXCL</code> options for the open function are specified, the open will fail if the file already exists. We also said that the check for the existence of the file and the creation of the file was performed as an atomic operation. If we didn’t have this atomic operation, we might try:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">if</span> ((fd = open(path, O_WRONLY)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (errno == ENOENT) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((fd = creat(path, mode)) &lt; <span class="number">0</span>)</span><br><span class="line">            err_sys(<span class="string">"creat error"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        err_sys(<span class="string">"open error"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>The problem occurs if the file is created by another process between the <code>open</code> and the <code>creat</code>. If the file is created by another process between these two function calls, and if that other process writes something to the file, that data is erased when this <code>creat</code> is executed. Combining the test for existence and the creation into a single atomic operation avoids this problem.</p><p>In general, the term <em>atomic operation</em> refers to an operation that might be composed of multiple steps. If the operation is performed atomically, either all the steps are performed (on success) or none are performed (on failure). It must not be possible for only a subset of the steps to be performed.</p><h2 id="5-dup-and-dup2-Functions"><a href="#5-dup-and-dup2-Functions" class="headerlink" title="5 dup and dup2 Functions"></a>5 <code>dup</code> and <code>dup2</code> Functions</h2><p>An existing file descriptor is duplicated by either of the following functions:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dup</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dup2</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">int</span> fd2)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The new file descriptor returned by <code>dup</code> is guaranteed to be the lowest-numbered available file descriptor.</p><p>The new file descriptor that is returned as the value of the functions shares the same file table entry as the <code>fd</code> argument. We show this in Figure 3.9.</p><center><img src="/images/2018/12/28.png" alt=""></center><p>Each descriptor has its own set of file descriptor flags.</p><h2 id="6-sync-fsync-and-fdatasync-Functions"><a href="#6-sync-fsync-and-fdatasync-Functions" class="headerlink" title="6 sync, fsync, and fdatasync Functions"></a>6 <code>sync</code>, <code>fsync</code>, and <code>fdatasync</code> Functions</h2><p>Traditional implementations of the UNIX System have a buffer cache or page cache in the kernel through which most disk I/O passes. When we write data to a file, the data is normally copied by the kernel into one of its buffers and queued for writing to disk at some later time. This is called <em>delayed write</em>.</p><p>The kernel eventually writes all the delayed-write blocks to disk.<br>To ensure consistency of the file system on disk with the contents of the buffer cache, the <code>sync</code>, <code>fsync</code>, and <code>fdatasync</code> functions are provided.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fsync</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fdatasync</span><span class="params">(<span class="keyword">int</span> fd)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sync</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p><p>The <code>sync</code> function simply queues all the modified block buffers for writing and returns; it does not wait for the disk writes to take place.</p><p>The function <code>sync</code> is normally called periodically (usually every 30 seconds) from a system daemon, often called <code>update</code>. This guarantees regular flushing of the kernel’s block buffers.</p><p>The function <code>fsync</code> refers only to a single file, specified by the file descriptor <code>fd</code>, and waits for the disk writes to complete before returning. This function is used when an application, such as a database, needs to be sure that the modified blocks have been written to the disk.</p><p>The <code>fdatasync</code> function is similar to <code>fsync</code>, but it affects only the data portions of a file. With <code>fsync</code>, the file’s attributes are also updated synchronously.</p><h2 id="7-fcntl-Function"><a href="#7-fcntl-Function" class="headerlink" title="7 fcntl Function"></a>7 <code>fcntl</code> Function</h2><p>The <code>fcntl</code> function can change the properties of a file that is already open.</p><p>The <code>fcntl</code> function is used for five different purposes.</p><ol><li>Duplicate an existing descriptor (cmd = <code>F_DUPFD</code> or <code>F_DUPFD_CLOEXEC</code>)</li><li>Get/set file descriptor flags (cmd = <code>F_GETFD</code> or <code>F_SETFD</code>)</li><li>Get/set file status flags (cmd = <code>F_GETFL</code> or <code>F_SETFL</code>)</li><li>Get/set asynchronous I/O ownership (cmd = <code>F_GETOWN</code> or <code>F_SETOWN</code>)</li><li>Get/set record locks (cmd = <code>F_GETLK</code>, <code>F_SETLK</code>, or <code>F_SETLKW</code>)</li></ol><h2 id="8-ioctl-Function"><a href="#8-ioctl-Function" class="headerlink" title="8 ioctl Function"></a>8 <code>ioctl</code> Function</h2><p>The <code>ioctl</code> function has always been the catchall for I/O operations.<br>Terminal I/O was the biggest user of this function.Each device driver can define its own set of <code>ioctl</code> commands. The system, however, provides generic <code>ioctl</code> commands for different classes of devices.</p><h2 id="9-dev-fd"><a href="#9-dev-fd" class="headerlink" title="9 /dev/fd"></a>9 <code>/dev/fd</code></h2><p>Newer systems provide a directory named <code>/dev/fd</code> whose entries are files named 0, 1, 2, and so on.</p><p>The main use of the <code>/dev/fd</code> files is from the shell. It allows programs that use pathname arguments to handle standard input and standard output in the same manner as other pathnames.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The functions described in this chapter are often referred to as &lt;em&gt;unbuffered I/O&lt;/em&gt;.The term &lt;em&gt;unbuffered&lt;/em&gt; means that each &lt;code&gt;read&lt;/code&gt; or &lt;code&gt;write&lt;/code&gt; invokes a system call in the kernel.
    
    </summary>
    
      <category term="linux" scheme="http://liujunming.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://liujunming.github.io/tags/linux/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Timing Measurements</title>
    <link href="http://liujunming.github.io/2018/12/15/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Timing-Measurements/"/>
    <id>http://liujunming.github.io/2018/12/15/Understanding-the-Linux-Kernel-读书笔记-Timing-Measurements/</id>
    <published>2018-12-15T00:57:28.000Z</published>
    <updated>2018-12-15T06:53:14.598Z</updated>
    
    <content type="html"><![CDATA[<p>We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel:</p><ul><li>Keeping the current time and date so they can be returned to user programs through the time(), ftime(), and gettimeofday() APIs and used by the kernel itself as timestamps for files and network packets</li><li>Maintaining timers—mechanisms that are able to notify the kernel or a user program that a certain interval of time has elapsed<a id="more"></a></li></ul><h2 id="1-Clock-and-Timer-Circuits"><a href="#1-Clock-and-Timer-Circuits" class="headerlink" title="1 Clock and Timer Circuits"></a>1 Clock and Timer Circuits</h2><p>The <em>clock circuits</em> are used both to keep track of the current time of day and to make precise time measurements. The <em>timer circuits</em> are programmed by the kernel, so that they issue interrupts at a fixed, predefined frequency.</p><h3 id="1-1-Real-Time-Clock-RTC"><a href="#1-1-Real-Time-Clock-RTC" class="headerlink" title="1.1 Real Time Clock (RTC)"></a>1.1 Real Time Clock (RTC)</h3><p>All PCs include a clock called <em>Real Time Clock</em> (RTC), which is independent of the CPU and all other chips.<br>Linux uses the RTC only to derive the time and date.</p><h3 id="1-2-Time-Stamp-Counter-TSC"><a href="#1-2-Time-Stamp-Counter-TSC" class="headerlink" title="1.2 Time Stamp Counter (TSC)"></a>1.2 Time Stamp Counter (TSC)</h3><p>Starting with the Pentium, 80×86 microprocessors sport a counter that is increased at each clock signal. The counter is accessible through the 64-bit <em>Time Stamp Counter</em> (TSC) register. When using this register, the kernel has to take into consideration the frequency of the clock signal: if, for instance, the clock ticks at 1 GHz, the Time Stamp Counter is increased once every nanosecond.<br>Linux may take advantage of this register to get much more accurate time measurements than those delivered by the Programmable Interval Timer.</p><h3 id="1-3-Programmable-Interval-Timer-PIT"><a href="#1-3-Programmable-Interval-Timer-PIT" class="headerlink" title="1.3 Programmable Interval Timer (PIT)"></a>1.3 Programmable Interval Timer (PIT)</h3><p>Besides the Real Time Clock and the Time Stamp Counter, IBM-compatible PCs include another type of time-measuring device called <em>Programmable Interval Timer</em> (PIT ). The role of a PIT is similar to the alarm clock of a microwave oven: it makes the user aware that the cooking time interval has elapsed. Instead of ringing a bell, this device issues a special interrupt called timer interrupt, which notifies the kernel that one more time interval has elapsed. Another difference from the alarm clock is that the PIT goes on issuing interrupts forever at some fixed frequency established by the kernel. </p><h3 id="1-4-CPU-Local-Timer"><a href="#1-4-CPU-Local-Timer" class="headerlink" title="1.4 CPU Local Timer"></a>1.4 CPU Local Timer</h3><p>The local APIC present in recent 80×86 microprocessors provides yet another time-measuring device: the <em>CPU local timer</em>.<br>The CPU local timer is a device similar to the Programmable Interval Timer just described that can issue one-shot or periodic interrupts. There are, however, a few differences:</p><ul><li>The APIC’s timer counter is 32bits long,while the PIT’s timer counter is 16 bits long;</li><li>The local APIC timer sends an interrupt only to its processor, while the PIT raises a global interrupt, which may be handled by any CPU in the system.</li><li>The APIC’s timer is based on the bus clock signal,the PIT, which makes use of its own clock signals, can be programmed in a more flexible way.</li></ul><h3 id="1-5-High-Precision-Event-Timer-HPET"><a href="#1-5-High-Precision-Event-Timer-HPET" class="headerlink" title="1.5 High Precision Event Timer (HPET)"></a>1.5 High Precision Event Timer (HPET)</h3><p>The HPET provides a number of hardware timers that can be exploited by the kernel.<br>The next generation of motherboards will likely support both the HPET and the 8254 PIT; in some future time, however, the HPET is expected to completely replace the PIT.</p><h3 id="1-6-ACPI-Power-Management-Timer"><a href="#1-6-ACPI-Power-Management-Timer" class="headerlink" title="1.6 ACPI Power Management Timer"></a>1.6 ACPI Power Management Timer</h3><p>The device is actually a simple counter increased at each clock tick.Its clock signal has a fixed frequency of roughly 3.58 MHz. </p><p>The ACPI Power Management Timer is preferable to the TSC if the operating system or the BIOS may dynamically lower the frequency or voltage of the CPU to save battery power. On the other hand, the high-frequency of the TSC counter is quite handy for measuring very small time intervals.</p><p>However, if an HPET device is present, it should always be preferred to the other circuits because of its richer architecture. </p><h2 id="2-The-Linux-Timekeeping-Architecture"><a href="#2-The-Linux-Timekeeping-Architecture" class="headerlink" title="2 The Linux Timekeeping Architecture"></a>2 The Linux Timekeeping Architecture</h2><p>Linux’s <em>timekeeping architecture</em> is the set of kernel data structures and functions related to the flow of time.</p><p>Linux’s timekeeping architecture depends also on the availability of the Time Stamp Counter (TSC), of the ACPI Power Management Timer, and of the High Precision Event Timer (HPET). The kernel uses two basic timekeeping functions: one to keep the current time up-to-date and another to count the number of nanoseconds that have elapsed within the current second. There are different ways to get the last value. Some methods are more precise and are available if the CPU has a Time Stamp Counter or a HPET; a less-precise method is used in the opposite case.</p><h3 id="2-1-Data-Structures-of-the-Timekeeping-Architecture"><a href="#2-1-Data-Structures-of-the-Timekeeping-Architecture" class="headerlink" title="2.1 Data Structures of the Timekeeping Architecture"></a>2.1 Data Structures of the Timekeeping Architecture</h3><h4 id="2-1-1-The-timer-object"><a href="#2-1-1-The-timer-object" class="headerlink" title="2.1.1 The timer object"></a>2.1.1 The timer object</h4><p>In order to handle the possible timer sources in a uniform way, the kernel makes use of a “timer object,” which is a descriptor of type <code>timer_opts</code>consisting of the timer name and of four standard methods.</p><p>The <code>cur_timer</code> variable stores the address of the timer object corresponding to the “best” timer source available in the system. </p><h4 id="2-1-2-The-jiffies-variable"><a href="#2-1-2-The-jiffies-variable" class="headerlink" title="2.1.2 The jiffies variable"></a>2.1.2 The jiffies variable</h4><p>The <code>jiffies</code> variable is a counter that stores the number of elapsed ticks since the system was started. It is increased by one when a timer interrupt occurs—that is, on every tick. </p><h4 id="2-1-3-The-xtime-variable"><a href="#2-1-3-The-xtime-variable" class="headerlink" title="2.1.3 The xtime variable"></a>2.1.3 The xtime variable</h4><p>The <code>xtime</code> variable stores the current time and date; it is a structure of type <code>timespec</code>.</p><h3 id="2-2-Timekeeping-Architecture-in-Uniprocessor-Systems"><a href="#2-2-Timekeeping-Architecture-in-Uniprocessor-Systems" class="headerlink" title="2.2 Timekeeping Architecture in Uniprocessor Systems"></a>2.2 Timekeeping Architecture in Uniprocessor Systems</h3><h3 id="2-3-Timekeeping-Architecture-in-Multiprocessor-Systems"><a href="#2-3-Timekeeping-Architecture-in-Multiprocessor-Systems" class="headerlink" title="2.3 Timekeeping Architecture in Multiprocessor Systems"></a>2.3 Timekeeping Architecture in Multiprocessor Systems</h3><h2 id="3-Updating-the-Time-and-Date"><a href="#3-Updating-the-Time-and-Date" class="headerlink" title="3 Updating the Time and Date"></a>3 Updating the Time and Date</h2><p>User programs get the current time and date from the xtime variable. The kernel must periodically update this variable, so that its value is always reasonably accurate.<br>The <code>update_times()</code> function, which is invoked by the global timer interrupt handler, updates the value of the <code>xtime</code> variable.</p><h2 id="4-Updating-System-Statistics"><a href="#4-Updating-System-Statistics" class="headerlink" title="4 Updating System Statistics"></a>4 Updating System Statistics</h2><p>The kernel, among the other time-related duties, must periodically collect various data used for:</p><ul><li>Checking the CPU resource limit of the running processes</li><li>Updating statistics about the local CPU workload</li><li>Computing the average system load</li><li>Profiling the kernel code</li></ul><h2 id="5-Software-Timers-and-Delay-Functions"><a href="#5-Software-Timers-and-Delay-Functions" class="headerlink" title="5 Software Timers and Delay Functions"></a>5 Software Timers and Delay Functions</h2><p>A <em>timer</em> is a software facility that allows functions to be invoked at some future moment, after a given time interval has elapsed; a <em>time-out</em> denotes a moment at which the time interval associated with a timer has elapsed.</p><p>Linux considers two types of timers called <em>dynamic timers</em> and <em>interval timers</em>. The first type is used by the kernel, while interval timers may be created by processes in User Mode.</p><p>Besides software timers, the kernel also makes use of <em>delay functions</em>, which execute a tight instruction loop until a given time interval elapses. </p><h3 id="5-1-Dynamic-Timers"><a href="#5-1-Dynamic-Timers" class="headerlink" title="5.1 Dynamic Timers"></a>5.1 Dynamic Timers</h3><h3 id="5-2-An-Application-of-Dynamic-Timers-the-nanosleep-System-Call"><a href="#5-2-An-Application-of-Dynamic-Timers-the-nanosleep-System-Call" class="headerlink" title="5.2 An Application of Dynamic Timers: the nanosleep( ) System Call"></a>5.2 An Application of Dynamic Timers: the nanosleep( ) System Call</h3><h3 id="5-3-Delay-Functions"><a href="#5-3-Delay-Functions" class="headerlink" title="5.3 Delay Functions"></a>5.3 Delay Functions</h3><p>Software timers are useless when the kernel must wait for a short time interval—let’s say, less than a few milliseconds. For instance, often a device driver has to wait for a predefined number of microseconds until the hardware completes some operation. Because a dynamic timer has a significant setup overhead and a rather large minimum wait time (1 millisecond), the device driver cannot conveniently use it.</p><h2 id="6-System-Calls-Related-to-Timing-Measurements"><a href="#6-System-Calls-Related-to-Timing-Measurements" class="headerlink" title="6 System Calls Related to Timing Measurements"></a>6 System Calls Related to Timing Measurements</h2><p>Several system calls allow User Mode processes to read and modify the time and date and to create timers. Let’s briefly review these and discuss how the kernel handles them.</p><h3 id="6-1-The-time-and-gettimeofday-System-Calls"><a href="#6-1-The-time-and-gettimeofday-System-Calls" class="headerlink" title="6.1 The time( ) and gettimeofday( ) System Calls"></a>6.1 The time( ) and gettimeofday( ) System Calls</h3><p>Processes in User Mode can get the current time and date by means of several system calls:</p><ul><li><code>time()</code><br>Returns the number of elapsed seconds since midnight at the start of January 1, 1970 (UTC).</li><li><code>gettimeofday()</code><br>Returns, in a data structure named timeval, the number of elapsed seconds since midnight of January 1, 1970 (UTC) and the number of elapsed microseconds in the last second.</li></ul><h3 id="6-2-The-adjtimex-System-Call"><a href="#6-2-The-adjtimex-System-Call" class="headerlink" title="6.2 The adjtimex( ) System Call"></a>6.2 The adjtimex( ) System Call</h3><h3 id="6-3-The-setitimer-and-alarm-System-Calls"><a href="#6-3-The-setitimer-and-alarm-System-Calls" class="headerlink" title="6.3 The setitimer( ) and alarm( ) System Calls"></a>6.3 The setitimer( ) and alarm( ) System Calls</h3><h3 id="6-4-System-Calls-for-POSIX-Timers"><a href="#6-4-System-Calls-for-POSIX-Timers" class="headerlink" title="6.4 System Calls for POSIX Timers"></a>6.4 System Calls for POSIX Timers</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keeping the current time and date so they can be returned to user programs through the time(), ftime(), and gettimeofday() APIs and used by the kernel itself as timestamps for files and network packets&lt;/li&gt;
&lt;li&gt;Maintaining timers—mechanisms that are able to notify the kernel or a user program that a certain interval of time has elapsed
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Kernel Synchronization</title>
    <link href="http://liujunming.github.io/2018/12/14/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Kernel-Synchronization/"/>
    <id>http://liujunming.github.io/2018/12/14/Understanding-the-Linux-Kernel-读书笔记-Kernel-Synchronization/</id>
    <published>2018-12-14T04:18:06.000Z</published>
    <updated>2018-12-14T10:38:57.246Z</updated>
    
    <content type="html"><![CDATA[<p>You could think of the kernel as a server that answers requests; these requests can come either from a process running on a CPU or an external device issuing an interrupt request. We make this analogy to underscore that parts of the kernel are not run serially, but in an interleaved way. Thus, they can give rise to race conditions, which must be controlled through proper synchronization techniques.<a id="more"></a> </p><h2 id="1-How-the-Kernel-Services-Requests"><a href="#1-How-the-Kernel-Services-Requests" class="headerlink" title="1 How the Kernel Services Requests"></a>1 How the Kernel Services Requests</h2><p>In the rest of this chapter, we will generally denote as “exceptions” both the system calls and the usual exceptions.</p><h3 id="1-1-Kernel-Preemption"><a href="#1-1-Kernel-Preemption" class="headerlink" title="1.1 Kernel Preemption"></a>1.1 Kernel Preemption</h3><p>内核抢占<br>In nonpreemptive kernels, the current process cannot be replaced unless it is about to switch to User Mode. Therefore, the main characteristic of a preemptive kernel is that a process running in Kernel Mode can be replaced by another process while in the middle of a kernel function.</p><p>The main motivation for making a kernel preemptive is to reduce the dispatch latency of the User Mode processes, that is, the delay between the time they become runnable and the time they actually begin running. </p><p>The kernel can be preempted only when it is executing an exception handler (in particular a system call) and the kernel preemption has not been explicitly disabled. The local CPU must have local interrupts enabled, otherwise kernel preemption is not performed.</p><p>Kernel preemption may happen either when a kernel control path (usually, an interrupt handler) is terminated, or when an exception handler reenables kernel preemption by means of <code>preempt_enable()</code>.</p><h2 id="2-Synchronization-Primitives"><a href="#2-Synchronization-Primitives" class="headerlink" title="2 Synchronization Primitives"></a>2 Synchronization Primitives</h2><center><img src="/images/2018/12/21.png" alt=""></center><h3 id="2-1-Per-CPU-Variables"><a href="#2-1-Per-CPU-Variables" class="headerlink" title="2.1 Per-CPU Variables"></a>2.1 Per-CPU Variables</h3><p>The simplest and most efficient synchronization technique consists of declaring kernel variables as <em>per-CPU variables</em>. Basically, a per-CPU variable is an array of data structures, one element per each CPU in the system. However, that the per-CPU variables can be used only in particular cases—basically, when it makes sense to logically split the data across the CPUs of the system.</p><p>Furthermore, per-CPU variables are prone to race conditions caused by kernel preemption, both in uniprocessor and multiprocessor systems. As a general rule, a kernel control path should access a per-CPU variable with kernel preemption disabled.</p><h3 id="2-2-Atomic-Operations"><a href="#2-2-Atomic-Operations" class="headerlink" title="2.2 Atomic Operations"></a>2.2 Atomic Operations</h3><p>Several assembly language instructions are of type “read-modify-write”—that is, they access a memory location twice, the first time to read the old value and the second time to write a new value.</p><p>The easiest way to prevent race conditions due to “read-modify-write” instructions is by ensuring that such operations are atomic at the chip level. Every such operation must be executed in a single instruction without being interrupted in the middle and avoiding accesses to the same memory location by other CPUs.</p><h3 id="2-3-Optimization-and-Memory-Barriers"><a href="#2-3-Optimization-and-Memory-Barriers" class="headerlink" title="2.3 Optimization and Memory Barriers"></a>2.3 Optimization and Memory Barriers</h3><p>When using optimizing compilers, you should never take for granted that instructions will be performed in the exact order in which they appear in the source code. For example, a compiler might reorder the assembly language instructions in such a way to optimize how registers are used. Moreover, modern CPUs usually execute several instructions in parallel and might reorder memory accesses. These kinds of reordering can greatly speed up the program.</p><p>When dealing with synchronization, however, reordering instructions must be avoided. Things would quickly become hairy if an instruction placed after a synchronization primitive is executed before the synchronization primitive itself. Therefore, all synchronization primitives act as optimization and memory barriers.</p><p>An <em>optimization barrier</em> primitive ensures that the assembly language instructions corresponding to C statements placed before the primitive are not mixed by the compiler with assembly language instructions corresponding to C statements placed after the primitive. In Linux the <code>barrier()</code> macro acts as an optimization barrier.</p><p>A <em>memory barrier</em> primitive ensures that the operations placed before the primitive are finished before starting the operations placed after the primitive. </p><p><img src="/images/2018/12/22.png" alt=""></p><p>Notice that in multiprocessor systems, all atomic operations described in the earlier section “Atomic Operations” act as memory barriers.</p><h3 id="2-4-Spin-Locks"><a href="#2-4-Spin-Locks" class="headerlink" title="2.4 Spin Locks"></a>2.4 Spin Locks</h3><p><em>Spin locks</em> are a special kind of lock designed to work in a multiprocessor environment. If the kernel control path finds the spin lock “open,” it acquires the lock and continues its execution. Conversely, if the kernel control path finds the lock “closed” by a kernel control path running on another CPU, it “spins” around, repeatedly executing a tight instruction loop, until the lock is released.</p><p>The instruction loop of spin locks represents a “busy wait.” The waiting kernel control path keeps running on the CPU, even if it has nothing to do besides waste time. Nevertheless, spin locks are usually convenient, because many kernel resources are locked for a fraction of a millisecond only; therefore, it would be far more time-consuming to release the CPU and reacquire it later.</p><p>As a general rule, <strong>kernel preemption is disabled in every critical region protected by spin locks</strong>. In the case of a uniprocessor system, the locks themselves are useless, and the spin lock primitives just disable or enable the kernel preemption. Please notice that kernel preemption is still enabled during the busy wait phase, thus a process waiting for a spin lock to be released could be replaced by a higher priority process.<br><img src="/images/2018/12/23.png" alt=""></p><h3 id="2-5-Read-Write-Spin-Locks"><a href="#2-5-Read-Write-Spin-Locks" class="headerlink" title="2.5 Read/Write Spin Locks"></a>2.5 Read/Write Spin Locks</h3><p><em>Read/write spin locks</em> have been introduced to increase the amount of concurrency inside the kernel. They allow several kernel control paths to simultaneously read the same data structure, as long as no kernel control path modifies it. If a kernel control path wishes to write to the structure, it must acquire the write version of the read/write lock, which grants exclusive access to the resource. Of course, allowing concurrent reads on data structures improves system performance.</p><p><strong>Getting and releasing a lock for reading</strong></p><ul><li><code>read_lock</code></li><li><code>read_unlock</code></li></ul><p><strong>Getting and releasing a lock for writing</strong></p><ul><li><code>write_lock</code></li><li><code>write_unlock</code></li></ul><h3 id="2-6-Seqlocks"><a href="#2-6-Seqlocks" class="headerlink" title="2.6 Seqlocks"></a>2.6 Seqlocks</h3><p>When using read/write spin locks, requests issued by kernel control paths to perform a <code>read_lock</code> or a <code>write_lock</code> operation have the same priority: readers must wait until the writer has finished and, similarly, a writer must wait until all readers have finished.</p><p><em>Seqlocks</em> are similar to read/write spin locks, except that they give a much higher priority to writers: in fact a writer is allowed to proceed even when readers are active. The good part of this strategy is that a writer never waits (unless another writer is active); the bad part is that a reader may sometimes be forced to read the same data several times until it gets a valid copy.</p><p>The critical regions of the readers should be short and writers should seldom acquire the seqlock</p><h3 id="2-7-Read-Copy-Update-RCU"><a href="#2-7-Read-Copy-Update-RCU" class="headerlink" title="2.7 Read-Copy Update (RCU)"></a>2.7 Read-Copy Update (RCU)</h3><p><a href="http://liujunming.top/2018/12/13/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Linux-%E7%9A%84-RCU-%E6%9C%BA%E5%88%B6/" target="_blank" rel="noopener">深入理解 Linux 的 RCU 机制</a><br><em>Read-copy update (RCU)</em> is yet another synchronization technique designed to protect data structures that are mostly accessed for reading by several CPUs. RCU is lock-free, that is, it uses no lock or counter shared by all CPUs; this is a great advantage over read/write spin locks and seqlocks, which have a high overhead due to cache line-snooping and invalidation.</p><p>How does RCU obtain the surprising result of synchronizing several CPUs without shared data structures? The key idea consists of limiting the scope of RCU as follows:</p><ol><li>Only data structures that are dynamically allocated and referenced by means of pointers can be protected by RCU.</li><li>No kernel control path can sleep inside a critical region protected by RCU.</li></ol><h3 id="2-8-Semaphores"><a href="#2-8-Semaphores" class="headerlink" title="2.8 Semaphores"></a>2.8 Semaphores</h3><p>Essentially, Semaphores implement a locking primitive that allows waiters to sleep until the desired resource becomes free.</p><p>Actually, Linux offers two kinds of semaphores:</p><ul><li>Kernel semaphores, which are used by kernel control paths</li><li>System V IPC semaphores, which are used by User Mode processes</li></ul><p>In this section, we focus on kernel semaphores.</p><p>A kernel semaphore is similar to a spin lock, in that it doesn’t allow a kernel control path to proceed unless the lock is open. However, whenever a kernel control path tries to acquire a busy resource protected by a kernel semaphore, the corresponding process is suspended. It becomes runnable again when the resource is released. Therefore, kernel semaphores can be acquired only by functions that are allowed to sleep; interrupt handlers and deferrable functions cannot use them.</p><p>The <code>init_MUTEX()</code> and <code>init_MUTEX_LOCKED()</code> functions may be used to initialize a semaphore for exclusive access.</p><p><strong>Getting and releasing semaphores</strong></p><ul><li><code>up()</code></li><li><code>down()</code></li></ul><h3 id="2-9-Read-Write-Semaphores"><a href="#2-9-Read-Write-Semaphores" class="headerlink" title="2.9 Read/Write Semaphores"></a>2.9 Read/Write Semaphores</h3><p>Read/write semaphores are similar to the read/write spin locks described earlier in the section “Read/Write Spin Locks,” except that waiting processes are suspended instead of spinning until the semaphore becomes open again.</p><p>Many kernel control paths may concurrently acquire a read/write semaphore for reading; however, every writer kernel control path must have exclusive access to the protected resource. Therefore, the semaphore can be acquired for writing only if no other kernel control path is holding it for either read or write access. Read/write semaphores improve the amount of concurrency inside the kernel and improve overall system performance.</p><p>The <code>down_read()</code> and <code>down_write()</code> functions acquire the read/write semaphore for reading and writing, respectively. Similarly, the <code>up_read()</code> and <code>up_write()</code> functions release a read/write semaphore previously acquired for reading and for writing.</p><h3 id="2-10-Completions"><a href="#2-10-Completions" class="headerlink" title="2.10 Completions"></a>2.10 Completions</h3><p>Linux 2.6 also makes use of another synchronization primitive similar to semaphores: <em>completions</em>. They have been introduced to solve a subtle race condition that occurs in multiprocessor systems when process A allocates a temporary semaphore variable, initializes it as closed MUTEX, passes its address to process B, and then invokes <code>down()</code> on it. Process A plans to destroy the semaphore as soon as it awakens. Later on, process B running on a different CPU invokes <code>up()</code> on the semaphore. However, in the current implementation <code>up()</code> and <code>down()</code> can execute concurrently on the same semaphore. Thus, process A can be woken up and destroy the temporary semaphore while process B is still executing the <code>up()</code> function. As a result, <code>up()</code>might attempt to access a data structure that no longer exists.</p><h3 id="2-11-Local-Interrupt-Disabling"><a href="#2-11-Local-Interrupt-Disabling" class="headerlink" title="2.11 Local Interrupt Disabling"></a>2.11 Local Interrupt Disabling</h3><p>Interrupt disabling is one of the key mechanisms used to ensure that a sequence of kernel statements is treated as a critical section. It allows a kernel control path to continue executing even when hardware devices issue IRQ signals, thus providing an effective way to protect data structures that are also accessed by interrupt handlers. By itself, however, local interrupt disabling does not protect against concurrent accesses to data structures by interrupt handlers running on other CPUs, so in multi-processor systems, local interrupt disabling is often coupled with spin locks.</p><p>The <code>local_irq_disable()</code> macro disables interrupts on the local CPU. The <code>local_irq_enable()</code> macro enables them.</p><h3 id="2-12-Disabling-and-Enabling-Deferrable-Functions"><a href="#2-12-Disabling-and-Enabling-Deferrable-Functions" class="headerlink" title="2.12 Disabling and Enabling Deferrable Functions"></a>2.12 Disabling and Enabling Deferrable Functions</h3><p>Deferrable functions can be executed at unpredictable times (essentially, on termination of hardware interrupt handlers). Therefore, data structures accessed by deferrable functions must be protected against race conditions.<br>The kernel sometimes needs to disable deferrable functions without disabling interrupts. Local deferrable functions can be enabled or disabled on the local CPU by acting on the softirq counter stored in the <code>preempt_count</code> field of the current’s <code>thread_info</code> descriptor.</p><h2 id="3-Synchronizing-Accesses-to-Kernel-Data-Structures"><a href="#3-Synchronizing-Accesses-to-Kernel-Data-Structures" class="headerlink" title="3 Synchronizing Accesses to Kernel Data Structures"></a>3 Synchronizing Accesses to Kernel Data Structures</h2><p>Usually, the following rule of thumb is adopted by kernel developers: <em>always keep the concurrency level as high as possible in the system</em>.</p><p>In turn, the concurrency level in the system depends on two main factors:</p><ul><li>The number of I/O devices that operate concurrently</li><li>The number of CPUs that do productive work</li></ul><p>To maximize I/O throughput, interrupts should be disabled for very short periods of time. To use CPUs efficiently, synchronization primitives based on spin locks should be avoided whenever possible.</p><h3 id="3-1-Choosing-Among-Spin-Locks-Semaphores-and-Interrupt-Disabling"><a href="#3-1-Choosing-Among-Spin-Locks-Semaphores-and-Interrupt-Disabling" class="headerlink" title="3.1 Choosing Among Spin Locks, Semaphores, and Interrupt Disabling"></a>3.1 Choosing Among Spin Locks, Semaphores, and Interrupt Disabling</h3><p>Generally speaking, choosing the synchronization primitives depends on what kinds of kernel control paths access the data structure, as shown in Table 5-8. Remember that whenever a kernel control path acquires a spin lock (as well as a read/write lock, a seqlock, or a RCU “read lock”), disables the local interrupts, or disables the local softirqs, kernel preemption is automatically disabled.</p><p><img src="/images/2018/12/24.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;You could think of the kernel as a server that answers requests; these requests can come either from a process running on a CPU or an external device issuing an interrupt request. We make this analogy to underscore that parts of the kernel are not run serially, but in an interleaved way. Thus, they can give rise to race conditions, which must be controlled through proper synchronization techniques.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/tags/Concurrency/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 Linux 的 RCU 机制</title>
    <link href="http://liujunming.github.io/2018/12/13/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Linux-%E7%9A%84-RCU-%E6%9C%BA%E5%88%B6/"/>
    <id>http://liujunming.github.io/2018/12/13/深入理解-Linux-的-RCU-机制/</id>
    <published>2018-12-13T10:59:18.000Z</published>
    <updated>2018-12-14T02:37:43.017Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Read-copy update (RCU) is a synchronization mechanism. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, <strong>RCU supports concurrency between a single updater and multiple readers</strong>. <a id="more"></a>RCU ensures that reads are coherent by maintaining multiple versions of objects and ensuring that they are not freed up until all pre-existing read-side critical sections complete. RCU defines and uses efficient and scalable mechanisms for publishing and reading new versions of an object, and also for deferring the collection of old versions. These mechanisms distribute the work among read and update paths in such a way as to make read paths extremely fast.</p><p>RCU is made up of three fundamental mechanisms, the first being used for insertion, the second being used for deletion, and the third being used to allow readers to tolerate concurrent insertions and deletions. These mechanisms are described in the following sections, which focus on applying RCU to linked lists:</p><ol><li>Publish-Subscribe Mechanism (for insertion)</li><li>Wait For Pre-Existing RCU Readers to Complete (for deletion)</li><li>Maintain Multiple Versions of Recently Updated Objects (for readers)</li></ol><h3 id="1-1-Publish-Subscribe-Mechanism"><a href="#1-1-Publish-Subscribe-Mechanism" class="headerlink" title="1.1 Publish-Subscribe Mechanism"></a>1.1 Publish-Subscribe Mechanism</h3><p>One key attribute of RCU is the ability to safely scan data, even though that data is being modified concurrently. To provide this ability for concurrent insertion, RCU uses what can be thought of as a publish-subscribe mechanism. For example, consider an initially <code>NULL</code> global pointer <code>gp</code> that is to be modified to point to a newly allocated and initialized data structure. The following code fragment (with the addition of appropriate locking) might be used for this purpose:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> *<span class="title">gp</span> = <span class="title">NULL</span>;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line">   </span><br><span class="line">p = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">gp = p;</span><br></pre></td></tr></table></figure></p><p>Unfortunately, there is nothing forcing the compiler and CPU to execute the last four assignment statements in order. If the assignment to <code>gp</code> happens before the initialization of <code>p</code>‘s fields, then concurrent readers could see the uninitialized values. Memory barriers are required to keep things ordered, but memory barriers are notoriously difficult to use. We therefore encapsulate them into a primitive <code>rcu_assign_pointer()</code> that has publication semantics. The last four lines would then be as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">rcu_assign_pointer(gp, p);</span><br></pre></td></tr></table></figure></p><p>The <code>rcu_assign_pointer()</code> would <em>publish</em> the new structure, forcing both the compiler and the CPU to execute the assignment to <code>gp</code> after the assignments to the fields referenced by <code>p</code>.</p><p>However, it is not sufficient to only enforce ordering at the updater, as the reader must enforce proper ordering as well. Consider for example the following code fragment:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p = gp;</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Although this code fragment might well seem immune to misordering, unfortunately, the <a href="http://www.rdrop.com/users/paulmck/scalability/paper/ordering.2007.09.19a.pdf" target="_blank" rel="noopener">DEC Alpha CPU</a>  and value-speculation compiler optimizations can cause the values of <code>p-&gt;a</code>, <code>p-&gt;b</code>, and <code>p-&gt;c</code> to be fetched before the value of <code>p</code>!</p><p>Clearly, we need to prevent this sort of skullduggery on the part of both the compiler and the CPU. The <code>rcu_dereference()</code> primitive uses whatever memory-barrier instructions and compiler directives are required for this purpose:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rcu_read_lock();</span><br><span class="line">p = rcu_dereference(gp);</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br></pre></td></tr></table></figure></p><p>The <code>rcu_dereference()</code> primitive can thus be thought of as <em>subscribing</em> to a given value of the specified pointer, guaranteeing that subsequent dereference operations will see any initialization that occurred before the corresponding publish (<code>rcu_assign_pointer()</code>) operation. The <code>rcu_read_lock()</code> and <code>rcu_read_unlock()</code> calls are absolutely required: they define the extent of the RCU read-side critical section. Their purpose is explained in the next section, however, they never spin or block, nor do they prevent the <code>list_add_rcu()</code> from executing concurrently. </p><p>Although <code>rcu_assign_pointer()</code> and <code>rcu_dereference()</code> can in theory be used to construct any conceivable RCU-protected data structure, in practice it is often better to use higher-level constructs. Therefore, the <code>rcu_assign_pointer()</code> and <code>rcu_dereference()</code> primitives have been embedded in special RCU variants of Linux’s list-manipulation API. Linux has two variants of doubly linked list, the circular <code>struct list_head</code> and the linear <code>struct hlist_head</code>/<code>struct hlist_node</code> pair. The former is laid out as follows, where the green boxes represent the list header and the blue boxes represent the elements in the list.</p><center><img src="/images/2018/12/6.jpg" alt=""></center><p>Adapting the pointer-publish example for the linked list gives the following:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">list</span>;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line">LIST_HEAD(head);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line"> </span><br><span class="line">p = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">p-&gt;a = <span class="number">1</span>;</span><br><span class="line">p-&gt;b = <span class="number">2</span>;</span><br><span class="line">p-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_add_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;head);</span><br></pre></td></tr></table></figure></p><p>Line 15 must be protected by some synchronization mechanism (most commonly some sort of lock) to prevent multiple <code>list_add()</code> instances from executing concurrently. However, such synchronization does not prevent this <code>list_add()</code> from executing concurrently with RCU readers.</p><p>Subscribing to an RCU-protected list is straightforward:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rcu_read_lock();</span><br><span class="line">list_for_each_entry_rcu(p, head, <span class="built_in">list</span>) &#123;</span><br><span class="line">    do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);</span><br><span class="line">&#125;</span><br><span class="line">rcu_read_unlock();</span><br></pre></td></tr></table></figure></p><p>The <code>list_add_rcu()</code> primitive publishes an entry into the specified list, guaranteeing that the corresponding <code>list_for_each_entry_rcu()</code> invocation will properly subscribe to this same entry.</p><p>The set of RCU publish and subscribe primitives are shown in the following table, along with additional primitives to “unpublish”, or retract:</p><center><img src="/images/2018/12/7.png" alt=""></center><p>Note that the <code>list_replace_rcu()</code>, <code>list_del_rcu()</code>, <code>hlist_replace_rcu()</code>, and <code>hlist_del_rcu()</code> APIs add a complication. When is it safe to free up the data element that was replaced or removed? In particular, how can we possibly know when all the readers have released their references to that data element?</p><p>These questions are addressed in the following section.</p><h3 id="1-2-Wait-For-Pre-Existing-RCU-Readers-to-Complete"><a href="#1-2-Wait-For-Pre-Existing-RCU-Readers-to-Complete" class="headerlink" title="1.2 Wait For Pre-Existing RCU Readers to Complete"></a>1.2 Wait For Pre-Existing RCU Readers to Complete</h3><p>In its most basic form, RCU is a way of waiting for things to finish. Of course, there are a great many other ways of waiting for things to finish, including reference counts, reader-writer locks, events, and so on. The great advantage of RCU is that it can wait for each of (say) 20,000 different things without having to explicitly track each and every one of them, and without having to worry about the performance degradation, scalability limitations, complex deadlock scenarios, and memory-leak hazards that are inherent in schemes using explicit tracking.</p><p>In RCU’s case, the things waited on are called “RCU read-side critical sections”. An RCU read-side critical section starts with an <code>rcu_read_lock()</code> primitive, and ends with a corresponding <code>rcu_read_unlock()</code> primitive. RCU read-side critical sections can be nested, and may contain pretty much any code, as long as that code does not explicitly block or sleep (although a special form of RCU called “<a href="https://lwn.net/Articles/202847/" target="_blank" rel="noopener">SRCU</a>“ does permit general sleeping in SRCU read-side critical sections). If you abide by these conventions, you can use RCU to wait for <em>any</em> desired piece of code to complete.</p><p>RCU accomplishes this feat by indirectly determining when these other things have finished, as has been described elsewhere for <a href="http://www.rdrop.com/users/paulmck/RCU/whatisRCU.html" target="_blank" rel="noopener">RCU Classic</a> and <a href="https://lwn.net/Articles/253651/" target="_blank" rel="noopener">realtime RCU</a>.</p><p>In particular, as shown in the following figure, RCU is a way of waiting for pre-existing RCU read-side critical sections to completely finish, including memory operations executed by those critical sections.</p><p><center><img src="/images/2018/12/8.png" alt=""></center><br>However, note that RCU read-side critical sections that begin after the beginning of a given grace period can and will extend beyond the end of that grace period.</p><p>The following pseudocode shows the basic form of algorithms that use RCU to wait for readers:</p><ol><li>Make a change, for example, replace an element in a linked list.</li><li>Wait for all pre-existing RCU read-side critical sections to completely finish (for example, by using the <code>synchronize_rcu()</code> primitive). The key observation here is that subsequent RCU read-side critical sections have no way to gain a reference to the newly removed element.</li><li>Clean up, for example, free the element that was replaced above.</li></ol><p>The following code fragment, adapted from those in the previous section, demonstrates this process, with field <code>a</code> being the search key:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">list</span>;</span></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line">LIST_HEAD(head);</span><br><span class="line"> </span><br><span class="line"><span class="comment">/* . . . */</span></span><br><span class="line"> </span><br><span class="line">p = search(head, key);</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span>) &#123;</span><br><span class="line">    <span class="comment">/* Take appropriate action, unlock, and return. */</span></span><br><span class="line">&#125;</span><br><span class="line">q = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">*q = *p;</span><br><span class="line">q-&gt;b = <span class="number">2</span>;</span><br><span class="line">q-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_replace_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;q-&gt;<span class="built_in">list</span>);</span><br><span class="line">synchronize_rcu();</span><br><span class="line">kfree(p);</span><br></pre></td></tr></table></figure></p><p>Lines 19, 20, and 21 implement the three steps called out above. Lines 16-19 gives RCU (“read-copy update”) its name: while permitting concurrent reads, line 16 <em>copies</em> and lines 17-19 do an <em>update</em>.</p><p>The <code>synchronize_rcu()</code>must wait for all RCU read-side critical sections to complete.</p><p>RCU Classic read-side critical sections delimited by <code>rcu_read_lock()</code> and <code>rcu_read_unlock()</code> are not permitted to block or sleep.</p><p>What exactly do RCU readers see when traversing a concurrently updated list? This question is addressed in the following section.</p><h3 id="1-3-Maintain-Multiple-Versions-of-Recently-Updated-Objects"><a href="#1-3-Maintain-Multiple-Versions-of-Recently-Updated-Objects" class="headerlink" title="1.3 Maintain Multiple Versions of Recently Updated Objects"></a>1.3 Maintain Multiple Versions of Recently Updated Objects</h3><p>This section demonstrates how RCU maintains multiple versions of lists to accommodate synchronization-free readers. Two examples are presented showing how an element that might be referenced by a given reader must remain intact while that reader remains in its RCU read-side critical section. The first example demonstrates deletion of a list element, and the second example demonstrates replacement of an element.</p><h4 id="Example-1-Maintaining-Multiple-Versions-During-Deletion"><a href="#Example-1-Maintaining-Multiple-Versions-During-Deletion" class="headerlink" title="Example 1: Maintaining Multiple Versions During Deletion"></a>Example 1: Maintaining Multiple Versions During Deletion</h4><p>To start the “deletion” example, we will modify lines 11-21 in the example in the previous section as follows:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">p = search(head, key);</span><br><span class="line"><span class="keyword">if</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    list_del_rcu(&amp;p-&gt;<span class="built_in">list</span>);</span><br><span class="line">    synchronize_rcu();</span><br><span class="line">    kfree(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>The initial state of the list, including the pointer <code>p</code>, is as follows.</p><p><center><img src="/images/2018/12/9.jpg" alt=""></center><br>The triples in each element represent the values of fields <code>a</code>, <code>b</code>, and <code>c</code>, respectively. The red borders on each element indicate that readers might be holding references to them, and because readers do not synchronize directly with updaters, readers might run concurrently with this entire replacement process. Please note that we have omitted the backwards pointers and the link from the tail of the list to the head for clarity.</p><p>After the <code>list_del_rcu()</code> on line 3 has completed, the <code>5</code>,<code>6</code>,<code>7</code> element has been removed from the list, as shown below. Since readers do not synchronize directly with updaters, readers might be concurrently scanning this list. These concurrent readers might or might not see the newly removed element, depending on timing. However, readers that were delayed just after fetching a pointer to the newly removed element might see the old version of the list for quite some time after the removal. Therefore, we now have two versions of the list, one with element <code>5</code>,<code>6</code>,<code>7</code> and one without. The border of the <code>5</code>,<code>6</code>,<code>7</code> element is still red, indicating that readers might be referencing it.</p><p><center><img src="/images/2018/12/10.jpg" alt=""></center><br>Please note that readers are not permitted to maintain references to element ,<code>5</code>,<code>6</code>,<code>7</code> after exiting from their RCU read-side critical sections. Therefore, once the <code>synchronize_rcu()</code> on line 4 completes, so that all pre-existing readers are guaranteed to have completed, there can be no more readers referencing this element, as indicated by its black border below. We are thus back to a single version of the list.</p><p><center><img src="/images/2018/12/11.jpg" alt=""></center><br>At this point, the <code>5</code>,<code>6</code>,<code>7</code> element may safely be freed, as shown below:</p><p><center><img src="/images/2018/12/12.jpg" alt=""></center><br>At this point, we have completed the deletion of element <code>5</code>,<code>6</code>,<code>7</code>. The following section covers replacement.</p><h4 id="Example-2-Maintaining-Multiple-Versions-During-Replacement"><a href="#Example-2-Maintaining-Multiple-Versions-During-Replacement" class="headerlink" title="Example 2: Maintaining Multiple Versions During Replacement"></a>Example 2: Maintaining Multiple Versions During Replacement</h4><p>To start the replacement example, here are the last few lines of the example in the previous section:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q = kmalloc(<span class="keyword">sizeof</span>(*p), GFP_KERNEL);</span><br><span class="line">*q = *p;</span><br><span class="line">q-&gt;b = <span class="number">2</span>;</span><br><span class="line">q-&gt;c = <span class="number">3</span>;</span><br><span class="line">list_replace_rcu(&amp;p-&gt;<span class="built_in">list</span>, &amp;q-&gt;<span class="built_in">list</span>);</span><br><span class="line">synchronize_rcu();</span><br><span class="line">kfree(p);</span><br></pre></td></tr></table></figure></p><p>The initial state of the list, including the pointer <code>p</code>, is the same as for the deletion example:</p><p><center><img src="/images/2018/12/13.jpg" alt=""></center><br>As before, the triples in each element represent the values of fields <code>a</code>, <code>b</code>, and <code>c</code>, respectively. The red borders on each element indicate that readers might be holding references to them, and because readers do not synchronize directly with updaters, readers might run concurrently with this entire replacement process. Please note that we again omit the backwards pointers and the link from the tail of the list to the head for clarity.<br>Line 1 <code>kmalloc()</code>s a replacement element, as follows:</p><p><center><img src="/images/2018/12/14.jpg" alt=""></center><br>Line 2 copies the old element to the new one:</p><p><center><img src="/images/2018/12/15.jpg" alt=""></center><br>Line 3 updates <code>q-&gt;b</code> to the value “2”:</p><p><center><img src="/images/2018/12/16.jpg" alt=""></center><br>Line 4 updates q-&gt;c to the value “3”:</p><p><center><img src="/images/2018/12/17.jpg" alt=""></center><br>Now, line 5 does the replacement, so that the new element is finally visible to readers. At this point, as shown below, we have two versions of the list. Pre-existing readers might see the <code>5</code>,<code>6</code>,<code>7</code> element, but new readers will instead see the <code>5</code>,<code>2</code>,<code>3</code> element. But any given reader is guaranteed to see some well-defined list.</p><p><center><img src="/images/2018/12/18.jpg" alt=""></center><br>After the <code>synchronize_rcu()</code> on line 6 returns, a grace period will have elapsed, and so all reads that started before the <code>list_replace_rcu()</code> will have completed. In particular, any readers that might have been holding references to the <code>5</code>,<code>6</code>,<code>7</code> element are guaranteed to have exited their RCU read-side critical sections, and are thus prohibited from continuing to hold a reference. Therefore, there can no longer be any readers holding references to the old element, as indicated by the thin black border around the <code>5</code>,<code>6</code>,<code>7</code> element below. As far as the readers are concerned, we are back to having a single version of the list, but with the new element in place of the old.</p><p><center><img src="/images/2018/12/19.jpg" alt=""></center><br>After the <code>kfree()</code> on line 7 completes, the list will appear as follows:</p><p><center><img src="/images/2018/12/20.jpg" alt=""></center></p><h2 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2 Usage"></a>2 Usage</h2><p><a href="http://lwn.net/Articles/263130/" target="_blank" rel="noopener">What is RCU? Part 2: Usage</a><br><a href="https://lwn.net/Articles/609973/" target="_blank" rel="noopener">The RCU API tables</a><br><a href="https://github.com/jinb-park/rcu_example/blob/master/list_rcu_example.c" target="_blank" rel="noopener">list_rcu_example</a>是一个具体实例，可以仔细研究下代码。</p><hr><p>参考资料：</p><ol><li><a href="https://www.kernel.org/doc/Documentation/RCU/whatisRCU.txt" target="_blank" rel="noopener">whatisRCU</a></li><li><a href="https://www.wikiwand.com/en/Read-copy-update" target="_blank" rel="noopener">wikiwand Read-copy-update</a></li><li><a href="https://lwn.net/Articles/262464/" target="_blank" rel="noopener">What is RCU, Fundamentally?</a></li><li><a href="https://pdos.csail.mit.edu/6.828/2018/readings/rcu-decade-later.pdf" target="_blank" rel="noopener">RCU Usage In the Linux Kernel: One Decade Later</a></li><li><a href="https://github.com/jinb-park/rcu_example" target="_blank" rel="noopener">rcu_example</a></li><li><a href="https://www.cnblogs.com/qcloud1001/p/7755331.html" target="_blank" rel="noopener">深入理解 Linux 的 RCU 机制</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1 Introduction&quot;&gt;&lt;/a&gt;1 Introduction&lt;/h2&gt;&lt;p&gt;Read-copy update (RCU) is a synchronization mechanism. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, &lt;strong&gt;RCU supports concurrency between a single updater and multiple readers&lt;/strong&gt;.
    
    </summary>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/categories/Concurrency/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="Concurrency" scheme="http://liujunming.github.io/tags/Concurrency/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Linux Kernel 读书笔记 -Interrupts and Exceptions</title>
    <link href="http://liujunming.github.io/2018/12/04/Understanding-the-Linux-Kernel-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Interrupts-and-Exceptions/"/>
    <id>http://liujunming.github.io/2018/12/04/Understanding-the-Linux-Kernel-读书笔记-Interrupts-and-Exceptions/</id>
    <published>2018-12-04T09:01:41.000Z</published>
    <updated>2018-12-14T05:56:02.319Z</updated>
    
    <content type="html"><![CDATA[<p>An <em>interrupt</em> is usually defined as an event that alters the sequence of instructions executed by a processor.</p><p>Intel microprocessor manuals designate synchronous and asynchronous interrupts as <em>exceptions</em> and <em>interrupts</em>.<a id="more"></a> We’ll occasionally use the term “interrupt signal” to designate both types together (synchronous as well as asynchronous).</p><p>Interrupts are issued by interval timers and I/O devices.</p><p>Exceptions, on the other hand, are caused either by programming errors or by anomalous conditions that must be handled by the kernel.</p><h2 id="1-The-Role-of-Interrupt-Signals"><a href="#1-The-Role-of-Interrupt-Signals" class="headerlink" title="1 The Role of Interrupt Signals"></a>1 The Role of Interrupt Signals</h2><p>When an interrupt signal arrives, the CPU must stop what it’s currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the <code>eip</code> and <code>cs</code> registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter.</p><p>There is a key difference between interrupt handling and process switching: the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred. As a kernel control path, the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down).</p><h2 id="2-Interrupts-and-Exceptions"><a href="#2-Interrupts-and-Exceptions" class="headerlink" title="2 Interrupts and Exceptions"></a>2 Interrupts and Exceptions</h2><ul><li>Interrupts</li><li>Exceptions<ul><li>Processor-detected exceptions<ul><li>Faults</li><li>Traps</li><li>Aborts</li></ul></li><li>Programmed exceptions</li></ul></li></ul><p><strong>Processor-detected exceptions:</strong> These are further divided into three groups, depending on the value of the eip register that is saved on the Kernel Mode stack when the CPU control unit raises the exception.</p><p><strong>Traps:</strong>The saved value of eip is the address of the instruction that should be executed after the one that caused the trap.</p><p><strong>Aborts:</strong>A serious error occurred; the control unit is in trouble, and it may be unable to store in the eip register the precise location of the instruction causing the exception. Aborts are used to report severe errors, such as hardware failures and invalid or inconsistent values in system tables.</p><p><strong>Programmed exceptions:</strong>Occur at the request of the programmer. Programmed exceptions are handled by the control unit as traps; they are often called <code>software interrupts</code>. Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event.</p><p>Each interrupt or exception is identified by a number ranging from 0 to 255; Intel calls this 8-bit unsigned number a <em>vector</em>. The vectors of nonmaskable interrupts and exceptions are fixed, while those of maskable interrupts can be altered by programming the Interrupt Controller.</p><h3 id="2-1-IRQs-and-Interrupts"><a href="#2-1-IRQs-and-Interrupts" class="headerlink" title="2.1 IRQs and Interrupts"></a>2.1 IRQs and Interrupts</h3><p>Each hardware device controller capable of issuing interrupt requests usually has a single output line designated as the <code>Interrupt ReQuest (IRQ)</code> line.All existing IRQ lines are connected to the input pins of a hardware circuit called the <code>Programmable Interrupt Controller(PIC)</code>.</p><center><img src="/images/2018/12/1.JPG" alt=""></center><p><strong>The Advanced Programmable Interrupt Controller (APIC)</strong><br> However, if the system includes two or more CPUs, this approach is no longer valid and more sophisticated PICs are needed.<br> <center><img src="/images/2018/12/2.png" alt=""></center></p><p> Besides distributing interrupts among processors, the multi-APIC system allows CPUs to generate <code>interprocessor interrupts(IPI)</code>. </p><h3 id="2-2-Exceptions"><a href="#2-2-Exceptions" class="headerlink" title="2.2 Exceptions"></a>2.2 Exceptions</h3><p>Each exception is handled by a specific exception handler, which usually sends a Unix signal to the process that caused the exception.</p><h3 id="2-3-Interrupt-Descriptor-Table"><a href="#2-3-Interrupt-Descriptor-Table" class="headerlink" title="2.3 Interrupt Descriptor Table"></a>2.3 Interrupt Descriptor Table</h3><p>A system table called <code>Interrupt Descriptor Table (IDT)</code> associates each interrupt or exception vector with the address of the corresponding interrupt or exception handler. </p><p>The IDT may include three types of descriptors;<br> <center><img src="/images/2018/12/3.png" alt=""></center><br>Linux uses interrupt gates to handle interrupts and trap gates to handle exceptions.</p><h3 id="2-4-Hardware-Handling-of-Interrupts-and-Exceptions"><a href="#2-4-Hardware-Handling-of-Interrupts-and-Exceptions" class="headerlink" title="2.4 Hardware Handling of Interrupts and Exceptions"></a>2.4 Hardware Handling of Interrupts and Exceptions</h3><p>After the interrupt or exception is processed, the corresponding handler must relinquish control to the interrupted process.</p><h2 id="3-Nested-Execution-of-Exception-and-Interrupt-Handlers"><a href="#3-Nested-Execution-of-Exception-and-Interrupt-Handlers" class="headerlink" title="3 Nested Execution of Exception and Interrupt Handlers"></a>3 Nested Execution of Exception and Interrupt Handlers</h2><p>Every interrupt or exception gives rise to a kernel control path or separate sequence of instructions that execute in Kernel Mode on behalf of the current process.<br> <center><img src="/images/2018/12/4.png" alt=""></center><br>The price to pay for allowing nested kernel control paths is that an interrupt handler must never block, that is, 中断处理程序运行期间不能发生进程切换.</p><p>An interrupt handler may preempt both other interrupt handlers and exception handlers. Conversely, an exception handler never preempts an interrupt handler. Interrupt handlers never perform operations that can induce page faults, and thus, potentially, a process switch.</p><p>On multiprocessor systems, several kernel control paths may execute concurrently. Moreover, a kernel control path associated with an exception may start executing on a CPU and, due to a process switch, migrate to another CPU.</p><h2 id="4-Initializing-the-Interrupt-Descriptor-Table"><a href="#4-Initializing-the-Interrupt-Descriptor-Table" class="headerlink" title="4 Initializing the Interrupt Descriptor Table"></a>4 Initializing the Interrupt Descriptor Table</h2><h2 id="5-Exception-Handling"><a href="#5-Exception-Handling" class="headerlink" title="5 Exception Handling"></a>5 Exception Handling</h2><p>When one of them occurs, the kernel sends a signal to the process that caused the exception to notify it of an anomalous condition. </p><p>Exception handlers have a standard structure consisting of three steps:</p><ol><li>Save the contents of most registers in the Kernel Mode stack (this part is coded in assembly language).</li><li>Handle the exception by means of a high-level C function.</li><li>Exit from the handler by means of the <code>ret_from_exception()</code> function.</li></ol><h3 id="5-1-Saving-the-Registers-for-the-Exception-Handler"><a href="#5-1-Saving-the-Registers-for-the-Exception-Handler" class="headerlink" title="5.1 Saving the Registers for the Exception Handler"></a>5.1 Saving the Registers for the Exception Handler</h3><h3 id="5-2-Entering-and-Leaving-the-Exception-Handler"><a href="#5-2-Entering-and-Leaving-the-Exception-Handler" class="headerlink" title="5.2 Entering and Leaving the Exception Handler"></a>5.2 Entering and Leaving the Exception Handler</h3><p>Store the hardware error code and the exception vector in the process descriptor of current, and then send a suitable signal to that process:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">current-&gt;thread.error_code = error_code;</span><br><span class="line">current-&gt;thread.trap_no = <span class="built_in">vector</span>;</span><br><span class="line">force_sig(sig_number, current);</span><br></pre></td></tr></table></figure></p><p>The current process takes care of the signal right after the termination of the exception handler. The signal will be handled either in User Mode by the process’s own signal handler (if it exists) or in Kernel Mode. In the latter case, the kernel usually kills the process.</p><h2 id="6-Interrupt-Handling"><a href="#6-Interrupt-Handling" class="headerlink" title="6 Interrupt Handling"></a>6 Interrupt Handling</h2><p>Most exceptions are handled simply by sending a Unix signal to the process that caused the exception. The action to be taken is thus deferred until the process receives the signal; as a result, the kernel is able to process the exception quickly.</p><p>Interrupt handling depends on the type of interrupt. </p><ul><li>I/O interrupts</li><li>Timer interrupts</li><li>Interprocessor interrupts</li></ul><h3 id="6-1-I-O-Interrupt-Handling"><a href="#6-1-I-O-Interrupt-Handling" class="headerlink" title="6.1 I/O Interrupt Handling"></a>6.1 I/O Interrupt Handling</h3><p>In the PCI bus architecture, for instance, several devices may share the same IRQ line. This means that the interrupt vector alone does not tell the whole story. </p><ul><li>IRQ sharing</li><li>IRQ dynamic allocation</li></ul><p>The interrupt handler executes several <code>interrupt service routines (ISRs)</code>.</p><p>Not all actions to be performed when an interrupt occurs have the same urgency.Long noncritical operations should be deferred, because while an interrupt handler is running, the signals on the corresponding IRQ line are temporarily ignored. Most important, the process on behalf of which an interrupt handler is executed must always stay in the TASK_RUNNING state, or a system freeze can occur.Therefore, interrupt handlers cannot perform any blocking procedure such as an I/O disk operation. Linux divides the actions to be performed following an interrupt into three classes:</p><ul><li>Critical</li><li>Noncritical</li><li>Noncritical deferrable</li></ul><p>Regardless of the kind of circuit that caused the interrupt, all I/O interrupt handlers perform the same four basic actions:</p><ol><li>Save the IRQ value and the register’s contents on the Kernel Mode stack.</li><li>Send an acknowledgment to the PIC that is servicing the IRQ line, thus allowing it to issue further interrupts.</li><li>Execute the interrupt service routines (ISRs) associated with all the devices that share the IRQ.</li><li>Terminate by jumping to the <code>ret_from_intr()</code> address.</li></ol><center><img src="/images/2018/12/5.png" alt=""></center><h2 id="7-Softirqs-and-Tasklets"><a href="#7-Softirqs-and-Tasklets" class="headerlink" title="7 Softirqs and Tasklets"></a>7 Softirqs and Tasklets</h2><p>We mentioned earlier in the section “Interrupt Handling” that several tasks among those executed by the kernel are not critical: they can be deferred for a long period of time, if necessary.</p><p>The deferrable tasks can execute with all interrupts enabled. Taking them out of the interrupt handler helps keep kernel response time small. This is a very important property for many time-critical applications that expect their interrupt requests to be serviced in a few milliseconds.</p><p>Linux 2.6 answers such a challenge by using two kinds of non-urgent interruptible kernel functions: the so-called deferrable functions (softirqs and tasklets), and those executed by means of some work queues.</p><p>Softirqs and tasklets are strictly correlated, because tasklets are implemented on top of softirqs. As a matter of fact, the term “softirq,” which appears in the kernel source code, often denotes both kinds of deferrable functions.</p><h2 id="8-Work-Queues"><a href="#8-Work-Queues" class="headerlink" title="8 Work Queues"></a>8 Work Queues</h2><p>The <em>work queues</em> allow kernel functions to be activated (much like deferrable functions) and later executed by special kernel threads called <em>worker threads</em>.</p><p>Despite their similarities, deferrable functions and work queues are quite different. The main difference is that deferrable functions run in interrupt context while functions in work queues run in process context. Running in process context is the only way to execute functions that can block. No process switch can take place in interrupt context. A function in a work queue is executed by a kernel thread,</p><hr><p>参考资料：</p><ol><li><a href="http://home.ustc.edu.cn/~boj/courses/linux_kernel/2_int.html" target="_blank" rel="noopener">Linux源代码阅读——中断</a></li><li><a href="http://www.wowotech.net/irq_subsystem/interrupt_subsystem_architecture.html" target="_blank" rel="noopener">Linux kernel的中断子系统</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-linuxkernelint/index.html" target="_blank" rel="noopener">Linux 内核中断内幕</a></li><li><a href="https://my.oschina.net/fileoptions/blog/918164" target="_blank" rel="noopener">linux内核之中断实现原理</a></li><li><a href="https://www.tldp.org/LDP/lkmpg/2.6/html/x1256.html" target="_blank" rel="noopener">Interrupt Handlers</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;An &lt;em&gt;interrupt&lt;/em&gt; is usually defined as an event that alters the sequence of instructions executed by a processor.&lt;/p&gt;
&lt;p&gt;Intel microprocessor manuals designate synchronous and asynchronous interrupts as &lt;em&gt;exceptions&lt;/em&gt; and &lt;em&gt;interrupts&lt;/em&gt;.
    
    </summary>
    
      <category term="Kernel" scheme="http://liujunming.github.io/categories/Kernel/"/>
    
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
      <category term="读书笔记" scheme="http://liujunming.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Fix-Mapped Linear Addresses</title>
    <link href="http://liujunming.github.io/2018/11/28/Fix-Mapped-Linear-Addresses/"/>
    <id>http://liujunming.github.io/2018/11/28/Fix-Mapped-Linear-Addresses/</id>
    <published>2018-11-28T02:15:51.000Z</published>
    <updated>2018-11-28T03:59:22.179Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>fixmap是一段固定地址映射，kernel预留的一段虚拟地址空间，虚拟地址是在编译的时候确定。fixmap可以用来做什么？kernel启动初期，由于此时的kernel已经运行在虚拟地址上。因此我们访问具体的物理地址是不行的，必须建立虚拟地址和物理地址的映射，然后通过虚拟地址访问才可以。例如：dtb中包含bootloader传递过来的内存信息，我们需要解析dtb，但是我们得到的是dtb的物理地址。因此访问之前必须创建映射，创建映射又需要内存系统。但是由于所有的内存管理子系统还没有ready，因此我们不能使用ioremap接口创建映射，为此kernel提出fixmap的解决方案。<br><a id="more"></a></p><h2 id="fixmap空间分配"><a href="#fixmap空间分配" class="headerlink" title="fixmap空间分配"></a>fixmap空间分配</h2><p>fixmap虚拟地址空间又被平均分成两个部分permanent fixed addresses和temporary fixed addresses。permanent fixed addresses是永久映射，temporary fixed addresses是临时映射。永久映射是指在建立的映射关系在kernel阶段不会改变，仅供特定模块一直使用。临时映射就是模块使用前创建映射，使用后解除映射。</p><p>With respect to variable pointers, fix-mapped linear addresses are more efficient. In fact, dereferencing a variable pointer requires one memory access more than dereferencing an immediate constant address. Moreover, checking the value of a variable pointer before dereferencing it is a good programming practice; conversely, the check is never required for a constant linear address.</p><p>具体函数可以参考Understanding the Linux Kernel p72.</p><hr><p>参考资料：</p><ol><li><a href="https://zohead.com/archives/linux-kernel-learning-memory-addressing/" target="_blank" rel="noopener">zohead</a></li><li><a href="https://www.spinics.net/lists/newbies/msg31797.html" target="_blank" rel="noopener">What is fixmaps?</a></li><li><a href="http://www.wowotech.net/memory_management/440.html" target="_blank" rel="noopener">fixmap addresses原理</a></li><li><a href="http://students.mimuw.edu.pl/ZSO/Wyklady/04_pamiec/4_pamiec_en.html#highmem" target="_blank" rel="noopener">Mapping frames from highmem</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;fixmap是一段固定地址映射，kernel预留的一段虚拟地址空间，虚拟地址是在编译的时候确定。fixmap可以用来做什么？kernel启动初期，由于此时的kernel已经运行在虚拟地址上。因此我们访问具体的物理地址是不行的，必须建立虚拟地址和物理地址的映射，然后通过虚拟地址访问才可以。例如：dtb中包含bootloader传递过来的内存信息，我们需要解析dtb，但是我们得到的是dtb的物理地址。因此访问之前必须创建映射，创建映射又需要内存系统。但是由于所有的内存管理子系统还没有ready，因此我们不能使用ioremap接口创建映射，为此kernel提出fixmap的解决方案。&lt;br&gt;
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>内核页表和进程页表</title>
    <link href="http://liujunming.github.io/2018/11/27/%E5%86%85%E6%A0%B8%E9%A1%B5%E8%A1%A8%E5%92%8C%E8%BF%9B%E7%A8%8B%E9%A1%B5%E8%A1%A8/"/>
    <id>http://liujunming.github.io/2018/11/27/内核页表和进程页表/</id>
    <published>2018-11-27T14:00:35.000Z</published>
    <updated>2018-11-28T02:41:18.710Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自：<a href="http://blog.chinaunix.net/uid-14528823-id-4334619.html" target="_blank" rel="noopener">chinaunix</a></p><p>初学内核时，经常被“内核页表”和“进程页表”搞晕，不知道这到底是个啥东东，跟我们平时理解的页表有和关系。</p><ul><li><p>内核页表：即书上说的<strong>主内核页表</strong>，在内核中其实就是一段内存，存放在主内核页全局目录init_mm.pgd(swapper_pg_dir)中，硬件并不直接使用。</p></li><li><p>进程页表：每个进程自己的页表，放在进程自身的页目录task_struct.pgd中。</p></li></ul><a id="more"></a><p>在保护模式下，从硬件角度看，其运行的基本对象为“进程”(或线程)，而寻址则依赖于“进程页表”，在进程调度而进行上下文切换时，会进行页表的切换：即将新进程的pgd(页目录)加载到CR3寄存器中。从这个角度看，其实是完全没有用到“内核页表”的，那么“内核页表”有什么用呢？跟“进程页表”有什么关系呢？</p><p>1、内核页表中的内容为所有进程共享，每个进程都有自己的“进程页表”，“进程页表”中映射的线性地址包括两部分：</p><ul><li>用户态</li><li>内核态</li></ul><p>其中，内核态地址对应的相关页表项，对于所有进程来说都是相同的(因为内核空间对所有进程来说都是共享的)，而这部分页表内容其实就来源于“内核页表”，即每个进程的“进程页表”中内核态地址相关的页表项都是“内核页表”的一个拷贝。<br>2、“内核页表”由内核自己维护并更新，在vmalloc区发生page fault时，将“内核页表”同步到“进程页表”中。以32位系统为例，内核页表主要包含两部分：</p><ul><li>线性映射区</li><li>vmalloc区</li></ul><p>其中，线性映射区即通过<code>TASK_SIZE</code>偏移进行映射的区域，对32系统来说就是0-896M这部分区域，映射对应的虚拟地址区域为<code>TASK_SIZE~TASK_SIZE+896M</code>。这部分区域在内核初始化时就已经完成映射，并创建好相应的页表，即这部分虚拟内存区域不会发生page fault。</p><p>vmalloc区，为<code>896M~896M+128M</code>，这部分区域用于映射高端内存，有三种映射方式：vmalloc、固定、临时，这里就不详述了。。<br>以vmalloc为例(最常使用)，这部分区域对应的线性地址在内核使用vmalloc分配内存时，其实就已经分配了相应的物理内存，并做了相应的映射，建立了相应的页表项，但相关页表项仅写入了“内核页表”，并没有实时更新到“进程页表中”，内核在这里使用了“延迟更新”的策略，将“进程页表”真正更新推迟到第一次访问相关线性地址，发生page fault时，此时在<a href="https://elixir.bootlin.com/linux/v4.19.4/source/arch/x86/mm/fault.c#L1244" target="_blank" rel="noopener">page fault</a>的处理流程中进行“进程页表”的更新：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 缺页地址位于内核空间。并不代表异常发生于内核空间，有可能是用户</span></span><br><span class="line"><span class="comment"> * 态访问了内核空间的地址。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (unlikely(fault_in_kernel_space(address))) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!(error_code &amp; (PF_RSVD | PF_USER | PF_PROT))) &#123;</span><br><span class="line">        <span class="comment">//检查发生缺页的地址是否在vmalloc区，是则进行相应的处理</span></span><br><span class="line">        <span class="keyword">if</span> (vmalloc_fault(address) &gt;= <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  * 对于发生缺页异常的指针位于vmalloc区情况的处理，主要是将</span></span><br><span class="line"><span class="comment">  * 主内核页表向当前进程的内核页表同步。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">static</span> noinline __<span class="function">kprobes <span class="keyword">int</span> <span class="title">vmalloc_fault</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">long</span> address)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> pgd_paddr;</span><br><span class="line">    <span class="keyword">pmd_t</span> *pmd_k;</span><br><span class="line">    <span class="keyword">pte_t</span> *pte_k;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Make sure we are in vmalloc area: */</span></span><br><span class="line">    <span class="comment">/* 区域检查 */</span></span><br><span class="line">    <span class="keyword">if</span> (!(address &gt;= VMALLOC_START &amp;&amp; address &lt; VMALLOC_END))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    WARN_ON_ONCE(in_nmi());</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Synchronize this task's top level page-table</span></span><br><span class="line"><span class="comment">     * with the 'reference' page table.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * Do _not_ use "current" here. We might be inside</span></span><br><span class="line"><span class="comment">     * an interrupt in the middle of a task switch..</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">     <span class="comment">/*获取pgd(最顶级页目录)地址，直接从CR3寄存器中读取。</span></span><br><span class="line"><span class="comment">     *不要通过current获取，因为缺页异常可能在上下文切换的过程中发生，</span></span><br><span class="line"><span class="comment">     *此时如果通过current获取，则可能会出问题*/</span></span><br><span class="line">    pgd_paddr = read_cr3();</span><br><span class="line">    <span class="comment">//从主内核页表中，同步vmalloc区发生缺页异常地址对应的页表</span></span><br><span class="line">    pmd_k = vmalloc_sync_one(__va(pgd_paddr), address);</span><br><span class="line">    <span class="keyword">if</span> (!pmd_k)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">//如果同步后，相应的PTE还不存在，则说明该地址有问题了</span></span><br><span class="line">    pte_k = pte_offset_kernel(pmd_k, address);</span><br><span class="line">    <span class="keyword">if</span> (!pte_present(*pte_k))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此时，问题来了，为什么需要内核页表呢？<br>详情可以参考<a href="http://bbs.chinaunix.net/thread-4190879-1-1.html" target="_blank" rel="noopener">关于内核页表初始化的问题</a>。<br><strong>目的</strong>主要是为了让cpu从real mode平稳过渡到protected mode，read mode下分页尚未开启。</p><p>更多细节请参考<a href="http://students.mimuw.edu.pl/ZSO/Wyklady/04_pamiec/4_pamiec_en.html#tablice_stron_jadra" target="_blank" rel="noopener">Kernel page tables</a></p><hr><p>参考资料：</p><ol><li><a href="http://bbs.chinaunix.net/thread-4190879-1-1.html" target="_blank" rel="noopener">关于内核页表初始化的问题</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自：&lt;a href=&quot;http://blog.chinaunix.net/uid-14528823-id-4334619.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;chinaunix&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;初学内核时，经常被“内核页表”和“进程页表”搞晕，不知道这到底是个啥东东，跟我们平时理解的页表有和关系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;内核页表：即书上说的&lt;strong&gt;主内核页表&lt;/strong&gt;，在内核中其实就是一段内存，存放在主内核页全局目录init_mm.pgd(swapper_pg_dir)中，硬件并不直接使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;进程页表：每个进程自己的页表，放在进程自身的页目录task_struct.pgd中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="内存管理" scheme="http://liujunming.github.io/categories/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="内存管理" scheme="http://liujunming.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
      <category term="Kernel" scheme="http://liujunming.github.io/tags/Kernel/"/>
    
  </entry>
  
</feed>
